# Comparing `tmp/nzpyida-0.2.2.6.tar.gz` & `tmp/nzpyida-0.3.3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist\nzpyida-0.2.2.6.tar", last modified: Mon Feb 13 16:39:02 2023, max compression
+gzip compressed data, was "nzpyida-0.3.3.tar", last modified: Wed May 10 13:25:45 2023, max compression
```

## Comparing `nzpyida-0.2.2.6.tar` & `nzpyida-0.3.3.tar`

### file list

```diff
@@ -1,119 +1,163 @@
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:02.038007 nzpyida-0.2.2.6/
--rw-rw-rw-   0        0        0     1595 2021-10-01 02:28:49.000000 nzpyida-0.2.2.6/LICENSE.txt
--rw-rw-rw-   0        0        0      260 2021-10-01 02:28:49.000000 nzpyida-0.2.2.6/MANIFEST.in
--rw-rw-rw-   0        0        0     1789 2023-02-13 16:39:02.040784 nzpyida-0.2.2.6/PKG-INFO
--rw-rw-rw-   0        0        0      173 2022-09-07 22:33:19.000000 nzpyida-0.2.2.6/README.rst
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:00.652750 nzpyida-0.2.2.6/docs/
--rw-rw-rw-   0        0        0     7885 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/Makefile
--rwxrwxrwx   0        0        0     7260 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/make.bat
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:01.079542 nzpyida-0.2.2.6/docs/source/
--rw-rw-rw-   0        0        0     1063 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/association_rules.rst
--rw-rw-rw-   0        0        0     3634 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/base.rst
--rw-rw-rw-   0        0        0    12309 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/conf.py
--rw-rw-rw-   0        0        0     6630 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/feature_selection.rst
--rw-rw-rw-   0        0        0     6782 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/frame.rst
--rw-rw-rw-   0        0        0     2588 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/geoFrame.rst
--rw-rw-rw-   0        0        0     5835 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/geoSeries.rst
--rw-rw-rw-   0        0        0  2620149 2021-10-01 02:28:49.000000 nzpyida-0.2.2.6/docs/source/geospatial.rst
--rw-rw-rw-   0        0        0     1740 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/ibm.png
--rw-rw-rw-   0        0        0 42730617 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/docs/source/index.rst
--rw-rw-rw-   0        0        0     3032 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/install.rst
--rw-rw-rw-   0        0        0     1150 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/kc.ico
--rw-rw-rw-   0        0        0      895 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/kmeans.rst
--rw-rw-rw-   0        0        0     2037 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/legal.rst
--rw-rw-rw-   0        0        0      378 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/ml.rst
--rw-rw-rw-   0        0        0      963 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/naive_bayes.rst
--rw-rw-rw-   0        0        0  8189314 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/docs/source/start.rst
--rw-rw-rw-   0        0        0     1532 2021-10-01 02:27:15.000000 nzpyida-0.2.2.6/docs/source/utils.rst
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:01.182000 nzpyida-0.2.2.6/nzpyida/
--rw-rw-rw-   0        0        0     1031 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/__init__.py
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:01.310112 nzpyida-0.2.2.6/nzpyida/ae/
--rw-rw-rw-   0        0        0      919 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/ae/__init__.py
--rw-rw-rw-   0        0        0     3610 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/ae/apply.py
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:01.363804 nzpyida-0.2.2.6/nzpyida/ae/client code examples/
--rw-rw-rw-   0        0        0     4220 2022-09-07 22:57:41.000000 nzpyida-0.2.2.6/nzpyida/ae/client code examples/customer_churn_prediction_nps.py
--rw-rw-rw-   0        0        0     5085 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/ae/client code examples/host_spus_compare_inza_kdd_measure_accuracy.py
--rw-rw-rw-   0        0        0    10725 2022-09-07 22:57:41.000000 nzpyida-0.2.2.6/nzpyida/ae/client code examples/host_spus_compare_inza_weather_train_pred.py
--rw-rw-rw-   0        0        0     2423 2022-08-22 18:31:41.000000 nzpyida-0.2.2.6/nzpyida/ae/client code examples/house_pricing.py
--rw-rw-rw-   0        0        0      240 2021-10-01 02:30:56.000000 nzpyida-0.2.2.6/nzpyida/ae/client code examples/install_package_test.py
--rw-rw-rw-   0        0        0     4806 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/ae/client code examples/stock_prediction_nps side.py
--rw-rw-rw-   0        0        0    12646 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/ae/client code examples/stock_prediction_nps_wlm.py
--rw-rw-rw-   0        0        0     1015 2022-10-10 14:37:06.000000 nzpyida-0.2.2.6/nzpyida/ae/client code examples/test_example_do_not_upload.py
--rw-rw-rw-   0        0        0     1501 2022-10-06 06:45:55.000000 nzpyida-0.2.2.6/nzpyida/ae/client code examples/test_example_do_not_upload1.py
--rw-rw-rw-   0        0        0     6146 2022-05-23 17:00:52.000000 nzpyida-0.2.2.6/nzpyida/ae/groupedapply.py
--rw-rw-rw-   0        0        0     1712 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/ae/install.py
--rw-rw-rw-   0        0        0     4526 2022-05-23 17:00:52.000000 nzpyida-0.2.2.6/nzpyida/ae/result_builder.py
--rw-rw-rw-   0        0        0     7267 2022-09-19 21:57:17.000000 nzpyida-0.2.2.6/nzpyida/ae/shaper.py
--rw-rw-rw-   0        0        0     3914 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/ae/tapply.py
--rw-rw-rw-   0        0        0     6731 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/ae/tapply_class.py
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:01.369706 nzpyida-0.2.2.6/nzpyida/ae/tests/
--rw-rw-rw-   0        0        0    23498 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/ae/tests/test_pyida.py
--rw-rw-rw-   0        0        0     6551 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/aggregation.py
--rw-rw-rw-   0        0        0    95607 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/base.py
--rw-rw-rw-   0        0        0     2411 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/exceptions.py
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:01.473955 nzpyida-0.2.2.6/nzpyida/feature_selection/
--rw-rw-rw-   0        0        0      991 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/__init__.py
--rw-rw-rw-   0        0        0     4702 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/chisquared.py
--rw-rw-rw-   0        0        0     7851 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/correlation.py
--rw-rw-rw-   0        0        0     5674 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/discretize.py
--rw-rw-rw-   0        0        0     5248 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/entropy.py
--rw-rw-rw-   0        0        0     4351 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/gain_ratio.py
--rw-rw-rw-   0        0        0     5283 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/gini.py
--rw-rw-rw-   0        0        0     3457 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/info_gain.py
--rw-rw-rw-   0        0        0     3188 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/private.py
--rw-rw-rw-   0        0        0     5391 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/symmetric_uncertainty.py
--rw-rw-rw-   0        0        0     4573 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/feature_selection/tstats.py
--rw-rw-rw-   0        0        0     7898 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/filtering.py
--rw-rw-rw-   0        0        0    92691 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/frame.py
--rw-rw-rw-   0        0        0    40412 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/geoFrame.py
--rw-rw-rw-   0        0        0    82676 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/geoSeries.py
--rw-rw-rw-   0        0        0     5116 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/indexing.py
--rw-rw-rw-   0        0        0    18461 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/internals.py
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:01.576096 nzpyida-0.2.2.6/nzpyida/learn/
--rw-rw-rw-   0        0        0      807 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/learn/__init__.py
--rw-rw-rw-   0        0        0    26615 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/learn/association_rules.py
--rw-rw-rw-   0        0        0    22059 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/learn/kmeans.py
--rw-rw-rw-   0        0        0    22508 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/learn/naive_bayes.py
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:01.689406 nzpyida-0.2.2.6/nzpyida/sampledata/
--rw-rw-rw-   0        0        0      756 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/sampledata/__init__.py
--rw-rw-rw-   0        0        0     1106 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/sampledata/iris.py
--rw-rw-rw-   0        0        0     4009 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/sampledata/iris.txt
--rw-rw-rw-   0        0        0     1292 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/sampledata/swiss.py
--rw-rw-rw-   0        0        0     1805 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/sampledata/swiss.txt
--rw-rw-rw-   0        0        0     1430 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/sampledata/titanic.py
--rw-rw-rw-   0        0        0    68594 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/sampledata/titanic.txt
--rw-rw-rw-   0        0        0     2399 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/series.py
--rw-rw-rw-   0        0        0    13562 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/sql.py
--rw-rw-rw-   0        0        0    39330 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/statistics.py
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:02.018208 nzpyida-0.2.2.6/nzpyida/tests/
--rw-rw-rw-   0        0        0    13818 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/conftest.py
--rw-rw-rw-   0        0        0     3329 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_aggregation.py
--rw-rw-rw-   0        0        0     1420 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_association_rules.py
--rw-rw-rw-   0        0        0     8579 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_base.py
--rw-rw-rw-   0        0        0     6525 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_base_connexion.py
--rw-rw-rw-   0        0        0     6866 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_base_private.py
--rw-rw-rw-   0        0        0    10430 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_base_table_manipulation.py
--rw-rw-rw-   0        0        0    23872 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_feature_selection.py
--rw-rw-rw-   0        0        0     3242 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_filtering.py
--rw-rw-rw-   0        0        0    15677 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_frame.py
--rw-rw-rw-   0        0        0     1430 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_frame_connexion.py
--rw-rw-rw-   0        0        0     2501 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_frame_private.py
--rw-rw-rw-   0        0        0    11504 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_geoFrame.py
--rw-rw-rw-   0        0        0    11608 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_geoSeries.py
--rw-rw-rw-   0        0        0     2837 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_indexing.py
--rw-rw-rw-   0        0        0     1005 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_internals.py
--rw-rw-rw-   0        0        0     1385 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_kmeans.py
--rw-rw-rw-   0        0        0     1388 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_naive_bayes.py
--rw-rw-rw-   0        0        0      880 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_series.py
--rw-rw-rw-   0        0        0     1492 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_sorting.py
--rw-rw-rw-   0        0        0     8520 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_statistics.py
--rw-rw-rw-   0        0        0     1027 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/tests/test_utils.py
--rw-rw-rw-   0        0        0    11409 2021-10-01 02:28:50.000000 nzpyida-0.2.2.6/nzpyida/utils.py
-drwxrwxrwx   0        0        0        0 2023-02-13 16:39:01.261646 nzpyida-0.2.2.6/nzpyida.egg-info/
--rw-rw-rw-   0        0        0     1789 2023-02-13 16:38:59.000000 nzpyida-0.2.2.6/nzpyida.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     3210 2023-02-13 16:38:59.000000 nzpyida-0.2.2.6/nzpyida.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-02-13 16:38:59.000000 nzpyida-0.2.2.6/nzpyida.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0      164 2023-02-13 16:38:59.000000 nzpyida-0.2.2.6/nzpyida.egg-info/requires.txt
--rw-rw-rw-   0        0        0        8 2023-02-13 16:38:59.000000 nzpyida-0.2.2.6/nzpyida.egg-info/top_level.txt
--rw-rw-rw-   0        0        0      206 2023-02-13 16:39:02.065945 nzpyida-0.2.2.6/setup.cfg
--rw-rw-rw-   0        0        0     3215 2023-02-13 16:38:37.000000 nzpyida-0.2.2.6/setup.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.313223 nzpyida-0.3.3/
+-rw-r--r--   0 mpl        (501) staff       (20)       25 2023-05-10 12:49:36.000000 nzpyida-0.3.3/.gitignore
+-rw-r--r--   0 mpl        (501) staff       (20)     1562 2023-05-10 12:49:36.000000 nzpyida-0.3.3/LICENSE.txt
+-rw-r--r--   0 mpl        (501) staff       (20)      222 2023-05-10 13:10:06.000000 nzpyida-0.3.3/MANIFEST.in
+-rw-r--r--   0 mpl        (501) staff       (20)     4680 2023-05-10 13:25:45.313328 nzpyida-0.3.3/PKG-INFO
+-rw-r--r--   0 mpl        (501) staff       (20)     3265 2023-05-10 12:49:36.000000 nzpyida-0.3.3/README.md
+-rw-r--r--   0 mpl        (501) staff       (20)      168 2023-05-10 12:49:36.000000 nzpyida-0.3.3/README.rst
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.289837 nzpyida-0.3.3/docs/
+-rw-r--r--   0 mpl        (501) staff       (20)     6148 2023-05-10 08:22:56.000000 nzpyida-0.3.3/docs/.DS_Store
+-rw-r--r--   0 mpl        (501) staff       (20)     7669 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/Makefile
+-rw-r--r--   0 mpl        (501) staff       (20)     6997 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/make.bat
+-rw-r--r--   0 mpl        (501) staff       (20)       41 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/requirements.txt
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.292975 nzpyida-0.3.3/docs/source/
+-rw-r--r--   0 mpl        (501) staff       (20)      110 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/analytics.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     3433 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/base.rst
+-rw-r--r--   0 mpl        (501) staff       (20)    11958 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/conf.py
+-rw-r--r--   0 mpl        (501) staff       (20)      419 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/exploration.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     6407 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/frame.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     2499 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/geoFrame.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     5123 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/geoSeries.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     4191 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/geospatial.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     1740 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/ibm.png
+-rw-r--r--   0 mpl        (501) staff       (20)     4041 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/index.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     1553 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/install.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     1150 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/kc.ico
+-rw-r--r--   0 mpl        (501) staff       (20)     2024 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/legal.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     2263 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/predictive.rst
+-rw-r--r--   0 mpl        (501) staff       (20)    23914 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/start.rst
+-rw-r--r--   0 mpl        (501) staff       (20)      400 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/transform.rst
+-rw-r--r--   0 mpl        (501) staff       (20)     1444 2023-05-10 12:49:36.000000 nzpyida-0.3.3/docs/source/utils.rst
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.295496 nzpyida-0.3.3/nzpyida/
+-rw-r--r--   0 mpl        (501) staff       (20)     1017 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/__init__.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.297644 nzpyida-0.3.3/nzpyida/ae/
+-rw-r--r--   0 mpl        (501) staff       (20)      893 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/__init__.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3496 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/apply.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.298868 nzpyida-0.3.3/nzpyida/ae/client code examples/
+-rw-r--r--   0 mpl        (501) staff       (20)     4080 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/client code examples/customer_churn_prediction_nps.py
+-rw-r--r--   0 mpl        (501) staff       (20)     4929 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/client code examples/host_spus_compare_inza_kdd_measure_accuracy.py
+-rw-r--r--   0 mpl        (501) staff       (20)    10386 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/client code examples/host_spus_compare_inza_weather_train_pred.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2329 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/client code examples/house_pricing.py
+-rw-r--r--   0 mpl        (501) staff       (20)      231 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/client code examples/install_package_test.py
+-rw-r--r--   0 mpl        (501) staff       (20)     4615 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/client code examples/stock_prediction_nps side.py
+-rw-r--r--   0 mpl        (501) staff       (20)    12151 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/client code examples/stock_prediction_nps_wlm.py
+-rw-r--r--   0 mpl        (501) staff       (20)     5983 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/groupedapply.py
+-rw-r--r--   0 mpl        (501) staff       (20)     1645 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/install.py
+-rw-r--r--   0 mpl        (501) staff       (20)     4399 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/result_builder.py
+-rw-r--r--   0 mpl        (501) staff       (20)     7082 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/shaper.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3785 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/tapply.py
+-rw-r--r--   0 mpl        (501) staff       (20)     6543 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/tapply_class.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.299048 nzpyida-0.3.3/nzpyida/ae/tests/
+-rw-r--r--   0 mpl        (501) staff       (20)    22335 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/ae/tests/test_pyida.py
+-rw-r--r--   0 mpl        (501) staff       (20)     6392 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/aggregation.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.299776 nzpyida-0.3.3/nzpyida/analytics/
+-rw-r--r--   0 mpl        (501) staff       (20)     1596 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/__init__.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2529 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/auto_delete_context.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.300304 nzpyida-0.3.3/nzpyida/analytics/exploration/
+-rw-r--r--   0 mpl        (501) staff       (20)      401 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/exploration/__init__.py
+-rw-r--r--   0 mpl        (501) staff       (20)     9102 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/exploration/distribution.py
+-rw-r--r--   0 mpl        (501) staff       (20)    32901 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/exploration/relation_identification.py
+-rw-r--r--   0 mpl        (501) staff       (20)     8695 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/model_manager.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.302462 nzpyida-0.3.3/nzpyida/analytics/predictive/
+-rw-r--r--   0 mpl        (501) staff       (20)      400 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/__init__.py
+-rw-r--r--   0 mpl        (501) staff       (20)    13034 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/association_rules.py
+-rw-r--r--   0 mpl        (501) staff       (20)     9919 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/bisecting_kmeans.py
+-rw-r--r--   0 mpl        (501) staff       (20)     6581 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/classification.py
+-rw-r--r--   0 mpl        (501) staff       (20)    10009 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/decision_trees.py
+-rw-r--r--   0 mpl        (501) staff       (20)    10454 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/kmeans.py
+-rw-r--r--   0 mpl        (501) staff       (20)     9603 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/knn.py
+-rw-r--r--   0 mpl        (501) staff       (20)     5588 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/linear_regression.py
+-rw-r--r--   0 mpl        (501) staff       (20)     6019 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/naive_bayes.py
+-rw-r--r--   0 mpl        (501) staff       (20)     6258 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/predictive_modeling.py
+-rw-r--r--   0 mpl        (501) staff       (20)     5721 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/regression.py
+-rw-r--r--   0 mpl        (501) staff       (20)     8356 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/regression_trees.py
+-rw-r--r--   0 mpl        (501) staff       (20)    12287 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/predictive/two_step_clustering.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.304886 nzpyida-0.3.3/nzpyida/analytics/tests/
+-rw-r--r--   0 mpl        (501) staff       (20)     1896 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/conftest.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3439 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_association_rules.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3552 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_auto_delete_context.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2397 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_bisecting_kmeans.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2759 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_decision_trees.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2482 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_discretization.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2265 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_kmeans.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2671 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_knn.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2504 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_linear_regression.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3289 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_model_manager.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2695 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_naive_bayes.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2393 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_preparation.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2475 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_regression_trees.py
+-rw-r--r--   0 mpl        (501) staff       (20)    20233 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_relation_identification.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2419 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/tests/test_two_step_clustering.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.305394 nzpyida-0.3.3/nzpyida/analytics/transform/
+-rw-r--r--   0 mpl        (501) staff       (20)      400 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/transform/__init__.py
+-rw-r--r--   0 mpl        (501) staff       (20)    10642 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/transform/discretization.py
+-rw-r--r--   0 mpl        (501) staff       (20)     7634 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/transform/preparation.py
+-rw-r--r--   0 mpl        (501) staff       (20)     5061 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/analytics/utils.py
+-rw-r--r--   0 mpl        (501) staff       (20)    93174 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/base.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2331 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/exceptions.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.307138 nzpyida-0.3.3/nzpyida/feature_selection/
+-rw-r--r--   0 mpl        (501) staff       (20)      957 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/__init__.py
+-rw-r--r--   0 mpl        (501) staff       (20)     4562 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/chisquared.py
+-rw-r--r--   0 mpl        (501) staff       (20)     7631 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/correlation.py
+-rw-r--r--   0 mpl        (501) staff       (20)     5517 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/discretize.py
+-rw-r--r--   0 mpl        (501) staff       (20)     5089 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/entropy.py
+-rw-r--r--   0 mpl        (501) staff       (20)     4228 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/gain_ratio.py
+-rw-r--r--   0 mpl        (501) staff       (20)     5106 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/gini.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3350 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/info_gain.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3100 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/private.py
+-rw-r--r--   0 mpl        (501) staff       (20)     5235 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/symmetric_uncertainty.py
+-rw-r--r--   0 mpl        (501) staff       (20)     4437 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/feature_selection/tstats.py
+-rw-r--r--   0 mpl        (501) staff       (20)     7710 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/filtering.py
+-rw-r--r--   0 mpl        (501) staff       (20)    90170 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/frame.py
+-rw-r--r--   0 mpl        (501) staff       (20)    39332 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/geoFrame.py
+-rw-r--r--   0 mpl        (501) staff       (20)    80555 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/geoSeries.py
+-rw-r--r--   0 mpl        (501) staff       (20)     4998 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/indexing.py
+-rw-r--r--   0 mpl        (501) staff       (20)    18273 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/internals.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.307772 nzpyida-0.3.3/nzpyida/learn/
+-rw-r--r--   0 mpl        (501) staff       (20)      786 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/learn/__init__.py
+-rw-r--r--   0 mpl        (501) staff       (20)    26008 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/learn/association_rules.py
+-rw-r--r--   0 mpl        (501) staff       (20)    21533 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/learn/kmeans.py
+-rw-r--r--   0 mpl        (501) staff       (20)    21981 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/learn/naive_bayes.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.308849 nzpyida-0.3.3/nzpyida/sampledata/
+-rw-r--r--   0 mpl        (501) staff       (20)      736 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/sampledata/__init__.py
+-rw-r--r--   0 mpl        (501) staff       (20)     1071 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/sampledata/iris.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3858 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/sampledata/iris.txt
+-rw-r--r--   0 mpl        (501) staff       (20)     1255 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/sampledata/swiss.py
+-rw-r--r--   0 mpl        (501) staff       (20)     1757 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/sampledata/swiss.txt
+-rw-r--r--   0 mpl        (501) staff       (20)     1386 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/sampledata/titanic.py
+-rw-r--r--   0 mpl        (501) staff       (20)    67702 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/sampledata/titanic.txt
+-rw-r--r--   0 mpl        (501) staff       (20)     2329 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/series.py
+-rw-r--r--   0 mpl        (501) staff       (20)    13343 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/sql.py
+-rw-r--r--   0 mpl        (501) staff       (20)    38179 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/statistics.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.313042 nzpyida-0.3.3/nzpyida/tests/
+-rw-r--r--   0 mpl        (501) staff       (20)    13403 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/conftest.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3202 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_aggregation.py
+-rw-r--r--   0 mpl        (501) staff       (20)     1368 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_association_rules.py
+-rw-r--r--   0 mpl        (501) staff       (20)     8378 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_base.py
+-rw-r--r--   0 mpl        (501) staff       (20)     6371 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_base_connexion.py
+-rw-r--r--   0 mpl        (501) staff       (20)     6705 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_base_private.py
+-rw-r--r--   0 mpl        (501) staff       (20)    10210 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_base_table_manipulation.py
+-rw-r--r--   0 mpl        (501) staff       (20)    23587 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_feature_selection.py
+-rw-r--r--   0 mpl        (501) staff       (20)     3146 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_filtering.py
+-rw-r--r--   0 mpl        (501) staff       (20)    15208 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_frame.py
+-rw-r--r--   0 mpl        (501) staff       (20)     1384 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_frame_connexion.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2403 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_frame_private.py
+-rw-r--r--   0 mpl        (501) staff       (20)    11236 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_geoFrame.py
+-rw-r--r--   0 mpl        (501) staff       (20)    11335 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_geoSeries.py
+-rw-r--r--   0 mpl        (501) staff       (20)     2730 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_indexing.py
+-rw-r--r--   0 mpl        (501) staff       (20)      968 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_internals.py
+-rw-r--r--   0 mpl        (501) staff       (20)     1333 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_kmeans.py
+-rw-r--r--   0 mpl        (501) staff       (20)     1336 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_naive_bayes.py
+-rw-r--r--   0 mpl        (501) staff       (20)      850 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_series.py
+-rw-r--r--   0 mpl        (501) staff       (20)     1429 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_sorting.py
+-rw-r--r--   0 mpl        (501) staff       (20)     8359 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_statistics.py
+-rw-r--r--   0 mpl        (501) staff       (20)      987 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/tests/test_utils.py
+-rw-r--r--   0 mpl        (501) staff       (20)    11065 2023-05-10 12:49:36.000000 nzpyida-0.3.3/nzpyida/utils.py
+drwxr-xr-x   0 mpl        (501) staff       (20)        0 2023-05-10 13:25:45.296364 nzpyida-0.3.3/nzpyida.egg-info/
+-rw-r--r--   0 mpl        (501) staff       (20)     4680 2023-05-10 13:25:45.000000 nzpyida-0.3.3/nzpyida.egg-info/PKG-INFO
+-rw-r--r--   0 mpl        (501) staff       (20)     4815 2023-05-10 13:25:45.000000 nzpyida-0.3.3/nzpyida.egg-info/SOURCES.txt
+-rw-r--r--   0 mpl        (501) staff       (20)        1 2023-05-10 13:25:45.000000 nzpyida-0.3.3/nzpyida.egg-info/dependency_links.txt
+-rw-r--r--   0 mpl        (501) staff       (20)      164 2023-05-10 13:25:45.000000 nzpyida-0.3.3/nzpyida.egg-info/requires.txt
+-rw-r--r--   0 mpl        (501) staff       (20)        8 2023-05-10 13:25:45.000000 nzpyida-0.3.3/nzpyida.egg-info/top_level.txt
+-rw-r--r--   0 mpl        (501) staff       (20)      177 2023-05-10 13:25:45.313616 nzpyida-0.3.3/setup.cfg
+-rw-r--r--   0 mpl        (501) staff       (20)     2969 2023-05-10 13:23:21.000000 nzpyida-0.3.3/setup.py
```

### Comparing `nzpyida-0.2.2.6/LICENSE.txt` & `nzpyida-0.3.3/LICENSE.txt`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-nzpyida license
-===============
-
-Copyright (c) 2015  IBM Corp.
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are
-met:
-
-    * Redistributions of source code must retain the above copyright
-       notice, this list of conditions and the following disclaimer.
-
-    * Redistributions in binary form must reproduce the above
-       copyright notice, this list of conditions and the following
-       disclaimer in the documentation and/or other materials provided
-       with the distribution.
-
-    * Neither the name of the copyright holder nor the names of any
-       contributors may be used to endorse or promote products derived
-       from this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS
-"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
-LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
-A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
-OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
-LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+nzpyida license
+===============
+
+Copyright (c) 2015  IBM Corp.
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+
+    * Redistributions of source code must retain the above copyright
+       notice, this list of conditions and the following disclaimer.
+
+    * Redistributions in binary form must reproduce the above
+       copyright notice, this list of conditions and the following
+       disclaimer in the documentation and/or other materials provided
+       with the distribution.
+
+    * Neither the name of the copyright holder nor the names of any
+       contributors may be used to endorse or promote products derived
+       from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
```

### Comparing `nzpyida-0.2.2.6/docs/make.bat` & `nzpyida-0.3.3/docs/make.bat`

 * *Files 12% similar despite different names*

```diff
@@ -1,263 +1,263 @@
-@ECHO OFF
-
-REM Command file for Sphinx documentation
-
-if "%SPHINXBUILD%" == "" (
-	set SPHINXBUILD=sphinx-build
-)
-set BUILDDIR=build
-set ALLSPHINXOPTS=-d %BUILDDIR%/doctrees %SPHINXOPTS% source
-set I18NSPHINXOPTS=%SPHINXOPTS% source
-if NOT "%PAPER%" == "" (
-	set ALLSPHINXOPTS=-D latex_paper_size=%PAPER% %ALLSPHINXOPTS%
-	set I18NSPHINXOPTS=-D latex_paper_size=%PAPER% %I18NSPHINXOPTS%
-)
-
-if "%1" == "" goto help
-
-if "%1" == "help" (
-	:help
-	echo.Please use `make ^<target^>` where ^<target^> is one of
-	echo.  html       to make standalone HTML files
-	echo.  dirhtml    to make HTML files named index.html in directories
-	echo.  singlehtml to make a single large HTML file
-	echo.  pickle     to make pickle files
-	echo.  json       to make JSON files
-	echo.  htmlhelp   to make HTML files and a HTML help project
-	echo.  qthelp     to make HTML files and a qthelp project
-	echo.  devhelp    to make HTML files and a Devhelp project
-	echo.  epub       to make an epub
-	echo.  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter
-	echo.  text       to make text files
-	echo.  man        to make manual pages
-	echo.  texinfo    to make Texinfo files
-	echo.  gettext    to make PO message catalogs
-	echo.  changes    to make an overview over all changed/added/deprecated items
-	echo.  xml        to make Docutils-native XML files
-	echo.  pseudoxml  to make pseudoxml-XML files for display purposes
-	echo.  linkcheck  to check all external links for integrity
-	echo.  doctest    to run all doctests embedded in the documentation if enabled
-	echo.  coverage   to run coverage check of the documentation if enabled
-	goto end
-)
-
-if "%1" == "clean" (
-	for /d %%i in (%BUILDDIR%\*) do rmdir /q /s %%i
-	del /q /s %BUILDDIR%\*
-	goto end
-)
-
-
-REM Check if sphinx-build is available and fallback to Python version if any
-%SPHINXBUILD% 1>NUL 2>NUL
-if errorlevel 9009 goto sphinx_python
-goto sphinx_ok
-
-:sphinx_python
-
-set SPHINXBUILD=python -m sphinx.__init__
-%SPHINXBUILD% 2> nul
-if errorlevel 9009 (
-	echo.
-	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
-	echo.installed, then set the SPHINXBUILD environment variable to point
-	echo.to the full path of the 'sphinx-build' executable. Alternatively you
-	echo.may add the Sphinx directory to PATH.
-	echo.
-	echo.If you don't have Sphinx installed, grab it from
-	echo.http://sphinx-doc.org/
-	exit /b 1
-)
-
-:sphinx_ok
-
-
-if "%1" == "html" (
-	%SPHINXBUILD% -b html %ALLSPHINXOPTS% %BUILDDIR%/html
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The HTML pages are in %BUILDDIR%/html.
-	goto end
-)
-
-if "%1" == "dirhtml" (
-	%SPHINXBUILD% -b dirhtml %ALLSPHINXOPTS% %BUILDDIR%/dirhtml
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The HTML pages are in %BUILDDIR%/dirhtml.
-	goto end
-)
-
-if "%1" == "singlehtml" (
-	%SPHINXBUILD% -b singlehtml %ALLSPHINXOPTS% %BUILDDIR%/singlehtml
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The HTML pages are in %BUILDDIR%/singlehtml.
-	goto end
-)
-
-if "%1" == "pickle" (
-	%SPHINXBUILD% -b pickle %ALLSPHINXOPTS% %BUILDDIR%/pickle
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished; now you can process the pickle files.
-	goto end
-)
-
-if "%1" == "json" (
-	%SPHINXBUILD% -b json %ALLSPHINXOPTS% %BUILDDIR%/json
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished; now you can process the JSON files.
-	goto end
-)
-
-if "%1" == "htmlhelp" (
-	%SPHINXBUILD% -b htmlhelp %ALLSPHINXOPTS% %BUILDDIR%/htmlhelp
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished; now you can run HTML Help Workshop with the ^
-.hhp project file in %BUILDDIR%/htmlhelp.
-	goto end
-)
-
-if "%1" == "qthelp" (
-	%SPHINXBUILD% -b qthelp %ALLSPHINXOPTS% %BUILDDIR%/qthelp
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished; now you can run "qcollectiongenerator" with the ^
-.qhcp project file in %BUILDDIR%/qthelp, like this:
-	echo.^> qcollectiongenerator %BUILDDIR%\qthelp\ibmdbpy.qhcp
-	echo.To view the help file:
-	echo.^> assistant -collectionFile %BUILDDIR%\qthelp\ibmdbpy.ghc
-	goto end
-)
-
-if "%1" == "devhelp" (
-	%SPHINXBUILD% -b devhelp %ALLSPHINXOPTS% %BUILDDIR%/devhelp
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished.
-	goto end
-)
-
-if "%1" == "epub" (
-	%SPHINXBUILD% -b epub %ALLSPHINXOPTS% %BUILDDIR%/epub
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The epub file is in %BUILDDIR%/epub.
-	goto end
-)
-
-if "%1" == "latex" (
-	%SPHINXBUILD% -b latex %ALLSPHINXOPTS% %BUILDDIR%/latex
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished; the LaTeX files are in %BUILDDIR%/latex.
-	goto end
-)
-
-if "%1" == "latexpdf" (
-	%SPHINXBUILD% -b latex %ALLSPHINXOPTS% %BUILDDIR%/latex
-	cd %BUILDDIR%/latex
-	make all-pdf
-	cd %~dp0
-	echo.
-	echo.Build finished; the PDF files are in %BUILDDIR%/latex.
-	goto end
-)
-
-if "%1" == "latexpdfja" (
-	%SPHINXBUILD% -b latex %ALLSPHINXOPTS% %BUILDDIR%/latex
-	cd %BUILDDIR%/latex
-	make all-pdf-ja
-	cd %~dp0
-	echo.
-	echo.Build finished; the PDF files are in %BUILDDIR%/latex.
-	goto end
-)
-
-if "%1" == "text" (
-	%SPHINXBUILD% -b text %ALLSPHINXOPTS% %BUILDDIR%/text
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The text files are in %BUILDDIR%/text.
-	goto end
-)
-
-if "%1" == "man" (
-	%SPHINXBUILD% -b man %ALLSPHINXOPTS% %BUILDDIR%/man
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The manual pages are in %BUILDDIR%/man.
-	goto end
-)
-
-if "%1" == "texinfo" (
-	%SPHINXBUILD% -b texinfo %ALLSPHINXOPTS% %BUILDDIR%/texinfo
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The Texinfo files are in %BUILDDIR%/texinfo.
-	goto end
-)
-
-if "%1" == "gettext" (
-	%SPHINXBUILD% -b gettext %I18NSPHINXOPTS% %BUILDDIR%/locale
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The message catalogs are in %BUILDDIR%/locale.
-	goto end
-)
-
-if "%1" == "changes" (
-	%SPHINXBUILD% -b changes %ALLSPHINXOPTS% %BUILDDIR%/changes
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.The overview file is in %BUILDDIR%/changes.
-	goto end
-)
-
-if "%1" == "linkcheck" (
-	%SPHINXBUILD% -b linkcheck %ALLSPHINXOPTS% %BUILDDIR%/linkcheck
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Link check complete; look for any errors in the above output ^
-or in %BUILDDIR%/linkcheck/output.txt.
-	goto end
-)
-
-if "%1" == "doctest" (
-	%SPHINXBUILD% -b doctest %ALLSPHINXOPTS% %BUILDDIR%/doctest
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Testing of doctests in the sources finished, look at the ^
-results in %BUILDDIR%/doctest/output.txt.
-	goto end
-)
-
-if "%1" == "coverage" (
-	%SPHINXBUILD% -b coverage %ALLSPHINXOPTS% %BUILDDIR%/coverage
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Testing of coverage in the sources finished, look at the ^
-results in %BUILDDIR%/coverage/python.txt.
-	goto end
-)
-
-if "%1" == "xml" (
-	%SPHINXBUILD% -b xml %ALLSPHINXOPTS% %BUILDDIR%/xml
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The XML files are in %BUILDDIR%/xml.
-	goto end
-)
-
-if "%1" == "pseudoxml" (
-	%SPHINXBUILD% -b pseudoxml %ALLSPHINXOPTS% %BUILDDIR%/pseudoxml
-	if errorlevel 1 exit /b 1
-	echo.
-	echo.Build finished. The pseudo-XML files are in %BUILDDIR%/pseudoxml.
-	goto end
-)
-
-:end
+@ECHO OFF
+
+REM Command file for Sphinx documentation
+
+if "%SPHINXBUILD%" == "" (
+	set SPHINXBUILD=sphinx-build
+)
+set BUILDDIR=build
+set ALLSPHINXOPTS=-d %BUILDDIR%/doctrees %SPHINXOPTS% source
+set I18NSPHINXOPTS=%SPHINXOPTS% source
+if NOT "%PAPER%" == "" (
+	set ALLSPHINXOPTS=-D latex_paper_size=%PAPER% %ALLSPHINXOPTS%
+	set I18NSPHINXOPTS=-D latex_paper_size=%PAPER% %I18NSPHINXOPTS%
+)
+
+if "%1" == "" goto help
+
+if "%1" == "help" (
+	:help
+	echo.Please use `make ^<target^>` where ^<target^> is one of
+	echo.  html       to make standalone HTML files
+	echo.  dirhtml    to make HTML files named index.html in directories
+	echo.  singlehtml to make a single large HTML file
+	echo.  pickle     to make pickle files
+	echo.  json       to make JSON files
+	echo.  htmlhelp   to make HTML files and a HTML help project
+	echo.  qthelp     to make HTML files and a qthelp project
+	echo.  devhelp    to make HTML files and a Devhelp project
+	echo.  epub       to make an epub
+	echo.  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter
+	echo.  text       to make text files
+	echo.  man        to make manual pages
+	echo.  texinfo    to make Texinfo files
+	echo.  gettext    to make PO message catalogs
+	echo.  changes    to make an overview over all changed/added/deprecated items
+	echo.  xml        to make Docutils-native XML files
+	echo.  pseudoxml  to make pseudoxml-XML files for display purposes
+	echo.  linkcheck  to check all external links for integrity
+	echo.  doctest    to run all doctests embedded in the documentation if enabled
+	echo.  coverage   to run coverage check of the documentation if enabled
+	goto end
+)
+
+if "%1" == "clean" (
+	for /d %%i in (%BUILDDIR%\*) do rmdir /q /s %%i
+	del /q /s %BUILDDIR%\*
+	goto end
+)
+
+
+REM Check if sphinx-build is available and fallback to Python version if any
+%SPHINXBUILD% 1>NUL 2>NUL
+if errorlevel 9009 goto sphinx_python
+goto sphinx_ok
+
+:sphinx_python
+
+set SPHINXBUILD=python -m sphinx.__init__
+%SPHINXBUILD% 2> nul
+if errorlevel 9009 (
+	echo.
+	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
+	echo.installed, then set the SPHINXBUILD environment variable to point
+	echo.to the full path of the 'sphinx-build' executable. Alternatively you
+	echo.may add the Sphinx directory to PATH.
+	echo.
+	echo.If you don't have Sphinx installed, grab it from
+	echo.http://sphinx-doc.org/
+	exit /b 1
+)
+
+:sphinx_ok
+
+
+if "%1" == "html" (
+	%SPHINXBUILD% -b html %ALLSPHINXOPTS% %BUILDDIR%/html
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The HTML pages are in %BUILDDIR%/html.
+	goto end
+)
+
+if "%1" == "dirhtml" (
+	%SPHINXBUILD% -b dirhtml %ALLSPHINXOPTS% %BUILDDIR%/dirhtml
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The HTML pages are in %BUILDDIR%/dirhtml.
+	goto end
+)
+
+if "%1" == "singlehtml" (
+	%SPHINXBUILD% -b singlehtml %ALLSPHINXOPTS% %BUILDDIR%/singlehtml
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The HTML pages are in %BUILDDIR%/singlehtml.
+	goto end
+)
+
+if "%1" == "pickle" (
+	%SPHINXBUILD% -b pickle %ALLSPHINXOPTS% %BUILDDIR%/pickle
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished; now you can process the pickle files.
+	goto end
+)
+
+if "%1" == "json" (
+	%SPHINXBUILD% -b json %ALLSPHINXOPTS% %BUILDDIR%/json
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished; now you can process the JSON files.
+	goto end
+)
+
+if "%1" == "htmlhelp" (
+	%SPHINXBUILD% -b htmlhelp %ALLSPHINXOPTS% %BUILDDIR%/htmlhelp
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished; now you can run HTML Help Workshop with the ^
+.hhp project file in %BUILDDIR%/htmlhelp.
+	goto end
+)
+
+if "%1" == "qthelp" (
+	%SPHINXBUILD% -b qthelp %ALLSPHINXOPTS% %BUILDDIR%/qthelp
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished; now you can run "qcollectiongenerator" with the ^
+.qhcp project file in %BUILDDIR%/qthelp, like this:
+	echo.^> qcollectiongenerator %BUILDDIR%\qthelp\nzpyida.qhcp
+	echo.To view the help file:
+	echo.^> assistant -collectionFile %BUILDDIR%\qthelp\nzpyida.ghc
+	goto end
+)
+
+if "%1" == "devhelp" (
+	%SPHINXBUILD% -b devhelp %ALLSPHINXOPTS% %BUILDDIR%/devhelp
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished.
+	goto end
+)
+
+if "%1" == "epub" (
+	%SPHINXBUILD% -b epub %ALLSPHINXOPTS% %BUILDDIR%/epub
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The epub file is in %BUILDDIR%/epub.
+	goto end
+)
+
+if "%1" == "latex" (
+	%SPHINXBUILD% -b latex %ALLSPHINXOPTS% %BUILDDIR%/latex
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished; the LaTeX files are in %BUILDDIR%/latex.
+	goto end
+)
+
+if "%1" == "latexpdf" (
+	%SPHINXBUILD% -b latex %ALLSPHINXOPTS% %BUILDDIR%/latex
+	cd %BUILDDIR%/latex
+	make all-pdf
+	cd %~dp0
+	echo.
+	echo.Build finished; the PDF files are in %BUILDDIR%/latex.
+	goto end
+)
+
+if "%1" == "latexpdfja" (
+	%SPHINXBUILD% -b latex %ALLSPHINXOPTS% %BUILDDIR%/latex
+	cd %BUILDDIR%/latex
+	make all-pdf-ja
+	cd %~dp0
+	echo.
+	echo.Build finished; the PDF files are in %BUILDDIR%/latex.
+	goto end
+)
+
+if "%1" == "text" (
+	%SPHINXBUILD% -b text %ALLSPHINXOPTS% %BUILDDIR%/text
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The text files are in %BUILDDIR%/text.
+	goto end
+)
+
+if "%1" == "man" (
+	%SPHINXBUILD% -b man %ALLSPHINXOPTS% %BUILDDIR%/man
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The manual pages are in %BUILDDIR%/man.
+	goto end
+)
+
+if "%1" == "texinfo" (
+	%SPHINXBUILD% -b texinfo %ALLSPHINXOPTS% %BUILDDIR%/texinfo
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The Texinfo files are in %BUILDDIR%/texinfo.
+	goto end
+)
+
+if "%1" == "gettext" (
+	%SPHINXBUILD% -b gettext %I18NSPHINXOPTS% %BUILDDIR%/locale
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The message catalogs are in %BUILDDIR%/locale.
+	goto end
+)
+
+if "%1" == "changes" (
+	%SPHINXBUILD% -b changes %ALLSPHINXOPTS% %BUILDDIR%/changes
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.The overview file is in %BUILDDIR%/changes.
+	goto end
+)
+
+if "%1" == "linkcheck" (
+	%SPHINXBUILD% -b linkcheck %ALLSPHINXOPTS% %BUILDDIR%/linkcheck
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Link check complete; look for any errors in the above output ^
+or in %BUILDDIR%/linkcheck/output.txt.
+	goto end
+)
+
+if "%1" == "doctest" (
+	%SPHINXBUILD% -b doctest %ALLSPHINXOPTS% %BUILDDIR%/doctest
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Testing of doctests in the sources finished, look at the ^
+results in %BUILDDIR%/doctest/output.txt.
+	goto end
+)
+
+if "%1" == "coverage" (
+	%SPHINXBUILD% -b coverage %ALLSPHINXOPTS% %BUILDDIR%/coverage
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Testing of coverage in the sources finished, look at the ^
+results in %BUILDDIR%/coverage/python.txt.
+	goto end
+)
+
+if "%1" == "xml" (
+	%SPHINXBUILD% -b xml %ALLSPHINXOPTS% %BUILDDIR%/xml
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The XML files are in %BUILDDIR%/xml.
+	goto end
+)
+
+if "%1" == "pseudoxml" (
+	%SPHINXBUILD% -b pseudoxml %ALLSPHINXOPTS% %BUILDDIR%/pseudoxml
+	if errorlevel 1 exit /b 1
+	echo.
+	echo.Build finished. The pseudo-XML files are in %BUILDDIR%/pseudoxml.
+	goto end
+)
+
+:end
```

### Comparing `nzpyida-0.2.2.6/docs/source/base.rst` & `nzpyida-0.3.3/docs/source/base.rst`

 * *Files 19% similar despite different names*

```diff
@@ -1,206 +1,206 @@
-.. highlight:: python
-
-IdaDataBase
-***********
-
-Connect to Db2
-==============
-
-.. currentmodule:: ibmdbpy.base
-
-.. autoclass:: IdaDataBase
-
-   .. automethod:: __init__
-
-.. rubric:: Methods
-
-DataBase Exploration
-====================
-
-current_schema
---------------
-.. automethod:: IdaDataBase.current_schema
-
-show_tables
------------
-.. automethod:: IdaDataBase.show_tables
-
-show_models
------------
-.. automethod:: IdaDataBase.show_models
-
-exists_table_or_view
---------------------
-.. automethod:: IdaDataBase.exists_table_or_view
-
-exists_table
-------------
-.. automethod:: IdaDataBase.exists_table
-
-exists_view
------------
-.. automethod:: IdaDataBase.exists_view
-
-exists_model
-------------
-.. automethod:: IdaDataBase.exists_model
-
-is_table_or_view
-----------------
-.. automethod:: IdaDataBase.is_table_or_view
-
-is_table
---------
-.. automethod:: IdaDataBase.is_table
-
-is_view
--------
-.. automethod:: IdaDataBase.is_view
-
-is_model
---------
-.. automethod:: IdaDataBase.is_model
-
-ida_query
----------
-.. automethod:: IdaDataBase.ida_query
-
-ida_scalar_query
-----------------
-.. automethod:: IdaDataBase.ida_scalar_query
-
-Upload DataFrames
-=================
-
-as_idadataframe
----------------
-.. automethod:: IdaDataBase.as_idadataframe
-
-Delete DataBase Objects
-=======================
-
-drop_table
-----------
-.. automethod:: IdaDataBase.drop_table
-
-drop_view
----------
-.. automethod:: IdaDataBase.drop_view
-
-drop_model
-----------
-.. automethod:: IdaDataBase.drop_model
-
-DataBase Modification
-=====================
-
-rename
-------
-.. automethod:: IdaDataBase.rename
-
-add_column_id
--------------
-.. automethod:: IdaDataBase.add_column_id
-
-delete_column
--------------
-.. automethod:: IdaDataBase.delete_column
-
-append
-------
-.. automethod:: IdaDataBase.append
-
-
-Connection Management
-=====================
-
-commit
-------
-.. automethod:: IdaDataBase.commit
-
-rollback
---------
-.. automethod:: IdaDataBase.rollback
-
-close
------
-.. automethod:: IdaDataBase.close
-
-reconnect
----------
-.. automethod:: IdaDataBase.reconnect
-
-Private Methods
-===============
-
-_exists
--------
-.. automethod:: IdaDataBase._exists
-
-_is
----
-.. automethod:: IdaDataBase._is
-
-_drop
------
-.. automethod:: IdaDataBase._drop
-
-_upper_columns
---------------
-.. automethod:: IdaDataBase._upper_columns
-
-_get_name_and_schema
---------------------
-.. automethod:: IdaDataBase._get_name_and_schema
-
-_get_valid_tablename
---------------------
-.. automethod:: IdaDataBase._get_valid_tablename
-
-_get_valid_viewname
---------------------
-.. automethod:: IdaDataBase._get_valid_viewname
-
-_get_valid_modelname
---------------------
-.. automethod:: IdaDataBase._get_valid_modelname
-
-_create_table
--------------
-.. automethod:: IdaDataBase._create_table
-
-_create_view
-------------
-.. automethod:: IdaDataBase._create_view
-
-_insert_into_database
----------------------
-.. automethod:: IdaDataBase._insert_into_database
-
-_prepare_and_execute
---------------------
-.. automethod:: IdaDataBase._prepare_and_execute
-
-_check_procedure
-----------------
-.. automethod:: IdaDataBase._check_procedure
-
-_call_stored_procedure
-----------------------
-.. automethod:: IdaDataBase._call_stored_procedure
-
-_autocommit
------------
-.. automethod:: IdaDataBase._autocommit
-
-_check_connection
------------------
-.. automethod:: IdaDataBase._check_connection
-
-_retrieve_cache
----------------
-.. automethod:: IdaDataBase._retrieve_cache
-
-_reset_attributes
------------------
+.. highlight:: python
+
+IdaDataBase
+***********
+
+Connect to Netezza
+==============
+
+.. currentmodule:: nzpyida.base
+
+.. autoclass:: IdaDataBase
+
+   .. automethod:: __init__
+
+.. rubric:: Methods
+
+DataBase Exploration
+====================
+
+current_schema
+--------------
+.. automethod:: IdaDataBase.current_schema
+
+show_tables
+-----------
+.. automethod:: IdaDataBase.show_tables
+
+show_models
+-----------
+.. automethod:: IdaDataBase.show_models
+
+exists_table_or_view
+--------------------
+.. automethod:: IdaDataBase.exists_table_or_view
+
+exists_table
+------------
+.. automethod:: IdaDataBase.exists_table
+
+exists_view
+-----------
+.. automethod:: IdaDataBase.exists_view
+
+exists_model
+------------
+.. automethod:: IdaDataBase.exists_model
+
+is_table_or_view
+----------------
+.. automethod:: IdaDataBase.is_table_or_view
+
+is_table
+--------
+.. automethod:: IdaDataBase.is_table
+
+is_view
+-------
+.. automethod:: IdaDataBase.is_view
+
+is_model
+--------
+.. automethod:: IdaDataBase.is_model
+
+ida_query
+---------
+.. automethod:: IdaDataBase.ida_query
+
+ida_scalar_query
+----------------
+.. automethod:: IdaDataBase.ida_scalar_query
+
+Upload DataFrames
+=================
+
+as_idadataframe
+---------------
+.. automethod:: IdaDataBase.as_idadataframe
+
+Delete DataBase Objects
+=======================
+
+drop_table
+----------
+.. automethod:: IdaDataBase.drop_table
+
+drop_view
+---------
+.. automethod:: IdaDataBase.drop_view
+
+drop_model
+----------
+.. automethod:: IdaDataBase.drop_model
+
+DataBase Modification
+=====================
+
+rename
+------
+.. automethod:: IdaDataBase.rename
+
+add_column_id
+-------------
+.. automethod:: IdaDataBase.add_column_id
+
+delete_column
+-------------
+.. automethod:: IdaDataBase.delete_column
+
+append
+------
+.. automethod:: IdaDataBase.append
+
+
+Connection Management
+=====================
+
+commit
+------
+.. automethod:: IdaDataBase.commit
+
+rollback
+--------
+.. automethod:: IdaDataBase.rollback
+
+close
+-----
+.. automethod:: IdaDataBase.close
+
+reconnect
+---------
+.. automethod:: IdaDataBase.reconnect
+
+Private Methods
+===============
+
+_exists
+-------
+.. automethod:: IdaDataBase._exists
+
+_is
+---
+.. automethod:: IdaDataBase._is
+
+_drop
+-----
+.. automethod:: IdaDataBase._drop
+
+_upper_columns
+--------------
+.. automethod:: IdaDataBase._upper_columns
+
+_get_name_and_schema
+--------------------
+.. automethod:: IdaDataBase._get_name_and_schema
+
+_get_valid_tablename
+--------------------
+.. automethod:: IdaDataBase._get_valid_tablename
+
+_get_valid_viewname
+--------------------
+.. automethod:: IdaDataBase._get_valid_viewname
+
+_get_valid_modelname
+--------------------
+.. automethod:: IdaDataBase._get_valid_modelname
+
+_create_table
+-------------
+.. automethod:: IdaDataBase._create_table
+
+_create_view
+------------
+.. automethod:: IdaDataBase._create_view
+
+_insert_into_database
+---------------------
+.. automethod:: IdaDataBase._insert_into_database
+
+_prepare_and_execute
+--------------------
+.. automethod:: IdaDataBase._prepare_and_execute
+
+_check_procedure
+----------------
+.. automethod:: IdaDataBase._check_procedure
+
+_call_stored_procedure
+----------------------
+.. automethod:: IdaDataBase._call_stored_procedure
+
+_autocommit
+-----------
+.. automethod:: IdaDataBase._autocommit
+
+_check_connection
+-----------------
+.. automethod:: IdaDataBase._check_connection
+
+_retrieve_cache
+---------------
+.. automethod:: IdaDataBase._retrieve_cache
+
+_reset_attributes
+-----------------
 .. automethod:: IdaDataBase._reset_attributes
```

### Comparing `nzpyida-0.2.2.6/docs/source/conf.py` & `nzpyida-0.3.3/docs/source/conf.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,375 +1,375 @@
-# -*- coding: utf-8 -*-
-#
-# ibmdbpy documentation build configuration file, created by
-# sphinx-quickstart on Wed Jul 20 18:25:07 2016.
-#
-# This file is execfile()d with the current directory set to its
-# containing dir.
-#
-# Note that not all possible configuration values are present in this
-# autogenerated file.
-#
-# All configuration values have a default; values that are commented out
-# serve to show the default.
-
-#standard_library.install_aliases()
-import sys
-import os
-import shlex
-# If extensions (or modules to document with autodoc) are in another directory,
-# add these directories to sys.path here. If the directory is relative to the
-# documentation root, use os.path.abspath to make it absolute, like shown here.
-sys.path.insert(0, os.pardir)
-basepath = os.path.dirname(__file__)
-filepath = os.path.abspath(os.path.join(basepath, "..", "..", "ibmdbpy"))
-sys.path.append(filepath)
-# -- General configuration ------------------------------------------------
-
-# If your documentation needs a minimal Sphinx version, state it here.
-#needs_sphinx = '1.0'
-
-# Add any Sphinx extension module names here, as strings. They can be
-# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
-# ones.
-extensions = [
-    'sphinx.ext.autodoc','numpydoc',
-    'sphinx.ext.autosummary',
-    'sphinx.ext.intersphinx',
-    'sphinx.ext.todo',
-    'sphinx.ext.coverage',
-    'sphinx.ext.imgmath',
-    'sphinx.ext.ifconfig',
-    'sphinx.ext.viewcode',
-    'IPython.sphinxext.ipython_console_highlighting',
-    'IPython.sphinxext.ipython_directive'
-]
-
-# Avoid annoying warnings
-numpydoc_show_class_members = False
-
-# Add any paths that contain templates here, relative to this directory.
-templates_path = ['_templates']
-
-# The suffix(es) of source filenames.
-# You can specify multiple suffix as a list of string:
-# source_suffix = ['.rst', '.md']
-source_suffix = '.rst'
-
-# The encoding of source files.
-#source_encoding = 'utf-8-sig'
-
-# The master toctree document.
-master_doc = 'index'
-
-# General information about the project.
-project = u'ibmdbpy'
-copyright = u'2016, IBM Corp'
-author = u'Edouard Fouché,Avipsa Roy'
-
-# The version info for the project you're documenting, acts as replacement for
-# |version| and |release|, also used in various other places throughout the
-# built documents.
-#
-# The short X.Y version.
-version = u'0.1.6'
-# The full version, including alpha/beta/rc tags.
-release = u'0.1.6 beta'
-
-# The language for content autogenerated by Sphinx. Refer to documentation
-# for a list of supported languages.
-#
-# This is also used if you do content translation via gettext catalogs.
-# Usually you set "language" from the command line for these cases.
-language = 'english'
-
-# There are two options for replacing |today|: either, you set today to some
-# non-false value, then it is used:
-#today = ''
-# Else, today_fmt is used as the format for a strftime call.
-#today_fmt = '%B %d, %Y'
-
-# List of patterns, relative to source directory, that match files and
-# directories to ignore when looking for source files.
-exclude_patterns = []
-exclude_trees = ['build']
-
-# The reST default role (used for this markup: `text`) to use for all
-# documents.
-#default_role = None
-
-# If true, '()' will be appended to :func: etc. cross-reference text.
-add_function_parentheses = True
-
-# If true, the current module name will be prepended to all description
-# unit titles (such as .. function::).
-add_module_names = True
-
-# If true, sectionauthor and moduleauthor directives will be shown in the
-# output. They are ignored by default.
-show_authors = True
-
-# The name of the Pygments (syntax highlighting) style to use.
-pygments_style = 'sphinx'
-
-# A list of ignored prefixes for module index sorting.
-#modindex_common_prefix = []
-
-# If true, keep warnings as "system message" paragraphs in the built documents.
-#keep_warnings = False
-
-# If true, `todo` and `todoList` produce output, else they produce nothing.
-todo_include_todos = True
-
-
-# -- Options for HTML output ----------------------------------------------
-
-# The theme to use for HTML and HTML Help pages.  See the documentation for
-# a list of builtin themes.
-html_theme = 'sphinx_rtd_theme'
-
-# Theme options are theme-specific and customize the look and feel of a theme
-# further.  For a list of options available for each theme, see the
-# documentation.
-#html_theme_options = {}
-
-# Add any paths that contain custom themes here, relative to this directory.
-#html_theme_path = []
-
-# The name for this set of Sphinx documents.  If None, it defaults to
-# "<project> v<release> documentation".
-#html_title = None
-
-# A shorter title for the navigation bar.  Default is the same as html_title.
-#html_short_title = None
-
-# The name of an image file (relative to this directory) to place at the top
-# of the sidebar.
-html_logo = 'ibm.png'
-
-# The name of an image file (within the static path) to use as favicon of the
-# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
-# pixels large.
-html_favicon = 'kc.ico'
-
-# Add any paths that contain custom static files (such as style sheets) here,
-# relative to this directory. They are copied after the builtin static files,
-# so a file named "default.css" will overwrite the builtin "default.css".
-html_static_path = ['_static']
-
-# Add any extra paths that contain custom files (such as robots.txt or
-# .htaccess) here, relative to this directory. These files are copied
-# directly to the root of the documentation.
-#html_extra_path = []
-
-# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
-# using the given strftime format.
-html_last_updated_fmt = '%b %d, %Y'
-
-# If true, SmartyPants will be used to convert quotes and dashes to
-# typographically correct entities.
-html_use_smartypants = True
-
-# Custom sidebar templates, maps document names to template names.
-#html_sidebars = {}
-
-# Additional templates that should be rendered to pages, maps page names to
-# template names.
-#html_additional_pages = {}
-
-# If false, no module index is generated.
-html_domain_indices = True
-
-# If false, no index is generated.
-html_use_index = True
-
-# If true, the index is split into individual pages for each letter.
-html_split_index = False
-
-# If true, links to the reST sources are added to the pages.
-html_show_sourcelink = True
-
-# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
-html_show_sphinx = True
-
-# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
-html_show_copyright = True
-
-# If true, an OpenSearch description file will be output, and all pages will
-# contain a <link> tag referring to it.  The value of this option must be the
-# base URL from which the finished HTML is served.
-#html_use_opensearch = ''
-
-# This is the file name suffix for HTML files (e.g. ".xhtml").
-#html_file_suffix = None
-
-# Language to be used for generating the HTML full-text search index.
-# Sphinx supports the following languages:
-#   'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'
-#   'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'
-html_search_language = 'en'
-
-# A dictionary with options for the search language support, empty by default.
-# Now only 'ja' uses this config value
-#html_search_options = {'type': 'default'}
-
-# The name of a javascript file (relative to the configuration directory) that
-# implements a search results scorer. If empty, the default will be used.
-#html_search_scorer = 'scorer.js'
-
-# Output file base name for HTML help builder.
-htmlhelp_basename = 'ibmdbpydoc'
-
-# -- Options for LaTeX output ---------------------------------------------
-
-latex_elements = {
-# The paper size ('letterpaper' or 'a4paper').
-#'papersize': 'letterpaper',
-
-# The font size ('10pt', '11pt' or '12pt').
-#'pointsize': '10pt',
-
-# Additional stuff for the LaTeX preamble.
-#'preamble': '',
-
-# Latex figure (float) alignment
-#'figure_align': 'htbp',
-}
-
-# Grouping the document tree into LaTeX files. List of tuples
-# (source start file, target name, title,
-#  author, documentclass [howto, manual, or own class]).
-latex_documents = [
-    (master_doc, 'ibmdbpy.tex', u'ibmdbpy Documentation',
-     u'Edouard Fouché,Avipsa Roy', 'manual'),
-]
-
-# The name of an image file (relative to this directory) to place at the top of
-# the title page.
-#latex_logo = None
-
-# For "manual" documents, if this is true, then toplevel headings are parts,
-# not chapters.
-#latex_use_parts = False
-
-# If true, show page references after internal links.
-#latex_show_pagerefs = False
-
-# If true, show URL addresses after external links.
-#latex_show_urls = False
-
-# Documents to append as an appendix to all manuals.
-#latex_appendices = []
-
-# If false, no module index is generated.
-#latex_domain_indices = True
-
-
-# -- Options for manual page output ---------------------------------------
-
-# One entry per manual page. List of tuples
-# (source start file, name, description, authors, manual section).
-man_pages = [
-    (master_doc, 'ibmdbpy', u'ibmdbpy Documentation',
-     [author], 1)
-]
-
-# If true, show URL addresses after external links.
-#man_show_urls = False
-
-
-# -- Options for Texinfo output -------------------------------------------
-
-# Grouping the document tree into Texinfo files. List of tuples
-# (source start file, target name, title, author,
-#  dir menu entry, description, category)
-texinfo_documents = [
-    (master_doc, 'ibmdbpy', u'ibmdbpy Documentation',
-     author, 'ibmdbpy', 'One line description of project.',
-     'Miscellaneous'),
-]
-
-# Documents to append as an appendix to all manuals.
-#texinfo_appendices = []
-
-# If false, no module index is generated.
-#texinfo_domain_indices = True
-
-# How to display URL addresses: 'footnote', 'no', or 'inline'.
-#texinfo_show_urls = 'footnote'
-
-# If true, do not generate a @detailmenu in the "Top" node's menu.
-#texinfo_no_detailmenu = False
-
-
-# -- Options for Epub output ----------------------------------------------
-
-# Bibliographic Dublin Core info.
-epub_title = project
-epub_author = author
-epub_publisher = author
-epub_copyright = copyright
-
-# The basename for the epub file. It defaults to the project name.
-#epub_basename = project
-
-# The HTML theme for the epub output. Since the default themes are not
-# optimized for small screen space, using the same theme for HTML and epub
-# output is usually not wise. This defaults to 'epub', a theme designed to save
-# visual space.
-#epub_theme = 'epub'
-
-# The language of the text. It defaults to the language option
-# or 'en' if the language is not set.
-#epub_language = ''
-
-# The scheme of the identifier. Typical schemes are ISBN or URL.
-#epub_scheme = ''
-
-# The unique identifier of the text. This can be a ISBN number
-# or the project homepage.
-#epub_identifier = ''
-
-# A unique identification for the text.
-#epub_uid = ''
-
-# A tuple containing the cover image and cover page html template filenames.
-#epub_cover = ()
-
-# A sequence of (type, uri, title) tuples for the guide element of content.opf.
-#epub_guide = ()
-
-# HTML files that should be inserted before the pages created by sphinx.
-# The format is a list of tuples containing the path and title.
-#epub_pre_files = []
-
-# HTML files that should be inserted after the pages created by sphinx.
-# The format is a list of tuples containing the path and title.
-#epub_post_files = []
-
-# A list of files that should not be packed into the epub file.
-epub_exclude_files = ['search.html']
-
-# The depth of the table of contents in toc.ncx.
-#epub_tocdepth = 3
-
-# Allow duplicate toc entries.
-#epub_tocdup = True
-
-# Choose between 'default' and 'includehidden'.
-#epub_tocscope = 'default'
-
-# Fix unsupported image types using the Pillow.
-#epub_fix_images = False
-
-# Scale large images.
-#epub_max_image_width = 0
-
-# How to display URL addresses: 'footnote', 'no', or 'inline'.
-#epub_show_urls = 'inline'
-
-# If false, no index is generated.
-#epub_use_index = True
-
-
-# Example configuration for intersphinx: refer to the Python standard library.
-intersphinx_mapping = {'https://docs.python.org/': None}
+# -*- coding: utf-8 -*-
+#
+# nzpyida documentation build configuration file, created by
+# sphinx-quickstart on Wed Jul 20 18:25:07 2016.
+#
+# This file is execfile()d with the current directory set to its
+# containing dir.
+#
+# Note that not all possible configuration values are present in this
+# autogenerated file.
+#
+# All configuration values have a default; values that are commented out
+# serve to show the default.
+
+#standard_library.install_aliases()
+import sys
+import os
+import shlex
+# If extensions (or modules to document with autodoc) are in another directory,
+# add these directories to sys.path here. If the directory is relative to the
+# documentation root, use os.path.abspath to make it absolute, like shown here.
+sys.path.insert(0, os.pardir)
+basepath = os.path.dirname(__file__)
+filepath = os.path.abspath(os.path.join(basepath, "..", "..", "nzpyida"))
+sys.path.append(filepath)
+# -- General configuration ------------------------------------------------
+
+# If your documentation needs a minimal Sphinx version, state it here.
+#needs_sphinx = '1.0'
+
+# Add any Sphinx extension module names here, as strings. They can be
+# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
+# ones.
+extensions = [
+    'sphinx.ext.autodoc','numpydoc',
+    'sphinx.ext.autosummary',
+    'sphinx.ext.intersphinx',
+    'sphinx.ext.todo',
+    'sphinx.ext.coverage',
+    'sphinx.ext.imgmath',
+    'sphinx.ext.ifconfig',
+    'sphinx.ext.viewcode',
+    'IPython.sphinxext.ipython_console_highlighting',
+    'IPython.sphinxext.ipython_directive'
+]
+
+# Avoid annoying warnings
+#numpydoc_show_class_members = False
+
+# Add any paths that contain templates here, relative to this directory.
+templates_path = ['_templates']
+
+# The suffix(es) of source filenames.
+# You can specify multiple suffix as a list of string:
+# source_suffix = ['.rst', '.md']
+source_suffix = '.rst'
+
+# The encoding of source files.
+#source_encoding = 'utf-8-sig'
+
+# The master toctree document.
+master_doc = 'index'
+
+# General information about the project.
+project = u'nzpyida'
+copyright = u'2016-2023, IBM Corp'
+author = u''
+
+# The version info for the project you're documenting, acts as replacement for
+# |version| and |release|, also used in various other places throughout the
+# built documents.
+#
+# The short X.Y version.
+version = u'0.3'
+# The full version, including alpha/beta/rc tags.
+release = u'0.x.x beta'
+
+# The language for content autogenerated by Sphinx. Refer to documentation
+# for a list of supported languages.
+#
+# This is also used if you do content translation via gettext catalogs.
+# Usually you set "language" from the command line for these cases.
+language = 'english'
+
+# There are two options for replacing |today|: either, you set today to some
+# non-false value, then it is used:
+#today = ''
+# Else, today_fmt is used as the format for a strftime call.
+#today_fmt = '%B %d, %Y'
+
+# List of patterns, relative to source directory, that match files and
+# directories to ignore when looking for source files.
+exclude_patterns = []
+exclude_trees = ['build']
+
+# The reST default role (used for this markup: `text`) to use for all
+# documents.
+#default_role = None
+
+# If true, '()' will be appended to :func: etc. cross-reference text.
+add_function_parentheses = True
+
+# If true, the current module name will be prepended to all description
+# unit titles (such as .. function::).
+add_module_names = True
+
+# If true, sectionauthor and moduleauthor directives will be shown in the
+# output. They are ignored by default.
+show_authors = True
+
+# The name of the Pygments (syntax highlighting) style to use.
+pygments_style = 'sphinx'
+
+# A list of ignored prefixes for module index sorting.
+#modindex_common_prefix = []
+
+# If true, keep warnings as "system message" paragraphs in the built documents.
+#keep_warnings = False
+
+# If true, `todo` and `todoList` produce output, else they produce nothing.
+todo_include_todos = True
+
+
+# -- Options for HTML output ----------------------------------------------
+
+# The theme to use for HTML and HTML Help pages.  See the documentation for
+# a list of builtin themes.
+html_theme = 'sphinx_rtd_theme'
+
+# Theme options are theme-specific and customize the look and feel of a theme
+# further.  For a list of options available for each theme, see the
+# documentation.
+#html_theme_options = {}
+
+# Add any paths that contain custom themes here, relative to this directory.
+#html_theme_path = []
+
+# The name for this set of Sphinx documents.  If None, it defaults to
+# "<project> v<release> documentation".
+#html_title = None
+
+# A shorter title for the navigation bar.  Default is the same as html_title.
+#html_short_title = None
+
+# The name of an image file (relative to this directory) to place at the top
+# of the sidebar.
+html_logo = 'ibm.png'
+
+# The name of an image file (within the static path) to use as favicon of the
+# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
+# pixels large.
+html_favicon = 'kc.ico'
+
+# Add any paths that contain custom static files (such as style sheets) here,
+# relative to this directory. They are copied after the builtin static files,
+# so a file named "default.css" will overwrite the builtin "default.css".
+html_static_path = ['_static']
+
+# Add any extra paths that contain custom files (such as robots.txt or
+# .htaccess) here, relative to this directory. These files are copied
+# directly to the root of the documentation.
+#html_extra_path = []
+
+# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
+# using the given strftime format.
+html_last_updated_fmt = '%b %d, %Y'
+
+# If true, SmartyPants will be used to convert quotes and dashes to
+# typographically correct entities.
+html_use_smartypants = False
+
+# Custom sidebar templates, maps document names to template names.
+#html_sidebars = {}
+
+# Additional templates that should be rendered to pages, maps page names to
+# template names.
+#html_additional_pages = {}
+
+# If false, no module index is generated.
+html_domain_indices = True
+
+# If false, no index is generated.
+html_use_index = True
+
+# If true, the index is split into individual pages for each letter.
+html_split_index = False
+
+# If true, links to the reST sources are added to the pages.
+html_show_sourcelink = False
+
+# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
+html_show_sphinx = True
+
+# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
+html_show_copyright = True
+
+# If true, an OpenSearch description file will be output, and all pages will
+# contain a <link> tag referring to it.  The value of this option must be the
+# base URL from which the finished HTML is served.
+#html_use_opensearch = ''
+
+# This is the file name suffix for HTML files (e.g. ".xhtml").
+#html_file_suffix = None
+
+# Language to be used for generating the HTML full-text search index.
+# Sphinx supports the following languages:
+#   'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'
+#   'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'
+html_search_language = 'en'
+
+# A dictionary with options for the search language support, empty by default.
+# Now only 'ja' uses this config value
+#html_search_options = {'type': 'default'}
+
+# The name of a javascript file (relative to the configuration directory) that
+# implements a search results scorer. If empty, the default will be used.
+#html_search_scorer = 'scorer.js'
+
+# Output file base name for HTML help builder.
+htmlhelp_basename = 'nzpyidadoc'
+
+# -- Options for LaTeX output ---------------------------------------------
+
+latex_elements = {
+# The paper size ('letterpaper' or 'a4paper').
+#'papersize': 'letterpaper',
+
+# The font size ('10pt', '11pt' or '12pt').
+#'pointsize': '10pt',
+
+# Additional stuff for the LaTeX preamble.
+#'preamble': '',
+
+# Latex figure (float) alignment
+#'figure_align': 'htbp',
+}
+
+# Grouping the document tree into LaTeX files. List of tuples
+# (source start file, target name, title,
+#  author, documentclass [howto, manual, or own class]).
+latex_documents = [
+    (master_doc, 'nzpyida.tex', u'nzpyida Documentation',
+     u'', 'manual'),
+]
+
+# The name of an image file (relative to this directory) to place at the top of
+# the title page.
+#latex_logo = None
+
+# For "manual" documents, if this is true, then toplevel headings are parts,
+# not chapters.
+#latex_use_parts = False
+
+# If true, show page references after internal links.
+#latex_show_pagerefs = False
+
+# If true, show URL addresses after external links.
+#latex_show_urls = False
+
+# Documents to append as an appendix to all manuals.
+#latex_appendices = []
+
+# If false, no module index is generated.
+#latex_domain_indices = True
+
+
+# -- Options for manual page output ---------------------------------------
+
+# One entry per manual page. List of tuples
+# (source start file, name, description, authors, manual section).
+man_pages = [
+    (master_doc, 'nzpyida', u'nzpyida Documentation',
+     [author], 1)
+]
+
+# If true, show URL addresses after external links.
+#man_show_urls = False
+
+
+# -- Options for Texinfo output -------------------------------------------
+
+# Grouping the document tree into Texinfo files. List of tuples
+# (source start file, target name, title, author,
+#  dir menu entry, description, category)
+texinfo_documents = [
+    (master_doc, 'nzpyida', u'nzpyida Documentation',
+     author, 'nzpyida', 'Python interface to in-database analytics algorithms and custom ML/analytics execution inside Netezza.',
+     'Miscellaneous'),
+]
+
+# Documents to append as an appendix to all manuals.
+#texinfo_appendices = []
+
+# If false, no module index is generated.
+#texinfo_domain_indices = True
+
+# How to display URL addresses: 'footnote', 'no', or 'inline'.
+#texinfo_show_urls = 'footnote'
+
+# If true, do not generate a @detailmenu in the "Top" node's menu.
+#texinfo_no_detailmenu = False
+
+
+# -- Options for Epub output ----------------------------------------------
+
+# Bibliographic Dublin Core info.
+epub_title = project
+epub_author = author
+epub_publisher = author
+epub_copyright = copyright
+
+# The basename for the epub file. It defaults to the project name.
+#epub_basename = project
+
+# The HTML theme for the epub output. Since the default themes are not
+# optimized for small screen space, using the same theme for HTML and epub
+# output is usually not wise. This defaults to 'epub', a theme designed to save
+# visual space.
+#epub_theme = 'epub'
+
+# The language of the text. It defaults to the language option
+# or 'en' if the language is not set.
+#epub_language = ''
+
+# The scheme of the identifier. Typical schemes are ISBN or URL.
+#epub_scheme = ''
+
+# The unique identifier of the text. This can be a ISBN number
+# or the project homepage.
+#epub_identifier = ''
+
+# A unique identification for the text.
+#epub_uid = ''
+
+# A tuple containing the cover image and cover page html template filenames.
+#epub_cover = ()
+
+# A sequence of (type, uri, title) tuples for the guide element of content.opf.
+#epub_guide = ()
+
+# HTML files that should be inserted before the pages created by sphinx.
+# The format is a list of tuples containing the path and title.
+#epub_pre_files = []
+
+# HTML files that should be inserted after the pages created by sphinx.
+# The format is a list of tuples containing the path and title.
+#epub_post_files = []
+
+# A list of files that should not be packed into the epub file.
+epub_exclude_files = ['search.html']
+
+# The depth of the table of contents in toc.ncx.
+#epub_tocdepth = 3
+
+# Allow duplicate toc entries.
+#epub_tocdup = True
+
+# Choose between 'default' and 'includehidden'.
+#epub_tocscope = 'default'
+
+# Fix unsupported image types using the Pillow.
+#epub_fix_images = False
+
+# Scale large images.
+#epub_max_image_width = 0
+
+# How to display URL addresses: 'footnote', 'no', or 'inline'.
+#epub_show_urls = 'inline'
+
+# If false, no index is generated.
+#epub_use_index = True
+
+
+# Example configuration for intersphinx: refer to the Python standard library.
+intersphinx_mapping = {'https://docs.python.org/': None}
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `nzpyida-0.2.2.6/docs/source/frame.rst` & `nzpyida-0.3.3/docs/source/frame.rst`

 * *Files 22% similar despite different names*

```diff
@@ -1,375 +1,375 @@
-.. highlight:: python
-
-IdaDataFrame
-************
-
-Open an IdaDataFrame Object.
-============================
-
-.. currentmodule:: ibmdbpy.frame
-
-.. autoclass:: IdaDataFrame
-
-   .. automethod:: __init__
-
-.. rubric:: Methods
-
-
-DataFrame introspection
-=======================
-
-internal_state
---------------
-.. autoattribute:: IdaDataFrame.internal_state
-
-indexer
--------
-.. autoattribute:: IdaDataFrame.indexer
-
-type
-----
-.. autoattribute:: IdaDataFrame.type
-
-dtypes
-------
-.. autoattribute:: IdaDataFrame.dtypes
-
-index
------
-.. autoattribute:: IdaDataFrame.index
-
-columns
--------
-.. autoattribute:: IdaDataFrame.columns
-
-axes
-----
-.. autoattribute:: IdaDataFrame.axes
-
-shape
------
-.. autoattribute:: IdaDataFrame.shape
-
-empty
------
-.. autoattribute:: IdaDataFrame.empty
-
-__len__
--------
-.. automethod:: IdaDataFrame.__len__
-
-__iter__
----------
-.. automethod:: IdaDataFrame.__iter__
-
-DataFrame modification
-======================
-
-Selection, Projection
----------------------
-.. automethod:: IdaDataFrame.__getitem__
-
-Selection and Projection are also possible using the ``ibmdbpy.Loc`` object stored in ``IdaDataFrame.loc``.
-
-.. autoclass:: ibmdbpy.indexing.Loc
-	:members:
-
-
-Filtering
----------
-
-.. automethod:: IdaDataFrame.delete_na 
-
-.. autoclass:: ibmdbpy.filtering.FilterQuery
-	:members:
-
-.. automethod:: IdaDataFrame.__lt__
-
-.. automethod:: IdaDataFrame.__le__
-
-.. automethod:: IdaDataFrame.__eq__
-
-.. automethod:: IdaDataFrame.__ne__
-
-.. automethod:: IdaDataFrame.__ge__
-
-.. automethod:: IdaDataFrame.__gt__
-
-
-Feature Engineering
--------------------
-
-.. automethod:: IdaDataFrame.__setitem__
-
-.. automethod:: ibmdbpy.aggregation.aggregate_idadf
-
-.. currentmodule:: ibmdbpy.frame
-
-.. automethod:: IdaDataFrame.__add__
-
-.. automethod:: IdaDataFrame.__radd__
-
-.. automethod:: IdaDataFrame.__div__
-
-.. automethod:: IdaDataFrame.__rdiv__
-
-.. automethod:: IdaDataFrame.__truediv__
-
-.. automethod:: IdaDataFrame.__rtruediv__
-
-.. automethod:: IdaDataFrame.__floordiv__
-
-.. automethod:: IdaDataFrame.__rfloordiv__
-
-.. automethod:: IdaDataFrame.__mod__
-
-.. automethod:: IdaDataFrame.__rmod__
-
-.. automethod:: IdaDataFrame.__mul__
-
-.. automethod:: IdaDataFrame.__rmul__
-
-.. automethod:: IdaDataFrame.__neg__
-
-.. automethod:: IdaDataFrame.__rpos__
-
-.. automethod:: IdaDataFrame.__pow__
-
-.. automethod:: IdaDataFrame.__rpow__
-
-.. automethod:: IdaDataFrame.__sub__
-
-.. automethod:: IdaDataFrame.__rsub__
-
-.. automethod:: IdaDataFrame.__delitem__
-
-.. automethod:: IdaDataFrame.save_as
-
-DataBase Features
-=================
-
-exists
-------
-.. automethod:: IdaDataFrame.exists
-
-is_view / is_table
-------------------
-.. automethod:: IdaDataFrame.is_view
-
-.. automethod:: IdaDataFrame.is_table
-
-
-get_primary_key
----------------
-.. automethod:: IdaDataFrame.get_primary_key
-
-ida_query
----------
-.. automethod:: IdaDataFrame.ida_query
-
-ida_scalar_query
-----------------
-.. automethod:: IdaDataFrame.ida_scalar_query
-
-
-Data Exploration
-================
-
-head
-----
-.. automethod:: IdaDataFrame.head
-
-tail
-----
-.. automethod:: IdaDataFrame.tail
-
-pivot_table
------------
-.. automethod:: IdaDataFrame.pivot_table
-
-sort
-----
-.. automethod:: IdaDataFrame.sort
-
-
-Descriptive Statistics
-======================
-
-describe
---------
-.. automethod:: IdaDataFrame.describe
-
-cov (covariance)
-----------------
-.. automethod:: IdaDataFrame.cov
-
-corr (correlation)
-------------------
-.. automethod:: IdaDataFrame.corr
-
-quantile
---------
-.. automethod:: IdaDataFrame.quantile
-
-mad (mean absolute deviation)
------------------------------
-.. automethod:: IdaDataFrame.mad
-
-min (minimum)
--------------
-.. automethod:: IdaDataFrame.min
-
-max (maximum)
--------------
-.. automethod:: IdaDataFrame.max
-
-count
------
-.. automethod:: IdaDataFrame.count
-
-count_distinct
---------------
-.. automethod:: IdaDataFrame.count_distinct
-
-std (standard deviation)
-------------------------
-.. automethod:: IdaDataFrame.std
-
-var (variance)
---------------
-.. automethod:: IdaDataFrame.var
-
-mean
-----
-.. automethod:: IdaDataFrame.mean
-
-sum
----
-.. automethod:: IdaDataFrame.sum
-
-median
-------
-.. automethod:: IdaDataFrame.median
-
-Import as DataFrame
-===================
-
-as_dataframe
-------------
-.. automethod:: IdaDataFrame.as_dataframe
-
-Connection Management
-=====================
-
-commit
-------
-.. automethod:: IdaDataFrame.commit
-
-rollback
---------
-.. automethod:: IdaDataFrame.rollback
-
-
-Private Methods
-===============
-
-_clone
-------
-.. automethod:: IdaDataFrame._clone
-
-_clone_as_serie
----------------
-.. automethod:: IdaDataFrame._clone_as_serie
-
-_get_type
----------
-.. automethod:: IdaDataFrame._get_type
-
-_get_columns
-------------
-.. automethod:: IdaDataFrame._get_columns
-
-_get_all_columns_in_table
--------------------------
-.. automethod:: IdaDataFrame._get_all_columns_in_table
-
-_get_index
-----------
-.. automethod:: IdaDataFrame._get_index
-
-_get_shape
-----------
-.. automethod:: IdaDataFrame._get_shape
-
-_get_columns_dtypes
--------------------
-.. automethod:: IdaDataFrame._get_columns_dtypes
-
-_reset_attributes
------------------
-.. automethod:: IdaDataFrame._reset_attributes
-
-_table_def
-----------
-.. automethod:: IdaDataFrame._table_def
-
-_get_numerical_columns
-----------------------
-.. automethod:: IdaDataFrame._get_numerical_columns
-
-_get_categorical_columns
-------------------------
-.. automethod:: IdaDataFrame._get_categorical_columns
-
-_prepare_and_execute
---------------------
-.. automethod:: IdaDataFrame._prepare_and_execute
-
-_autocommit
------------
-.. automethod:: IdaDataFrame._autocommit
-
-_combine_check
---------------
-.. automethod:: IdaDataFrame._combine_check
-
-These functions are defined in ibmdbpy.statistics but apply to IdaDataFrames.
-
-
-
-
-_numeric_stats
---------------
-.. autofunction:: ibmdbpy.statistics._numeric_stats
-
-_get_percentiles
-----------------
-.. autofunction:: ibmdbpy.statistics._get_percentiles
-
-_categorical_stats
-------------------
-.. autofunction:: ibmdbpy.statistics._categorical_stats
-
-_get_number_of_nas
-------------------
-.. autofunction:: ibmdbpy.statistics._get_number_of_nas
-
-_count_level
-------------
-.. autofunction:: ibmdbpy.statistics._count_level
-
-_count_level_groupby
---------------------
-.. autofunction:: ibmdbpy.statistics._count_level_groupby
-
-_factors_count
---------------------
-.. autofunction:: ibmdbpy.statistics._factors_count
-
-_factors_sum
---------------------
-.. autofunction:: ibmdbpy.statistics._factors_sum
-
-_factors_avg
---------------------
-.. autofunction:: ibmdbpy.statistics._factors_avg
+.. highlight:: python
+
+IdaDataFrame
+************
+
+Open an IdaDataFrame Object.
+============================
+
+.. currentmodule:: nzpyida.frame
+
+.. autoclass:: IdaDataFrame
+
+   .. automethod:: __init__
+
+.. rubric:: Methods
+
+
+DataFrame introspection
+=======================
+
+internal_state
+--------------
+.. autoattribute:: IdaDataFrame.internal_state
+
+indexer
+-------
+.. autoattribute:: IdaDataFrame.indexer
+
+type
+----
+.. autoattribute:: IdaDataFrame.type
+
+dtypes
+------
+.. autoattribute:: IdaDataFrame.dtypes
+
+index
+-----
+.. autoattribute:: IdaDataFrame.index
+
+columns
+-------
+.. autoattribute:: IdaDataFrame.columns
+
+axes
+----
+.. autoattribute:: IdaDataFrame.axes
+
+shape
+-----
+.. autoattribute:: IdaDataFrame.shape
+
+empty
+-----
+.. autoattribute:: IdaDataFrame.empty
+
+__len__
+-------
+.. automethod:: IdaDataFrame.__len__
+
+__iter__
+---------
+.. automethod:: IdaDataFrame.__iter__
+
+DataFrame modification
+======================
+
+Selection, Projection
+---------------------
+.. automethod:: IdaDataFrame.__getitem__
+
+Selection and Projection are also possible using the ``nzpyida.Loc`` object stored in ``IdaDataFrame.loc``.
+
+.. autoclass:: nzpyida.indexing.Loc
+	:members:
+
+
+Filtering
+---------
+
+.. automethod:: IdaDataFrame.delete_na 
+
+.. autoclass:: nzpyida.filtering.FilterQuery
+	:members:
+
+.. automethod:: IdaDataFrame.__lt__
+
+.. automethod:: IdaDataFrame.__le__
+
+.. automethod:: IdaDataFrame.__eq__
+
+.. automethod:: IdaDataFrame.__ne__
+
+.. automethod:: IdaDataFrame.__ge__
+
+.. automethod:: IdaDataFrame.__gt__
+
+
+Feature Engineering
+-------------------
+
+.. automethod:: IdaDataFrame.__setitem__
+
+.. automethod:: nzpyida.aggregation.aggregate_idadf
+
+.. currentmodule:: nzpyida.frame
+
+.. automethod:: IdaDataFrame.__add__
+
+.. automethod:: IdaDataFrame.__radd__
+
+.. automethod:: IdaDataFrame.__div__
+
+.. automethod:: IdaDataFrame.__rdiv__
+
+.. automethod:: IdaDataFrame.__truediv__
+
+.. automethod:: IdaDataFrame.__rtruediv__
+
+.. automethod:: IdaDataFrame.__floordiv__
+
+.. automethod:: IdaDataFrame.__rfloordiv__
+
+.. automethod:: IdaDataFrame.__mod__
+
+.. automethod:: IdaDataFrame.__rmod__
+
+.. automethod:: IdaDataFrame.__mul__
+
+.. automethod:: IdaDataFrame.__rmul__
+
+.. automethod:: IdaDataFrame.__neg__
+
+.. automethod:: IdaDataFrame.__rpos__
+
+.. automethod:: IdaDataFrame.__pow__
+
+.. automethod:: IdaDataFrame.__rpow__
+
+.. automethod:: IdaDataFrame.__sub__
+
+.. automethod:: IdaDataFrame.__rsub__
+
+.. automethod:: IdaDataFrame.__delitem__
+
+.. automethod:: IdaDataFrame.save_as
+
+DataBase Features
+=================
+
+exists
+------
+.. automethod:: IdaDataFrame.exists
+
+is_view / is_table
+------------------
+.. automethod:: IdaDataFrame.is_view
+
+.. automethod:: IdaDataFrame.is_table
+
+
+get_primary_key
+---------------
+.. automethod:: IdaDataFrame.get_primary_key
+
+ida_query
+---------
+.. automethod:: IdaDataFrame.ida_query
+
+ida_scalar_query
+----------------
+.. automethod:: IdaDataFrame.ida_scalar_query
+
+
+Data Exploration
+================
+
+head
+----
+.. automethod:: IdaDataFrame.head
+
+tail
+----
+.. automethod:: IdaDataFrame.tail
+
+pivot_table
+-----------
+.. automethod:: IdaDataFrame.pivot_table
+
+sort
+----
+.. automethod:: IdaDataFrame.sort
+
+
+Descriptive Statistics
+======================
+
+describe
+--------
+.. automethod:: IdaDataFrame.describe
+
+cov (covariance)
+----------------
+.. automethod:: IdaDataFrame.cov
+
+corr (correlation)
+------------------
+.. automethod:: IdaDataFrame.corr
+
+quantile
+--------
+.. automethod:: IdaDataFrame.quantile
+
+mad (mean absolute deviation)
+-----------------------------
+.. automethod:: IdaDataFrame.mad
+
+min (minimum)
+-------------
+.. automethod:: IdaDataFrame.min
+
+max (maximum)
+-------------
+.. automethod:: IdaDataFrame.max
+
+count
+-----
+.. automethod:: IdaDataFrame.count
+
+count_distinct
+--------------
+.. automethod:: IdaDataFrame.count_distinct
+
+std (standard deviation)
+------------------------
+.. automethod:: IdaDataFrame.std
+
+var (variance)
+--------------
+.. automethod:: IdaDataFrame.var
+
+mean
+----
+.. automethod:: IdaDataFrame.mean
+
+sum
+---
+.. automethod:: IdaDataFrame.sum
+
+median
+------
+.. automethod:: IdaDataFrame.median
+
+Import as DataFrame
+===================
+
+as_dataframe
+------------
+.. automethod:: IdaDataFrame.as_dataframe
+
+Connection Management
+=====================
+
+commit
+------
+.. automethod:: IdaDataFrame.commit
+
+rollback
+--------
+.. automethod:: IdaDataFrame.rollback
+
+
+Private Methods
+===============
+
+_clone
+------
+.. automethod:: IdaDataFrame._clone
+
+_clone_as_serie
+---------------
+.. automethod:: IdaDataFrame._clone_as_serie
+
+_get_type
+---------
+.. automethod:: IdaDataFrame._get_type
+
+_get_columns
+------------
+.. automethod:: IdaDataFrame._get_columns
+
+_get_all_columns_in_table
+-------------------------
+.. automethod:: IdaDataFrame._get_all_columns_in_table
+
+_get_index
+----------
+.. automethod:: IdaDataFrame._get_index
+
+_get_shape
+----------
+.. automethod:: IdaDataFrame._get_shape
+
+_get_columns_dtypes
+-------------------
+.. automethod:: IdaDataFrame._get_columns_dtypes
+
+_reset_attributes
+-----------------
+.. automethod:: IdaDataFrame._reset_attributes
+
+_table_def
+----------
+.. automethod:: IdaDataFrame._table_def
+
+_get_numerical_columns
+----------------------
+.. automethod:: IdaDataFrame._get_numerical_columns
+
+_get_categorical_columns
+------------------------
+.. automethod:: IdaDataFrame._get_categorical_columns
+
+_prepare_and_execute
+--------------------
+.. automethod:: IdaDataFrame._prepare_and_execute
+
+_autocommit
+-----------
+.. automethod:: IdaDataFrame._autocommit
+
+_combine_check
+--------------
+.. automethod:: IdaDataFrame._combine_check
+
+These functions are defined in nzpyida.statistics but apply to IdaDataFrames.
+
+
+
+
+_numeric_stats
+--------------
+.. autofunction:: nzpyida.statistics._numeric_stats
+
+_get_percentiles
+----------------
+.. autofunction:: nzpyida.statistics._get_percentiles
+
+_categorical_stats
+------------------
+.. autofunction:: nzpyida.statistics._categorical_stats
+
+_get_number_of_nas
+------------------
+.. autofunction:: nzpyida.statistics._get_number_of_nas
+
+_count_level
+------------
+.. autofunction:: nzpyida.statistics._count_level
+
+_count_level_groupby
+--------------------
+.. autofunction:: nzpyida.statistics._count_level_groupby
+
+_factors_count
+--------------------
+.. autofunction:: nzpyida.statistics._factors_count
+
+_factors_sum
+--------------------
+.. autofunction:: nzpyida.statistics._factors_sum
+
+_factors_avg
+--------------------
+.. autofunction:: nzpyida.statistics._factors_avg
```

### Comparing `nzpyida-0.2.2.6/docs/source/geoFrame.rst` & `nzpyida-0.3.3/docs/source/geoFrame.rst`

 * *Files 27% similar despite different names*

```diff
@@ -1,93 +1,93 @@
-.. highlight:: python
-
-IdaGeoDataFrame
-***************
-An IdaGeoDataFrame is a reference to a spatial table in a remote Db2 instance.
-
-The most important property of an IdaGeoDataFrame is that it always has a reference to one IdaGeoSeries column
-that holds a special status. This IdaGeoSeries is referred to as the IdaGeoDataFrame‘s “geometry”.
-When a spatial method is applied to an IdaGeoDataFrame (or a spatial attribute like area is called),
-this commands will always act on the “geometry” attribute.
-
-The “geometry” attribute – no matter its name – can be accessed through the geometry attribute
-of an IdaGeoDataFrame.
-
-Open an IdaGeoDataFrame
-=======================
-.. currentmodule:: ibmdbpy.geoFrame
-
-.. autoclass:: IdaGeoDataFrame
-
-   .. automethod:: __init__
-
-Set geometry
-------------
-.. automethod:: IdaGeoDataFrame.set_geometry
-
-Create from an IdaDataFrame
----------------------------
-.. automethod:: IdaGeoDataFrame.from_IdaDataFrame
-
-Get the geometry attribute
---------------------------
-.. autoattribute:: IdaGeoDataFrame.geometry
-
-Geospatial Methods that return an IdaGeoDataFrame
-=================================================
-Some geospatial methods operate on two IdaGeoDataFrames to return a result as a boolean or a new geometry.
-Such methods can be accessed with the IdaGeoDataFrame object to return a new IdaGeoDataFrame with three columns
-respectively, indexer of the first IdaGeoDataFrame with which the method is called, the indexer of the second
-IdaGeoDataFrame which is passed as an argument to the method and a third column which contains the result of the
-geometric operation between the geometry columns of the first and second IdaGeoDataFrames.
-
-Contains
---------
-.. automethod:: IdaGeoDataFrame.contains
-
-Crosses
--------
-.. automethod:: IdaGeoDataFrame.crosses
-
-Difference
-----------
-.. automethod:: IdaGeoDataFrame.difference
-
-Disjoint
---------
-.. automethod:: IdaGeoDataFrame.disjoint
-
-Distance
---------
-.. automethod:: IdaGeoDataFrame.distance
-
-Equals
-------
-.. automethod:: IdaGeoDataFrame.equals
-
-Intersection
-------------
-.. automethod:: IdaGeoDataFrame.intersection
-
-Intersects
-----------
-.. automethod:: IdaGeoDataFrame.intersects
-
-Mbr_Intersects
---------------
-.. automethod:: IdaGeoDataFrame.mbr_intersects
-
-Overlaps
---------
-.. automethod:: IdaGeoDataFrame.overlaps
-
-Touches
--------
-.. automethod:: IdaGeoDataFrame.touches
-
-Union
------
-.. automethod:: IdaGeoDataFrame.union
-
-Within
-------
-.. automethod:: IdaGeoDataFrame.within
+.. highlight:: python
+
+IdaGeoDataFrame
+***************
+An IdaGeoDataFrame is a reference to a spatial table in a remote Netezza instance.
+
+The most important property of an IdaGeoDataFrame is that it always has a reference to one IdaGeoSeries column
+that holds a special status. This IdaGeoSeries is referred to as the IdaGeoDataFrame‘s “geometry”.
+When a spatial method is applied to an IdaGeoDataFrame (or a spatial attribute like area is called),
+this commands will always act on the “geometry” attribute.
+
+The “geometry” attribute – no matter its name – can be accessed through the geometry attribute
+of an IdaGeoDataFrame.
+
+Open an IdaGeoDataFrame
+=======================
+.. currentmodule:: nzpyida.geoFrame
+
+.. autoclass:: IdaGeoDataFrame
+
+   .. automethod:: __init__
+
+Set geometry
+------------
+.. automethod:: IdaGeoDataFrame.set_geometry
+
+Create from an IdaDataFrame
+---------------------------
+.. automethod:: IdaGeoDataFrame.from_IdaDataFrame
+
+Get the geometry attribute
+--------------------------
+.. autoattribute:: IdaGeoDataFrame.geometry
+
+Geospatial Methods that return an IdaGeoDataFrame
+=================================================
+Some geospatial methods operate on two IdaGeoDataFrames to return a result as a boolean or a new geometry.
+Such methods can be accessed with the IdaGeoDataFrame object to return a new IdaGeoDataFrame with three columns
+respectively, indexer of the first IdaGeoDataFrame with which the method is called, the indexer of the second
+IdaGeoDataFrame which is passed as an argument to the method and a third column which contains the result of the
+geometric operation between the geometry columns of the first and second IdaGeoDataFrames.
+
+Contains
+--------
+.. automethod:: IdaGeoDataFrame.contains
+
+Crosses
+-------
+.. automethod:: IdaGeoDataFrame.crosses
+
+Difference
+----------
+.. automethod:: IdaGeoDataFrame.difference
+
+Disjoint
+--------
+.. automethod:: IdaGeoDataFrame.disjoint
+
+Distance
+--------
+.. automethod:: IdaGeoDataFrame.distance
+
+Equals
+------
+.. automethod:: IdaGeoDataFrame.equals
+
+Intersection
+------------
+.. automethod:: IdaGeoDataFrame.intersection
+
+Intersects
+----------
+.. automethod:: IdaGeoDataFrame.intersects
+
+Mbr_Intersects
+--------------
+.. automethod:: IdaGeoDataFrame.mbr_intersects
+
+Overlaps
+--------
+.. automethod:: IdaGeoDataFrame.overlaps
+
+Touches
+-------
+.. automethod:: IdaGeoDataFrame.touches
+
+Union
+-----
+.. automethod:: IdaGeoDataFrame.union
+
+Within
+------
+.. automethod:: IdaGeoDataFrame.within
```

### Comparing `nzpyida-0.2.2.6/docs/source/ibm.png` & `nzpyida-0.3.3/docs/source/ibm.png`

 * *Files identical despite different names*

### Comparing `nzpyida-0.2.2.6/docs/source/kc.ico` & `nzpyida-0.3.3/docs/source/kc.ico`

 * *Files identical despite different names*

### Comparing `nzpyida-0.2.2.6/docs/source/legal.rst` & `nzpyida-0.3.3/docs/source/legal.rst`

 * *Files 21% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-.. highlight:: python
-
-License
-*******
-
-Ibmdbpy is a community project that is distributed under the New BDS License (3-clauses). The BSD license grants you almost unlimited freedom of use as long as the following conditions are met:
-
-	* You include the BSD copyright notice (see below).
-	* The name of IBM or any ibmdbpy contributor is not used to endorse or promote software derived from ibmdbpy without explicit permission.
-
-
-License::
-
-	Copyright (c) 2015  IBM Corp.
-	All rights reserved.
-
-	Redistribution and use in source and binary forms, with or without
-	modification, are permitted provided that the following conditions are
-	met:
-
-	    * Redistributions of source code must retain the above copyright
-	       notice, this list of conditions and the following disclaimer.
-
-	    * Redistributions in binary form must reproduce the above
-	       copyright notice, this list of conditions and the following
-	       disclaimer in the documentation and/or other materials provided
-	       with the distribution.
-
-	    * Neither the name of the copyright holder nor the names of any
-	       contributors may be used to endorse or promote products derived
-	       from this software without specific prior written permission.
-
-	THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS
-	"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
-	LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
-	A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
-	OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-	SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
-	LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-	DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-	THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-	(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-	OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+.. highlight:: python
+
+License
+*******
+
+nzpyida is a community project that is distributed under the New BDS License (3-clauses). The BSD license grants you almost unlimited freedom of use as long as the following conditions are met:
+
+	* You include the BSD copyright notice (see below).
+	* The name of IBM or any nzpyida and/or ibmdbpy contributor is not used to endorse or promote software derived from nzpyida and/or ibmdbpy without explicit permission.
+
+
+License::
+
+	Copyright (c) 2015  IBM Corp.
+	All rights reserved.
+
+	Redistribution and use in source and binary forms, with or without
+	modification, are permitted provided that the following conditions are
+	met:
+
+	    * Redistributions of source code must retain the above copyright
+	       notice, this list of conditions and the following disclaimer.
+
+	    * Redistributions in binary form must reproduce the above
+	       copyright notice, this list of conditions and the following
+	       disclaimer in the documentation and/or other materials provided
+	       with the distribution.
+
+	    * Neither the name of the copyright holder nor the names of any
+	       contributors may be used to endorse or promote products derived
+	       from this software without specific prior written permission.
+
+	THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS
+	"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+	LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+	A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+	OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+	SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+	LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+	DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+	THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+	(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+	OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
```

### Comparing `nzpyida-0.2.2.6/nzpyida/__init__.py` & `nzpyida-0.3.3/nzpyida/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,27 +1,27 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-from .base import IdaDataBase
-from .frame import IdaDataFrame
-from .series import IdaSeries
-from .geoFrame import IdaGeoDataFrame
-from .geoSeries import IdaGeoSeries
-
-__all__ = ['learn', 'sampledata', 'tests', 'aggregation', 
-		   'base', 'exceptions', 'filtering', 'frame', 'indexing', 
-		   'internals', 'series', 'sql', 'statistics', 'utils', 'geoFrame',
-             'geoSeries']
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+from .base import IdaDataBase
+from .frame import IdaDataFrame
+from .series import IdaSeries
+from .geoFrame import IdaGeoDataFrame
+from .geoSeries import IdaGeoSeries
+
+__all__ = ['learn', 'sampledata', 'tests', 'aggregation', 
+		   'base', 'exceptions', 'filtering', 'frame', 'indexing', 
+		   'internals', 'series', 'sql', 'statistics', 'utils', 'geoFrame',
+             'geoSeries', 'analytics']
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/__init__.py` & `nzpyida-0.3.3/nzpyida/ae/__init__.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,26 +1,26 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-# -----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-# -----------------------------------------------------------------------------
-
-
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-from .tapply import NZFunTApply
-from .apply import NZFunApply
-from .groupedapply import NZFunGroupedApply
-from .tapply_class import NZClassTApply
-from .install import NZInstall
-
-standard_library.install_aliases()
-
-__all__ = ['NZFunTApply', 'NZClassTApply', 'NZFunApply', 'NZFunGroupedApply', 'NZInstall']
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+# -----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+# -----------------------------------------------------------------------------
+
+
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+from .tapply import NZFunTApply
+from .apply import NZFunApply
+from .groupedapply import NZFunGroupedApply
+from .tapply_class import NZClassTApply
+from .install import NZInstall
+
+standard_library.install_aliases()
+
+__all__ = ['NZFunTApply', 'NZClassTApply', 'NZFunApply', 'NZFunGroupedApply', 'NZInstall']
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/apply.py` & `nzpyida-0.3.3/nzpyida/ae/tapply.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,114 +1,129 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-# -----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-# -----------------------------------------------------------------------------
-
-"""
-In-Database User defined functions
-"""
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-
-import inspect
-import textwrap
-from builtins import dict
-from future import standard_library
-
-from nzpyida.ae import shaper, result_builder
-
-standard_library.install_aliases()
-
-
-class NZFunApply(object):
-
-    def __init__(self, df,  output_signature, output_table=None, fun_ref=None, code_str=None, fun_name=None, columns=None, merge_output_with_df=False, id='ID'):
-        """
-        Constructor for tapply
-        """
-
-
-        self.table_name = df.internal_state.current_state
-        self.df = df
-        self.db = df._idadb
-
-        self.fun = fun_ref
-        self.fun_name = fun_name
-        self.code_str = code_str
-        self.output_table = output_table
-        self.output_signature =output_signature
-        self.merge_output = merge_output_with_df
-        self.id = id
-
-        # convert the columns index object to a list
-        if columns:
-            self.columns = columns
-        else:
-            self.columns = self.df.columns.tolist()
-
-
-    def get_result(self):
-
-        if self.code_str and self.fun_name is None:
-            raise Exception("fun_name is required")
-
-
-
-        if self.code_str:
-         code_string = self.build_udtf_shaper_ae_code_fun_as_str(self.code_str, self.fun_name, self.columns, self.output_signature)
-        else :
-         code_string = self.build_udtf_shaper_ae_code_fun_as_ref(self.fun, self.columns, self.output_signature)
-
-
-        compressions_string = result_builder.compress_columns_string(self.columns)
-
-        # send the code as dynamic variable to ae function
-        columns_string = compressions_string + ",'CODE_TO_EXECUTE=" + "\"" + code_string + "\"" + "'"
-
-
-
-        ae_name ="nzpyida..py_udtf_any"
-
-        query = "select ae_output.* from " + \
-                " (select * from " + self.table_name + ") as input_t" + \
-                ", table with final (" + ae_name + "(" + columns_string + ")) as ae_output"
-
-
-        result = result_builder.build_result(self.output_table,self.merge_output, self.db, self.df, self.output_signature, self.table_name, query, self.id)
-        return result
-
-
-
-    def build_udtf_shaper_ae_code_fun_as_ref(self, fun, columns, output_signature):
-
-        return self.build_udtf_shaper_ae_code_fun_as_str(inspect.getsource(fun).lstrip(), fun.__name__, columns,
-                                                  output_signature)
-
-
-    def build_udtf_shaper_ae_code_fun_as_str(self, code_str, fun_name, columns, output_signature):
-        # we need extra single quotes for correct escaping
-
-        fun_code = code_str
-
-        fun_code = fun_code.replace("'", "''")
-
-        base_code = shaper.get_base_shaper_apply(columns, fun_name, output_signature)
-
-        run_string = textwrap.dedent(""" BaseShaperUdtf.run()""")
-        final_code = base_code + "\n" + textwrap.indent(fun_code, '     ')
-        final_code = final_code + "\n" + run_string
-
-        return inspect.cleandoc(final_code)
-
-
-
-
-
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+# -----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+# -----------------------------------------------------------------------------
+
+"""
+In-Database User defined functions
+"""
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+
+import inspect
+import textwrap
+from builtins import dict
+from future import standard_library
+
+from nzpyida.ae import shaper, result_builder
+
+standard_library.install_aliases()
+
+
+class NZFunTApply(object):
+
+    def __init__(self, df,  parallel,  output_signature=None, output_table=None, fun_ref=None, code_str=None,  fun_name=None, columns=None, merge_output_with_df=False, id='ID'):
+        """
+        Constructor for tapply
+        """
+        self.table_name = df.internal_state.current_state
+        self.df = df
+        self.db = df._idadb
+        self.fun = fun_ref
+        self.fun_name =fun_name
+        self.code_str = code_str
+        self.output_table = output_table
+        self.output_signature = output_signature
+        self.parallel =parallel
+        self.merge_output = merge_output_with_df
+        self.id = id
+
+        # convert the columns index object to a list
+        if columns:
+            self.columns = columns
+        else:
+            self.columns = self.df.columns.tolist()
+
+
+
+
+
+    def get_result(self):
+
+        if self.code_str and self.fun_name is None:
+            raise Exception("fun_name is required")
+        # we need a comma separated string of column values
+        columns_string = ",".join(self.columns)
+
+
+        # get the default ae class coupled with client ml code
+
+
+        if self.code_str:
+         code_string = self.build_udtf_shaper_ae_code_fun_as_str(self.code_str, self.fun_name, self.columns, self.output_signature)
+        else :
+         code_string = self.build_udtf_shaper_ae_code_fun_as_ref(self.fun, self.columns, self.output_signature)
+
+
+        # send the code as dynamic variable to ae function
+        columns_string = columns_string + ",'CODE_TO_EXECUTE=" + "\"" + code_string + "\"" + "'"
+
+
+
+        if not self.parallel:
+            ae_name = "nzpyida..py_udtf_host"
+        else:
+            ae_name ="nzpyida..py_udtf_any"
+
+        query = "select ae_output.* from " + \
+                " (select * from " + self.table_name + ") as input_t" + \
+                ", table with final (" + ae_name + "(" + columns_string + ")) as ae_output"
+
+
+        result = result_builder.build_result(self.output_table, self.merge_output, self.db, self.df,
+                                             self.output_signature, self.table_name, query, self.id)
+        return result
+
+
+
+
+
+    def build_udtf_shaper_ae_code_fun_as_ref(self, fun, columns, output_signature):
+
+        return self.build_udtf_shaper_ae_code_fun_as_str(inspect.getsource(fun).lstrip(), fun.__name__, columns,
+                                                         output_signature)
+
+
+
+
+    def build_udtf_shaper_ae_code_fun_as_str(self, code_str, fun_name, columns, output_signature):
+        # we need extra single quotes for correct escaping
+
+        fun_code = code_str
+
+        fun_code = fun_code.replace("'", "''")
+
+        base_code = shaper.get_base_shaper_tapply(columns, fun_name, output_signature)
+
+
+
+        run_string = textwrap.dedent(""" BaseShaperUdtf.run()""")
+        final_code = base_code + "\n" + textwrap.indent(fun_code, '     ')
+        final_code = final_code + "\n" + run_string
+
+
+
+
+
+        return inspect.cleandoc(final_code)
+
+
+
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/client code examples/customer_churn_prediction_nps.py` & `nzpyida-0.3.3/nzpyida/ae/client code examples/customer_churn_prediction_nps.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,141 +1,141 @@
-from nzpyida import IdaDataBase, IdaDataFrame
-
-
-from nzpyida.ae import NZFunGroupedApply
-
-
-#
-#nzpy dsn
-dsn ={
-    "database":"customer_churn",
-     "port" :5480,
-     "host" : "xxxx",
-     "securityLevel":0,
-     "logLevel":3
-
-
-}
-
-
-
-idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True )
-
-
-print(idadb)
-
-idadf = IdaDataFrame(idadb, 'customer_churn')
-
-
-print(idadf.dtypes)
-code_str_host_spus = """def log_reg_ml(self, df):
-            from sklearn.model_selection import cross_val_score
-            from sklearn.impute import SimpleImputer
-            from sklearn.linear_model import LogisticRegression
-            from sklearn.model_selection import train_test_split
-
-            from sklearn.preprocessing import LabelEncoder
-            import numpy as np
-            
-            
-            
-            
-
-            # data preparation
-            imputed_df = df.copy()
-            ds_size = len(imputed_df)
-            temp_dict = dict()
-
-
-            columns = imputed_df.columns
-
-            for column in columns:
-                if column=='ID':
-                    continue
-
-                if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-                  if imputed_df[column].isnull().sum()==len(imputed_df):
-                     imputed_df[column] = imputed_df[column].fillna(0)
-
-                  else :
-
-                     imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-                     transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))         
-                     imputed_df[column] = transformed_column
-
-                if (imputed_df[column].dtype == 'object'):
-                    # impute missing values for categorical variables
-                    imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                    imputed_df[column] = imputed_df[column].astype('str')
-                    le = LabelEncoder()
-
-                    le.fit(imputed_df[column])
-                    # print(le.classes_)
-                    imputed_df[column] = le.transform(imputed_df[column])
-                    temp_dict[column] = le
-
-
-
-            # Create a decision tree
-            lr = LogisticRegression()
-            X = imputed_df.drop(['EXITED'], axis=1)
-            y = imputed_df['EXITED']
-            X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
-
-
-            lr.fit(X_train, y_train)
-
-            accuracy = lr.score(X_test, y_test)    
-            #print(accuracy)
-
-
-
-            pred_df = X_test.copy()
-
-
-            y_pred= (lr.predict_proba(X_test)[:,1]>=0.2)
-
-            pred_df['EXITED'] = y_pred
-            pred_df['DATASET_SIZE'] = ds_size
-            pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
-            
-
-
-
-
-            original_columns = pred_df.columns
-
-            for column in original_columns:
-
-             if column in temp_dict:   
-               pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
-               #print(pred_df)
-
-            def print_output(x):
-                row = [x['CUSTOMERID'],x['EXITED'], x['DATASET_SIZE']]
-                self.output(row)
-
-
-            pred_df.apply(print_output, axis=1)
-
-
-
-
-
-
-"""
-
-output_signature = {'CUSTOMERID':'float', 'EXITED_PRED': 'int',  'DATASET_SIZE': 'int'}
-import time
-
-start = time.time()
-
-nz_groupapply = NZFunGroupedApply(df=idadf, code_str=code_str_host_spus, index='GEOGRAPHY', fun_name="log_reg_ml",
-                                  output_signature=output_signature, merge_output_with_df=True, id='CUSTOMERID')
-# nz_groupapply = NZFunGroupedApply(df=idadf,  code_str=code_str_host_spus, index='LOCATION', fun_name="decision_tree_ml")
-result = nz_groupapply.get_result()
-result = result.as_dataframe()
-print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
-print(result)
-end = time.time()
+from nzpyida import IdaDataBase, IdaDataFrame
+
+
+from nzpyida.ae import NZFunGroupedApply
+
+
+#
+#nzpy dsn
+dsn ={
+    "database":"customer_churn",
+     "port" :5480,
+     "host" : "xxxx",
+     "securityLevel":0,
+     "logLevel":3
+
+
+}
+
+
+
+idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True )
+
+
+print(idadb)
+
+idadf = IdaDataFrame(idadb, 'customer_churn')
+
+
+print(idadf.dtypes)
+code_str_host_spus = """def log_reg_ml(self, df):
+            from sklearn.model_selection import cross_val_score
+            from sklearn.impute import SimpleImputer
+            from sklearn.linear_model import LogisticRegression
+            from sklearn.model_selection import train_test_split
+
+            from sklearn.preprocessing import LabelEncoder
+            import numpy as np
+            
+            
+            
+            
+
+            # data preparation
+            imputed_df = df.copy()
+            ds_size = len(imputed_df)
+            temp_dict = dict()
+
+
+            columns = imputed_df.columns
+
+            for column in columns:
+                if column=='ID':
+                    continue
+
+                if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+                  if imputed_df[column].isnull().sum()==len(imputed_df):
+                     imputed_df[column] = imputed_df[column].fillna(0)
+
+                  else :
+
+                     imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+                     transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))         
+                     imputed_df[column] = transformed_column
+
+                if (imputed_df[column].dtype == 'object'):
+                    # impute missing values for categorical variables
+                    imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                    imputed_df[column] = imputed_df[column].astype('str')
+                    le = LabelEncoder()
+
+                    le.fit(imputed_df[column])
+                    # print(le.classes_)
+                    imputed_df[column] = le.transform(imputed_df[column])
+                    temp_dict[column] = le
+
+
+
+            # Create a decision tree
+            lr = LogisticRegression()
+            X = imputed_df.drop(['EXITED'], axis=1)
+            y = imputed_df['EXITED']
+            X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
+
+
+            lr.fit(X_train, y_train)
+
+            accuracy = lr.score(X_test, y_test)    
+            #print(accuracy)
+
+
+
+            pred_df = X_test.copy()
+
+
+            y_pred= (lr.predict_proba(X_test)[:,1]>=0.2)
+
+            pred_df['EXITED'] = y_pred
+            pred_df['DATASET_SIZE'] = ds_size
+            pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
+            
+
+
+
+
+            original_columns = pred_df.columns
+
+            for column in original_columns:
+
+             if column in temp_dict:   
+               pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
+               #print(pred_df)
+
+            def print_output(x):
+                row = [x['CUSTOMERID'],x['EXITED'], x['DATASET_SIZE']]
+                self.output(row)
+
+
+            pred_df.apply(print_output, axis=1)
+
+
+
+
+
+
+"""
+
+output_signature = {'CUSTOMERID':'float', 'EXITED_PRED': 'int',  'DATASET_SIZE': 'int'}
+import time
+
+start = time.time()
+
+nz_groupapply = NZFunGroupedApply(df=idadf, code_str=code_str_host_spus, index='GEOGRAPHY', fun_name="log_reg_ml",
+                                  output_signature=output_signature, merge_output_with_df=True, id='CUSTOMERID')
+# nz_groupapply = NZFunGroupedApply(df=idadf,  code_str=code_str_host_spus, index='LOCATION', fun_name="decision_tree_ml")
+result = nz_groupapply.get_result()
+result = result.as_dataframe()
+print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
+print(result)
+end = time.time()
 print(end - start)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/client code examples/host_spus_compare_inza_kdd_measure_accuracy.py` & `nzpyida-0.3.3/nzpyida/ae/client code examples/host_spus_compare_inza_kdd_measure_accuracy.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,157 +1,157 @@
-from nzpyida import IdaDataBase, IdaDataFrame
-
-from nzpyida.ae import NZFunTApply, NZClassTApply
-from nzpyida.ae import NZFunGroupedApply
-
-idadb = IdaDataBase('kddcup99', 'admin', 'password')
-print(idadb)
-
-idadf = IdaDataFrame(idadb, 'KDDCUP99')
-print(idadf.head())
-
-
-
-def decision_tree_ml_host(self, df):
-
-    from sklearn.model_selection import cross_val_score
-    from sklearn.impute import SimpleImputer
-    from sklearn.tree import DecisionTreeClassifier
-
-    from sklearn.preprocessing import LabelEncoder
-    import numpy as np
-
-
-    result = df.groupby('PROTOCOL_TYPE')
-
-
-    for name, group in result:
-        # print(name)
-
-        # print(group)
-        def decision_tree_classifier(df):
-            imputed_df = df.copy()
-
-
-
-            # remove columns which have only null values
-            columns = imputed_df.columns
-            for column in columns:
-                if imputed_df[column].isnull().sum() == len(imputed_df):
-                    imputed_df = imputed_df.drop(column, 1)
-
-            columns = imputed_df.columns
-
-            for column in columns:
-
-                if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-                    imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-
-                if (imputed_df[column].dtype == 'object'):
-                    # impute missing values for categorical variables
-                    imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                    imputed_df[column] = imputed_df[column].astype('str')
-                    le = LabelEncoder()
-                    # print(imputed_df[column].unique())
-
-                    le.fit(imputed_df[column].unique())
-                    # print(le.classes_)
-                    imputed_df[column] = le.transform(imputed_df[column])
-
-            X = imputed_df.drop('ATTACK_TYPE', axis=1)
-            y = imputed_df['ATTACK_TYPE']
-
-            # Create a decision tree
-            dt = DecisionTreeClassifier(max_depth=5)
-
-            cvscores_3 = cross_val_score(dt, X, y, cv=3)
-
-
-
-            self.output([len(imputed_df), name, np.mean(cvscores_3)])
-
-        ml_result = decision_tree_classifier(df=group)
-
-
-
-
-output_signature = {'dataset_size':'int', 'protocol_type':'str', 'classifier_accuracy':'float'}
-
-import time
-start = time.time()
-nz_tapply = NZFunTApply(df=idadf, fun_ref =decision_tree_ml_host,  parallel=False,  output_signature=output_signature)
-result = nz_tapply.get_result()
-result = result.as_dataframe()
-print("Host only execution")
-print(result)
-print("\n")
-end = time.time()
-print(end - start)
-
-def decision_tree_ml(self, df):
-    from sklearn.model_selection import cross_val_score
-    from sklearn.impute import SimpleImputer
-    from sklearn.tree import DecisionTreeClassifier
-
-    from sklearn.preprocessing import LabelEncoder
-    import numpy as np
-
-    location = df.PROTOCOL_TYPE[0]
-
-    # data preparation
-    imputed_df = df.copy()
-    ds_size = len(imputed_df)
-
-
-
-    #remove columns which have only null values
-    columns = imputed_df.columns
-    for column in columns:
-        if imputed_df[column].isnull().sum()==len(imputed_df):
-            imputed_df=imputed_df.drop(column, 1)
-
-    columns = imputed_df.columns
-
-    for column in columns:
-
-        if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-            imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-            imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-
-        if (imputed_df[column].dtype == 'object'):
-            # impute missing values for categorical variables
-            imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-            imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-            imputed_df[column] = imputed_df[column].astype('str')
-            le = LabelEncoder()
-            #print(imputed_df[column].unique())
-
-            le.fit(imputed_df[column].unique())
-            # print(le.classes_)
-            imputed_df[column] = le.transform(imputed_df[column])
-
-
-
-    X = imputed_df.drop('ATTACK_TYPE', axis=1)
-    y = imputed_df['ATTACK_TYPE']
-
-    # Create a decision tree
-    dt = DecisionTreeClassifier(max_depth=5)
-
-    cvscores_3 = cross_val_score(dt, X, y, cv=3)
-
-    self.output(ds_size, location, np.mean(cvscores_3))
-
-
-
-
-import time
-start = time.time()
-nz_groupapply = NZFunGroupedApply(df=idadf, index='PROTOCOL_TYPE', fun_ref=decision_tree_ml,  output_signature=output_signature)
-result = nz_groupapply.get_result()
-result = result.as_dataframe()
-print("Parallel Execution on SPUs: \n")
-print(result)
-end = time.time()
+from nzpyida import IdaDataBase, IdaDataFrame
+
+from nzpyida.ae import NZFunTApply, NZClassTApply
+from nzpyida.ae import NZFunGroupedApply
+
+idadb = IdaDataBase('kddcup99', 'admin', 'password')
+print(idadb)
+
+idadf = IdaDataFrame(idadb, 'KDDCUP99')
+print(idadf.head())
+
+
+
+def decision_tree_ml_host(self, df):
+
+    from sklearn.model_selection import cross_val_score
+    from sklearn.impute import SimpleImputer
+    from sklearn.tree import DecisionTreeClassifier
+
+    from sklearn.preprocessing import LabelEncoder
+    import numpy as np
+
+
+    result = df.groupby('PROTOCOL_TYPE')
+
+
+    for name, group in result:
+        # print(name)
+
+        # print(group)
+        def decision_tree_classifier(df):
+            imputed_df = df.copy()
+
+
+
+            # remove columns which have only null values
+            columns = imputed_df.columns
+            for column in columns:
+                if imputed_df[column].isnull().sum() == len(imputed_df):
+                    imputed_df = imputed_df.drop(column, 1)
+
+            columns = imputed_df.columns
+
+            for column in columns:
+
+                if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+                    imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+
+                if (imputed_df[column].dtype == 'object'):
+                    # impute missing values for categorical variables
+                    imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                    imputed_df[column] = imputed_df[column].astype('str')
+                    le = LabelEncoder()
+                    # print(imputed_df[column].unique())
+
+                    le.fit(imputed_df[column].unique())
+                    # print(le.classes_)
+                    imputed_df[column] = le.transform(imputed_df[column])
+
+            X = imputed_df.drop('ATTACK_TYPE', axis=1)
+            y = imputed_df['ATTACK_TYPE']
+
+            # Create a decision tree
+            dt = DecisionTreeClassifier(max_depth=5)
+
+            cvscores_3 = cross_val_score(dt, X, y, cv=3)
+
+
+
+            self.output([len(imputed_df), name, np.mean(cvscores_3)])
+
+        ml_result = decision_tree_classifier(df=group)
+
+
+
+
+output_signature = {'dataset_size':'int', 'protocol_type':'str', 'classifier_accuracy':'float'}
+
+import time
+start = time.time()
+nz_tapply = NZFunTApply(df=idadf, fun_ref =decision_tree_ml_host,  parallel=False,  output_signature=output_signature)
+result = nz_tapply.get_result()
+result = result.as_dataframe()
+print("Host only execution")
+print(result)
+print("\n")
+end = time.time()
+print(end - start)
+
+def decision_tree_ml(self, df):
+    from sklearn.model_selection import cross_val_score
+    from sklearn.impute import SimpleImputer
+    from sklearn.tree import DecisionTreeClassifier
+
+    from sklearn.preprocessing import LabelEncoder
+    import numpy as np
+
+    location = df.PROTOCOL_TYPE[0]
+
+    # data preparation
+    imputed_df = df.copy()
+    ds_size = len(imputed_df)
+
+
+
+    #remove columns which have only null values
+    columns = imputed_df.columns
+    for column in columns:
+        if imputed_df[column].isnull().sum()==len(imputed_df):
+            imputed_df=imputed_df.drop(column, 1)
+
+    columns = imputed_df.columns
+
+    for column in columns:
+
+        if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+            imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+            imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+
+        if (imputed_df[column].dtype == 'object'):
+            # impute missing values for categorical variables
+            imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+            imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+            imputed_df[column] = imputed_df[column].astype('str')
+            le = LabelEncoder()
+            #print(imputed_df[column].unique())
+
+            le.fit(imputed_df[column].unique())
+            # print(le.classes_)
+            imputed_df[column] = le.transform(imputed_df[column])
+
+
+
+    X = imputed_df.drop('ATTACK_TYPE', axis=1)
+    y = imputed_df['ATTACK_TYPE']
+
+    # Create a decision tree
+    dt = DecisionTreeClassifier(max_depth=5)
+
+    cvscores_3 = cross_val_score(dt, X, y, cv=3)
+
+    self.output(ds_size, location, np.mean(cvscores_3))
+
+
+
+
+import time
+start = time.time()
+nz_groupapply = NZFunGroupedApply(df=idadf, index='PROTOCOL_TYPE', fun_ref=decision_tree_ml,  output_signature=output_signature)
+result = nz_groupapply.get_result()
+result = result.as_dataframe()
+print("Parallel Execution on SPUs: \n")
+print(result)
+end = time.time()
 print(end - start)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/client code examples/host_spus_compare_inza_weather_train_pred.py` & `nzpyida-0.3.3/nzpyida/ae/client code examples/host_spus_compare_inza_weather_train_pred.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,340 +1,340 @@
-
-
-from nzpyida import IdaDataBase, IdaDataFrame
-
-from nzpyida.ae import NZFunTApply
-from nzpyida.ae import NZFunApply
-from nzpyida.ae import NZFunGroupedApply
-
-
-#jdbc dsn
-#dsn = "jdbc:netezza://xxxx:5480/weather"
-
-
-
-
-
-#nzpy dsn
-dsn ={
-    "database":"weather",
-     "port" :5480,
-     "host" : "xxxx",
-     "securityLevel":0,
-     "logLevel":0
-
-
-}
-
-#odbc dsn
-dsn='fyre'
-idadb = IdaDataBase(dsn, 'admin', 'password',verbose=True)
-
-
-
-#query = "select * from weather_new limit 1000"
-print(idadb)
-
-idadf = IdaDataFrame(idadb, 'WEATHER')
-print(idadf.dtypes)
-
-#query = 'select * from weather limit 10000'
-
-
-
-
-#df = idadf.ida_query(query)
-
-
-code_str_host = """def decision_tree_ml_host(self, df):
-
-    from sklearn.model_selection import cross_val_score
-    from sklearn.impute import SimpleImputer
-    from sklearn.tree import DecisionTreeClassifier
-    from sklearn.model_selection import train_test_split
-    import datetime
-
-    from sklearn.preprocessing import LabelEncoder
-    import numpy as np
-
-    result = df.groupby('LOCATION')
-    #result = df.groupby(pd.qcut(df['ID'], q=3))
-
-    # result = idadf.ida_query(query, autocommit=True)
-
-    for name, group in result:
-        # print(name)
-
-        # print(group)
-        def decision_tree_classifier(df):
-            imputed_df = df.copy()
-            ds_size = len(imputed_df)
-            temp_dict = dict()
-
-
-
-
-
-            columns = imputed_df.columns
-
-            for column in columns:
-                if column=='ID':
-                    continue
-
-                if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-                    if imputed_df[column].isnull().sum()==len(imputed_df):
-                     imputed_df[column] = imputed_df[column].fillna(0)
-
-                    else :
-                     imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-                     transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                     imputed_df[column] = transformed_column
-
-
-                if (imputed_df[column].dtype == 'object'):
-                    # impute missing values for categorical variables
-                    imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                    imputed_df[column] = imputed_df[column].astype(str)
-                    le = LabelEncoder()
-                    # print(imputed_df[column].unique())
-
-                    le.fit(imputed_df[column])
-                    # print(le.classes_)
-                    imputed_df[column] = le.transform(imputed_df[column])
-                    temp_dict[column] = le
-
-            X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
-            y = imputed_df['RAINTOMORROW']
-            X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
-            #X_train_mod = X_train.drop(['RISK_MM'],axis=1)
-            #X_test_mod = X_test.drop(['RISK_MM'],axis=1)
-            dt = DecisionTreeClassifier(max_depth=5)
-            dt.fit(X_train, y_train)
-
-            accuracy = dt.score(X_test, y_test)    
-            #print(accuracy)
-
-
-
-            pred_df = X_test.copy()
-
-
-            y_pred= dt.predict(X_test)
-            
-
-            pred_df['RAINTOMORROW'] = y_pred
-            
-            pred_df['DATASET_SIZE'] = ds_size
-            pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
-            #test_date = pd.to_datetime(pred_df['DATE'])
-            #pred_df['TEST_DATE'] = pd.to_datetime('2020-10-01') 
-            #pred_df['TEST_DATE']= pred_df['TEST_DATE'].dt.date
-            pred_df['TEST_DATE'] =10
-            
-            
-           
-            
-            
-            
-
-
-
-
-            original_columns = pred_df.columns
-
-            for column in original_columns:
-
-             if column in temp_dict:   
-               pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
-               #print(pred_df)
-
-            def print_output(x):
-                row = [x['ID'], x['TEST_DATE'],  x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
-                
-                self.output(row)
-
-
-            pred_df.apply(print_output, axis=1)
-                      
-            return pred_df
-            
- 
-
-        ml_result = decision_tree_classifier(df=group)
-
-        """
-
-
-
-
-
-output_signature = {'ID':'int', 'TEST_DATE':'date',  'RAINTOMORROW_PRED' :'bool',  'DATASET_SIZE':'int', 'CLASSIFIER_ACCURACY':'float'}
-
-
-import time
-start = time.time()
-
-nz_tapply = NZFunTApply(df=idadf, code_str=code_str_host, fun_name='decision_tree_ml_host', parallel=False,  output_signature=output_signature, merge_output_with_df=True)
-result_idadf = nz_tapply.get_result()
-result_df = result_idadf.as_dataframe()
-idadb.drop_table(result_idadf.tablename)
-print("\n")
-#pd.set_option('display.max_columns', None)
-#pd.set_option('display.width', None)
-#pd.set_option('display.max_rows', None)
-#pd.set_option('display.max_colwidth', -1)
-print(result_df)
-end = time.time()
-print(end - start)
-
-groups = result_df.groupby("LOCATION")
-for name, group in groups:
-    print(name + ":" + str(len(group)))
-
-
-code_str_apply="""
-def apply_fun(self, x):
-    from math import sqrt
-    max_temp = x[1]
-    id = x[0]
-    fahren_max_temp = (max_temp*1.8)+32
-    row = [id, max_temp,  fahren_max_temp]
-    self.output(row)"""
-output_signature = {'ID':'int', 'RESULT_MAX_TEMP' :'float', 'RESULT_FAHREN_MAX_TEMP' : 'float'}
-nz_apply = NZFunApply(df=idadf, code_str= code_str_apply, fun_name="apply_fun", columns=['ID', 'MAXTEMP'], output_signature=output_signature, merge_output_with_df=True)
-result = nz_apply.get_result()
-result=result.as_dataframe()
-print(result)
-
-
-
-end = time.time()
-print(end - start)
-
-
-code_str_host_spus="""def decision_tree_ml(self, df):
-            from sklearn.model_selection import cross_val_score
-            from sklearn.impute import SimpleImputer
-            from sklearn.tree import DecisionTreeClassifier
-            from sklearn.model_selection import train_test_split
-
-            from sklearn.preprocessing import LabelEncoder
-            import numpy as np
-
-    
-
-            # data preparation
-            imputed_df = df.copy()
-            ds_size = len(imputed_df)
-            temp_dict = dict()
-      
-
-            columns = imputed_df.columns
-
-            for column in columns:
-                if column=='ID':
-                    continue
-
-                if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-                  if imputed_df[column].isnull().sum()==len(imputed_df):
-                     imputed_df[column] = imputed_df[column].fillna(0)
-                      
-                  else :
-           
-                     imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-                     transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))         
-                     imputed_df[column] = transformed_column
-                   
-                if (imputed_df[column].dtype == 'object'):
-                    # impute missing values for categorical variables
-                    imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                    imputed_df[column] = imputed_df[column].astype('str')
-                    le = LabelEncoder()
-                  
-                    le.fit(imputed_df[column])
-                    # print(le.classes_)
-                    imputed_df[column] = le.transform(imputed_df[column])
-                    temp_dict[column] = le
-            
-            
-            
-            # Create a decision tree
-            dt = DecisionTreeClassifier(max_depth=5)
-            X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
-            y = imputed_df['RAINTOMORROW']
-            X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
-           
-           
-            dt.fit(X_train, y_train)
-  
-            accuracy = dt.score(X_test, y_test)    
-            #print(accuracy)
-    
- 
-   
-            pred_df = X_test.copy()
-            
-    
-            y_pred= dt.predict(X_test)
-    
-            pred_df['RAINTOMORROW'] = y_pred
-            pred_df['DATASET_SIZE'] = ds_size
-            pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
-   
-            
-  
-
-            original_columns = pred_df.columns
-  
-            for column in original_columns:
-        
-             if column in temp_dict:   
-               pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
-               #print(pred_df)
-    
-            def print_output(x):
-                row = [x['ID'], x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
-                self.output(row)
-            
-                
-            pred_df.apply(print_output, axis=1)
-          
-            
-
-    
-
-       
-"""
-
-
-output_signature = {'ID':'int', 'RAINTOMORROW_PRED' :'str',  'DATASET_SIZE':'int', 'CLASSIFIER_ACCURACY':'float'}
-import time
-start = time.time()
-
-nz_groupapply = NZFunGroupedApply(df=idadf,  code_str=code_str_host_spus, index='LOCATION', fun_name="decision_tree_ml", output_signature=output_signature, merge_output_with_df=True)
-#nz_groupapply = NZFunGroupedApply(df=idadf,  code_str=code_str_host_spus, index='LOCATION', fun_name="decision_tree_ml")
-result = nz_groupapply.get_result()
-
-print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
-print(result)
-groups = result.as_dataframe().groupby("LOCATION")
-for name, group in groups:
-    print (name +":"+str(len(group)))
-end = time.time()
-print(end - start)
-
-
-
-import time
-start = time.time()
-output_signature = {'ID':'int', 'RAINTOMORROW_PRED' :'bool',  'DATASET_SIZE':'int', 'CLASSIFIER_ACCURACY':'float'}
-nz_groupapply = NZFunTApply(df=idadf, code_str=code_str_host_spus, fun_name ="decision_tree_ml", parallel=True, output_signature=output_signature)
-result = nz_groupapply.get_result()
-result = result.as_dataframe()
-print("Host +SPUs execution - slicing on a default column- ML function for the entire slices")
-print(result)
-print("\n")
-
-end = time.time()
+
+
+from nzpyida import IdaDataBase, IdaDataFrame
+
+from nzpyida.ae import NZFunTApply
+from nzpyida.ae import NZFunApply
+from nzpyida.ae import NZFunGroupedApply
+
+
+#jdbc dsn
+#dsn = "jdbc:netezza://xxxx:5480/weather"
+
+
+
+
+
+#nzpy dsn
+dsn ={
+    "database":"weather",
+     "port" :5480,
+     "host" : "xxxx",
+     "securityLevel":0,
+     "logLevel":0
+
+
+}
+
+#odbc dsn
+dsn='fyre'
+idadb = IdaDataBase(dsn, 'admin', 'password',verbose=True)
+
+
+
+#query = "select * from weather_new limit 1000"
+print(idadb)
+
+idadf = IdaDataFrame(idadb, 'WEATHER')
+print(idadf.dtypes)
+
+#query = 'select * from weather limit 10000'
+
+
+
+
+#df = idadf.ida_query(query)
+
+
+code_str_host = """def decision_tree_ml_host(self, df):
+
+    from sklearn.model_selection import cross_val_score
+    from sklearn.impute import SimpleImputer
+    from sklearn.tree import DecisionTreeClassifier
+    from sklearn.model_selection import train_test_split
+    import datetime
+
+    from sklearn.preprocessing import LabelEncoder
+    import numpy as np
+
+    result = df.groupby('LOCATION')
+    #result = df.groupby(pd.qcut(df['ID'], q=3))
+
+    # result = idadf.ida_query(query, autocommit=True)
+
+    for name, group in result:
+        # print(name)
+
+        # print(group)
+        def decision_tree_classifier(df):
+            imputed_df = df.copy()
+            ds_size = len(imputed_df)
+            temp_dict = dict()
+
+
+
+
+
+            columns = imputed_df.columns
+
+            for column in columns:
+                if column=='ID':
+                    continue
+
+                if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+                    if imputed_df[column].isnull().sum()==len(imputed_df):
+                     imputed_df[column] = imputed_df[column].fillna(0)
+
+                    else :
+                     imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+                     transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                     imputed_df[column] = transformed_column
+
+
+                if (imputed_df[column].dtype == 'object'):
+                    # impute missing values for categorical variables
+                    imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                    imputed_df[column] = imputed_df[column].astype(str)
+                    le = LabelEncoder()
+                    # print(imputed_df[column].unique())
+
+                    le.fit(imputed_df[column])
+                    # print(le.classes_)
+                    imputed_df[column] = le.transform(imputed_df[column])
+                    temp_dict[column] = le
+
+            X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
+            y = imputed_df['RAINTOMORROW']
+            X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
+            #X_train_mod = X_train.drop(['RISK_MM'],axis=1)
+            #X_test_mod = X_test.drop(['RISK_MM'],axis=1)
+            dt = DecisionTreeClassifier(max_depth=5)
+            dt.fit(X_train, y_train)
+
+            accuracy = dt.score(X_test, y_test)    
+            #print(accuracy)
+
+
+
+            pred_df = X_test.copy()
+
+
+            y_pred= dt.predict(X_test)
+            
+
+            pred_df['RAINTOMORROW'] = y_pred
+            
+            pred_df['DATASET_SIZE'] = ds_size
+            pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
+            #test_date = pd.to_datetime(pred_df['DATE'])
+            #pred_df['TEST_DATE'] = pd.to_datetime('2020-10-01') 
+            #pred_df['TEST_DATE']= pred_df['TEST_DATE'].dt.date
+            pred_df['TEST_DATE'] =10
+            
+            
+           
+            
+            
+            
+
+
+
+
+            original_columns = pred_df.columns
+
+            for column in original_columns:
+
+             if column in temp_dict:   
+               pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
+               #print(pred_df)
+
+            def print_output(x):
+                row = [x['ID'], x['TEST_DATE'],  x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
+                
+                self.output(row)
+
+
+            pred_df.apply(print_output, axis=1)
+                      
+            return pred_df
+            
+ 
+
+        ml_result = decision_tree_classifier(df=group)
+
+        """
+
+
+
+
+
+output_signature = {'ID':'int', 'TEST_DATE':'date',  'RAINTOMORROW_PRED' :'bool',  'DATASET_SIZE':'int', 'CLASSIFIER_ACCURACY':'float'}
+
+
+import time
+start = time.time()
+
+nz_tapply = NZFunTApply(df=idadf, code_str=code_str_host, fun_name='decision_tree_ml_host', parallel=False,  output_signature=output_signature, merge_output_with_df=True)
+result_idadf = nz_tapply.get_result()
+result_df = result_idadf.as_dataframe()
+idadb.drop_table(result_idadf.tablename)
+print("\n")
+#pd.set_option('display.max_columns', None)
+#pd.set_option('display.width', None)
+#pd.set_option('display.max_rows', None)
+#pd.set_option('display.max_colwidth', -1)
+print(result_df)
+end = time.time()
+print(end - start)
+
+groups = result_df.groupby("LOCATION")
+for name, group in groups:
+    print(name + ":" + str(len(group)))
+
+
+code_str_apply="""
+def apply_fun(self, x):
+    from math import sqrt
+    max_temp = x[1]
+    id = x[0]
+    fahren_max_temp = (max_temp*1.8)+32
+    row = [id, max_temp,  fahren_max_temp]
+    self.output(row)"""
+output_signature = {'ID':'int', 'RESULT_MAX_TEMP' :'float', 'RESULT_FAHREN_MAX_TEMP' : 'float'}
+nz_apply = NZFunApply(df=idadf, code_str= code_str_apply, fun_name="apply_fun", columns=['ID', 'MAXTEMP'], output_signature=output_signature, merge_output_with_df=True)
+result = nz_apply.get_result()
+result=result.as_dataframe()
+print(result)
+
+
+
+end = time.time()
+print(end - start)
+
+
+code_str_host_spus="""def decision_tree_ml(self, df):
+            from sklearn.model_selection import cross_val_score
+            from sklearn.impute import SimpleImputer
+            from sklearn.tree import DecisionTreeClassifier
+            from sklearn.model_selection import train_test_split
+
+            from sklearn.preprocessing import LabelEncoder
+            import numpy as np
+
+    
+
+            # data preparation
+            imputed_df = df.copy()
+            ds_size = len(imputed_df)
+            temp_dict = dict()
+      
+
+            columns = imputed_df.columns
+
+            for column in columns:
+                if column=='ID':
+                    continue
+
+                if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+                  if imputed_df[column].isnull().sum()==len(imputed_df):
+                     imputed_df[column] = imputed_df[column].fillna(0)
+                      
+                  else :
+           
+                     imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+                     transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))         
+                     imputed_df[column] = transformed_column
+                   
+                if (imputed_df[column].dtype == 'object'):
+                    # impute missing values for categorical variables
+                    imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+                    imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                    imputed_df[column] = imputed_df[column].astype('str')
+                    le = LabelEncoder()
+                  
+                    le.fit(imputed_df[column])
+                    # print(le.classes_)
+                    imputed_df[column] = le.transform(imputed_df[column])
+                    temp_dict[column] = le
+            
+            
+            
+            # Create a decision tree
+            dt = DecisionTreeClassifier(max_depth=5)
+            X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
+            y = imputed_df['RAINTOMORROW']
+            X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
+           
+           
+            dt.fit(X_train, y_train)
+  
+            accuracy = dt.score(X_test, y_test)    
+            #print(accuracy)
+    
+ 
+   
+            pred_df = X_test.copy()
+            
+    
+            y_pred= dt.predict(X_test)
+    
+            pred_df['RAINTOMORROW'] = y_pred
+            pred_df['DATASET_SIZE'] = ds_size
+            pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
+   
+            
+  
+
+            original_columns = pred_df.columns
+  
+            for column in original_columns:
+        
+             if column in temp_dict:   
+               pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
+               #print(pred_df)
+    
+            def print_output(x):
+                row = [x['ID'], x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
+                self.output(row)
+            
+                
+            pred_df.apply(print_output, axis=1)
+          
+            
+
+    
+
+       
+"""
+
+
+output_signature = {'ID':'int', 'RAINTOMORROW_PRED' :'str',  'DATASET_SIZE':'int', 'CLASSIFIER_ACCURACY':'float'}
+import time
+start = time.time()
+
+nz_groupapply = NZFunGroupedApply(df=idadf,  code_str=code_str_host_spus, index='LOCATION', fun_name="decision_tree_ml", output_signature=output_signature, merge_output_with_df=True)
+#nz_groupapply = NZFunGroupedApply(df=idadf,  code_str=code_str_host_spus, index='LOCATION', fun_name="decision_tree_ml")
+result = nz_groupapply.get_result()
+
+print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
+print(result)
+groups = result.as_dataframe().groupby("LOCATION")
+for name, group in groups:
+    print (name +":"+str(len(group)))
+end = time.time()
+print(end - start)
+
+
+
+import time
+start = time.time()
+output_signature = {'ID':'int', 'RAINTOMORROW_PRED' :'bool',  'DATASET_SIZE':'int', 'CLASSIFIER_ACCURACY':'float'}
+nz_groupapply = NZFunTApply(df=idadf, code_str=code_str_host_spus, fun_name ="decision_tree_ml", parallel=True, output_signature=output_signature)
+result = nz_groupapply.get_result()
+result = result.as_dataframe()
+print("Host +SPUs execution - slicing on a default column- ML function for the entire slices")
+print(result)
+print("\n")
+
+end = time.time()
 print(end - start)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/client code examples/house_pricing.py` & `nzpyida-0.3.3/nzpyida/ae/client code examples/house_pricing.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,95 +1,95 @@
-from nzpyida.ae.install import NZInstall
-nzpy_dsn = {
-        "database":"housing",
-         "port" :5480,
-        "host": "9.30.250.118",
-        "securityLevel":3,
-        "logLevel":0
-       }
-import pandas as pd
-from nzpyida import IdaDataBase, IdaDataFrame
-from nzpyida.ae import NZFunTApply
-idadb = IdaDataBase(nzpy_dsn, uid="admin", pwd="password", verbose=True)
-
-#idadb.ida_query("create view houseprice_temp as select * from houseprice")
-
-#idadb.ida_query("alter table houseprice add column date_str varchar(1000)")
-#idadb.ida_query("update  houseprice set date_str = cast(ID as varchar(1000))")
-#idadf = IdaDataFrame(idadb,"HOUSEPRICE")
-idadf = IdaDataFrame(idadb,"HOUSEPRICE_TEMP")
-
-pd.set_option('display.max_rows', None)
-pd.set_option('display.max_columns', None)
-print(idadf.head())
-print(idadf.dtypes)
-#package_name = 'sklearn'
-#nzinstall = NZInstall(idadb, package_name)
-#result = nzinstall.getResultCode()
-#print(result)
-code_house_prediction_model = """def house_prediction_model(self,df):
-            import numpy as np
-            import pandas as pd
-            import nzaeCppWrapper
-
-            import sklearn
-
-            from sklearn.model_selection import cross_val_score
-            from sklearn.model_selection import train_test_split
-            from sklearn.linear_model import LinearRegression
-            # from sklearn.ensemble import GradientBoostingRegression
-            # from xgboost import XGBRegressor
-            from sklearn.metrics import r2_score,mean_squared_error
-            from datetime import datetime
-
-            # Getting the current date and time
-            #dt = datetime.now()
-
-            # getting the timestamp
-            #ts = datetime.timestamp(dt)
-
-
-
-            data = df.copy()
-
-            #id1 = data['ID'][0]
-            #id = id1/100000
-            #data['test_field'] = str(ts)
-            
-           
-
-            
-
-
-            
-            
-
-            
-
-
-            
-
-            
-
-
-           
-
-            self.output(id,0.85, 'hello')
-
-
-
-
-
-
-
-
-
-
-
-"""
-
-output_signature = {'ID': 'float', 'accuracy': 'double', 'date':'str'}
-
-nz_fun_tapply = NZFunTApply(df=idadf, code_str=code_house_prediction_model, fun_name="house_prediction_model",
-                            parallel=False,  output_signature=output_signature)
-
+from nzpyida.ae.install import NZInstall
+nzpy_dsn = {
+        "database":"housing",
+         "port" :5480,
+        "host": "9.30.250.118",
+        "securityLevel":3,
+        "logLevel":0
+       }
+import pandas as pd
+from nzpyida import IdaDataBase, IdaDataFrame
+from nzpyida.ae import NZFunTApply
+idadb = IdaDataBase(nzpy_dsn, uid="admin", pwd="password", verbose=True)
+
+#idadb.ida_query("create view houseprice_temp as select * from houseprice")
+
+#idadb.ida_query("alter table houseprice add column date_str varchar(1000)")
+#idadb.ida_query("update  houseprice set date_str = cast(ID as varchar(1000))")
+#idadf = IdaDataFrame(idadb,"HOUSEPRICE")
+idadf = IdaDataFrame(idadb,"HOUSEPRICE_TEMP")
+
+pd.set_option('display.max_rows', None)
+pd.set_option('display.max_columns', None)
+print(idadf.head())
+print(idadf.dtypes)
+#package_name = 'sklearn'
+#nzinstall = NZInstall(idadb, package_name)
+#result = nzinstall.getResultCode()
+#print(result)
+code_house_prediction_model = """def house_prediction_model(self,df):
+            import numpy as np
+            import pandas as pd
+            import nzaeCppWrapper
+
+            import sklearn
+
+            from sklearn.model_selection import cross_val_score
+            from sklearn.model_selection import train_test_split
+            from sklearn.linear_model import LinearRegression
+            # from sklearn.ensemble import GradientBoostingRegression
+            # from xgboost import XGBRegressor
+            from sklearn.metrics import r2_score,mean_squared_error
+            from datetime import datetime
+
+            # Getting the current date and time
+            #dt = datetime.now()
+
+            # getting the timestamp
+            #ts = datetime.timestamp(dt)
+
+
+
+            data = df.copy()
+
+            #id1 = data['ID'][0]
+            #id = id1/100000
+            #data['test_field'] = str(ts)
+            
+           
+
+            
+
+
+            
+            
+
+            
+
+
+            
+
+            
+
+
+           
+
+            self.output(id,0.85, 'hello')
+
+
+
+
+
+
+
+
+
+
+
+"""
+
+output_signature = {'ID': 'float', 'accuracy': 'double', 'date':'str'}
+
+nz_fun_tapply = NZFunTApply(df=idadf, code_str=code_house_prediction_model, fun_name="house_prediction_model",
+                            parallel=False,  output_signature=output_signature)
+
 print(nz_fun_tapply.get_result())
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/client code examples/stock_prediction_nps side.py` & `nzpyida-0.3.3/nzpyida/ae/client code examples/stock_prediction_nps side.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,191 +1,191 @@
-
-from nzpyida import IdaDataBase, IdaDataFrame
-
-from nzpyida.ae import NZFunGroupedApply
-
-dsn ='bank'
-
-idadb = IdaDataBase(dsn, 'admin', 'password')
-print(idadb)
-
-
-idadf = IdaDataFrame(idadb, 'stocks')
-print(idadf.head())
-
-
-query = 'select * from stocks'
-
-df = idadf.ida_query(query)
-print(df.dtypes)
-
-
-code_str_host_spus = """def stocks_rf_ml(self, df):
-
-    import numpy as np
-
-    #how many days in future you need to predict
-    future_days = -1
-
-    from sklearn.impute import SimpleImputer
-    from sklearn.metrics import mean_squared_error
-    from sklearn.ensemble import RandomForestRegressor
-    from sklearn.model_selection import train_test_split
-
-
-    from sklearn.preprocessing import LabelEncoder
-
-    temp_dict = dict()
-    
-    
-    
-    df['DATE'] = pd.to_datetime(df['DATE'])
-    df = df.sort_values(by='DATE')
-    df['DATE']=df['DATE'].dt.date
-
-    # data preparation
-
-    imputed_df = df.copy()
-    
-    
-    # add the future close price column and shift by the required days
-
-    imputed_df['FUTURE_CLOSE_PRICE'] = imputed_df['ADJCLOSE'].shift(future_days)
-
-
-
-    # add technical indicators
-    for n in [14,30,50,200]:
-
-     # create the moving average indicator 
-     imputed_df['ma'+str(n)] = imputed_df['ADJCLOSE'].ewm(span=n,adjust=False).mean()
-
-
-
-    ds_size = len(imputed_df)
-    #imputed_df.dropna(inplace=True)
-
-
-    columns = imputed_df.columns
-    for column in columns:
-
-        if column=='ID':
-            continue
-
-        #impute missing values 
-        # mean for numerical and 'missing' for categorical
-        if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-
-
-            if imputed_df[column].isnull().sum()==len(imputed_df):
-
-                imputed_df[column] = imputed_df[column].fillna(0)
-
-
-            else :
-
-                imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-
-                transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-
-
-                imputed_df[column] = transformed_column
-
-
-        if (imputed_df[column].dtype == 'object'):
-            # impute missing values for categorical variables
-
-
-            imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-            imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-            imputed_df[column] = imputed_df[column].astype('str')
-
-            le = LabelEncoder()
-
-            le.fit(imputed_df[column])
-                    # print(le.classes_)
-            imputed_df[column] = le.transform(imputed_df[column])
-            temp_dict[column] = le
-
-
-    # Create a random forest regressor
-    rf = RandomForestRegressor(n_estimators=200)
-
-    X = imputed_df.drop(['FUTURE_CLOSE_PRICE'], axis=1)
-    y = imputed_df['FUTURE_CLOSE_PRICE']
-
-    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42)
-
-
-    rf.fit(X_train,y_train)
-
-
-
-
-    pred_df = X_test.copy()
-    pred_df['FUTURE_CLOSE_PRICE'] = y_test
-
-    y_pred = rf.predict(X_test)
-    pred_df['FUTURE_CLOSE_PRICE_PRED'] =y_pred
-
-
-
-    accuracy  = rf.score(X_test, y_test)    
-
-    rms = mean_squared_error(y_test, y_pred)
-
-
-    pred_df['DATASET_SIZE'] = ds_size
-    pred_df['accuracy']=round(accuracy,2)
-    pred_df['mean_square_error']=round(rms)
-
-
-    #for all the columns that had label encoders, do an inverse transform
-
-    original_columns = pred_df.columns
-
-    for column in original_columns:
-
-     if column in temp_dict:   
-      pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
-
-    #pred_df['DATE'] = pd.to_datetime(pred_df['DATE'])
-
-
-
-    #pred_df = pred_df.sort_values(by='DATE')
-    #pred_df['DATE']=pred_df['DATE'].dt.date
-
-    def print_output(x):
-                row = [x['ID'], x['FUTURE_CLOSE_PRICE'], x['FUTURE_CLOSE_PRICE_PRED'], x['DATASET_SIZE'], x['accuracy']]
-               
-                
-                self.output(row)
-
-
-    pred_df.apply(print_output, axis=1)
-
-
-
-"""
-
-output_signature= {'ID': 'float', 'FUTURE_CLOSE_PRICE': 'double', 'FUTURE_CLOSE_PRICE_PRED': 'double',
-                   'DATASET_SIZE': 'int', 'ACCURACY': 'float'}
-
-
-import time
-
-start = time.time()
-
-nz_groupapply = NZFunGroupedApply(df=idadf, code_str=code_str_host_spus, index='TICKER', fun_name="stocks_rf_ml",
-                                  output_signature=output_signature, merge_output_with_df=True)
-
-result = nz_groupapply.get_result()
-result = result.as_dataframe()
-print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
-print(result)
-end = time.time()
-print(end - start)
-
-
-
-
+
+from nzpyida import IdaDataBase, IdaDataFrame
+
+from nzpyida.ae import NZFunGroupedApply
+
+dsn ='bank'
+
+idadb = IdaDataBase(dsn, 'admin', 'password')
+print(idadb)
+
+
+idadf = IdaDataFrame(idadb, 'stocks')
+print(idadf.head())
+
+
+query = 'select * from stocks'
+
+df = idadf.ida_query(query)
+print(df.dtypes)
+
+
+code_str_host_spus = """def stocks_rf_ml(self, df):
+
+    import numpy as np
+
+    #how many days in future you need to predict
+    future_days = -1
+
+    from sklearn.impute import SimpleImputer
+    from sklearn.metrics import mean_squared_error
+    from sklearn.ensemble import RandomForestRegressor
+    from sklearn.model_selection import train_test_split
+
+
+    from sklearn.preprocessing import LabelEncoder
+
+    temp_dict = dict()
+    
+    
+    
+    df['DATE'] = pd.to_datetime(df['DATE'])
+    df = df.sort_values(by='DATE')
+    df['DATE']=df['DATE'].dt.date
+
+    # data preparation
+
+    imputed_df = df.copy()
+    
+    
+    # add the future close price column and shift by the required days
+
+    imputed_df['FUTURE_CLOSE_PRICE'] = imputed_df['ADJCLOSE'].shift(future_days)
+
+
+
+    # add technical indicators
+    for n in [14,30,50,200]:
+
+     # create the moving average indicator 
+     imputed_df['ma'+str(n)] = imputed_df['ADJCLOSE'].ewm(span=n,adjust=False).mean()
+
+
+
+    ds_size = len(imputed_df)
+    #imputed_df.dropna(inplace=True)
+
+
+    columns = imputed_df.columns
+    for column in columns:
+
+        if column=='ID':
+            continue
+
+        #impute missing values 
+        # mean for numerical and 'missing' for categorical
+        if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+
+
+            if imputed_df[column].isnull().sum()==len(imputed_df):
+
+                imputed_df[column] = imputed_df[column].fillna(0)
+
+
+            else :
+
+                imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+
+                transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+
+
+                imputed_df[column] = transformed_column
+
+
+        if (imputed_df[column].dtype == 'object'):
+            # impute missing values for categorical variables
+
+
+            imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+            imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+            imputed_df[column] = imputed_df[column].astype('str')
+
+            le = LabelEncoder()
+
+            le.fit(imputed_df[column])
+                    # print(le.classes_)
+            imputed_df[column] = le.transform(imputed_df[column])
+            temp_dict[column] = le
+
+
+    # Create a random forest regressor
+    rf = RandomForestRegressor(n_estimators=200)
+
+    X = imputed_df.drop(['FUTURE_CLOSE_PRICE'], axis=1)
+    y = imputed_df['FUTURE_CLOSE_PRICE']
+
+    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42)
+
+
+    rf.fit(X_train,y_train)
+
+
+
+
+    pred_df = X_test.copy()
+    pred_df['FUTURE_CLOSE_PRICE'] = y_test
+
+    y_pred = rf.predict(X_test)
+    pred_df['FUTURE_CLOSE_PRICE_PRED'] =y_pred
+
+
+
+    accuracy  = rf.score(X_test, y_test)    
+
+    rms = mean_squared_error(y_test, y_pred)
+
+
+    pred_df['DATASET_SIZE'] = ds_size
+    pred_df['accuracy']=round(accuracy,2)
+    pred_df['mean_square_error']=round(rms)
+
+
+    #for all the columns that had label encoders, do an inverse transform
+
+    original_columns = pred_df.columns
+
+    for column in original_columns:
+
+     if column in temp_dict:   
+      pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
+
+    #pred_df['DATE'] = pd.to_datetime(pred_df['DATE'])
+
+
+
+    #pred_df = pred_df.sort_values(by='DATE')
+    #pred_df['DATE']=pred_df['DATE'].dt.date
+
+    def print_output(x):
+                row = [x['ID'], x['FUTURE_CLOSE_PRICE'], x['FUTURE_CLOSE_PRICE_PRED'], x['DATASET_SIZE'], x['accuracy']]
+               
+                
+                self.output(row)
+
+
+    pred_df.apply(print_output, axis=1)
+
+
+
+"""
+
+output_signature= {'ID': 'float', 'FUTURE_CLOSE_PRICE': 'double', 'FUTURE_CLOSE_PRICE_PRED': 'double',
+                   'DATASET_SIZE': 'int', 'ACCURACY': 'float'}
+
+
+import time
+
+start = time.time()
+
+nz_groupapply = NZFunGroupedApply(df=idadf, code_str=code_str_host_spus, index='TICKER', fun_name="stocks_rf_ml",
+                                  output_signature=output_signature, merge_output_with_df=True)
+
+result = nz_groupapply.get_result()
+result = result.as_dataframe()
+print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
+print(result)
+end = time.time()
+print(end - start)
+
+
+
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/groupedapply.py` & `nzpyida-0.3.3/nzpyida/ae/groupedapply.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,163 +1,163 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-# -----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-# -----------------------------------------------------------------------------
-
-"""
-In-Database User defined functions
-"""
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-
-import inspect
-import textwrap
-from builtins import dict
-from future import standard_library
-
-from nzpyida.ae import shaper, result_builder
-
-standard_library.install_aliases()
-
-
-class NZFunGroupedApply(object):
-
-    def __init__(self, df, index, output_signature, output_table=None, fun_ref=None,  code_str=None,  fun_name=None, columns=None, merge_output_with_df=False, id='ID'):
-        """
-        Constructor for tapply
-        """
-        self.table_name = df.internal_state.current_state
-        self.df = df
-        self.db = df._idadb
-        self.fun = fun_ref
-        self.fun_name = fun_name
-        self.id = id
-
-        self.code_str= code_str
-        self.merge_output= merge_output_with_df
-
-        self.index = index
-        self.output_table = output_table
-        self.output_signature = output_signature
-        self.parallel = True
-
-        # convert the columns index object to a list
-        if columns:
-            self.columns=columns
-        else :
-            self.columns = self.df.columns.tolist()
-
-
-    def get_result(self):
-        if self.code_str and self.fun_name is None:
-            raise Exception("fun_name is required")
-        # we need a comma separated string of column values
-        #columns_string = ",".join(self.columns)
-        # print(columns_string)
-
-
-
-        if self.code_str:
-            code_string = self.build_udtf_shaper_ae_code_fun_as_str(self.code_str, self.fun_name, self.columns,
-                                                                    self.output_signature)
-        else:
-            code_string = self.build_udtf_shaper_ae_code_fun_as_ref(self.fun, self.columns, self.output_signature)
-
-
-
-        compressions_string = result_builder.compress_columns_string(self.columns)
-
-        query = ""
-        # send the code as dynamic variable to ae function
-        columns_string = compressions_string + ",'CODE_TO_EXECUTE=" + "\"" + code_string + "\"" + "'"
-        if self.parallel is False:
-            ae_name = "nzpyida..py_udtf_host"
-            query = "select ae_output.* from " + \
-                    " (select * from " + self.db_name + ") as input_t" + \
-                    ", table with final (" + ae_name + columns_string + ")) as ae_output"
-        else:
-            ae_name = "nzpyida..py_udtf_any"
-            query = "(select row_number() over (partition by " + self.index + " order by " + self.index + ") as  rn,  count(*)  over (partition" + \
-                    " by " + self.index + ") as   ct,    " + self.table_name + ".*   from " + self.table_name + ") as input_t"
-            query = "select ae_output.* from " + \
-                    query + \
-                    ", table with final (" + ae_name + "(rn,ct," + columns_string + ")) as ae_output"
-        result = result_builder.build_result(self.output_table, self.merge_output, self.db, self.df,
-                                             self.output_signature, self.table_name, query, self.id)
-        return result
-
-
-
-    def build_udtf_shaper_ae_code_fun_as_str(self, code_str, fun_name, columns, output_signature):
-        # we need extra single quotes for correct escaping
-
-        fun_code = code_str
-        fun_code = fun_code.replace("'", "''")
-
-
-        if self.parallel is False:
-            base_code = self.get_base_shaper_host(columns, fun_name, output_signature)
-        else:
-            base_code = shaper.get_base_shaper_groupedapply(columns, fun_name, output_signature)
-
-        run_string = textwrap.dedent(""" BaseShaperUdtf.run()""")
-
-        final_code = base_code + "\n" + textwrap.indent(fun_code, '     ')
-        final_code = final_code+"\n"+run_string
-
-
-
-        return inspect.cleandoc(final_code)
-
-
-
-
-    def build_udtf_shaper_ae_code_fun_as_ref(self, fun, columns, output_signature):
-
-
-        return self.build_udtf_shaper_ae_code_fun_as_str(inspect.getsource(fun).lstrip(), fun.__name__, columns,
-                                                         output_signature)
-
-
-
-
-    def get_base_shaper_host(self, columns, fun_name, output_signature):
-
-        output_signature_str = shaper.get_base_shaper(output_signature)
-
-
-
-
-
-        code_string = """               import nzae
-                                        import pandas as pd
-                                        class BaseShaperUdtf(nzae.Ae):
-                                             def _runUdtf(self):
-                                               rows_list=[]
-                                               for row in self:
-                                                 rows_list.append(row)
-                                                 #rows_list.append(str(self.getDatasliceId())+str(self.getHardwareId()))  
-                                               df = pd.DataFrame(rows_list, columns=""" + str(columns) + """ )
-                                               value = self.""" + fun_name + """(df)
-                                               self.output(value)
-                                               #self.output(str(len(rows_list)), str(rows_list[12000]))
-
-                                             def _runShaper(self):
-                                               """ + textwrap.indent(output_signature_str, '                      ') + """
-
-
-
-
-                                               """
-
-        code_string = code_string.replace("'", "''")
-        return inspect.cleandoc(code_string)
-
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+# -----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+# -----------------------------------------------------------------------------
+
+"""
+In-Database User defined functions
+"""
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+
+import inspect
+import textwrap
+from builtins import dict
+from future import standard_library
+
+from nzpyida.ae import shaper, result_builder
+
+standard_library.install_aliases()
+
+
+class NZFunGroupedApply(object):
+
+    def __init__(self, df, index, output_signature, output_table=None, fun_ref=None,  code_str=None,  fun_name=None, columns=None, merge_output_with_df=False, id='ID'):
+        """
+        Constructor for tapply
+        """
+        self.table_name = df.internal_state.current_state
+        self.df = df
+        self.db = df._idadb
+        self.fun = fun_ref
+        self.fun_name = fun_name
+        self.id = id
+
+        self.code_str= code_str
+        self.merge_output= merge_output_with_df
+
+        self.index = index
+        self.output_table = output_table
+        self.output_signature = output_signature
+        self.parallel = True
+
+        # convert the columns index object to a list
+        if columns:
+            self.columns=columns
+        else :
+            self.columns = self.df.columns.tolist()
+
+
+    def get_result(self):
+        if self.code_str and self.fun_name is None:
+            raise Exception("fun_name is required")
+        # we need a comma separated string of column values
+        #columns_string = ",".join(self.columns)
+        # print(columns_string)
+
+
+
+        if self.code_str:
+            code_string = self.build_udtf_shaper_ae_code_fun_as_str(self.code_str, self.fun_name, self.columns,
+                                                                    self.output_signature)
+        else:
+            code_string = self.build_udtf_shaper_ae_code_fun_as_ref(self.fun, self.columns, self.output_signature)
+
+
+
+        compressions_string = result_builder.compress_columns_string(self.columns)
+
+        query = ""
+        # send the code as dynamic variable to ae function
+        columns_string = compressions_string + ",'CODE_TO_EXECUTE=" + "\"" + code_string + "\"" + "'"
+        if self.parallel is False:
+            ae_name = "nzpyida..py_udtf_host"
+            query = "select ae_output.* from " + \
+                    " (select * from " + self.db_name + ") as input_t" + \
+                    ", table with final (" + ae_name + columns_string + ")) as ae_output"
+        else:
+            ae_name = "nzpyida..py_udtf_any"
+            query = "(select row_number() over (partition by " + self.index + " order by " + self.index + ") as  rn,  count(*)  over (partition" + \
+                    " by " + self.index + ") as   ct,    " + self.table_name + ".*   from " + self.table_name + ") as input_t"
+            query = "select ae_output.* from " + \
+                    query + \
+                    ", table with final (" + ae_name + "(rn,ct," + columns_string + ")) as ae_output"
+        result = result_builder.build_result(self.output_table, self.merge_output, self.db, self.df,
+                                             self.output_signature, self.table_name, query, self.id)
+        return result
+
+
+
+    def build_udtf_shaper_ae_code_fun_as_str(self, code_str, fun_name, columns, output_signature):
+        # we need extra single quotes for correct escaping
+
+        fun_code = code_str
+        fun_code = fun_code.replace("'", "''")
+
+
+        if self.parallel is False:
+            base_code = self.get_base_shaper_host(columns, fun_name, output_signature)
+        else:
+            base_code = shaper.get_base_shaper_groupedapply(columns, fun_name, output_signature)
+
+        run_string = textwrap.dedent(""" BaseShaperUdtf.run()""")
+
+        final_code = base_code + "\n" + textwrap.indent(fun_code, '     ')
+        final_code = final_code+"\n"+run_string
+
+
+
+        return inspect.cleandoc(final_code)
+
+
+
+
+    def build_udtf_shaper_ae_code_fun_as_ref(self, fun, columns, output_signature):
+
+
+        return self.build_udtf_shaper_ae_code_fun_as_str(inspect.getsource(fun).lstrip(), fun.__name__, columns,
+                                                         output_signature)
+
+
+
+
+    def get_base_shaper_host(self, columns, fun_name, output_signature):
+
+        output_signature_str = shaper.get_base_shaper(output_signature)
+
+
+
+
+
+        code_string = """               import nzae
+                                        import pandas as pd
+                                        class BaseShaperUdtf(nzae.Ae):
+                                             def _runUdtf(self):
+                                               rows_list=[]
+                                               for row in self:
+                                                 rows_list.append(row)
+                                                 #rows_list.append(str(self.getDatasliceId())+str(self.getHardwareId()))  
+                                               df = pd.DataFrame(rows_list, columns=""" + str(columns) + """ )
+                                               value = self.""" + fun_name + """(df)
+                                               self.output(value)
+                                               #self.output(str(len(rows_list)), str(rows_list[12000]))
+
+                                             def _runShaper(self):
+                                               """ + textwrap.indent(output_signature_str, '                      ') + """
+
+
+
+
+                                               """
+
+        code_string = code_string.replace("'", "''")
+        return inspect.cleandoc(code_string)
+
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/install.py` & `nzpyida-0.3.3/nzpyida/ae/install.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,67 +1,67 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-# -----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-# -----------------------------------------------------------------------------
-
-"""
-In-Database User defined functions
-"""
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-
-import inspect
-import textwrap
-from builtins import dict
-from future import standard_library
-from nzpyida import IdaDataBase, IdaDataFrame
-
-from nzpyida.ae import shaper, result_builder
-
-standard_library.install_aliases()
-
-
-class NZInstall(object):
-
-    def __init__(self, idadb, package_name):
-        """
-        Constructor for install
-        """
-        self.package_name=package_name
-        self.db =idadb
-
-
-
-    def  getResultCode(self):
-
-        
-        ae_name = "nzpyida..py_udtf_install"
-
-        output_signature = {'ResultCode': 'int'}
-        base_code = shaper.get_base_shaper_install(output_signature, self.package_name)
-
-        run_string = textwrap.dedent(""" BaseShaperUdtf.run()""")
-
-        final_code = base_code + "\n" + run_string
-
-        columns_string = "'CODE_TO_EXECUTE=" + "\"" + final_code + "\"" + "'"
-        query = "select * from table with final (" + ae_name + "(" + columns_string + ")) "
-
-
-        result = self.db.ida_query(query)
-
-        if(len(result.values)>0):
-            return result.values[0]
-
-
-
-
-
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+# -----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+# -----------------------------------------------------------------------------
+
+"""
+In-Database User defined functions
+"""
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+
+import inspect
+import textwrap
+from builtins import dict
+from future import standard_library
+from nzpyida import IdaDataBase, IdaDataFrame
+
+from nzpyida.ae import shaper, result_builder
+
+standard_library.install_aliases()
+
+
+class NZInstall(object):
+
+    def __init__(self, idadb, package_name):
+        """
+        Constructor for install
+        """
+        self.package_name=package_name
+        self.db =idadb
+
+
+
+    def  getResultCode(self):
+
+        
+        ae_name = "nzpyida..py_udtf_install"
+
+        output_signature = {'ResultCode': 'int'}
+        base_code = shaper.get_base_shaper_install(output_signature, self.package_name)
+
+        run_string = textwrap.dedent(""" BaseShaperUdtf.run()""")
+
+        final_code = base_code + "\n" + run_string
+
+        columns_string = "'CODE_TO_EXECUTE=" + "\"" + final_code + "\"" + "'"
+        query = "select * from table with final (" + ae_name + "(" + columns_string + ")) "
+
+
+        result = self.db.ida_query(query)
+
+        if(len(result.values)>0):
+            return result.values[0]
+
+
+
+
+
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/result_builder.py` & `nzpyida-0.3.3/nzpyida/ae/result_builder.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,127 +1,127 @@
-from nzpyida import IdaDataFrame
-
-
-def build_result(output_table, merge_output, db, df, output_signature, table_name, query, id):
-
-    if output_table and merge_output is False:
-        if db.exists_table(output_table):
-            raise NameError("table name already exists..choose a different name")
-
-        create_string = "create table " + output_table + " as "
-        query = create_string + query
-        result = df.ida_query(query, autocommit=True)
-        idadf = IdaDataFrame(db, output_table)
-        return idadf
-    if output_table and merge_output is True:
-        if db.exists_table(output_table):
-            raise NameError("table name already exists..choose a different name")
-
-        # check for duplicate columns
-
-        for column in output_signature:
-
-
-
-            if column in df.columns.tolist():
-                if (column == id):
-                    continue
-
-                raise ValueError("column " + column + " duplicated in the output table")
-
-
-        # create a random table for merging the two tables
-        output_table_tmp = db._get_valid_tablename(prefix="pyida_")
-        create_string = "create table " + output_table_tmp + " as "
-        query = create_string + query
-        result = df.ida_query(query, autocommit=True)
-
-        # join the two
-        columns_str = ""
-        for column in output_signature:
-
-            if column == id:
-                continue
-            columns_str = columns_str + "link." + column + ","
-        #if len(columns_str) > 0:
-            #columns_str = columns_str[:-1]
-
-        query = "create table " + output_table + " as  select  " + columns_str + " base.*  from  " + output_table_tmp + " as link INNER JOIN " \
-                                                                                                                             " " + table_name + "  as base on link." + id + " = base." + id + ";"
-
-
-        result = df.ida_query(query, autocommit=True)
-        db.drop_table(output_table_tmp)
-        idadf = IdaDataFrame(db, output_table)
-        return idadf
-    if output_table is None and merge_output is True:
-
-        # create a random table and store query results
-        output_table_tmp = db._get_valid_tablename(prefix="pyida_")
-        create_string = "create table " + output_table_tmp + " as "
-        query = create_string + query
-        result = df.ida_query(query, autocommit=True)
-
-
-        # join the two
-
-        # check if there are duplicate columns in the two tables
-
-        for column in output_signature:
-
-            if column in df.columns.tolist():
-                if (column == id):
-                    continue
-                raise ValueError("column "+column+" duplicated in the output table")
-
-        columns_str = ""
-        for column in output_signature:
-            if column == id:
-                continue
-            columns_str = columns_str + "link. " + column + ","
-        if len(columns_str) > 0:
-            columns_str = columns_str[:-1]
-        df_output_table = db._get_valid_tablename(prefix="df_")
-
-        query = "create table "+df_output_table+" as  select  " + columns_str + " , base.*  from  " + output_table_tmp + " as link INNER JOIN " \
-                                                                                                                        " " + table_name+"  as base on link." + id + " = base." +id + ";"
-
-
-        result = df.ida_query(query, autocommit=True)
-
-
-        idadf = IdaDataFrame(db, df_output_table)
-
-        db.drop_table(output_table_tmp)
-
-        return idadf
-
-    df_output_table = db._get_valid_tablename(prefix="df_")
-    query = "create table " + df_output_table + " as " + query
-    result = df.ida_query(query)
-
-    idadf = IdaDataFrame(db, df_output_table)
-    return idadf
-
-
-def compress_columns_string(columns):
-
-
-    columns_count = len(columns)
-
-    compressions_count = columns_count // 64
-
-    residual_columns_count = columns_count % 64
-
-
-    compressions_string = ''
-    for i in range(0, compressions_count):
-        columns_string = ",".join(columns[(i * 64):(i + 64)])
-
-        compressions_string = compressions_string + "NZAEMULTIARG(" + columns_string + "),  "
-
-    columns_string = ",".join(
-        columns[(compressions_count * 64):((compressions_count * 64) + residual_columns_count)])
-    compressions_string = compressions_string +columns_string
-
-
-    return compressions_string
+from nzpyida import IdaDataFrame
+
+
+def build_result(output_table, merge_output, db, df, output_signature, table_name, query, id):
+
+    if output_table and merge_output is False:
+        if db.exists_table(output_table):
+            raise NameError("table name already exists..choose a different name")
+
+        create_string = "create table " + output_table + " as "
+        query = create_string + query
+        result = df.ida_query(query, autocommit=True)
+        idadf = IdaDataFrame(db, output_table)
+        return idadf
+    if output_table and merge_output is True:
+        if db.exists_table(output_table):
+            raise NameError("table name already exists..choose a different name")
+
+        # check for duplicate columns
+
+        for column in output_signature:
+
+
+
+            if column in df.columns.tolist():
+                if (column == id):
+                    continue
+
+                raise ValueError("column " + column + " duplicated in the output table")
+
+
+        # create a random table for merging the two tables
+        output_table_tmp = db._get_valid_tablename(prefix="pyida_")
+        create_string = "create table " + output_table_tmp + " as "
+        query = create_string + query
+        result = df.ida_query(query, autocommit=True)
+
+        # join the two
+        columns_str = ""
+        for column in output_signature:
+
+            if column == id:
+                continue
+            columns_str = columns_str + "link." + column + ","
+        #if len(columns_str) > 0:
+            #columns_str = columns_str[:-1]
+
+        query = "create table " + output_table + " as  select  " + columns_str + " base.*  from  " + output_table_tmp + " as link INNER JOIN " \
+                                                                                                                             " " + table_name + "  as base on link." + id + " = base." + id + ";"
+
+
+        result = df.ida_query(query, autocommit=True)
+        db.drop_table(output_table_tmp)
+        idadf = IdaDataFrame(db, output_table)
+        return idadf
+    if output_table is None and merge_output is True:
+
+        # create a random table and store query results
+        output_table_tmp = db._get_valid_tablename(prefix="pyida_")
+        create_string = "create table " + output_table_tmp + " as "
+        query = create_string + query
+        result = df.ida_query(query, autocommit=True)
+
+
+        # join the two
+
+        # check if there are duplicate columns in the two tables
+
+        for column in output_signature:
+
+            if column in df.columns.tolist():
+                if (column == id):
+                    continue
+                raise ValueError("column "+column+" duplicated in the output table")
+
+        columns_str = ""
+        for column in output_signature:
+            if column == id:
+                continue
+            columns_str = columns_str + "link. " + column + ","
+        if len(columns_str) > 0:
+            columns_str = columns_str[:-1]
+        df_output_table = db._get_valid_tablename(prefix="df_")
+
+        query = "create table "+df_output_table+" as  select  " + columns_str + " , base.*  from  " + output_table_tmp + " as link INNER JOIN " \
+                                                                                                                        " " + table_name+"  as base on link." + id + " = base." +id + ";"
+
+
+        result = df.ida_query(query, autocommit=True)
+
+
+        idadf = IdaDataFrame(db, df_output_table)
+
+        db.drop_table(output_table_tmp)
+
+        return idadf
+
+    df_output_table = db._get_valid_tablename(prefix="df_")
+    query = "create table " + df_output_table + " as " + query
+    result = df.ida_query(query)
+
+    idadf = IdaDataFrame(db, df_output_table)
+    return idadf
+
+
+def compress_columns_string(columns):
+
+
+    columns_count = len(columns)
+
+    compressions_count = columns_count // 64
+
+    residual_columns_count = columns_count % 64
+
+
+    compressions_string = ''
+    for i in range(0, compressions_count):
+        columns_string = ",".join(columns[(i * 64):(i + 64)])
+
+        compressions_string = compressions_string + "NZAEMULTIARG(" + columns_string + "),  "
+
+    columns_string = ",".join(
+        columns[(compressions_count * 64):((compressions_count * 64) + residual_columns_count)])
+    compressions_string = compressions_string +columns_string
+
+
+    return compressions_string
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/shaper.py` & `nzpyida-0.3.3/nzpyida/ae/shaper.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,185 +1,185 @@
-import inspect
-import textwrap
-
-
-def get_base_shaper_tapply(columns, fun_name, output_signature):
-    output_signature_str = get_base_shaper(output_signature)
-
-
-
-
-    # output_signature_final = inspect.cleandoc(output_signature_str)
-    code_string = """import nzae
-                            import pandas as pd
-                            class BaseShaperUdtf(nzae.Ae):
-                                 def _runUdtf(self):
-                                   
-                                   df = pd.DataFrame(self.__iter__(), columns=""" + str(columns) + """ )
-                                   value = self.""" + fun_name + """(df)
-                                   #rows_list.append(str(self.getDatasliceId())+str(self.getHardwareId()))  
-                                  
-
-                                 def _runShaper(self):
-                                    
-                                    """ + textwrap.indent(output_signature_str, '                      ') + """
-                                    
-
-
-
-
-                                   """
-
-    code_string = code_string.replace("'", "''")
-    return inspect.cleandoc(code_string)
-
-def get_base_shaper_groupedapply(columns, fun_name, output_signature):
-    output_signature_str = get_base_shaper(output_signature)
-
-
-
-
-        # output_signature_final = inspect.cleandoc(output_signature_str)
-    code_string = """import nzae
-                        import pandas as pd
-                        class BaseShaperUdtf(nzae.Ae):
-                             def _runUdtf(self):
-                               rows_list=[]
-                               for row in self:
-                                  row_number = row[0]
-                                  count = row[1]
-
-                                  if row_number==1:
-                                    rows_list=[]
-                                  rows_list.append(row[2:])
-                                  #rows_list.append(str(self.getDatasliceId())+str(self.getHardwareId()))  
-                                  if row_number==count:                                                              
-                                   df = pd.DataFrame(rows_list, columns=""" + str(columns) + """ )
-                                   value = self.""" + fun_name + """(df)
-                                   #self.output(value)
-                                   #self.output(len(rows_list))
-
-
-
-
-                             def _runShaper(self):
-                               
-                                     """ + textwrap.indent(output_signature_str, '                      ') + """
-
-
-
-                               """
-
-    code_string = code_string.replace("'", "''")
-    return inspect.cleandoc(code_string)
-
-def get_base_shaper(output_signature):
-    output_signature_str = ""
-
-
-    for column in output_signature:
-
-        if output_signature[column] == 'int' :
-            column_val = 'self.DATA_TYPE__INT32'
-
-            output_signature_str += """
-                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
-        if output_signature[column] == 'int64':
-            column_val = 'self.DATA_TYPE__INT64'
-
-            output_signature_str += """
-                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
-
-        if output_signature[column] == 'bool' :
-            column_val = 'self.DATA_TYPE__BOOL'
-            output_signature_str += """
-                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
-
-        if output_signature[column] == 'datetime' or  output_signature[column] == 'date':
-
-            
-            column_val = 'self.DATA_TYPE__DATE'
-            output_signature_str += """
-                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
-
-        if output_signature[column] == 'float' or output_signature[column] == 'float64':
-            column_val = 'self.DATA_TYPE__FLOAT'
-            output_signature_str += """
-         
-                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
-
-
-        if output_signature[column] == 'double':
-            column_val = 'self.DATA_TYPE__DOUBLE'
-            output_signature_str += """
-                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
-        if output_signature[column] == 'str' or output_signature[column] == 'object':
-            column_val = 'self.DATA_TYPE__VARIABLE'
-            output_signature_str += """
-                                                       self.addOutputColumnString('""" + column + """',""" + column_val + """,1000) """
-    return output_signature_str
-
-
-def get_base_shaper_apply(columns, fun_name, output_signature):
-
-    output_signature_str = get_base_shaper(output_signature)
-    # output_signature_final = inspect.cleandoc(output_signature_str)
-    code_string = """import nzae
-                            import pandas as pd
-                            class BaseShaperUdtf(nzae.Ae):
-                                 def _runUdtf(self):
-                                  
-                                   for row in self:
-                                     row = self.apply_fun(row)  
-                                   
-
-                                 def _runShaper(self):
-                                   """ + textwrap.indent(output_signature_str, '                      ') + """
-
-
-
-
-                                   """
-
-    code_string = code_string.replace("'", "''")
-    return inspect.cleandoc(code_string)
-
-
-def get_base_shaper_install(output_signature, package_name):
-    output_signature_str = get_base_shaper(output_signature)
-    # output_signature_final = inspect.cleandoc(output_signature_str)
-    code_string = """import nzae
-                            
-                            class BaseShaperUdtf(nzae.Ae):
-                                 def _runUdtf(self):
-
-                                    import subprocess
-                                    cwd = os.getcwd()
-                                    
-                                    for row in self:
-                                                                
-                                        
-                                        res = subprocess.call([\'sh', \'/nz/export/ae/adapters/python3x/11/lib/installPackage.sh',  """+"\'"+package_name+"\'"+"""])
-                                       
-                                        self.output(res)
-                                       
-                                    
-
-
-                                 def _runShaper(self):
-                                   """ + textwrap.indent(output_signature_str, '       ') + """
-
-
-
-
-                                   """
-
-    code_string = code_string.replace("'", "''")
-    return inspect.cleandoc(code_string)
-
-
-
-
-
-
-
-
+import inspect
+import textwrap
+
+
+def get_base_shaper_tapply(columns, fun_name, output_signature):
+    output_signature_str = get_base_shaper(output_signature)
+
+
+
+
+    # output_signature_final = inspect.cleandoc(output_signature_str)
+    code_string = """import nzae
+                            import pandas as pd
+                            class BaseShaperUdtf(nzae.Ae):
+                                 def _runUdtf(self):
+                                   
+                                   df = pd.DataFrame(self.__iter__(), columns=""" + str(columns) + """ )
+                                   value = self.""" + fun_name + """(df)
+                                   #rows_list.append(str(self.getDatasliceId())+str(self.getHardwareId()))  
+                                  
+
+                                 def _runShaper(self):
+                                    
+                                    """ + textwrap.indent(output_signature_str, '                      ') + """
+                                    
+
+
+
+
+                                   """
+
+    code_string = code_string.replace("'", "''")
+    return inspect.cleandoc(code_string)
+
+def get_base_shaper_groupedapply(columns, fun_name, output_signature):
+    output_signature_str = get_base_shaper(output_signature)
+
+
+
+
+        # output_signature_final = inspect.cleandoc(output_signature_str)
+    code_string = """import nzae
+                        import pandas as pd
+                        class BaseShaperUdtf(nzae.Ae):
+                             def _runUdtf(self):
+                               rows_list=[]
+                               for row in self:
+                                  row_number = row[0]
+                                  count = row[1]
+
+                                  if row_number==1:
+                                    rows_list=[]
+                                  rows_list.append(row[2:])
+                                  #rows_list.append(str(self.getDatasliceId())+str(self.getHardwareId()))  
+                                  if row_number==count:                                                              
+                                   df = pd.DataFrame(rows_list, columns=""" + str(columns) + """ )
+                                   value = self.""" + fun_name + """(df)
+                                   #self.output(value)
+                                   #self.output(len(rows_list))
+
+
+
+
+                             def _runShaper(self):
+                               
+                                     """ + textwrap.indent(output_signature_str, '                      ') + """
+
+
+
+                               """
+
+    code_string = code_string.replace("'", "''")
+    return inspect.cleandoc(code_string)
+
+def get_base_shaper(output_signature):
+    output_signature_str = ""
+
+
+    for column in output_signature:
+
+        if output_signature[column] == 'int' :
+            column_val = 'self.DATA_TYPE__INT32'
+
+            output_signature_str += """
+                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
+        if output_signature[column] == 'int64':
+            column_val = 'self.DATA_TYPE__INT64'
+
+            output_signature_str += """
+                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
+
+        if output_signature[column] == 'bool' :
+            column_val = 'self.DATA_TYPE__BOOL'
+            output_signature_str += """
+                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
+
+        if output_signature[column] == 'datetime' or  output_signature[column] == 'date':
+
+            
+            column_val = 'self.DATA_TYPE__DATE'
+            output_signature_str += """
+                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
+
+        if output_signature[column] == 'float' or output_signature[column] == 'float64':
+            column_val = 'self.DATA_TYPE__FLOAT'
+            output_signature_str += """
+         
+                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
+
+
+        if output_signature[column] == 'double':
+            column_val = 'self.DATA_TYPE__DOUBLE'
+            output_signature_str += """
+                                                       self.addOutputColumn('""" + column + """',""" + column_val + """) """
+        if output_signature[column] == 'str' or output_signature[column] == 'object':
+            column_val = 'self.DATA_TYPE__VARIABLE'
+            output_signature_str += """
+                                                       self.addOutputColumnString('""" + column + """',""" + column_val + """,1000) """
+    return output_signature_str
+
+
+def get_base_shaper_apply(columns, fun_name, output_signature):
+
+    output_signature_str = get_base_shaper(output_signature)
+    # output_signature_final = inspect.cleandoc(output_signature_str)
+    code_string = """import nzae
+                            import pandas as pd
+                            class BaseShaperUdtf(nzae.Ae):
+                                 def _runUdtf(self):
+                                  
+                                   for row in self:
+                                     row = self.apply_fun(row)  
+                                   
+
+                                 def _runShaper(self):
+                                   """ + textwrap.indent(output_signature_str, '                      ') + """
+
+
+
+
+                                   """
+
+    code_string = code_string.replace("'", "''")
+    return inspect.cleandoc(code_string)
+
+
+def get_base_shaper_install(output_signature, package_name):
+    output_signature_str = get_base_shaper(output_signature)
+    # output_signature_final = inspect.cleandoc(output_signature_str)
+    code_string = """import nzae
+                            
+                            class BaseShaperUdtf(nzae.Ae):
+                                 def _runUdtf(self):
+
+                                    import subprocess
+                                    cwd = os.getcwd()
+                                    
+                                    for row in self:
+                                                                
+                                        
+                                        res = subprocess.call([\'sh', \'/nz/export/ae/adapters/python3x/11/lib/installPackage.sh',  """+"\'"+package_name+"\'"+"""])
+                                       
+                                        self.output(res)
+                                       
+                                    
+
+
+                                 def _runShaper(self):
+                                   """ + textwrap.indent(output_signature_str, '       ') + """
+
+
+
+
+                                   """
+
+    code_string = code_string.replace("'", "''")
+    return inspect.cleandoc(code_string)
+
+
+
+
+
+
+
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/tapply_class.py` & `nzpyida-0.3.3/nzpyida/ae/tapply_class.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,188 +1,188 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-# -----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-# -----------------------------------------------------------------------------
-
-"""
-In-Database User defined functions
-"""
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-
-import inspect
-import textwrap
-from nzpyida import IdaDataFrame
-from builtins import dict
-from future import standard_library
-
-standard_library.install_aliases()
-
-
-class NZClassTApply(object):
-
-    def __init__(self, df, fun, input_class,  parallel,  output_signature, output_table=None):
-        """
-        Constructor for NZClassTApply
-        """
-        self.db_name = df.internal_state.current_state
-        self.df = df
-        self.fun = fun
-        self.db = df._idadb
-        self.input_class = input_class
-
-
-
-        self.output_table = output_table
-        self.output_signature = output_signature
-        self.parallel =parallel
-
-        # convert the columns index object to a list
-        self.columns = self.df.columns.tolist()
-
-    def get_result(self):
-        # we need a comma separated string of column values
-        columns_string = ",".join(self.columns)
-        # print(columns_string)
-
-        # get the default ae class coupled with client ml code
-        # code_string = self.build_udtf_ae_code(self.fun, self.columns)
-        code_string = self.build_udtf_shaper_ae_code(self.fun, self.columns, self.output_signature)
-
-        # path_to_execute = "/nz/export/ae/adapters/python3x/11/aes/test.py"
-        # code_string = "print(4+9)"
-
-        # send the code as dynamic variable to ae function
-        columns_string = columns_string + ",'CODE_TO_EXECUTE=" + "\"" + code_string + "\"" + "'"
-
-        print("db name is " + self.db_name)
-        # print(columns_string)
-        if not self.parallel:
-            ae_name = "py_udtf_host"
-        else:
-            ae_name ="py_udtf_any"
-        query = "select ae_output.* from " + \
-                " (select * from " + self.db_name + ") as input_t" + \
-                ", table with final (" + ae_name + "(" + columns_string + ")) as ae_output"
-
-        # print(query)
-        if self.output_table:
-            if self.db.exists_table(self.output_table):
-              create_string = "insert into "+self.output_table+" "
-            else:
-              create_string = "create table " + self.output_table + " as "
-            query = create_string+query
-            result = self.df.ida_query(query)
-            idadf = IdaDataFrame(self.db, self.output_table)
-            df = idadf.as_dataframe()
-            return df
-        result = self.df.ida_query(query)
-        return result
-
-
-
-
-
-
-    def build_udtf_shaper_ae_code(self, fun, columns, output_signature):
-        # we need extra single quotes for correct escaping
-
-        input_class = self.input_class
-        input_class_name = input_class.__name__
-        input_class_code = inspect.getsource(input_class)
-
-
-
-        input_class_code=input_class_code.replace(input_class_name, input_class_name+"(nzae.Ae)")
-        input_class_code = input_class_code.replace("'", "''")
-
-
-
-
-
-        import_string = """ import nzae
-                            import pandas as pd
-                            """
-        import_string = inspect.cleandoc(import_string)
-
-        base_code = self.get_base_shaper(columns, fun, output_signature)
-
-
-        run_string = input_class_name+".run()"
-        run_string = inspect.cleandoc(textwrap.dedent(run_string))
-
-        final_code = import_string +"\n"+input_class_code+"\n"+base_code+"\n"+run_string
-
-        #final_code = base_code + "\n" + textwrap.indent(fun_code, '     ') \
-                    # + run_string
-
-
-        print_string = """
-        print(3+4)
-
-        """
-
-        return inspect.cleandoc(final_code)
-
-
-
-    def get_base_shaper(self, columns, fun, output_signature):
-        fun_name = fun.__name__
-        output_signature_str = ""
-        print(len(output_signature))
-        for i in range(len(output_signature)):
-            column_signature = output_signature[i]
-            col_sig_list = column_signature.split('=')
-            if col_sig_list[1]=='int':
-                col_sig_list[1]= 'self.DATA_TYPE__INT32'
-                output_signature_str += """
-                            self.addOutputColumn('""" + col_sig_list[0] + """',""" + col_sig_list[
-                    1] + """) """
-            if col_sig_list[1] == 'float':
-                col_sig_list[1] = 'self.DATA_TYPE__FLOAT'
-                output_signature_str += """
-                            self.addOutputColumn('""" + col_sig_list[0] + """',""" + col_sig_list[
-                    1] + """) """
-            if col_sig_list[1] == 'double':
-                col_sig_list[1] = 'self.DATA_TYPE__DOUBLE'
-                output_signature_str += """
-                            self.addOutputColumn('""" + col_sig_list[0] + """',""" + \
-                                        col_sig_list[
-                                            1] + """) """
-            if col_sig_list[1] == 'str':
-                col_sig_list[1] = 'self.DATA_TYPE__VARIABLE'
-                output_signature_str += """
-                            self.addOutputColumnString('""" + col_sig_list[
-                    0] + """',""" + \
-                                        col_sig_list[
-                                            1] + """,100) """
-
-
-
-
-        #output_signature_final = inspect.cleandoc(output_signature_str)
-        code_string ="""def _runUdtf(self):
-                               rows_list=[]
-                               for row in self:
-                                 rows_list.append(row)
-                               df = pd.DataFrame(rows_list, columns=""" + str(columns) + """ )
-                               self.""" + fun_name + """(df)
-                               
-                        def _runShaper(self):
-                               """+textwrap.indent(output_signature_str, '           ') + """
-                                                          
-        
-                             
-                               
-                               """
-
-        code_string = code_string.replace("'", "''")
-        return inspect.cleandoc(code_string)
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+# -----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+# -----------------------------------------------------------------------------
+
+"""
+In-Database User defined functions
+"""
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+
+import inspect
+import textwrap
+from nzpyida import IdaDataFrame
+from builtins import dict
+from future import standard_library
+
+standard_library.install_aliases()
+
+
+class NZClassTApply(object):
+
+    def __init__(self, df, fun, input_class,  parallel,  output_signature, output_table=None):
+        """
+        Constructor for NZClassTApply
+        """
+        self.db_name = df.internal_state.current_state
+        self.df = df
+        self.fun = fun
+        self.db = df._idadb
+        self.input_class = input_class
+
+
+
+        self.output_table = output_table
+        self.output_signature = output_signature
+        self.parallel =parallel
+
+        # convert the columns index object to a list
+        self.columns = self.df.columns.tolist()
+
+    def get_result(self):
+        # we need a comma separated string of column values
+        columns_string = ",".join(self.columns)
+        # print(columns_string)
+
+        # get the default ae class coupled with client ml code
+        # code_string = self.build_udtf_ae_code(self.fun, self.columns)
+        code_string = self.build_udtf_shaper_ae_code(self.fun, self.columns, self.output_signature)
+
+        # path_to_execute = "/nz/export/ae/adapters/python3x/11/aes/test.py"
+        # code_string = "print(4+9)"
+
+        # send the code as dynamic variable to ae function
+        columns_string = columns_string + ",'CODE_TO_EXECUTE=" + "\"" + code_string + "\"" + "'"
+
+        print("db name is " + self.db_name)
+        # print(columns_string)
+        if not self.parallel:
+            ae_name = "py_udtf_host"
+        else:
+            ae_name ="py_udtf_any"
+        query = "select ae_output.* from " + \
+                " (select * from " + self.db_name + ") as input_t" + \
+                ", table with final (" + ae_name + "(" + columns_string + ")) as ae_output"
+
+        # print(query)
+        if self.output_table:
+            if self.db.exists_table(self.output_table):
+              create_string = "insert into "+self.output_table+" "
+            else:
+              create_string = "create table " + self.output_table + " as "
+            query = create_string+query
+            result = self.df.ida_query(query)
+            idadf = IdaDataFrame(self.db, self.output_table)
+            df = idadf.as_dataframe()
+            return df
+        result = self.df.ida_query(query)
+        return result
+
+
+
+
+
+
+    def build_udtf_shaper_ae_code(self, fun, columns, output_signature):
+        # we need extra single quotes for correct escaping
+
+        input_class = self.input_class
+        input_class_name = input_class.__name__
+        input_class_code = inspect.getsource(input_class)
+
+
+
+        input_class_code=input_class_code.replace(input_class_name, input_class_name+"(nzae.Ae)")
+        input_class_code = input_class_code.replace("'", "''")
+
+
+
+
+
+        import_string = """ import nzae
+                            import pandas as pd
+                            """
+        import_string = inspect.cleandoc(import_string)
+
+        base_code = self.get_base_shaper(columns, fun, output_signature)
+
+
+        run_string = input_class_name+".run()"
+        run_string = inspect.cleandoc(textwrap.dedent(run_string))
+
+        final_code = import_string +"\n"+input_class_code+"\n"+base_code+"\n"+run_string
+
+        #final_code = base_code + "\n" + textwrap.indent(fun_code, '     ') \
+                    # + run_string
+
+
+        print_string = """
+        print(3+4)
+
+        """
+
+        return inspect.cleandoc(final_code)
+
+
+
+    def get_base_shaper(self, columns, fun, output_signature):
+        fun_name = fun.__name__
+        output_signature_str = ""
+        print(len(output_signature))
+        for i in range(len(output_signature)):
+            column_signature = output_signature[i]
+            col_sig_list = column_signature.split('=')
+            if col_sig_list[1]=='int':
+                col_sig_list[1]= 'self.DATA_TYPE__INT32'
+                output_signature_str += """
+                            self.addOutputColumn('""" + col_sig_list[0] + """',""" + col_sig_list[
+                    1] + """) """
+            if col_sig_list[1] == 'float':
+                col_sig_list[1] = 'self.DATA_TYPE__FLOAT'
+                output_signature_str += """
+                            self.addOutputColumn('""" + col_sig_list[0] + """',""" + col_sig_list[
+                    1] + """) """
+            if col_sig_list[1] == 'double':
+                col_sig_list[1] = 'self.DATA_TYPE__DOUBLE'
+                output_signature_str += """
+                            self.addOutputColumn('""" + col_sig_list[0] + """',""" + \
+                                        col_sig_list[
+                                            1] + """) """
+            if col_sig_list[1] == 'str':
+                col_sig_list[1] = 'self.DATA_TYPE__VARIABLE'
+                output_signature_str += """
+                            self.addOutputColumnString('""" + col_sig_list[
+                    0] + """',""" + \
+                                        col_sig_list[
+                                            1] + """,100) """
+
+
+
+
+        #output_signature_final = inspect.cleandoc(output_signature_str)
+        code_string ="""def _runUdtf(self):
+                               rows_list=[]
+                               for row in self:
+                                 rows_list.append(row)
+                               df = pd.DataFrame(rows_list, columns=""" + str(columns) + """ )
+                               self.""" + fun_name + """(df)
+                               
+                        def _runShaper(self):
+                               """+textwrap.indent(output_signature_str, '           ') + """
+                                                          
+        
+                             
+                               
+                               """
+
+        code_string = code_string.replace("'", "''")
+        return inspect.cleandoc(code_string)
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/ae/tests/test_pyida.py` & `nzpyida-0.3.3/nzpyida/ae/tests/test_pyida.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,646 +1,628 @@
-import pytest
-from nzpyida import IdaDataBase, IdaDataFrame
-from nzpyida.ae import NZFunTApply, NZFunApply, NZFunGroupedApply
-from nzpyida.ae import NZInstall
-
-
-
-
-#nzpy dsn
-dsn ={
-    "database":"weather",
-     "port" :5480,
-     "host" : "169.63.46.17",
-     "securityLevel":0,
-     "logLevel":0
-}
-
-#odbc dsn
-#dsn='weather'
-
-def test_tapply_host_weather_train_pred():
-    idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True)
-    print(idadb)
-
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-
-    code_str_host = """def decision_tree_ml_host(self, df):
-
-        from sklearn.model_selection import cross_val_score
-        from sklearn.impute import SimpleImputer
-        from sklearn.tree import DecisionTreeClassifier
-        from sklearn.model_selection import train_test_split
-
-        from sklearn.preprocessing import LabelEncoder
-        import numpy as np
-
-        result = df.groupby('LOCATION')
-        #result = df.groupby(pd.qcut(df['ID'], q=3))
-
-        # result = idadf.ida_query(query, autocommit=True)
-
-        for name, group in result:
-            # print(name)
-
-            # print(group)
-            def decision_tree_classifier(df):
-                imputed_df = df.copy()
-                ds_size = len(imputed_df)
-                temp_dict = dict()
-
-
-
-
-
-                columns = imputed_df.columns
-
-                for column in columns:
-                    if column=='ID':
-                        continue
-
-                    if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-                        if imputed_df[column].isnull().sum()==len(imputed_df):
-                         imputed_df[column] = imputed_df[column].fillna(0)
-
-                        else :
-                         imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-                         transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                         imputed_df[column] = transformed_column
-
-
-                    if (imputed_df[column].dtype == 'object'):
-                        # impute missing values for categorical variables
-                        imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-                        imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                        imputed_df[column] = imputed_df[column].astype('str')
-                        le = LabelEncoder()
-                        # print(imputed_df[column].unique())
-
-                        le.fit(imputed_df[column])
-                        # print(le.classes_)
-                        imputed_df[column] = le.transform(imputed_df[column])
-                        temp_dict[column] = le
-
-                X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
-                y = imputed_df['RAINTOMORROW']
-                X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
-                #X_train_mod = X_train.drop(['RISK_MM'],axis=1)
-                #X_test_mod = X_test.drop(['RISK_MM'],axis=1)
-                dt = DecisionTreeClassifier(max_depth=5)
-                dt.fit(X_train, y_train)
-
-                accuracy = dt.score(X_test, y_test)    
-                #print(accuracy)
-
-
-
-                pred_df = X_test.copy()
-
-
-                y_pred= dt.predict(X_test)
-
-                pred_df['RAINTOMORROW'] = y_pred
-                pred_df['DATASET_SIZE'] = ds_size
-                pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
-
-
-
-
-                original_columns = pred_df.columns
-
-                for column in original_columns:
-
-                 if column in temp_dict:   
-                   pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
-                   #print(pred_df)
-
-                def print_output(x):
-                    row = [x['ID'], x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
-                    self.output(row)
-
-
-                pred_df.apply(print_output, axis=1)
-
-                return pred_df
-
-
-            ml_result = decision_tree_classifier(df=group)
-
-            """
-
-    output_signature = {'ID': 'int', 'RAINTOMORROW_PRED': 'str', 'DATASET_SIZE': 'int', 'CLASSIFIER_ACCURACY': 'float'}
-
-    import time
-    start = time.time()
-
-    nz_tapply = NZFunTApply(df=idadf, code_str=code_str_host, fun_name='decision_tree_ml_host', parallel=False,
-                            output_signature=output_signature, merge_output_with_df=True)
-
-    result = nz_tapply.get_result()
-    print("\n")
-    print(result)
-    end = time.time()
-    print(end - start)
-    result = result.as_dataframe()
-    print(result)
-    print(result.shape[0])
-    assert result.shape[0] == 35565, "number of records are not matching"
-    assert result.shape[1] == 28, "number of columns are not matching"
-
-
-def test_apply_weather_save_table_merge_withdf_duplicate_columns():
-    idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True)
-
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-    code_str_apply = """def apply_fun(self, x):
-        from math import sqrt
-        max_temp = x[3]
-        id = x[24]
-        fahren_max_temp = (max_temp*1.8)+32
-        row = [id, max_temp,  fahren_max_temp]
-        self.output(row)
-        """
-    output_signature = {'ID': 'int', 'MAXTEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
-    output_table = "temp_conversion"
-    if idadb.exists_table(output_table):
-        idadb.drop_table(output_table)
-
-    with pytest.raises(ValueError) as exception_msg:
-        nz_apply = NZFunApply(df=idadf, code_str=code_str_apply, fun_name='apply_fun', output_table=output_table,
-                              output_signature=output_signature, merge_output_with_df=True)
-        result = nz_apply.get_result()
-
-    assert exception_msg.match("column MAXTEMP duplicated in the output table")
-
-
-def test_apply_weather_save_table_merge_withdf():
-    idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True)
-
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-    code_str_apply = """def apply_fun(self, x):
-        from math import sqrt
-        max_temp = x[3]
-        id = x[24]
-        fahren_max_temp = (max_temp*1.8)+32
-        row = [id, max_temp,  fahren_max_temp]
-        self.output(row)
-        """
-    output_signature = {'ID': 'int', 'MAX_TEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
-    output_table = "temp_conversion"
-    if idadb.exists_table(output_table):
-        idadb.drop_table(output_table)
-
-    nz_apply = NZFunApply(df=idadf, code_str=code_str_apply, fun_name='apply_fun', output_table=output_table,
-                          output_signature=output_signature, merge_output_with_df=True)
-    result = nz_apply.get_result()
-    result = result.as_dataframe()
-    print(result)
-    assert result.shape[0] == 142193, "number of records are not matching"
-    assert result.shape[1] == 27, "number of columns are not matching"
-
-
-def test_summary():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-    print(idadb)
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-    result = idadf.summary()
-    print(result)
-    assert result.shape[0] == 25, "number of rows are not matching"
-    assert result.shape[1] == 13, "number of columns are not matching"
-
-
-def test_corr():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-    print(idadb)
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-    result_df = idadf.corr()
-    print(result_df)
-    assert result_df.shape[0] == 16, "number of rows are not matching"
-    assert result_df.shape[1] == 16, "number of columns are not matching"
-
-
-def test_train_test_split():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-    print(idadb)
-    if (idadb.exists_table("WEATHER_TRAIN")):
-        idadb.drop_table("WEATHER_TRAIN")
-
-    if (idadb.exists_table("WEATHER_TEST")):
-        idadb.drop_table("WEATHER_TEST")
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-    result = idadf.train_test_split(train_table='WEATHER_TRAIN', test_table='WEATHER_TEST', id='ID', fraction=0.75,
-                                    seed=42)
-
-    assert result[0] == 106645.0, "no of records are not matching"
-
-
-def test_cov():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-    print(idadb)
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-    result_df = idadf.cov()
-
-    print(result_df)
-    assert result_df.shape[0] == 16, "number of rows are not matching"
-    assert result_df.shape[1] == 16, "number of columns are not matching"
-
-
-def test_apply_weather_merge_withdf():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-    code_str_apply = """def apply_fun(self, x):
-            from math import sqrt
-            max_temp = x[3]
-            id = x[24]
-            fahren_max_temp = (max_temp*1.8)+32
-            row = [id, max_temp,  fahren_max_temp]
-            self.output(row)
-            """
-    output_signature = {'ID': 'int', 'MAX_TEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
-    nz_apply = NZFunApply(df=idadf, code_str=code_str_apply, fun_name='apply_fun',
-                          output_signature=output_signature, merge_output_with_df=True)
-    result = nz_apply.get_result()
-    print(result)
-    assert result.shape[0] == 142193, "number of records are not matching"
-    assert result.shape[1] == 27, "number of columns are not matching"
-
-
-def test_apply_weather_funstr():
-    idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True)
-    print(idadb)
-
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-    code_str_apply = """def apply_fun(self, x):
-                from math import sqrt
-                max_temp = x[3]
-                id = x[24]
-                fahren_max_temp = (max_temp*1.8)+32
-                row = [id, max_temp,  fahren_max_temp]
-                self.output(row)
-                """
-    output_signature = {'ID': 'int', 'MAX_TEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
-    nz_apply = NZFunApply(df=idadf, code_str=code_str_apply, fun_name='apply_fun',
-                          output_signature=output_signature)
-    result = nz_apply.get_result()
-    print(result)
-    assert result.shape[0] == 142193, "number of records are not matching"
-    assert result.shape[1] == 3, "number of columns are not matching"
-
-
-def test_apply_weather_funref():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-    print(idadb)
-
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-
-    def apply_fun(self, x):
-        from math import sqrt
-        max_temp = x[3]
-        id = x[24]
-        fahren_max_temp = (max_temp * 1.8) + 32
-        row = [id, max_temp, fahren_max_temp]
-        self.output(row)
-
-    output_signature = {'ID': 'int', 'MAX_TEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
-    nz_apply = NZFunApply(df=idadf, fun_ref=apply_fun,
-                          output_signature=output_signature)
-    result = nz_apply.get_result()
-    print(result)
-    assert result.shape[0] == 142193, "number of records are not matching"
-    assert result.shape[1] == 3, "number of columns are not matching"
-
-
-def test_tapply_weather_host_spus_train_pred():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-    print(idadb)
-
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-
-    code_str_host_spus = """def decision_tree_ml(self, df):
-             from sklearn.model_selection import cross_val_score
-             from sklearn.impute import SimpleImputer
-             from sklearn.tree import DecisionTreeClassifier
-             from sklearn.model_selection import train_test_split
-
-             from sklearn.preprocessing import LabelEncoder
-             import numpy as np
-
-
-
-             # data preparation
-             imputed_df = df.copy()
-             ds_size = len(imputed_df)
-             temp_dict = dict()
-
-
-             columns = imputed_df.columns
-
-             for column in columns:
-                 if column=='ID':
-                     continue
-
-                 if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-                   if imputed_df[column].isnull().sum()==len(imputed_df):
-                      imputed_df[column] = imputed_df[column].fillna(0)
-
-                   else :
-
-                      imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-                      transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))         
-                      imputed_df[column] = transformed_column
-
-                 if (imputed_df[column].dtype == 'object'):
-                     # impute missing values for categorical variables
-                     imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-                     imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                     imputed_df[column] = imputed_df[column].astype('str')
-                     le = LabelEncoder()
-
-                     le.fit(imputed_df[column])
-                     # print(le.classes_)
-                     imputed_df[column] = le.transform(imputed_df[column])
-                     temp_dict[column] = le
-
-
-
-             # Create a decision tree
-             dt = DecisionTreeClassifier(max_depth=5)
-             X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
-             y = imputed_df['RAINTOMORROW']
-             X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
-
-
-             dt.fit(X_train, y_train)
-
-             accuracy = dt.score(X_test, y_test)    
-             #print(accuracy)
-
-
-
-             pred_df = X_test.copy()
-
-
-             y_pred= dt.predict(X_test)
-
-             pred_df['RAINTOMORROW'] = y_pred
-             pred_df['DATASET_SIZE'] = ds_size
-             pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
-
-
-
-
-             original_columns = pred_df.columns
-
-             for column in original_columns:
-
-              if column in temp_dict:   
-                pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
-                #print(pred_df)
-
-             def print_output(x):
-                 row = [x['ID'], x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
-                 self.output(row)
-
-
-             pred_df.apply(print_output, axis=1)
-
-
-
-
-
-
- """
-
-    output_signature = {'ID': 'int', 'RAINTOMORROW_PRED': 'str', 'DATASET_SIZE': 'int', 'CLASSIFIER_ACCURACY': 'float'}
-    nz_tapply = NZFunTApply(df=idadf, code_str=code_str_host_spus, fun_name="decision_tree_ml", parallel=True,
-                            output_signature=output_signature)
-    result = nz_tapply.get_result()
-    print("Host +SPUs execution - slicing on a default column- ML function for the entire slices")
-    print(result)
-    assert result.shape[0] == 35551, "number of records are not matching"
-    assert result.shape[1] == 4, "number of columns are not matching"
-
-
-def test_groupedapply_weather_host_spus():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-    print(idadb)
-
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-
-    code_str_host_spus = """def decision_tree_ml(self, df):
-                from sklearn.model_selection import cross_val_score
-                from sklearn.impute import SimpleImputer
-                from sklearn.tree import DecisionTreeClassifier
-                from sklearn.model_selection import train_test_split
-
-                from sklearn.preprocessing import LabelEncoder
-                import numpy as np
-
-
-
-                # data preparation
-                imputed_df = df.copy()
-                ds_size = len(imputed_df)
-                temp_dict = dict()
-
-
-                columns = imputed_df.columns
-
-                for column in columns:
-                    if column=='ID':
-                        continue
-
-                    if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-                      if imputed_df[column].isnull().sum()==len(imputed_df):
-                         imputed_df[column] = imputed_df[column].fillna(0)
-
-                      else :
-
-                         imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-                         transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))         
-                         imputed_df[column] = transformed_column
-
-                    if (imputed_df[column].dtype == 'object'):
-                        # impute missing values for categorical variables
-                        imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-                        imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                        imputed_df[column] = imputed_df[column].astype('str')
-                        le = LabelEncoder()
-
-                        le.fit(imputed_df[column])
-                        # print(le.classes_)
-                        imputed_df[column] = le.transform(imputed_df[column])
-                        temp_dict[column] = le
-
-
-
-                # Create a decision tree
-                dt = DecisionTreeClassifier(max_depth=5)
-                X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
-                y = imputed_df['RAINTOMORROW']
-                X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
-
-
-                dt.fit(X_train, y_train)
-
-                accuracy = dt.score(X_test, y_test)    
-                #print(accuracy)
-
-
-
-                pred_df = X_test.copy()
-
-
-                y_pred= dt.predict(X_test)
-
-                pred_df['RAINTOMORROW'] = y_pred
-                pred_df['DATASET_SIZE'] = ds_size
-                pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
-
-
-
-
-                original_columns = pred_df.columns
-
-                for column in original_columns:
-
-                 if column in temp_dict:   
-                   pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
-                   #print(pred_df)
-
-                def print_output(x):
-                    row = [x['ID'], x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
-                    self.output(row)
-
-
-                pred_df.apply(print_output, axis=1)
-
-
-
-
-
-
-    """
-
-    output_signature = {'ID': 'int', 'RAINTOMORROW_PRED': 'str', 'DATASET_SIZE': 'int', 'CLASSIFIER_ACCURACY': 'float'}
-    import time
-    start = time.time()
-
-    nz_groupapply = NZFunGroupedApply(df=idadf, code_str=code_str_host_spus, index='LOCATION',
-                                      fun_name="decision_tree_ml", output_signature=output_signature,
-                                      merge_output_with_df=True)
-
-    result = nz_groupapply.get_result()
-    result = result.as_dataframe()
-    print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
-    print(result)
-    print(time.time() - start)
-    assert result.shape[0] == 35565, "number of records are not matching"
-    assert result.shape[1] == 28, "number of columns are not matching"
-
-
-def test_groupedapply_weather_host_spus_funref():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-    print(idadb)
-
-    idadf = IdaDataFrame(idadb, 'WEATHER')
-
-    def decision_tree_ml(self, df):
-        from sklearn.model_selection import cross_val_score
-        from sklearn.impute import SimpleImputer
-        from sklearn.tree import DecisionTreeClassifier
-        from sklearn.model_selection import train_test_split
-
-        from sklearn.preprocessing import LabelEncoder
-        import numpy as np
-
-        # data preparation
-        imputed_df = df.copy()
-        ds_size = len(imputed_df)
-        temp_dict = dict()
-
-        columns = imputed_df.columns
-
-        for column in columns:
-            if column == 'ID':
-                continue
-
-            if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
-                if imputed_df[column].isnull().sum() == len(imputed_df):
-                    imputed_df[column] = imputed_df[column].fillna(0)
-
-                else:
-
-                    imp = SimpleImputer(missing_values=np.nan, strategy='mean')
-                    transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                    imputed_df[column] = transformed_column
-
-            if (imputed_df[column].dtype == 'object'):
-                # impute missing values for categorical variables
-                imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
-                imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
-                imputed_df[column] = imputed_df[column].astype('str')
-                le = LabelEncoder()
-
-                le.fit(imputed_df[column])
-                # print(le.classes_)
-                imputed_df[column] = le.transform(imputed_df[column])
-                temp_dict[column] = le
-
-        # Create a decision tree
-        dt = DecisionTreeClassifier(max_depth=5)
-        X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
-        y = imputed_df['RAINTOMORROW']
-        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
-
-        dt.fit(X_train, y_train)
-
-        accuracy = dt.score(X_test, y_test)
-        # print(accuracy)
-
-        pred_df = X_test.copy()
-
-        y_pred = dt.predict(X_test)
-
-        pred_df['RAINTOMORROW'] = y_pred
-        pred_df['DATASET_SIZE'] = ds_size
-        pred_df['CLASSIFIER_ACCURACY'] = round(accuracy, 2)
-
-        original_columns = pred_df.columns
-
-        for column in original_columns:
-
-            if column in temp_dict:
-                pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
-                # print(pred_df)
-
-        def print_output(x):
-            row = [x['ID'], x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
-            self.output(row)
-
-        pred_df.apply(print_output, axis=1)
-
-    output_signature = {'ID': 'int', 'RAINTOMORROW_PRED': 'str', 'DATASET_SIZE': 'int', 'CLASSIFIER_ACCURACY': 'float'}
-    import time
-    start = time.time()
-
-    nz_groupapply = NZFunGroupedApply(df=idadf, index='LOCATION',
-                                      fun_ref=decision_tree_ml, output_signature=output_signature,
-                                      merge_output_with_df=True)
-
-    result = nz_groupapply.get_result()
-    print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
-    result = result.as_dataframe()
-    print(result)
-    print(time.time() - start)
-
-    assert result.shape[0] == 35565, "number of records are not matching"
-    assert result.shape[1] == 28, "number of columns are not matching"
-
-
-def test_install():
-    idadb = IdaDataBase(dsn, 'admin', 'password')
-    nzinstall = NZInstall(idadb, package_name='pandas')
-    result = nzinstall.getResultCode()
-    assert result == 0, "installation failed"
+import pytest
+from nzpyida import IdaDataBase, IdaDataFrame
+from nzpyida.ae import NZFunTApply, NZFunApply, NZFunGroupedApply
+from nzpyida.ae import NZInstall
+
+
+#nzpy dsn
+
+dsn= {
+    'host':'9.30.57.xxx',  # NOTE: replace with your test server name or IP
+    'port':5480,
+    'database':'telco',
+    'logLevel':0,
+    'securityLevel':0}
+
+#odbc dsn
+#dsn='weather'
+
+def test_tapply_host_weather_train_pred():
+    idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True)
+    print(idadb)
+
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+
+    code_str_host = """def decision_tree_ml_host(self, df):
+
+        from sklearn.model_selection import cross_val_score
+        from sklearn.impute import SimpleImputer
+        from sklearn.tree import DecisionTreeClassifier
+        from sklearn.model_selection import train_test_split
+
+        from sklearn.preprocessing import LabelEncoder
+        import numpy as np
+
+        result = df.groupby('LOCATION')
+        #result = df.groupby(pd.qcut(df['ID'], q=3))
+
+        # result = idadf.ida_query(query, autocommit=True)
+
+        for name, group in result:
+            # print(name)
+
+            # print(group)
+            def decision_tree_classifier(df):
+                imputed_df = df.copy()
+                ds_size = len(imputed_df)
+                temp_dict = dict()
+
+
+
+
+
+                columns = imputed_df.columns
+
+                for column in columns:
+                    if column=='ID':
+                        continue
+
+                    if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+                        if imputed_df[column].isnull().sum()==len(imputed_df):
+                         imputed_df[column] = imputed_df[column].fillna(0)
+
+                        else :
+                         imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+                         transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                         imputed_df[column] = transformed_column
+
+
+                    if (imputed_df[column].dtype == 'object'):
+                        # impute missing values for categorical variables
+                        imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+                        imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                        imputed_df[column] = imputed_df[column].astype('str')
+                        le = LabelEncoder()
+                        # print(imputed_df[column].unique())
+
+                        le.fit(imputed_df[column])
+                        # print(le.classes_)
+                        imputed_df[column] = le.transform(imputed_df[column])
+                        temp_dict[column] = le
+
+                X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
+                y = imputed_df['RAINTOMORROW']
+                X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
+                #X_train_mod = X_train.drop(['RISK_MM'],axis=1)
+                #X_test_mod = X_test.drop(['RISK_MM'],axis=1)
+                dt = DecisionTreeClassifier(max_depth=5)
+                dt.fit(X_train, y_train)
+
+                accuracy = dt.score(X_test, y_test)    
+                #print(accuracy)
+
+
+
+                pred_df = X_test.copy()
+
+
+                y_pred= dt.predict(X_test)
+
+                pred_df['RAINTOMORROW'] = y_pred
+                pred_df['DATASET_SIZE'] = ds_size
+                pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
+
+
+
+
+                original_columns = pred_df.columns
+
+                for column in original_columns:
+
+                 if column in temp_dict:   
+                   pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
+                   #print(pred_df)
+
+                def print_output(x):
+                    row = [x['ID'], x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
+                    self.output(row)
+
+
+                pred_df.apply(print_output, axis=1)
+
+                return pred_df
+
+
+            ml_result = decision_tree_classifier(df=group)
+
+            """
+
+    output_signature = {'ID': 'int', 'RAINTOMORROW_PRED': 'str', 'DATASET_SIZE': 'int', 'CLASSIFIER_ACCURACY': 'float'}
+
+    import time
+    start = time.time()
+
+    nz_tapply = NZFunTApply(df=idadf, code_str=code_str_host, fun_name='decision_tree_ml_host', parallel=False,
+                            output_signature=output_signature, merge_output_with_df=True)
+
+    result = nz_tapply.get_result()
+    print("\n")
+    print(result)
+    end = time.time()
+    print(end - start)
+    result = result.as_dataframe()
+    print(result)
+    print(result.shape[0])
+    assert result.shape[0] == 35565, "number of records are not matching"
+    assert result.shape[1] == 28, "number of columns are not matching"
+
+
+def test_apply_weather_save_table_merge_withdf_duplicate_columns():
+    idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True)
+
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+    code_str_apply = """def apply_fun(self, x):
+        from math import sqrt
+        max_temp = x[3]
+        id = x[24]
+        fahren_max_temp = (max_temp*1.8)+32
+        row = [id, max_temp,  fahren_max_temp]
+        self.output(row)
+        """
+    output_signature = {'ID': 'int', 'MAXTEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
+    output_table = "temp_conversion"
+    if idadb.exists_table(output_table):
+        idadb.drop_table(output_table)
+
+    with pytest.raises(ValueError) as exception_msg:
+        nz_apply = NZFunApply(df=idadf, code_str=code_str_apply, fun_name='apply_fun', output_table=output_table,
+                              output_signature=output_signature, merge_output_with_df=True)
+        result = nz_apply.get_result()
+
+    assert exception_msg.match("column MAXTEMP duplicated in the output table")
+
+
+def test_apply_weather_save_table_merge_withdf():
+    idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True)
+
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+    code_str_apply = """def apply_fun(self, x):
+        from math import sqrt
+        max_temp = x[3]
+        id = x[24]
+        fahren_max_temp = (max_temp*1.8)+32
+        row = [id, max_temp,  fahren_max_temp]
+        self.output(row)
+        """
+    output_signature = {'ID': 'int', 'MAX_TEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
+    output_table = "temp_conversion"
+    if idadb.exists_table(output_table):
+        idadb.drop_table(output_table)
+
+    nz_apply = NZFunApply(df=idadf, code_str=code_str_apply, fun_name='apply_fun', output_table=output_table,
+                          output_signature=output_signature, merge_output_with_df=True)
+    result = nz_apply.get_result()
+    result = result.as_dataframe()
+    print(result)
+    assert result.shape[0] == 142193, "number of records are not matching"
+    assert result.shape[1] == 27, "number of columns are not matching"
+
+
+def test_summary():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+    print(idadb)
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+    result = idadf.summary()
+    print(result)
+    assert result.shape[0] == 25, "number of rows are not matching"
+    assert result.shape[1] == 13, "number of columns are not matching"
+
+
+def test_corr():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+    print(idadb)
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+    result_df = idadf.corr()
+    print(result_df)
+    assert result_df.shape[0] == 16, "number of rows are not matching"
+    assert result_df.shape[1] == 16, "number of columns are not matching"
+
+
+def test_train_test_split():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+    print(idadb)
+    if (idadb.exists_table("WEATHER_TRAIN")):
+        idadb.drop_table("WEATHER_TRAIN")
+
+    if (idadb.exists_table("WEATHER_TEST")):
+        idadb.drop_table("WEATHER_TEST")
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+    result = idadf.train_test_split(train_table='WEATHER_TRAIN', test_table='WEATHER_TEST', id='ID', fraction=0.75,
+                                    seed=42)
+
+    assert result[0] == 106645.0, "no of records are not matching"
+
+
+def test_cov():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+    print(idadb)
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+    result_df = idadf.cov()
+
+    print(result_df)
+    assert result_df.shape[0] == 16, "number of rows are not matching"
+    assert result_df.shape[1] == 16, "number of columns are not matching"
+
+
+def test_apply_weather_merge_withdf():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+    code_str_apply = """def apply_fun(self, x):
+            from math import sqrt
+            max_temp = x[3]
+            id = x[24]
+            fahren_max_temp = (max_temp*1.8)+32
+            row = [id, max_temp,  fahren_max_temp]
+            self.output(row)
+            """
+    output_signature = {'ID': 'int', 'MAX_TEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
+    nz_apply = NZFunApply(df=idadf, code_str=code_str_apply, fun_name='apply_fun',
+                          output_signature=output_signature, merge_output_with_df=True)
+    result = nz_apply.get_result()
+    print(result)
+    assert result.shape[0] == 142193, "number of records are not matching"
+    assert result.shape[1] == 27, "number of columns are not matching"
+
+
+def test_apply_weather_funstr():
+    idadb = IdaDataBase(dsn, 'admin', 'password', verbose=True)
+    print(idadb)
+
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+    code_str_apply = """def apply_fun(self, x):
+                from math import sqrt
+                max_temp = x[3]
+                id = x[24]
+                fahren_max_temp = (max_temp*1.8)+32
+                row = [id, max_temp,  fahren_max_temp]
+                self.output(row)
+                """
+    output_signature = {'ID': 'int', 'MAX_TEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
+    nz_apply = NZFunApply(df=idadf, code_str=code_str_apply, fun_name='apply_fun',
+                          output_signature=output_signature)
+    result = nz_apply.get_result()
+    print(result)
+    assert result.shape[0] == 142193, "number of records are not matching"
+    assert result.shape[1] == 3, "number of columns are not matching"
+
+
+def test_apply_weather_funref():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+    print(idadb)
+
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+
+    def apply_fun(self, x):
+        from math import sqrt
+        max_temp = x[3]
+        id = x[24]
+        fahren_max_temp = (max_temp * 1.8) + 32
+        row = [id, max_temp, fahren_max_temp]
+        self.output(row)
+
+    output_signature = {'ID': 'int', 'MAX_TEMP': 'float', 'FAHREN_MAX_TEMP': 'float'}
+    nz_apply = NZFunApply(df=idadf, fun_ref=apply_fun,
+                          output_signature=output_signature)
+    result = nz_apply.get_result()
+    print(result)
+    assert result.shape[0] == 142193, "number of records are not matching"
+    assert result.shape[1] == 3, "number of columns are not matching"
+
+
+def test_tapply_weather_host_spus_train_pred():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+    print(idadb)
+
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+
+    code_str_host_spus = """def decision_tree_ml(self, df):
+             from sklearn.model_selection import cross_val_score
+             from sklearn.impute import SimpleImputer
+             from sklearn.tree import DecisionTreeClassifier
+             from sklearn.model_selection import train_test_split
+
+             from sklearn.preprocessing import LabelEncoder
+             import numpy as np
+
+
+
+             # data preparation
+             imputed_df = df.copy()
+             ds_size = len(imputed_df)
+             temp_dict = dict()
+
+
+             columns = imputed_df.columns
+
+             for column in columns:
+                 if column=='ID':
+                     continue
+
+                 if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+                   if imputed_df[column].isnull().sum()==len(imputed_df):
+                      imputed_df[column] = imputed_df[column].fillna(0)
+
+                   else :
+
+                      imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+                      transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))         
+                      imputed_df[column] = transformed_column
+
+                 if (imputed_df[column].dtype == 'object'):
+                     # impute missing values for categorical variables
+                     imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+                     imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                     imputed_df[column] = imputed_df[column].astype('str')
+                     le = LabelEncoder()
+
+                     le.fit(imputed_df[column])
+                     # print(le.classes_)
+                     imputed_df[column] = le.transform(imputed_df[column])
+                     temp_dict[column] = le
+
+
+
+             # Create a decision tree
+             dt = DecisionTreeClassifier(max_depth=5)
+             X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
+             y = imputed_df['RAINTOMORROW']
+             X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
+
+
+             dt.fit(X_train, y_train)
+
+             accuracy = dt.score(X_test, y_test)    
+             #print(accuracy)
+
+
+             dslices = self.getNumberOfDataSlices()
+             self.output(dslices, ds_size,accuracy)
+
+            
+
+
+
+
+             
+
+
+
+
+
+
+ """
+
+    output_signature = { "NUMBER_DATASLICES":'int','DATASET_SIZE': 'int', 'CLASSIFIER_ACCURACY': 'float', }
+    nz_tapply = NZFunTApply(df=idadf, code_str=code_str_host_spus, fun_name="decision_tree_ml", parallel=True,
+                            output_signature=output_signature)
+    result = nz_tapply.get_result()
+    print("Host +SPUs execution - slicing on a default column- ML function for the entire slices")
+    result = result.as_dataframe()
+    print(result)
+    dslices= result['NUMBER_DATASLICES'][0]
+    assert result.shape[0] == dslices, "number of data slices and ae instances are not matching"
+
+
+
+def test_groupedapply_weather_host_spus():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+    print(idadb)
+
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+
+    code_str_host_spus = """def decision_tree_ml(self, df):
+                from sklearn.model_selection import cross_val_score
+                from sklearn.impute import SimpleImputer
+                from sklearn.tree import DecisionTreeClassifier
+                from sklearn.model_selection import train_test_split
+
+                from sklearn.preprocessing import LabelEncoder
+                import numpy as np
+
+
+
+                # data preparation
+                imputed_df = df.copy()
+                ds_size = len(imputed_df)
+                temp_dict = dict()
+
+
+                columns = imputed_df.columns
+
+                for column in columns:
+                    if column=='ID':
+                        continue
+
+                    if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+                      if imputed_df[column].isnull().sum()==len(imputed_df):
+                         imputed_df[column] = imputed_df[column].fillna(0)
+
+                      else :
+
+                         imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+                         transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))         
+                         imputed_df[column] = transformed_column
+
+                    if (imputed_df[column].dtype == 'object'):
+                        # impute missing values for categorical variables
+                        imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+                        imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                        imputed_df[column] = imputed_df[column].astype('str')
+                        le = LabelEncoder()
+
+                        le.fit(imputed_df[column])
+                        # print(le.classes_)
+                        imputed_df[column] = le.transform(imputed_df[column])
+                        temp_dict[column] = le
+
+
+
+                # Create a decision tree
+                dt = DecisionTreeClassifier(max_depth=5)
+                X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
+                y = imputed_df['RAINTOMORROW']
+                X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=42, stratify=y)
+
+
+                dt.fit(X_train, y_train)
+
+                accuracy = dt.score(X_test, y_test)    
+                #print(accuracy)
+
+
+
+                pred_df = X_test.copy()
+
+
+                y_pred= dt.predict(X_test)
+
+                pred_df['RAINTOMORROW'] = y_pred
+                pred_df['DATASET_SIZE'] = ds_size
+                pred_df['CLASSIFIER_ACCURACY']=round(accuracy,2)
+
+
+
+
+                original_columns = pred_df.columns
+
+                for column in original_columns:
+
+                 if column in temp_dict:   
+                   pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
+                   #print(pred_df)
+
+                def print_output(x):
+                    row = [x['ID'], x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
+                    self.output(row)
+
+
+                pred_df.apply(print_output, axis=1)
+
+
+
+
+
+
+    """
+
+    output_signature = {'ID': 'int', 'RAINTOMORROW_PRED': 'str', 'DATASET_SIZE': 'int', 'CLASSIFIER_ACCURACY': 'float'}
+    import time
+    start = time.time()
+
+    nz_groupapply = NZFunGroupedApply(df=idadf, code_str=code_str_host_spus, index='LOCATION',
+                                      fun_name="decision_tree_ml", output_signature=output_signature,
+                                      merge_output_with_df=True)
+
+    result = nz_groupapply.get_result()
+    result = result.as_dataframe()
+    print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
+    print(result)
+    print(time.time() - start)
+    assert result.shape[0] == 35565, "number of records are not matching"
+    assert result.shape[1] == 28, "number of columns are not matching"
+
+
+def test_groupedapply_weather_host_spus_funref():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+    print(idadb)
+
+    idadf = IdaDataFrame(idadb, 'WEATHER')
+
+    def decision_tree_ml(self, df):
+        from sklearn.model_selection import cross_val_score
+        from sklearn.impute import SimpleImputer
+        from sklearn.tree import DecisionTreeClassifier
+        from sklearn.model_selection import train_test_split
+
+        from sklearn.preprocessing import LabelEncoder
+        import numpy as np
+
+        # data preparation
+        imputed_df = df.copy()
+        ds_size = len(imputed_df)
+        temp_dict = dict()
+
+        columns = imputed_df.columns
+
+        for column in columns:
+            if column == 'ID':
+                continue
+
+            if (imputed_df[column].dtype == 'float64' or imputed_df[column].dtype == 'int64'):
+                if imputed_df[column].isnull().sum() == len(imputed_df):
+                    imputed_df[column] = imputed_df[column].fillna(0)
+
+                else:
+
+                    imp = SimpleImputer(missing_values=np.nan, strategy='mean')
+                    transformed_column = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                    imputed_df[column] = transformed_column
+
+            if (imputed_df[column].dtype == 'object'):
+                # impute missing values for categorical variables
+                imp = SimpleImputer(missing_values=None, strategy='constant', fill_value='missing')
+                imputed_df[column] = imp.fit_transform(imputed_df[column].values.reshape(-1, 1))
+                imputed_df[column] = imputed_df[column].astype('str')
+                le = LabelEncoder()
+
+                le.fit(imputed_df[column])
+                # print(le.classes_)
+                imputed_df[column] = le.transform(imputed_df[column])
+                temp_dict[column] = le
+
+        # Create a decision tree
+        dt = DecisionTreeClassifier(max_depth=5)
+        X = imputed_df.drop(['RISK_MM', 'RAINTOMORROW'], axis=1)
+        y = imputed_df['RAINTOMORROW']
+        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
+
+        dt.fit(X_train, y_train)
+
+        accuracy = dt.score(X_test, y_test)
+        # print(accuracy)
+
+        pred_df = X_test.copy()
+
+        y_pred = dt.predict(X_test)
+
+        pred_df['RAINTOMORROW'] = y_pred
+        pred_df['DATASET_SIZE'] = ds_size
+        pred_df['CLASSIFIER_ACCURACY'] = round(accuracy, 2)
+
+        original_columns = pred_df.columns
+
+        for column in original_columns:
+
+            if column in temp_dict:
+                pred_df[column] = temp_dict[column].inverse_transform(pred_df[column])
+                # print(pred_df)
+
+        def print_output(x):
+            row = [x['ID'], x['RAINTOMORROW'], x['DATASET_SIZE'], x['CLASSIFIER_ACCURACY']]
+            self.output(row)
+
+        pred_df.apply(print_output, axis=1)
+
+    output_signature = {'ID': 'int', 'RAINTOMORROW_PRED': 'str', 'DATASET_SIZE': 'int', 'CLASSIFIER_ACCURACY': 'float'}
+    import time
+    start = time.time()
+
+    nz_groupapply = NZFunGroupedApply(df=idadf, index='LOCATION',
+                                      fun_ref=decision_tree_ml, output_signature=output_signature,
+                                      merge_output_with_df=True)
+
+    result = nz_groupapply.get_result()
+    print("Host+ SPUs execution - slicing on user selection -ML function for partitions within slices\n")
+    result = result.as_dataframe()
+    print(result)
+    print(time.time() - start)
+
+    assert result.shape[0] == 35565, "number of records are not matching"
+    assert result.shape[1] == 28, "number of columns are not matching"
+
+
+def test_install():
+    idadb = IdaDataBase(dsn, 'admin', 'password')
+    nzinstall = NZInstall(idadb, package_name='pandas')
+    result = nzinstall.getResultCode()
+    assert result == 0, "installation failed"
```

### Comparing `nzpyida-0.2.2.6/nzpyida/aggregation.py` & `nzpyida-0.3.3/nzpyida/aggregation.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,161 +1,161 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-The module contains the function that is used to modify or
-create columns in an IdaDataFrame based on aggreation
-"""
-
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-from builtins import str
-from future import standard_library
-standard_library.install_aliases()
-
-from numbers import Number
-from collections import OrderedDict
-
-import nzpyida
-from nzpyida.exceptions import IdaDataBaseError
-
-def aggregate_idadf(idadf, method, other, swap = False):
-    """
-    Modify internal state variables to represent the aggregation of columns
-    of an IdaDataFrame or IdaSeries in ibmdbpy.
-
-    The following comparison operators are supported: +, \*, /, -, //, %, \*\*.
-
-    The syntax is similar to Pandas.
-
-    Parameters
-    ----------
-    idadf : IdaDataFrame or IdaSeries
-        IdaDataFrame or IdaSerie on the left (if swap is False)
-    method : str
-        Aggregation method that is computed: the following values are 
-        admissible: "add","mul","div","sub","floordiv","mod","neg","pow"
-    other: Number or IdaDataFrame or IdaSeries
-        Another object that idadf will be aggregated with (on the right if swap is False).
-    swap : bool, default: False
-        Internally used to handle cases where the call is made reflexively,
-        that is when the main IdaDataFrame/IdaSeries is not on the left.
-        If swap is True, this also implies that other is not of type IdaDataFrame/IdaSeries.
-
-    Returns
-    -------
-    Aggregated IdaDataFrame or IdaSeries
-
-    Raises
-    ------
-    ValueError
-        Aggregation method not supported.
-    TypeError
-         Type not supported for aggregation.
-
-    Examples
-    --------
-    >>> idairis['SepalLength'] = idairis['SepalLength'] * 2
-    ...
-
-    Notes
-    -----
-    It is not possible to create aggregations between columns that are stored 
-    in different Db2 Warehouse tables.
-
-    """
-    def swap_manager(left, right, swap = False):
-        if swap:
-            left, right = right, left
-        return (left, right)
-
-    #Swap values in case of reflexive call
-    # TODO : Override in IdaSeries instead of including the logic here.
-    if swap:
-        idadf, other = other, idadf
-
-
-    if self._idadb._is_netezza_system():
-        power_function = " POW"
-    else:
-        power_function = " POWER"
-
-    simplemethod = {"add": " + ", "mul": " * ",  "div": " / ", "sub": " - "}
-    complexmethod = {"floordiv" : " FLOOR(%s/%s) ",
-                     "mod" : " MOD(%s,%s) ",
-                     "neg" : " -%s%s ",
-                     "pow" : power_function + "(%s,%s)"} # overflow risk, to handle
-
-    all_methods = list(simplemethod.keys())+list(complexmethod.keys())
-    if method not in all_methods:
-        raise ValueError("Admissible values for method argument are %s." %str(all_methods)[1:-1])
-
-    columndict = OrderedDict()
-
-    if isinstance(idadf, nzpyida.IdaDataFrame):
-
-        for index, column in enumerate(idadf.internal_state.columndict.keys()):
-            column_value = idadf.internal_state.columndict[column]
-            if other is None: # this is for now just the neg case
-                left, right = swap_manager(column_value, '')
-            elif isinstance(other, Number):
-                left, right = swap_manager(column_value, other, swap)
-            elif isinstance(other, nzpyida.IdaSeries):
-                left, right = swap_manager(column_value, "%s"%list(other.internal_state.columndict.values())[0], swap)
-            elif isinstance(other, nzpyida.IdaDataFrame):
-                if len(idadf.columns) != len(other.columns):
-                    if len(other.columns) != 1:
-                        raise IdaDataBaseError("Number of columns of other "+
-                                               "IdaDataFrame should be either "+
-                                               "equal to aggregated IdaDataFrame"+
-                                               "or equal to 1.")
-                    left, right = swap_manager("%s"%column_value, "%s"%list(other.internal_state.columndict.values())[0], swap)
-                else:
-                    left, right = swap_manager("%s"%column_value, "%s"%list(other.internal_state.columndict.values())[index], swap)
-            else:
-                raise TypeError("Aggregation method not supported. Unsupported type for aggregation: %s"%type(other))
-
-            if method in simplemethod:
-                columndict[column] = "(%s%s%s)"%(left, simplemethod[method], right)
-            elif method in complexmethod:
-                agg = complexmethod[method] %(left, right)
-                columndict[column] = "(%s)"%agg
-
-        newidadf = idadf._clone()
-        for key,value in columndict.items():
-            newidadf.internal_state.columndict[key] = value
-
-        newidadf.internal_state.update()
-        # REMARK: Don't need to reset some attributes ?
-        return newidadf
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        columnname = idadf.internal_state.columndict.keys()[0]
-        if other is None: # this is for now just the neg case
-            left, right = swap_manager("\"%s\""%columnname, '')
-        elif isinstance(other, Number):
-            left, right = swap_manager("\"%s\""%columnname, other, swap)
-        elif isinstance(other, nzpyida.IdaSeries):
-            left, right = swap_manager("\"%s\""%columnname, "\"%s\""%other.columns[0], swap)
-        else:
-            raise TypeError("Type not supported for aggregation: " + str(type(other)))
-
-        if method in simplemethod:
-            columndict[columnname] = "(%s%s%s)"%(left, simplemethod[method], right)
-        elif method in complexmethod:
-            agg = complexmethod[method] %(left, right)
-            columndict[columnname] = "(%s)"%agg
-
-        newidaseries = idadf._clone()
-        newidaseries.internal_state.columndict[key] = columndict[columnname]
-        newidaseries.internal_state.update()
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+The module contains the function that is used to modify or
+create columns in an IdaDataFrame based on aggreation
+"""
+
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+from builtins import str
+from future import standard_library
+standard_library.install_aliases()
+
+from numbers import Number
+from collections import OrderedDict
+
+import nzpyida
+from nzpyida.exceptions import IdaDataBaseError
+
+def aggregate_idadf(idadf, method, other, swap = False):
+    """
+    Modify internal state variables to represent the aggregation of columns
+    of an IdaDataFrame or IdaSeries in ibmdbpy.
+
+    The following comparison operators are supported: +, \*, /, -, //, %, \*\*.
+
+    The syntax is similar to Pandas.
+
+    Parameters
+    ----------
+    idadf : IdaDataFrame or IdaSeries
+        IdaDataFrame or IdaSerie on the left (if swap is False)
+    method : str
+        Aggregation method that is computed: the following values are 
+        admissible: "add","mul","div","sub","floordiv","mod","neg","pow"
+    other: Number or IdaDataFrame or IdaSeries
+        Another object that idadf will be aggregated with (on the right if swap is False).
+    swap : bool, default: False
+        Internally used to handle cases where the call is made reflexively,
+        that is when the main IdaDataFrame/IdaSeries is not on the left.
+        If swap is True, this also implies that other is not of type IdaDataFrame/IdaSeries.
+
+    Returns
+    -------
+    Aggregated IdaDataFrame or IdaSeries
+
+    Raises
+    ------
+    ValueError
+        Aggregation method not supported.
+    TypeError
+         Type not supported for aggregation.
+
+    Examples
+    --------
+    >>> idairis['SepalLength'] = idairis['SepalLength'] * 2
+    ...
+
+    Notes
+    -----
+    It is not possible to create aggregations between columns that are stored 
+    in different Db2 Warehouse tables.
+
+    """
+    def swap_manager(left, right, swap = False):
+        if swap:
+            left, right = right, left
+        return (left, right)
+
+    #Swap values in case of reflexive call
+    # TODO : Override in IdaSeries instead of including the logic here.
+    if swap:
+        idadf, other = other, idadf
+
+
+    if idadf._idadb._is_netezza_system():
+        power_function = " POW"
+    else:
+        power_function = " POWER"
+
+    simplemethod = {"add": " + ", "mul": " * ",  "div": " / ", "sub": " - "}
+    complexmethod = {"floordiv" : " FLOOR(%s/%s) ",
+                     "mod" : " MOD(%s,%s) ",
+                     "neg" : " -%s%s ",
+                     "pow" : power_function + "(%s,%s)"} # overflow risk, to handle
+
+    all_methods = list(simplemethod.keys())+list(complexmethod.keys())
+    if method not in all_methods:
+        raise ValueError("Admissible values for method argument are %s." %str(all_methods)[1:-1])
+
+    columndict = OrderedDict()
+
+    if isinstance(idadf, nzpyida.IdaDataFrame):
+
+        for index, column in enumerate(idadf.internal_state.columndict.keys()):
+            column_value = idadf.internal_state.columndict[column]
+            if other is None: # this is for now just the neg case
+                left, right = swap_manager(column_value, '')
+            elif isinstance(other, Number):
+                left, right = swap_manager(column_value, other, swap)
+            elif isinstance(other, nzpyida.IdaSeries):
+                left, right = swap_manager(column_value, "%s"%list(other.internal_state.columndict.values())[0], swap)
+            elif isinstance(other, nzpyida.IdaDataFrame):
+                if len(idadf.columns) != len(other.columns):
+                    if len(other.columns) != 1:
+                        raise IdaDataBaseError("Number of columns of other "+
+                                               "IdaDataFrame should be either "+
+                                               "equal to aggregated IdaDataFrame"+
+                                               "or equal to 1.")
+                    left, right = swap_manager("%s"%column_value, "%s"%list(other.internal_state.columndict.values())[0], swap)
+                else:
+                    left, right = swap_manager("%s"%column_value, "%s"%list(other.internal_state.columndict.values())[index], swap)
+            else:
+                raise TypeError("Aggregation method not supported. Unsupported type for aggregation: %s"%type(other))
+
+            if method in simplemethod:
+                columndict[column] = "(%s%s%s)"%(left, simplemethod[method], right)
+            elif method in complexmethod:
+                agg = complexmethod[method] %(left, right)
+                columndict[column] = "(%s)"%agg
+
+        newidadf = idadf._clone()
+        for key,value in columndict.items():
+            newidadf.internal_state.columndict[key] = value
+
+        newidadf.internal_state.update()
+        # REMARK: Don't need to reset some attributes ?
+        return newidadf
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        columnname = idadf.internal_state.columndict.keys()[0]
+        if other is None: # this is for now just the neg case
+            left, right = swap_manager("\"%s\""%columnname, '')
+        elif isinstance(other, Number):
+            left, right = swap_manager("\"%s\""%columnname, other, swap)
+        elif isinstance(other, nzpyida.IdaSeries):
+            left, right = swap_manager("\"%s\""%columnname, "\"%s\""%other.columns[0], swap)
+        else:
+            raise TypeError("Type not supported for aggregation: " + str(type(other)))
+
+        if method in simplemethod:
+            columndict[columnname] = "(%s%s%s)"%(left, simplemethod[method], right)
+        elif method in complexmethod:
+            agg = complexmethod[method] %(left, right)
+            columndict[columnname] = "(%s)"%agg
+
+        newidaseries = idadf._clone()
+        newidaseries.internal_state.columndict[key] = columndict[columnname]
+        newidaseries.internal_state.update()
         return newidaseries
```

### Comparing `nzpyida-0.2.2.6/nzpyida/base.py` & `nzpyida-0.3.3/nzpyida/base.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,2465 +1,2465 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-# For local installation do "pip install -e" in the ibmdbpy-master directory
-###############################################################################
-
-"""
-An IdaDataBase instance represents a reference to a remote Netezza Warehouse database
-maintaining attributes and methods for administration of the database.
-"""
-
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-from builtins import zip
-from builtins import int
-from builtins import str
-from future import standard_library
-standard_library.install_aliases()
-
-import os
-#from os import path
-import sys
-import math
-import random
-from time import time
-import datetime
-import warnings
-from copy import deepcopy
-
-from collections import OrderedDict
-
-
-import numpy as np
-import pandas as pd
-from pandas.io.sql import read_sql
-
-from lazy import lazy
-import six
-
-import nzpyida
-from nzpyida import sql
-from nzpyida.utils import timed, set_verbose, set_autocommit
-from nzpyida.exceptions import IdaDataBaseError, PrimaryKeyError
-
-class IdaDataBase(object):
-    """
-    An IdaDataBase instance represents a reference to a remote Netezza Warehouse
-    database. This is an abstraction layer for the remote connection. The
-    IdaDataBase interface provides several functions that enable basic database 
-    administration in pythonic syntax.
-
-    You can use either ODBC or JDBC or NZPY to connect to the database. The default
-    connection type is ODBC, which is the standard connection type for Windows 
-    users. To establish an ODBC connection, download an IBM Netezza driver and set
-    up your ODBC connection by specifying your connection protocol, port, and 
-    hostname. An ODBC connection on Linux or Mac might require more settings. 
-    For more information about how to establish an ODBC connection, see the 
-    pypyodbc documentation.
-
-    To connect with JDBC, install the optional external package jaydebeapi, 
-    download the ibm jdbc driver, and save it in your local nzpyida folder.
-    If you put the jdbc driver in the CLASSPATH variable or the folder that
-    contains it, it will work too. A C++ compiler adapted to the current python 
-    version, operating system, and architecture may also be required to install
-    jaydebeapi.
-
-    To connect with NZPY, specify the required parameters in dict format, there is
-    no need to install any drivers.
-
-    The instantiation of an IdaDataBase object is a mandatory step before 
-    creating IdaDataFrame objects because IdaDataFrames require an IdaDataBase 
-    as a parameter to be initialized. By convention, use only one instance of 
-    IdaDataBase per database. However, you can use several instances of 
-    IdaDataFrame per connection.
-    """
-
-    def __init__(self, dsn, uid='', pwd='', autocommit=True, verbose=False):
-        """
-        Open a database connection.
-
-        Parameters
-        ----------
-        dsn : str
-            Data Source Name (as specified in your ODBC settings) or JDBC URL 
-            string or NZPY dict
-
-        uid : str, optional
-            User ID.
-
-        pwd : str, optional
-            User password.
-
-        autocommit : bool, default: True
-            If True, automatically commits all operations.
-
-        verbose : bool, defaukt: True
-            If True, prints all SQL requests that are sent to the database. 
-
-        Attributes
-        ----------
-        data_source_name : str
-            Name of the referring DataBase.
-
-        _con_type : str
-            Type of the connection, either 'odbc' or 'jdbc' or 'nzpy'.
-
-        _connection_string : str
-            Connection string use for connecting via ODBC or JDBC or NZPY.
-
-        _con : connection object
-            Connection object to the remote Database.
-
-        _database_system: str
-            Underlying database system, either 'db2' or 'netezza'
-
-        _database_name: str
-            The name of the database the application is connected to.
-
-        _idadfs : list
-            List of IdaDataFrame objects opened under this connection.
-
-        Returns
-        -------
-        IdaDataBase object
-
-        Raises
-        ------
-        ImportError
-            JayDeBeApi is not installed.
-        IdaDataBaseError
-            * uid and pwd are defined both in uid, pwd parameters and dsn.
-            * The 'db2jcc4.jar' file is not in the ibmdbpy site-package repository.
-
-        Examples
-        --------
-
-        ODBC connection, userID and password are stored in ODBC settings:
-
-        >>> IdaDataBase(dsn="BLUDB") # ODBC Connection
-        <ibmdbpy.base.IdaDataBase at 0x9bec860>
-
-        ODBC connection, userID and password are not stored in ODBC settings:
-
-        >>> IdaDataBase(dsn="BLUDB", uid="<UID>", pwd="<PWD>")
-        <ibmdbpy.base.IdaDataBase at 0x9bec860>
-        
-        JDBC connection, full JDBC string:
-
-        >>> jdbc='jdbc:db2://<HOST>:<PORT>/<DBNAME>:user=<UID>;password=<PWD>'
-        >>> IdaDataBase(dsn=jdbc)
-        <ibmdbpy.base.IdaDataBase at 0x9bec860>
-
-        JDBC connectiom, JDBC string and seperate userID and password:
-
-        >>> jdbc = 'jdbc:db2://<HOST>:<PORT>/<DBNAME>'
-        >>> IdaDataBase(dsn=jdbc, uid="<UID>", pwd="<PWD>")
-        <ibmdbpy.base.IdaDataBase at 0x9bec860>
-
-
-         NZPY connection, userID and password are stored in NZPY dict:
-
-         nzpy_dsn ={
-        "database":"<DBNAME>",
-         "port" :<PORT>,
-        "host" : "<HOST>",
-        "securityLevel":<SECURITYLEVEL>,
-        "logLevel":<LOGLEVEL>,
-        "user":"<UID>",
-        "password":"<PWD>"
-        }
-
-        >>> IdaDataBase(dsn=nzpy_dsn)
-        <ibmdbpy.base.IdaDataBase at 0x9bec860>
-
-
-        NZPY connection, NZPY dict and seperate userID and password
-
-         nzpy_dsn ={
-        "database":"<DBNAME>",
-         "port" :<PORT>,
-        "host" : "<HOST>",
-        "securityLevel":<SECURITYLEVEL>,
-        "logLevel":<LOGLEVEL>
-
-        }
-
-        >>> IdaDataBase(dsn=nzpy_dsn, uid="<UID>", pwd="<PWD>")
-
-
-
-        """
-
-        if isinstance(dsn, dict)==False:
-            for arg,name in zip([dsn, uid, pwd],['dsn','uid','pwd']):
-              if not isinstance(arg, six.string_types):
-                raise TypeError("Argument '%s' of type %, expected : string type."%(name,type(arg)))
-
-        self.data_source_name = dsn
-
-
-        # default value for _database_system is db2
-        self._database_system = 'db2'
-        # first delimiter before the parameters in the jdbc-url
-        url_1stparam_del = ':'
-
-
-        # Detect if user attempt to connection with ODBC or JDBC
-        if isinstance(dsn,str):
-            if dsn.startswith('jdbc:'):
-              self._con_type = "jdbc"
-              if dsn.startswith('jdbc:netezza:'):
-                  self._database_system = 'netezza'
-                  # for Netezza the internal connection is always in autocommit mode
-                  # to allow explicit commits
-                  dsn += ';autocommit=false'
-                  url_1stparam_del = ';'
-              elif dsn.startswith('jdbc:db2:'):
-                  self._database_system = 'db2'
-              else:
-                  raise IdaDataBaseError(("The JDBC connection string is invalid for Db2 and Netezza. " +
-                                          "It has to start either with 'jdbc:db2:' or 'jdbc:netezza:'."))
-
-            else:
-                self._con_type='odbc'
-
-        elif isinstance(dsn, dict):
-            self._con_type = "nzpy"
-
-
-        self._idadfs = []
-
-
-
-        if self._con_type == 'nzpy':
-            #parse dict
-            host=''
-            port=0
-            database=''
-            securityLevel=0
-            logLevel=0
-            user =''
-            password=''
-            for key,value in dsn.items():
-                if(key=='host'):
-                    host=value
-                if (key=='port'):
-                    port=value
-                if (key=='database'):
-                    database=value
-                if (key=='securityLevel') :
-                    securityLevel=value
-                if (key=='logLevel') :
-                    logLevel=value
-                if (key == 'user'):
-                    user = value
-                if (key == 'password'):
-                    password = value
-
-            import nzpy;
-            missingCredentialsMsg = ("Missing credentials to connect via NZPY.")
-            ambiguousDefinitionMsg = ("Ambiguous definition of userID or password: " +
-                                      "Cannot be defined in uid and pwd parameters " +
-                                      "and in nzpy dict at the same time.")
-
-
-            uidSpecified = uid != ''
-            pwdSpecified = pwd != ''
-
-            if not ('user' in dsn):
-                if (uidSpecified):
-                    user= uid
-            elif (uidSpecified):
-                raise IdaDataBaseError(ambiguousDefinitionMsg)
-            uidSpecified = uidSpecified | ('user' in dsn)
-
-            if not ('password' in dsn):
-                if (pwdSpecified):
-                   password= pwd
-            elif (pwdSpecified):
-                raise IdaDataBaseError(ambiguousDefinitionMsg)
-            pwdSpecified = pwdSpecified | ('password' in dsn)
-
-            # throw an exception if either uid or pwd are specified,
-            # i.e. not both or none of the two
-            if (uidSpecified ^ pwdSpecified):
-                raise IdaDataBaseError(missingCredentialsMsg)
-
-
-
-            try:
-                if port <= 0:
-                    port = 5480
-                self._con = nzpy.connect(user=user, password=password,host=host, port=port,
-                                        database=database, securityLevel=securityLevel,logLevel=logLevel)
-            except Exception as e:
-                raise IdaDataBaseError(str(e))
-            self._connection_string ={'user':user,'password':password,'host':host,
-                'port':port, 'database':database, 'securityLevel':securityLevel,'logLevel':logLevel}
-            self._database_system = 'netezza'
-
-        if self._con_type == 'odbc' :
-
-            self._connection_string = "DSN=%s; UID=%s; PWD=%s;LONGDATACOMPAT=1;"%(dsn,uid,pwd)
-            """
-            Workaround for CLOB retrieval: 
-            Set the CLI/ODBC LongDataCompat keyword to 1. 
-            Doing so will force the CLI driver to make the 
-            following data type mappings
-            SQL_CLOB to SQL_LONGVARCHAR
-            SQL_BLOB to SQL_LONGVARBINARY
-            SQL_DBCLOB to SQL_WLONGVARCHAR
-            """
-
-            import pyodbc
-            try:
-                self._con = pyodbc.connect(self._connection_string)
-                self._con.setdecoding(pyodbc.SQL_CHAR, encoding='utf-8')
-                self._con.setdecoding(pyodbc.SQL_WCHAR, encoding='utf-8')
-                self._con.setencoding(encoding='utf-8')
-            except Exception as e:
-                raise IdaDataBaseError(str(e))
-            try:
-                self.ida_query("select count(*) from _V_OBJECT")
-                self._database_system = 'netezza'
-            except  Exception as e1:
-                try:
-                    self.ida_query("select CURRENT_SERVER from SYSIBM.SYSDUMMY1")
-                    self._database_system = 'db2'
-                except Exception as e2:
-                    errorMsg = ("The following errors occurred when trying to determine " +
-                                "if the database system is Netezza or Db2:\n" +
-                                "%s\n%s") % (str(e1), str(e2))
-                    raise IdaDataBaseError(errorMsg)
-
-        if self._con_type == 'jdbc':
-
-            missingCredentialsMsg = ("Missing credentials to connect via JDBC.")
-            ambiguousDefinitionMsg = ("Ambiguous definition of userID or password: " +
-                                      "Cannot be defined in uid and pwd parameters " +
-                                      "and in jdbc_url_string at the same time.")
-
-            # remove trailing ":" or ";" or spaces on dsn, we will replace.
-            dsn = dsn.rstrip(';: ')
-            # find parameters on dsn; if any exist, there will be an equals sign.
-            ix = dsn.find("=")
-
-            # if no parameters exist, then this is the complete dsn
-            if (ix < 0):
-                # nothing needs to be done, if uid and pwd are missing
-                # (uid and pwd are not needed for local database connections)
-                # otherwise both uid and pwd have to be specified
-                if uid and pwd:
-                    dsn = dsn + url_1stparam_del + 'user={};password={};'.format(uid, pwd)
-                elif not uid and not pwd:
-                   # Neither UID nor PWD have to be specified for local connections.
-                   # This assumes that if they're missing, the connection is local.
-                    pass
-                else:
-                    raise IdaDataBaseError(missingCredentialsMsg)
-            else:
-                # if we know there is at least one parameter; we can assume there exists a ":" before the parameter
-                # portion of the string in a correctly formatted dsn.  Therefore, just check for the existence of 
-                # the uid and pwd and add if they are missing.  If they are on the string, IGNORE the string as-is.
-
-                uidSpecified = uid != ''
-                pwdSpecified = pwd != ''
-
-                if not ('user=' in dsn):
-                    if (uidSpecified):
-                       dsn = dsn + ';user=' + uid
-                elif (uidSpecified):
-                    raise IdaDataBaseError(ambiguousDefinitionMsg)
-                uidSpecified = uidSpecified | ('user=' in dsn)
-
-                if not ('password=' in dsn):
-                    if (pwdSpecified):
-                        dsn = dsn + ';password=' + pwd
-                elif (pwdSpecified):
-                    raise IdaDataBaseError(ambiguousDefinitionMsg)
-                pwdSpecified = pwdSpecified | ('password=' in dsn)
-
-                # throw an exception if either uid or pwd are specified,
-                # i.e. not both or none of the two
-                if (uidSpecified^pwdSpecified):
-                    raise IdaDataBaseError(missingCredentialsMsg)
-
-                 # add trailing ";" to dsn.
-                dsn = dsn + ';'
-
-            jdbc_url = dsn
-
-            try:
-                import jaydebeapi
-                import jpype
-            except ImportError:
-                raise ImportError("Please install optional dependency jaydebeapi "+
-                            "to work with JDBC.")
-
-            # check versions
-            if jaydebeapi.__version__.startswith('0'):
-                # Older JaydeBeAPi versions where the connection information is specified through a list
-                # are not supported anymore.
-                # The connection information has to be included now in a single connection string.
-                message = ("Your JayDeBeApi module is not supported anymore.  Please install version 1.x or higher.")
-                raise IdaDataBaseError(message)
-
-            if jpype.__version__ != '0.6.3':
-                message = ("Your JPype1 version is not compatible with JayDeBeApi. Please install version 0.6.3.")
-                raise IdaDataBaseError(message)
-
-            here = os.path.abspath(os.path.dirname(__file__))
-            driver_not_found = ""
-            if self._is_netezza_system():
-                driverlibs = ["nzjdbc3.jar"]
-            else:
-                driverlibs = ["db2jcc4.jar", "db2jcc.jar"]
-
-            if (not jpype.isJVMStarted()):
-                classpath = os.getenv('CLASSPATH','')
-                jarpath = ''
-                platform = sys.platform
-
-                if verbose: print("Trying to find a path to the JDBC driver jar file in CLASSPATH (%s)."%classpath)
-
-                if platform == 'win32':
-                    classpaths = classpath.split(';')
-                else:
-                    classpaths = classpath.split(':')
-
-                if any(dl for dl in driverlibs if dl in classpath):
-                    # A path to the driver exists in the classpath variable
-                    jarpaths =  [jp for jp in classpaths if any([dl in jp for dl in driverlibs])]
-                    jarpath = ""
-                    
-                    while jarpaths: # check if a least one path exists
-                        jarpath = jarpaths.pop(0) # just take the first
-                        if os.path.isfile(jarpath): # Is the path correct?
-                            if platform == 'win32':
-                                jarpath = jarpath.split(':')[1].replace('\\', '/') # get rid of windows style formatting
-                            break
-                        else:
-                            if verbose: print("The path %s does not seem to be correct.\nTrying to recover..."%jarpath)
-                            jarpath = ""
-
-                if not jarpath: # Means the search for a direct path in the CLASSPATH was not successful
-                    def _get_jdbc_driver_from_folders(folders):
-                        jarpath = ''
-                        if platform == 'win32':
-                            jarpaths = [fld.split(':')[1].replace('\\', '/') + "/" + dl
-                                        for fld in folders for dl in driverlibs if os.path.isfile(fld + "/" + dl)]
-                            if jarpaths == []:
-                                # check if classpath contains paths with wildcard "*" at the end
-                                jarpaths = [fld.split(':')[1].replace('\\', '/')[:-1] + dl
-                                            for fld in folders for dl in driverlibs if
-                                            fld[-1:] == '*' and os.path.isfile(fld + "/" + dl)]
-                        else:
-                            jarpaths = [fld + "/" + dl for fld in folders for dl in driverlibs if
-                                        os.path.isfile(fld + "/" + dl)]
-                            if jarpaths == []:
-                                # check if classpath contains paths with wildcard "*" at the end
-                                jarpaths = [fld[:-1] + dl
-                                            for fld in folders for dl in driverlibs if
-                                            fld[-1:] == '*' and os.path.isfile(fld[:-1] + dl)]
-                        if jarpaths:
-                            return jarpaths[0]
-                        
-                    if classpath: # There is at least something in the classpath variable
-                        # Let us see if the jar in a folder of the classpath
-                        if verbose: print("Trying to find the JDBC driver in the folders of CLASSPATH (%s)."%classpath)
-
-                        jarpath = _get_jdbc_driver_from_folders(classpaths)
-                    
-                        if not jarpath: # jarpath is still ''
-                            if verbose: print("Trying to find the JDBC driver in the local folder of ibmdbpy (%s)"%here)
-                            # Try to get the path to the driver from the ibmdbpy folder, the last chance
-                            jarpath = _get_jdbc_driver_from_folders([here])
-
-                if jarpath:
-                    if verbose: print("Found it at %s!\nTrying to connect..."%jarpath)
-
-                jpype.startJVM(jpype.getDefaultJVMPath(), '-Djava.class.path=%s' % jarpath)
-
-
-            if self._is_netezza_system():
-                driver_not_found = ("HELP: The Netezza JDBC driver library 'nzjdbc3.jar' could not be found. "+
-                                    "Please, follow the instructions on "+
-                                    "'https://www.ibm.com/support/knowledgecenter/en/SS5FPD_1.0.0/com.ibm.ips.doc/postgresql/odbc/c_datacon_plg_overview.html' "+
-                                    "for downloading and installing the JDBC driver "+
-                                    "and put the file 'nzjdbc3.jar' in the CLASSPATH variable "+
-                                    "or in a folder that is in the CLASSPATH variable. Alternatively place "+
-                                    "it in the folder '%s'."%here)
-
-            else:
-                driver_not_found = ("HELP: The JDBC driver for IBM Db2 could "+
-                                    "not be found. Please download the latest JDBC Driver at the "+
-                                    "following address: 'https://www.ibm.com/support/pages/node/382667' "+
-                                    "and put the file 'db2jcc.jar' or 'db2jcc4.jar' in the CLASSPATH variable "+
-                                    "or in a folder that is in the CLASSPATH variable. Alternatively place "+
-                                    "it in the folder '%s'."%here)
-
-            self._connection_string = jdbc_url
-
-            try:
-                if self._is_netezza_system():
-                    self._con = jaydebeapi.connect('org.netezza.Driver', self._connection_string)
-                else:
-                    self._con = jaydebeapi.connect('com.ibm.db2.jcc.DB2Driver', self._connection_string)
-            except  Exception as e:
-                print(driver_not_found)
-                raise IdaDataBaseError(e)
-             
-            if verbose: print("Connection successful!")
-                
-            #add DB2GSE to the database FUNCTION PATH            
-            #query = "SET CURRENT FUNCTION PATH = CURRENT FUNCTION PATH, db2gse"
-            #self.ida_query(query)
-            #not anymore, reported problems with ODBC
-            #better mention DB2GSE explicitly when accessing its functions
-
-        # determine name of database
-        if self._is_netezza_system():
-            self._database_name = self.ida_scalar_query('select OBJNAME from _T_OBJECT where OBJID = CURRENT_DB;')
-        else:
-            self._database_name = self.ida_scalar_query('select CURRENT_SERVER from SYSIBM.SYSDUMMY1')
-
-        # Setting Autocommit and verbose environment variables
-        set_autocommit(autocommit)
-        set_verbose(verbose)
-
-    ###########################################################################
-    #### Data Exploration
-    ###########################################################################
-
-    @lazy
-    def current_schema(self):
-        """
-        Get the current user schema name as a string.
-
-        Returns
-        -------
-        str
-            User's schema name.
-
-        Examples
-        --------
-        >>> idadb.current_schema()
-        'DASHXXXXXX'
-        """
-        if self._is_netezza_system():
-          query = 'select TRIM(CURRENT_SCHEMA)'
-        else:
-          query = "SELECT TRIM(CURRENT_SCHEMA) FROM SYSIBM.SYSDUMMY1"
-        return self.ida_scalar_query(query)
-
-    def show_tables(self, show_all=False):
-        """
-        Show tables and views that are available in self. By default, this 
-        function shows only tables that belong to a user’s specific schema.
-
-        Parameters
-        ----------
-        show_all : bool
-            If True, all table and view names in the database are returned, 
-            not only those that belong to the user's schema.
-
-        Returns
-        -------
-        DataFrame
-             A data frame containing tables and views names in self with some 
-             additional information (TABSCHEMA, TABNAME, OWNER, TYPE). 
-
-        Examples
-        --------
-        >>> ida_db.show_tables()
-             TABSCHEMA           TABNAME       OWNER TYPE
-        0    DASHXXXXXX            SWISS  DASHXXXXXX    T
-        1    DASHXXXXXX             IRIS  DASHXXXXXX    T
-        2    DASHXXXXXX     VIEW_TITANIC  DASHXXXXXX    V
-        ...
-        >>> ida_db.show_tables(show_all = True)
-             TABSCHEMA           TABNAME       OWNER TYPE
-        0    DASHXXXXXX            SWISS  DASHXXXXXX    T
-        1    DASHXXXXXX             IRIS  DASHXXXXXX    T
-        2    DASHXXXXXX     VIEW_TITANIC  DASHXXXXXX    V
-        2      SYSTOOLS      IDAX_MODELS  DASH101631    A
-        ...
-
-        Notes
-        -----
-        show_tables implements a cache strategy. The cache is stored when the 
-        user calls the method with the argument show_all set to True. This 
-        improves performance because database table look ups are a very common 
-        operation. The cache gets updated each time a table or view is created 
-        or refreshed, each time a table or view is deleted, or when a new 
-        IdaDataFrame is opened.
-
-        """
-        #### DEVELOPERS FIX: UNCOMMENT WHEN DEAD LOCK
-        #show_all = False
-        ####
-
-        # Try to retrieve the cache
-        if show_all:
-            cache = self._retrieve_cache("cache_show_tables")
-            if cache is not None:
-                return cache
-
-        where_part = ""
-        if not show_all:
-            where_part = ("AND TABSCHEMA = '%s' " % self.current_schema)
-
-        if self._is_netezza_system():
-            query = ("SELECT SCHEMA as TABSCHEMA, TABLENAME as TABNAME, OWNER, 'T' as TYPE FROM _V_TABLE " +
-                     "WHERE DATABASE = '" + self._database_name + "' and OBJTYPE = 'TABLE' " + where_part +
-                     "UNION ALL " +
-                     "SELECT SCHEMA as TABSCHEMA, VIEWNAME as TABNAME, OWNER, 'V' as TYPE FROM _V_VIEW " +
-                     "WHERE DATABASE = '" + self._database_name + "' and OBJTYPE = 'VIEW' " + where_part)
-            query = ("SELECT SCHEMA as TABSCHEMA, OBJNAME as TABNAME, OWNER, " +
-                             "CASE WHEN OBJTYPE = 'TABLE' THEN 'T' ELSE 'V' END AS TYPE " +
-                      "FROM _V_OBJECTS  " +
-                      "WHERE OBJTYPE in ('TABLE', 'VIEW') " + where_part +
-                      "ORDER BY TABSCHEMA, TABNAME")
-        else:
-            query = ("SELECT DISTINCT TABSCHEMA, TABNAME, OWNER, TYPE " +
-                     "FROM SYSCAT.TABLES " +
-                     "WHERE OWNERTYPE = 'U' " + where_part +
-                     "ORDER BY TABSCHEMA, TABNAME")
-
-        data = self.ida_query(query)
-        
-        # Workaround for some ODBC version which does not get the entire
-        # string of the column name in the cursor descriptor. 
-        # This is hardcoded, so be careful
-        data.columns = ['TABSCHEMA', 'TABNAME', 'OWNER', 'TYPE']
-        
-        data = self._upper_columns(data)
-
-        # Db2 Warehouse FIX: schema "SAMPLES" and "GOSALES" saved with an extra blank,
-        # By doing so, we delete the extra blank.
-        # Note that this works because all cells are of type string.
-
-        # OLD version, created an unexpected bug in some wrong ODBC version
-        # TypeError: unorderable types: bytes() > int()
-        # less pythonic, do not use anymore 
-        #for col in data:
-        #    for index, val in enumerate(data[col]):
-        #        data[col][index] = val.strip()
-
-        # Can be done with a one liner :) 
-        data = data.apply(lambda x: x.apply(lambda x: x.strip()))
-
-        # Cache the result
-        if show_all is True:
-            self.cache_show_tables = data
-
-        return data
-
-    def show_models(self):
-        """
-        Show models that are available in the database.
-
-        Returns
-        -------
-        DataFrame
-
-        Examples
-        --------
-        >>> idadb.show_models()
-            MODELSCHEMA               MODELNAME       OWNER
-        0   DASHXXXXXX  KMEANS_10857_1434974511  DASHXXXXXX
-        1   DASHXXXXXX  KMEANS_11726_1434977692  DASHXXXXXX
-        2   DASHXXXXXX  KMEANS_11948_1434976568  DASHXXXXXX
-        """
-        if self._is_netezza_system():
-            list_models_stmt = ("SELECT MODELNAME, OWNER, CREATED, STATE, MININGFUNCTION, ALGORITHM, USERCATEGORY " +
-                                "FROM INZA.V_NZA_MODELS")
-            result_columns =  ['modelname', 'owner', 'created', 'state','miningfunction', 'algorithm', 'usercategory']
-        else:
-            list_models_stmt = "call IDAX.LIST_MODELS()"
-            result_columns = ['modelschema', 'modelname', 'owner', 'created', 'state',
-                              'miningfunction', 'algorithm', 'usercategory']
-
-        data = self.ida_query(list_models_stmt)
-
-        data.columns = result_columns
-
-        # Workaround for some ODBC version which does not get the entire
-        # string of the column name in the cursor descriptor. 
-        # This is hardcoded, so be careful
-        data = self._upper_columns(data)
-        return data
-
-    def exists_table_or_view(self, objectname):
-        """
-        Check if a table or view exists in self.
-
-        Parameters
-        ----------
-        objectname : str
-            Name of the table or view to check.
-
-        Returns
-        -------
-        bool
-
-        Raises
-        ------
-        TypeError
-            The object exists but is not of the expected type.
-
-        Examples
-        --------
-        >>> idadb.exists_table_or_view("NOT_EXISTING")
-        False
-        >>> idadb.exists_table_or_view("TABLE_OR_VIEW")
-        True
-        >>> idadb.exists_table_or_view("NO_TABLE_NOR_VIEW")
-        TypeError : "NO_TABLE_NOR_VIEW" exists in schema '?' but of type '?'
-        """
-        return self._exists(objectname,['T', 'V'])
-
-    def exists_table(self, tablename):
-        """
-        Check if a table exists in self.
-
-        Parameters
-        ----------
-        tablename : str
-            Name of the table to check.
-
-        Returns
-        -------
-        bool
-
-        Raises
-        ------
-        TypeError
-            The object exists but is not of the expected type.
-
-        Examples
-        --------
-        >>> idadb.exists_table("NOT_EXISTING")
-        False
-        >>> idadb.exists_table("TABLE")
-        True
-        >>> idadb.exists_table("NO_TABLE")
-        TypeError : "tablename" exists in schema "?" but of type '?'
-        """
-        return self._exists(tablename,['T'])
-
-    def exists_view(self, viewname):
-        """
-        Check if a view exists in self.
-
-        Parameters
-        ----------
-        viewname : str
-            Name of the view to check.
-
-        Returns
-        -------
-        bool
-
-        Raises
-        ------
-        TypeError
-            The object exists but is not of the expected type.
-
-        Examples
-        --------
-        >>> idadb.exists_view("NOT_EXISTING")
-        False
-        >>> idadb.exists_view("VIEW")
-        True
-        >>> idadb.exists_view("NO_VIEW")
-        TypeError : "viewname" exists in schema "?" but of type '?'
-        """
-        return self._exists(viewname,['V'])
-
-    def exists_model(self, modelname):
-        """
-        Check if a model exists in self.
-
-        Parameters
-        ----------
-        modelname : str
-            Name of the model to check. It should contain only alphanumeric
-            characters and underscores. All lower case characters will be
-            converted to upper case characters.
-
-        Returns
-        -------
-        bool
-
-        Raises
-        ------
-        TypeError
-            The object exists but is not of the expected type.
-
-        Examples
-        --------
-        >>> idadb.exists_model("MODEL")
-        True
-        >>> idadb.exists_model("NOT_EXISTING")
-        False
-        >>> idadb.exists_model("NO_MODEL")
-        TypeError : NO_MODEL exists but is not a model (of type '?')
-        """
-        modelname = nzpyida.utils.check_modelname(modelname)
-        if '.' in modelname:
-            modelschema, modelname = modelname.split('.')
-        else:
-            modelschema = self.current_schema
-
-        if self._is_netezza_system():
-            # on Netezza the model schema part of the model name is ignored
-            modelquery = "SELECT count(*) FROM INZA.V_NZA_MODELS WHERE MODELNAME ='%s'"%modelname
-            modelexists = self.ida_scalar_query(modelquery) >= 1
-            if modelexists:
-                return True
-        else:
-            # check if schema exists to avoid exception thrown by idax.list_models
-            schemaquery = "SELECT count(*) FROM SYSCAT.SCHEMATA WHERE SCHEMANAME = '%s'"%modelschema
-            schemaexists = self.ida_scalar_query(schemaquery) >= 1
-            if not schemaexists:
-                return False
-            data = self.ida_query("call idax.list_models('schema=%s, where=MODELNAME=''%s''')" % (modelschema, modelname))
-            if not data.empty:
-                return True
-
-        tablelist = self.show_tables(show_all=True)
-        tablelist = tablelist[(tablelist['TABSCHEMA']==modelschema) & (tablelist['TABNAME']==modelname)]
-        if len(tablelist):
-             tabletype = tablelist['TYPE'].values[0]
-             raise TypeError("%s.%s exists, but is not a model (of type '%s')"
-                             % (modelschema, modelname, tabletype))
-
-        else:
-            return False
-
-    def is_table_or_view(self, objectname):
-        """
-        Check if an object is a table or a view in self.
-
-        Parameters
-        ----------
-        objectname : str
-            Name of the object to check.
-
-        Returns
-        -------
-        bool
-
-        Raises
-        ------
-        ValueError
-            objectname doesn't exist in the database.
-
-
-        Examples
-        --------
-        >>> idadb.is_table_or_view("NO_TABLE")
-        False
-        >>> idadb.is_table_or_view("TABLE")
-        True
-        >>> idadb.is_table_or_view("NOT_EXISTING")
-        ValueError : NO_EXISTING does not exist in database
-        """
-        return self._is(objectname,['T','V'])
-
-    def is_table(self, tablename):
-        """
-        Check if an object is a table in self.
-
-        Parameters
-        ----------
-        tablename : str
-            Name of the table to check.
-
-        Returns
-        -------
-        bool
-
-        Raises
-        ------
-        ValueError
-            The object doesn't exist in the database.
-
-
-        Examples
-        --------
-        >>> idadb.is_table("NO_TABLE")
-        False
-        >>> idadb.is_table("TABLE")
-        True
-        >>> idadb.is_table("NOT_EXISTING")
-        ValueError : NO_EXISTING does not exist in database
-        """
-        return self._is(tablename,['T'])
-
-    def is_view(self, viewname):
-        """
-        Check if an object is a view in self.
-
-        Parameters
-        ----------
-        viewname : str
-            Name of the view to check.
-
-        Returns
-        -------
-        bool
-
-        Raises
-        ------
-        ValueError
-            The object doesn't exist in the database.
-
-        Examples
-        --------
-        >>> idadb.is_view("NO_VIEW")
-        False
-        >>> idadb.is_view("VIEW")
-        True
-        >>> idadb.is_view("NOT_EXISTING")
-        ValueError : NO_EXISTING does not exist in database
-        """
-        return self._is(viewname,['V'])
-
-    def is_model(self, modelname):
-        """
-        Check if an object is a model in self.
-
-        Parameters
-        ----------
-        modelname : str
-            Name of the model to check. It should contain only alphanumeric
-            characters and underscores. All lower case characters will be
-            converted to upper case characters.
-
-        Returns
-        -------
-        bool
-
-        Raises
-        ------
-        ValueError
-            The object doesn't exist in the database.
-
-        Examples
-        --------
-        >>> idadb.is_model("MODEL")
-        True
-        >>> idadb.is_model("NO_MODEL")
-        False
-        >>> idadb.is_model("NOT_EXISTING")
-        ValueError : NOT_EXISTING doesn't exist in database
-        """
-        modelname = nzpyida.utils.check_modelname(modelname)
-
-        if '.' in modelname:
-            modelname_noschema = modelname.split('.')[-1]
-        else:
-            modelname_noschema = modelname
-        data = self.show_models()
-        if not data.empty:
-            if modelname_noschema in data['MODELNAME'].values:
-                return True
-
-        # This part is executed if data is empty or model is not in data
-        try:
-            self.is_table_or_view(modelname)
-        except:
-            raise
-        else:
-            return False
-
-    def ida_query(self, query, silent=False, first_row_only=False, autocommit = False):
-        """
-        Prepare, execute and format the result of a query in a dataframe or
-        in a Tuple. If nothing is expected to be returned for the SQL command,
-        nothing is returned. 
-
-        Parameters
-        ----------
-        query : str
-            Query to be executed.
-        silent: bool, default: False
-             If True, the query is not printed in the python console even if 
-             the verbosity mode is activated (VERBOSE environment variable is 
-             equal to “True”). 
-        first_row_only : bool, default: False
-             If True, only the first row of the result is returned as a Tuple.
-        autocommit: bool, default: False
-            If True, the autocommit function is available.
-
-        Returns
-        -------
-        DataFrame or Tuple (if first_row_only=False)
-
-        Examples
-        --------
-        >>> idadb.ida_query("SELECT * FROM IRIS LIMIT 5")
-           sepal_length  sepal_width  petal_length  petal_width species
-        0           5.1          3.5           1.4          0.2  setosa
-        1           4.9          3.0           1.4          0.2  setosa
-        2           4.7          3.2           1.3          0.2  setosa
-        3           4.6          3.1           1.5          0.2  setosa
-        4           5.0          3.6           1.4          0.2  setosa
-
-        >>> idadb.ida_query("SELECT COUNT(*) FROM IRIS")
-        (150, 150, 150, 150)
-
-        Notes
-        -----
-        If first_row_only argument is True, then even if the actual result of 
-        the query is composed of several rows, only the first row will be 
-        returned.
-        """
-        self._check_connection()
-        return sql.ida_query(self, query, silent, first_row_only, autocommit)
-
-    def ida_scalar_query(self, query, silent=False, autocommit = False):
-        """
-        Prepare and execute a query and return only the first element as a 
-        string. If nothing is returned from the SQL query, an error occurs.
-
-        Parameters
-        ----------
-        query : str
-            Query to be executed.
-        silent: bool, default: False
-            If True, the query will not be printed in python console even if
-            verbosity mode is activated.
-        autocommit: bool, default: False
-            If True, the autocommit function is available.
-
-        Returns
-        -------
-        str or Number
-
-        Examples
-        --------
-        >>> idadb.ida_scalar_query("SELECT TRIM(CURRENT_SCHEMA) from SYSIBM.SYSDUMMY1")
-        'DASHXXXXX'
-
-        Notes
-        -----
-        Even if the actual result of the query is composed of several columns
-        and several rows, only the first element (top-left) will be returned.
-        """
-        self._check_connection()
-        return sql.ida_scalar_query(self, query, silent, autocommit)
-
-    ###############################################################################
-    #### Upload DataFrames
-    ###############################################################################
-
-    @timed
-    def as_idadataframe(self, dataframe, tablename=None, clear_existing=False, primary_key=None, indexer=None):
-        """
-        Upload a dataframe and return its corresponding IdaDataFrame. The target
-        table (tablename) will be created or replaced if the option clear_existing
-        is set to True.
-        
-        To add data to an existing tables, see IdaDataBase.append
-
-        Parameters
-        ----------
-        dataframe : DataFrame
-            Data to be uploaded, contained in a Pandas DataFrame.
-        tablename : str, optional
-            Name to be given to the table created in the database.
-            It should contain only alphanumeric characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If not given, a valid tablename is generated (for example, DATA_FRAME_X
-            where X is a random number).
-        clear_existing : bool
-            If set to True, a table will be replaced when a table with the same 
-            name already exists  in the database.
-        primary_key : str
-            Name of a column to be used as primary key.
-
-        Returns
-        -------
-        IdaDataFrame
-
-        Raises
-        ------
-        TypeError
-            * Argument dataframe is not of type pandas.DataFrame.
-            * The primary key argument is not a string.
-
-        NameError
-            * The name already exists in the database and clear_existing is False.
-            * The primary key argument doesn't correspond to a column.
-
-        PrimaryKeyError
-            The primary key contains non unique values.
-
-        Examples
-        --------
-        >>> from nzpyida.sampledata.iris import iris
-        >>> idadb.as_idadataframe(iris, "IRIS")
-        <ibmdbpy.frame.IdaDataFrame at 0xb34a898>
-        >>> idadb.as_idadataframe(iris, "IRIS")
-        NameError: IRIS already exists, choose a different name or use clear_existing option.
-        >>> idadb.as_idadataframe(iris, "IRIS2")
-        <ibmdbpy.frame.IdaDataFrame at 0xb375940>
-        >>> idadb.as_idadataframe(iris, "IRIS", clear_existing = True)
-        <ibmdbpy.frame.IdaDataFrame at 0xb371cf8>
-        
-        Notes
-        -----
-        This function is not intended to be used to add data to an existing table,
-        rather to create a new table from a dataframe. To add data to an existing
-        table, please consider using IdaDataBase.append 
-        """
-        if not isinstance(dataframe, pd.DataFrame):
-            raise TypeError("Argument dataframe is not of type Pandas.DataFrame")
-
-        if tablename is None:
-                tablename = self._get_valid_tablename(prefix="DATA_FRAME_")
-
-        tablename = nzpyida.utils.check_tablename(tablename)
-
-        if primary_key is not None:
-            if not isinstance(primary_key, six.string_types):
-                raise TypeError("The primary key argument should be a string")
-            if primary_key not in dataframe.columns:
-                raise ValueError("The primary key should be the name of a column" +
-                                 " in the given dataframe")
-            if len(dataframe[primary_key]) != len(set(dataframe[primary_key])):
-                raise PrimaryKeyError(primary_key + " cannot be a primary key for" +
-                                      "table " + tablename + " because it contains" +
-                                      " non unique values")
-
-        if self.exists_table_or_view(tablename):
-            if clear_existing:
-                try:
-                    self.drop_table(tablename)
-                except:
-                    self.drop_view(tablename)
-            else:
-                raise NameError(("%s already exists, choose a different name "+
-                                "or use clear_existing option.")%tablename)
-
-        self._create_table(dataframe, tablename, primary_key=primary_key)
-        idadf = nzpyida.frame.IdaDataFrame(self, tablename, indexer)
-        self.append(idadf, dataframe)
-
-        ############## Experimental ##################
-        # dataframe.to_sql(tablename, self._con, index=False)
-        # idadf = ibmdbpy.frame.IdaDataFrame(self, tablename, flavor='mysql',
-        #                                    schema = self.current_schema)
-
-        self._autocommit()
-
-        if primary_key:
-            idadf._indexer=primary_key
-        return idadf
-        
-    ###########################################################################
-    #### Delete DataBase objects
-    ###########################################################################
-
-    def drop_table(self, tablename):
-        """
-        Drop a table in the database.
-
-        Parameters
-        ----------
-        tablename : str
-            Name of the table to drop.
-
-        Raises
-        ------
-        ValueError
-            If the object does not exist.
-        TypeError
-            If the object is not a table.
-
-        Examples
-        --------
-        >>> idadb.drop_table("TABLE")
-        True
-        >>> idadb.drop_table("NO_TABLE")
-        TypeError : NO_TABLE  exists in schema '?' but of type '?'
-        >>> idadb.drop_table("NOT_EXISTING")
-        ValueError : NO_EXISTING doesn't exist in database
-
-        Notes
-        -----
-            This operation cannot be undone if autocommit mode is activated.
-        """
-        return self._drop(tablename, "T")
-
-    def drop_view(self, viewname):
-        """
-        Drop a view in the database.
-
-        Parameters
-        ----------
-        viewname : str
-            Name of the view to drop.
-
-        Raises
-        ------
-        ValueError
-            If the object does not exist.
-        TypeError
-            If the object is not a view.
-
-        Examples
-        --------
-        >>> idadb.drop_view("VIEW")
-        True
-        >>> idadb.drop_view("NO_VIEW")
-        TypeError : NO_VIEW exists in schema '?' but of type '?'
-        >>> idadb.drop_view("NOT_EXISTING")
-        ValueError : NO_EXISTING doesn't exist in database
-
-        Notes
-        -----
-            This operation cannot be undone if autocommit mode is activated.
-        """
-        return self._drop(viewname, "V")
-
-    def drop_model(self, modelname):
-        """
-        Drop a model in the database.
-
-        Parameters
-        ----------
-        modelname : str
-            Name of the model to drop.
-
-        Raises
-        ------
-        ValueError
-            If the object does not exist.
-        TypeError
-            if the object exists but is not a model.
-
-        Examples
-        --------
-        >>> idadb.drop_model("MODEL")
-        True
-        >>> idadb.drop_model("NO_MODEL")
-        TypeError : NO_MODEL exists in schema '?' but of type '?'
-        >>> idadb.drop_model("NOT_EXISTING")
-        ValueError : NOT_EXISTING does not exist in database
-
-        Notes
-        -----
-            This operation cannot be undone if autocommit mode is activated.
-
-        """
-
-        try:
-            self._call_stored_procedure("DROP_MODEL ", model = modelname)
-        except Exception as e:
-            try:
-                flag = self.exists_table(modelname)
-            except TypeError:
-                # Exists but is not a model (nor a table)
-                raise
-            else:
-                if flag:
-                    # It is a table so make it raise by calling exists_view
-                    self.exists_view(modelname)
-            value_error = ValueError(modelname + " does not exist in database")
-            six.raise_from(value_error, e)
-        else:
-            tables = self.show_tables()
-            if not tables.empty:
-                for table in tables['TABNAME']:
-                    if modelname in table:
-                        self.drop_table(table)
-            return True
-
-    @timed
-    def rename(self, idadf, newname):
-        """
-        Rename a table referenced by an IdaDataFrame in Db2 Warehouse.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-            IdaDataFrame object referencing the table to rename.
-        newname : str
-            Name to be given to self. It should contain only alphanumeric
-            characters and underscores. All lower case characters will be 
-            converted to upper case characters. The new name should not already 
-            exist in the database.
-
-        Raises
-        ------
-        ValueError
-            The new tablename is not valid.
-        TypeError
-            Rename function is supported only for table type.
-        NameError
-            The name of the object to be created is identical to an existing name.
-
-        Notes
-        -----
-            Upper case characters and numbers, optionally separated by 
-            underscores “_”, are valid characters.
-        """
-        # Actually we could support it for views too
-        # Question : Is it better to accept an idadf as argument or rather the name of the table?
-        oldname = idadf._name
-        newname = nzpyida.utils.check_tablename(newname)
-
-        if self.is_table(idadf._name):
-            if self._is_netezza_system():
-                query = "ALTER TABLE %s RENAME TO %s"%(idadf._name, newname)
-            else:
-                query = "RENAME TABLE %s TO %s"%(idadf._name, newname)
-            try:
-                self._prepare_and_execute(query)
-            except Exception as e:
-                if self._con_type == "odbc":
-                    raise NameError(e.value[-1])
-                else:
-                    if self._is_netezza_system():
-                        if "ERROR:  relation does not exist" in str(e.args[0]):
-                            raise ValueError("Object does not exist.")
-                        elif 'ERROR:  ALTER TABLE: object "%s" already exists'%newname in str(e.args[0]):
-                            raise NameError("The new name is identical to the old one")
-                        else:
-                            raise e
-                    else:
-                        sql_code = int(str(e.args[0]).split("SQLCODE=")[-1].split(",")[0])
-                        if sql_code == -601:
-                            raise NameError("The new name is identical to the old one")
-                        else:
-                            raise e
-
-            idadf._name = newname
-            idadf.tablename = newname
-            idadf.internal_state.name = newname
-            self._reset_attributes("cache_show_tables")
-
-            # Update name of all IdaDataFrame that were opened on this table
-            for idadf in self._idadfs:
-                if idadf._name == oldname:
-                    idadf._name = newname
-                    idadf.internal_state.name = newname # to refactor
-        else:
-            raise TypeError("Rename function is supported only for table type")
-
-    def add_column(self, idadf, column = None, colname = None, ncat=10):
-        """
-        Add physically a column to the dataset.
-        If column argument is set to None, a random column is generated which 
-        can take values for 1 to ncat.
-        Used for benchmark purpose. 
-        Note: This method is deprecated. Can probably be removed without impact 
-        """
-        if column is not None:
-            if column not in idadf.columns:
-                raise ValueError("Unknown column %s"%column)
-            dtype = idadf.dtypes['TYPENAME'][column]
-            if dtype == "VARCHAR":
-                dtype = dtype + "(255)"
-        else:
-            dtype = "INTEGER"
-        
-        if colname is None:
-            i = 0
-            colname = "COL%s"%i
-            while colname in idadf.columns:
-                i += 1
-                colname = "COL%s"%i
-        else:
-            if colname in idadf.columns:
-                raise ValueError("Column %s already exists"%colname)
-        
-        self.commit() 
-        query = "ALTER TABLE %s ADD \"%s\" %s"%(idadf._name, colname, dtype) 
-        print(query)         
-        try:
-            self._prepare_and_execute(query)
-        except:
-            print("Failed ! Trying again in 5 seconds")
-            #query2 = "REORG TABLE %s"%idadf._name
-            #print(query2)
-            import time
-            time.sleep(5)
-            self._prepare_and_execute(query)
-        self.commit()
-        
-        # force sleep 2 sec
-        
-        
-        if column is None:
-            # Random column
-            query = "UPDATE %s SET \"%s\" = RAND()* %s + 1"%(idadf._name, colname, ncat)
-        else:
-            query = "UPDATE %s SET \"%s\" = \"%s\""%(idadf._name, colname, column)
-        
-        print(query)
-        
-        try:
-            self._prepare_and_execute(query)
-        except:
-            print("Failed ! Trying again in 5 seconds")
-            import time
-            time.sleep(5)
-            self._prepare_and_execute(query)
-                
-            
-        self.commit()
-        idadf._reset_attributes(["columns", "dtypes", "shape"])
-        
-            
-    
-    @timed
-    def add_column_id(self, idadf, column_id="ID", destructive=False):
-        # TODO: Base the creation of the idea on the sorting of several columns
-        # (or all columns in case there are duplicated rows) so that the ID
-        # can be created in a determinstic and reproducible way
-        """
-        Add an ID column to an IdaDataFrame.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-            IdaDataFrame object to which an ID column will be added
-        column_id : str
-            Name of the ID column to add
-        destructive : bool
-            If set to True, the column will be added phisically in the database.
-            This can take time. If set to False, the column will be added virtually
-            in a view and a new IdaDataFrame is returned.
-
-        Raises
-        ------
-        TypeError
-            idadf is not an IdaDataFrame.
-        ValueError
-            The given column name already exists in the DataBase.
-
-        Notes
-        -----
-            The non-destructive creation of column IDs is not reliable, because row IDs are recalculated on the fly in a non-deterministic 
-            way each time a new view is produced. On the contrary, creating them destructively i.e physically is reliable but can take time. 
-            If no sorting has been done whatsoever before, row IDs will be created at random.
-            Improvement idea: create ID columns in a non-destructive way and base them on the sorting of a set of columns, 
-            defined by the user, or all columns if no column combination results in unique identifiers.
-        """
-        if isinstance(idadf, nzpyida.IdaSeries):
-            raise TypeError("Adding column ID is not supported for IdaSeries")
-        if not isinstance(idadf, nzpyida.IdaDataFrame):
-            raise TypeError("idadf is not an IdaDataFrame, type: %s"%type(idadf))
-        if column_id in idadf.columns:
-            raise ValueError("A column named '"+column_id+"' already exists." +
-                             " Please define a new column name using"+
-                             " column_id argument")
-
-        if destructive is True:
-
-            viewname = self._get_valid_tablename(prefix="VIEW_")
-            if self._is_netezza_system():
-                order_by = "ORDER BY NULL"
-            else:
-                order_by = ""
-            self._prepare_and_execute("CREATE VIEW " + viewname + " AS SELECT ((ROW_NUMBER() OVER("+ order_by +"))-1)" +
-                                      " AS \"" + column_id + "\", \""+
-                                      "\",\"".join(idadf._get_all_columns_in_table()) +
-                                      "\" FROM " + idadf._name)
-
-            # Initiate the modified table under a random name
-            tablename = self._get_valid_tablename(prefix="DATA_FRAME_")
-            if self._is_netezza_system():
-                 self._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s)"%(tablename,viewname))
-            else:
-                self._prepare_and_execute("CREATE TABLE %s LIKE %s"%(tablename,viewname))
-                self._prepare_and_execute("INSERT INTO %s (SELECT * FROM %s)"%(tablename,viewname))
-
-            # Drop the view and old table
-            self.drop_view(viewname)
-            self.drop_table(idadf._name)
-
-            # Give it the original name back
-            self._reset_attributes("cache_show_tables")
-            new_idadf = nzpyida.IdaDataFrame(self, tablename)
-            self.rename(new_idadf, idadf.tablename)
-
-            # Updating internal state
-            # prepend the columndict OrderedDict
-            items = idadf.internal_state.columndict.items()
-            idadf.internal_state.columndict = OrderedDict()
-            idadf.internal_state.columndict[column_id] = "\"" + column_id + "\""
-            for item in items:
-                idadf.internal_state.columndict[item[0]] = item[1]
-
-            idadf.internal_state.update()
-            idadf._reset_attributes(["columns"])
-            idadf.indexer = column_id
-        else:
-            # prepend the columndict OrderedDict
-            items = idadf.internal_state.columndict.items()
-            idadf.internal_state.columndict = OrderedDict()
-            if self._is_netezza_system():
-                order_by = "ORDER BY NULL"
-            else:
-                order_by = ""
-            idadf.internal_state.columndict[column_id] = "((ROW_NUMBER() OVER("+ order_by +"))-1)"
-            for item in items:
-                idadf.internal_state.columndict[item[0]] = item[1]
-            idadf.internal_state.update()
-            idadf._reset_attributes(["columns"])
-            idadf.indexer = column_id
-
-        # Reset attributes
-        idadf._reset_attributes(['shape', 'columns', 'axes', 'dtypes'])
-
-    def delete_column(self, idadf, column_name, destructive=False):
-        """
-        Delete a column in an idaDataFrame.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-            The IdaDataframe in which a column should be deleted.
-        column_name : str
-            Name of the column to delete.
-        destructive : bool
-            If set to True, the column is deleted in the database. Otherwise, 
-            it is deleted virtually, creating a view for the IdaDataFrame.
-
-        Raises
-        ------
-        TypeError
-            column_name should be a string.
-        ValueError
-            column_name refers to a column that doesn't exist in self.
-        """
-        if not isinstance(column_name, six.string_types):
-            raise TypeError("column_name is not of string type")
-        if column_name not in idadf.columns:
-            raise ValueError("%s refers to a columns that doesn't exists in self"%(column_name))
-
-        if destructive is True:
-            if column_name not in idadf._get_all_columns_in_table():
-                # Detect it is a virtual ID, the deletion cannot be destructive
-                return self.delete_column(idadf, column_name, destructive=False)
-
-            viewname = self._get_valid_tablename(prefix="VIEW_")
-            columnlist = list(idadf._get_all_columns_in_table())
-            columnlist.remove(column_name)
-
-            self._prepare_and_execute("CREATE VIEW " + viewname + " AS SELECT \""+
-                                      "\",\"".join(columnlist) +
-                                      "\" FROM " + idadf._name)
-
-            tablename = self._get_valid_tablename(prefix="DATA_FRAME_")
-
-            if self._is_netezza_system():
-                self._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s)"%(tablename,viewname))
-            else:
-                self._prepare_and_execute("CREATE TABLE " + tablename + " LIKE " + viewname)
-                self._prepare_and_execute("INSERT INTO " + tablename + " (SELECT * FROM " + viewname + ")")
-
-            # Drop the view and old table
-            self.drop_view(viewname)
-            self.drop_table(idadf._name)
-
-            # Give it the original name back
-            self._reset_attributes("cache_show_tables") # normally, no needed
-            new_idadf = nzpyida.IdaDataFrame(self, tablename, idadf.indexer)
-            self.rename(new_idadf, idadf.tablename)
-
-            # updating internal state
-            del idadf.internal_state.columndict[column_name]
-            idadf.internal_state.update()
-            self._reset_attributes("cache_show_tables")
-            idadf._reset_attributes(['shape', 'columns', 'dtypes'])
-
-        else:
-            del idadf.internal_state.columndict[column_name]
-            idadf.internal_state.update()
-            idadf._reset_attributes(["columns", "shape", "dtypes"])
-
-        if column_name == idadf.indexer:
-            idadf._reset_attributes(["_indexer"])
-
-
-    def append(self, idadf, df, maxnrow=None):
-        """
-        Append rows of a DataFrame to an IdaDataFrame. The DataFrame must have 
-        the same structure (same column names and datatypes). Optionally, the 
-        DataFrame to be added can be splitted into several chunks. This 
-        improves performance and prevents SQL overflows. By default, chunks are 
-        limited to 100.000 cells.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-            IdaDataFrame that receives data from dataframe df.
-        df : DataFrame
-            Dataframe whose rows are added to IdaDataFrame idadf.
-        maxnrow : int, optional
-            number corresponding to the maximum number of rows for each chunks.
-
-        Raises
-        ------
-        TypeError
-            * maxnrow should be an interger.
-            * Argument idadf should be an IdaDataFrame.
-            * Argument df should be a pandas DataFrame.
-        ValueErrpr
-            * maxnrow should be greater than 1 or nleft blank.
-            * Other should be a Pandas DataFrame.
-            * Other dataframe has not the same number of columns as self.
-            * Some columns in other have different names that are different from the names of the columns in self.
-        """
-        # SANITY CHECK : maxnrow
-        if maxnrow is None:
-            # Note : it has been measured on a big dataset (>1 million rows) that int(100000 / len(df.columns)) 
-            # performs better than the previous empirical value int(8000 / len(df.columns)) 
-            maxnrow = int(100000 / len(df.columns))
-        else:
-            if not isinstance(maxnrow, six.integer_types):
-                raise TypeError("maxnrow is not an integer")
-            if maxnrow < 1:
-                raise ValueError("maxnrow should be stricly positive or omitted")
-            if maxnrow > 15000:
-                warnings.warn("Performance may decrease if maxnrow is bigger than 15000", UserWarning)
-
-        # SANITY CHECK : idadf & other
-        if not isinstance(idadf, nzpyida.frame.IdaDataFrame):
-            raise TypeError("Argument idadf is not an IdaDataFrame")
-        if not isinstance(df, pd.DataFrame):
-            raise TypeError("Argument df is not of type pPandas.DataFrame")
-        if len(df.columns) != len(idadf.columns):
-            raise ValueError("(Ida)DataFrames don't have the same number of columns")
-        if any([column not in idadf.columns for column in [str(x).strip() for x in df.columns]]):
-            raise ValueError("Some column names do not match current object columns: \n" +
-                             "Expected : \t" + str(idadf.columns) + "\n" +
-                             "Found : \t" + str([x.strip() for x in df.columns]) + "\n")
-        if any([str(column_idadf) != str(column_other).strip() for column_idadf, column_other in
-                zip(idadf.columns, df.columns)]):
-            raise ValueError("Order or columns in other and" + idadf._name + "does not match.")
-
-        if df.shape[0] > 1.5 * maxnrow:
-            split_into = math.ceil(df.shape[0] / maxnrow)
-            split = np.array_split(df, split_into)
-            print("DataFrame will be splitted into " + str(split_into) +
-                  " chunks. (" + str(maxnrow) + " rows per chunk)")
-            for i, chunk in enumerate(split, 0):
-                percentage = int(i / split_into * 100)
-                print("Uploaded: " + str(percentage) + "%... ", end="\r")
-                try:
-                    self._insert_into_database(chunk, idadf.schema, idadf.tablename, silent=True)
-                except:
-                    raise
-            print("Uploaded: %s/%s... "%(split_into,split_into), end="")
-            print("[DONE]")
-        else:
-            print("Uploading %s rows (maxnrow was set to %s)"%(df.shape[0], maxnrow))
-            try:
-                self._insert_into_database(df, idadf.schema, idadf.tablename, silent=True)
-            except:
-                raise
-
-        idadf._reset_attributes(['shape', 'axes', 'dtypes', 'index'])
-
-    def merge(self, idadf, other, key):
-        # TODO:
-        pass
-
-    ###############################################################################
-    #### Connection management
-    ###############################################################################
-
-    def commit(self):
-        """
-        Commit operations in the database.
-
-        Notes
-        -----
-
-        All changes that are made in the database after the last commit, 
-        including those in the child IdaDataFrames, are commited.
-
-        If the environment variable ‘VERBOSE’ is set to True, the commit 
-        operations are notified in the console.
-        """
-        self._check_connection()  # Important
-        self._con.commit()
-        if os.getenv('VERBOSE') == 'True':
-            print("<< COMMIT >>")
-        self._reset_attributes("cache_show_tables")
-
-    def rollback(self):
-        """
-        Rollback operations in the database.
-
-        Notes
-        -----
-
-        All changes that are made in the database after the last commit, 
-        including those in the child IdaDataFrames, are discarded.
-        """
-        self._check_connection()  # Important
-        self._con.rollback()
-        if os.getenv('VERBOSE') == 'True':
-            print("<< ROLLBACK >>")
-        self._reset_attributes("cache_show_tables")
-
-    def close(self):
-        """
-        Close the IdaDataBase connection.
-
-        Notes
-        -----
-
-        If the environment variable ‘AUTOCOMMIT’ is set to True, then all 
-        changes after the last commit are committed, otherwise they are 
-        discarded.
-
-        """
-        if os.getenv('AUTOCOMMIT') == 'True':
-            self.commit()
-        else:
-            self.rollback()
-        self._reset_attributes("cache_show_tables")
-        self._con.close()
-        print("Connection closed.")
-
-    def reconnect(self):
-        """
-        Try to reopen the connection.
-        """
-        try:
-            self._check_connection()
-        except IdaDataBaseError:
-            if self._con_type == 'odbc':
-                import pyodbc
-                try:
-                    self._con = pyodbc.connect(self._connection_string)
-                except:
-                    raise
-                else:
-                    print("The connection was successfully restored")
-            elif self._con_type == 'nzpy':
-                import nzpy
-                try:
-                    self._con = nzpy.connect(**(self._connection_string))
-                except:
-                    raise
-                else:
-                    print("The connection was successfully restored")
-            elif self._con_type == 'jdbc':
-                try:
-                    import jaydebeapi
-                    if self._is_netezza_system():
-                        self._con = jaydebeapi.connect('org.netezza.Driver', self._connection_string)
-                    else:
-                        self._con = jaydebeapi.connect('com.ibm.db2.jcc.DB2Driver', self._connection_string)
-                except:
-                    raise
-                else:
-                    print("The connection was successfully restored")
-        else:
-            print("The connection for current IdaDataBase is valid")
-
-        ###############################################################################
-        #### Private methods
-        ###############################################################################
-
-    def __enter__(self):
-        """
-        Allow the object to be used with a "with" statement
-        """
-        return self
-
-    def __exit__(self):
-        """
-        Allow the object to be used with a "with" statement. Make sure the
-        connection is closed when the object get out of scope
-        """
-        self.close()
-
-    def _exists(self, objectname, typelist):
-        """
-        Check if an object of a certain type exists in Db2 Warehouse.
-
-        Notes
-        -----
-        For more information, see exists_table_or_view, exists_table, 
-        exists_view functions.
-        """
-        objectname = nzpyida.utils.check_tablename(objectname)
-
-        tablelist = self.show_tables(show_all=True)
-        schema, name = self._get_name_and_schema(objectname)
-        tablelist = tablelist[tablelist['TABSCHEMA'] == schema]
-
-        if len(tablelist):
-            if name in tablelist['TABNAME'].values:
-                tabletype = tablelist[tablelist['TABNAME'] == name]['TYPE'].values[0]
-                if tabletype in typelist:
-                    return True
-                else:
-                    raise TypeError("%s exists in schema %s but of type \"%s\""
-                                    %(objectname,schema,tabletype))
-            else:
-                return False
-        else:
-            return False
-
-    def _is(self, objectname, typelist):
-        """
-        Check if an existing object is of a certain type or in a list of types.
-
-        Notes
-        -----
-        For more information, see is_table_or_view, is_table, is_view functions.
-        """
-        objectname = nzpyida.utils.check_tablename(objectname)
-
-        tablelist = self.show_tables(show_all=True)
-        schema, name = self._get_name_and_schema(objectname)
-        tablelist = tablelist[tablelist['TABSCHEMA'] == schema]
-
-        if len(tablelist):
-            if name in tablelist['TABNAME'].values:
-                tabletype = tablelist[tablelist['TABNAME'] == name]['TYPE'].values[0]
-                if tabletype in typelist:
-                    return True
-                else:
-                    return False
-
-        raise ValueError("%s does not exist in database"%(objectname))
-
-    def _drop(self, objectname, object_type = "T"):
-        """
-        Drop an object in the table depending on its type.
-        Admissible type values are "T" (table) and "V" (view)
-
-        Notes
-        -----
-        For more information, seedrop_table and drop_view functions.
-
-        """
-        objectname = nzpyida.utils.check_tablename(objectname)
-
-        if object_type == "T":
-            to_drop = "TABLE"
-        elif object_type == "V":
-            to_drop = "VIEW"
-        else:
-            raise ValueError("Unknown type to drop")
-
-        try:
-            self._prepare_and_execute("DROP %s %s"%(to_drop,objectname))
-        except Exception as e:
-            if self._con_type == "odbc":
-                if e.value[0] == "42S02":
-                    raise ValueError(e.value[1])  # does not exist
-                if e.value[0] == "42809":
-                    raise TypeError(e.value[1])  # object is not of expected type
-            else:
-                if self._is_netezza_system():
-                    if "ERROR:  relation does not exist" in str(e.args[0]):
-                        raise ValueError("Object does not exist.")
-                    else:
-                        raise e
-                else:
-                    sql_code = int(str(e.args[0]).split("SQLCODE=")[-1].split(",")[0])
-                    if sql_code == -204:
-                        raise ValueError("Object does not exist.")
-                    elif sql_code == -159:
-                        raise TypeError("Object is not of expected type")
-                    else:
-                        raise e # let the expection raise anyway
-        else:
-            self._reset_attributes("cache_show_tables")
-            return True
-
-    def _upper_columns(self, dataframe):
-        # Could be moved to utils (then move in the test too)
-        """
-        Put every column name of a Pandas DataFrame in upper case.
-
-        Parameters
-        ----------
-        dataframe : DataFrame
-
-        Returns
-        -------
-        DataFrame
-        """
-        data = deepcopy(dataframe)
-        if len(data):
-            data.columns = [x.upper() for x in data.columns]
-        return data
-
-    def _get_name_and_schema(self, objectname):
-        """
-        Helper function that returns the name and the schema from an object 
-        name. Implicitly, if no schema name was given, it is assumed that user 
-        refers to the current schema.
-
-        Parameters
-        ----------
-        objectname : str
-            Name of the object to process. Can be either under the form
-            "SCHEMA.TABLE" or just "TABLE"
-
-        Returns
-        -------
-        tuple
-            A tuple composed of 2 strings containing the schema and the name.
-
-        Examples
-        --------
-        >>> _get_name_and_schema(SCHEMA.TABLE)
-        (SCHEMA, TABLE)
-        >>> _get_name_and_schema(TABLE)
-        (<current schema>, TABLE)
-        """
-        if '.' in objectname:
-            name = objectname.split('.')[-1]
-            schema = objectname.split('.')[0]
-        else:
-            name = objectname
-            schema = self.current_schema
-        return (schema, name)
-
-    def _get_valid_tablename(self, prefix="DATA_FRAME_"):
-        """
-        Generate a valid database table name.
-
-        Parameters
-        ----------
-        prefix : str, default: "DATA_FRAME_"
-            Prefix used to create the table name. The name is constructed using 
-            this pattern : <prefix>_X where <prefix> corresponds to the string 
-            parameter “prefix” capitalized and X corresponds to a pseudo 
-            randomly generated number (0-100000).
-
-        Returns
-        -------
-        str
-
-        Examples
-        --------
-        >>> idadb._get_valid_tablename()
-        'DATA_FRAME_49537_1434978215'
-        >>> idadb._get_valid_tablename("MYDATA_")
-        'MYDATA_65312_1434978215'
-        >>> idadb._get_valid_tablename("mydata_")
-        'MYDATA_78425_1434978215'
-        >>> idadb._get_valid_tablename("mydata$")
-        ValueError: Table name is not valid, only alphanumeric characters and underscores are allowed.
-        """
-        prefix = nzpyida.utils.check_tablename(prefix)
-        # We may assume that the generated name is unlikely to exist
-        name = "%s%s_%s" % (prefix, random.randint(0, 100000), int(time()))
-        return name
-
-    def _get_valid_viewname(self, prefix="VIEW_"):
-        """
-        Convenience function : Alternative name for _get_valid_tablename.
-
-        The parameter prefix has its optional value changed to "VIEW\_"".
-
-        Examples
-        --------
-        >>> idadb._get_valid_viewname()
-        'VIEW_49537_1434978215'
-        >>> idadb._get_valid_viewname("MYVIEW_")
-        'MYVIEW_65312_1434978215'
-        >>> idadb._get_valid_viewname("myview_")
-        'MYVIEW_78425_1434978215'
-        >>> idadb._get_valid_modelname("myview$")
-        ValueError: View name is not valid, only alphanumeric characters and underscores are allowed.
-        """
-        return self._get_valid_tablename(prefix)
-
-    def _get_valid_modelname(self, prefix="MODEL_"):
-        """
-        Convenience function : Alternative name for _get_valid_tablename.
-
-        Parameter prefix has its optional value changed to "MODEL\_".
-
-        Examples
-        --------
-        >>> idadb._get_valid_modelname()
-        'MODEL_49537_1434978215'
-        >>> idadb._get_valid_modelname("TEST_")
-        'TEST_65312_1434996318'
-        >>> idadb._get_valid_modelname("test_")
-        'TEST_78425_1435632423'
-        >>> idadb._get_valid_tablename("mymodel$")
-        ValueError: Table name is not valid, only alphanumeric characters and underscores are allowed.
-        """
-        return self._get_valid_tablename(prefix)
-
-    def _create_table(self, dataframe, tablename, primary_key=None):
-        """
-        Create a new table in the database by declaring its name and columns 
-        based on an existing DataFrame. It is possible declare a column as 
-        primary key.
-
-        Parameters
-        ----------
-        dataframe : DataFrame
-            Pandas DataFrame be used to initiate the table.
-        tablename : str
-            Name to be given to the table at its creation.
-        primary_key: str
-            Name of a column to declare as primary key.
-
-        Notes
-        -----
-        The columns and their data type is deducted from the Pandas DataFrame 
-        given as parameter.
-
-        Examples
-        --------
-        >>> from nzpyida.sampledata.iris import iris
-        >>> idadb._create_table(iris, "IRIS")
-        'IRIS'
-        >>> idadb._create_table(iris)
-        'DATA_FRAME_4956'
-        """
-        # TODO : Handle more types
-        # integer_attributes =
-        # ['int_', 'intc','BIGINT','REAL','DOUBLE','FLOAT','DECIMAL','NUMERIC']
-        # double_attributes =
-        # ['SMALLINT', 'INTEGER','BIGINT','REAL','DOUBLE','FLOAT','DECIMAL','NUMERIC']
-        # self._check_connection()
-
-        if not isinstance(dataframe, pd.DataFrame):
-            raise TypeError("_create_table is valid only for DataFrame objects")
-
-        if primary_key is not None:
-            if not isinstance(primary_key, six.string_types):
-                raise TypeError("primary_key argument should be a string")
-
-        # Check the tablename
-        tablename = nzpyida.utils.check_tablename(tablename)
-
-        # for Netezza we have to check if the schema exists already
-        # otherwise we have to create it
-        if self._is_netezza_system() & ("." in tablename):
-            schemaname, tabname = tablename.split(".")
-            if self.ida_scalar_query("SELECT COUNT(*) FROM _V_SCHEMA WHERE SCHEMA='%s'"%schemaname) == 0:
-                self.ida_query("CREATE SCHEMA " + schemaname)
-
-        column_string = ''
-        for column in dataframe.columns:
-            if dataframe.dtypes[column] in [object,bool]:
-                # Handle boolean type
-                if set(dataframe[column].unique()).issubset([True, False, 0, 1, np.nan]):
-                    column_string += "\"%s\" SMALLINT," % str(column).strip()
-                else:
-                    if column == primary_key:
-                        column_string += "\"%s\" VARCHAR(255) NOT NULL, PRIMARY KEY (\"%s\")," % (str(column).strip(), str(column).strip())
-                    else:
-                        column_string += "\"%s\" VARCHAR(255)," % str(column).strip()
-            elif dataframe.dtypes[column] == np.dtype('datetime64[ns]'):
-                # This is a first patch for handling dates
-                # TODO: Dates as timestamp in the database
-                column_string += "\"%s\" VARCHAR(255)," % str(column).strip()
-            else:
-                if dataframe.dtypes[column] in [np.int64, np.int, np.int8, np.int32]:
-                    if abs(dataframe[column].max()) < 2147483647/2: # might get bigger 
-                        datatype = "INT"
-                    else:
-                        datatype = "BIGINT"
-                else:
-                    datatype = "DOUBLE"
-                if column == primary_key:
-                    column_string += "\"%s\" %s NOT NULL, PRIMARY KEY (\"%s\")," % (str(column).strip(), datatype, str(column).strip())
-                else:
-                    column_string += "\"%s\" %s," %(str(column.strip()), datatype)
-        if column_string[-1] == ',':
-            column_string = column_string[:-1]
-
-        if '.' in tablename:
-            schema, tablename2 = tablename.split('.')
-        else:
-            schema = self.current_schema
-            tablename2 = tablename
-
-        create_table = "CREATE TABLE \"%s\".\"%s\" (%s)" % (schema, tablename2, column_string)
-
-        self._prepare_and_execute(create_table, autocommit=False)
-
-        # Save new table in cache
-        if hasattr(self, "cache_show_tables"):
-            record = (schema, tablename2, self.current_schema, 'T')
-            self.cache_show_tables.loc[len(self.cache_show_tables)] = np.array(record)
-
-        return "\"%s\".\"%s\"" % (schema, tablename2)
-
-    def _create_view(self, idadf, viewname = None):
-        """
-        Create a new view in the database from an existing table.
-
-        Parameters
-        ----------
-
-        idadf : IdaDataFrame
-            IdaDataFrame to be duplicated as view.
-
-        viewname : str, optional
-            Name to be given to the view at its creation. If not given, a 
-            random name will be generated automatically 
-
-        Returns
-        -------
-        str
-            View name.
-
-        Examples
-        --------
-        >>> idadf = IdaDataFrame(idadb, "IRIS")
-        >>> idadb._create_view(idadf)
-        'IDAR_VIEW_4956'
-        """
-        if not isinstance(idadf, nzpyida.frame.IdaDataFrame):
-            raise TypeError("_create_view is valid only for IdaDataFrame objects")
-
-        # Check the viewname
-        if viewname is not None:
-            viewname = nzpyida.utils.check_viewname(viewname)
-        else:
-            viewname = self._get_valid_viewname()
-
-        self._prepare_and_execute("CREATE VIEW \"%s\" AS SELECT * FROM %s" % (viewname, idadf._name))
-    
-        # Save new view in cache
-        if hasattr(self, "cache_show_tables"):
-            record = (self.current_schema, viewname, self.current_schema, 'V')
-            self.cache_show_tables.loc[len(self.cache_show_tables)] = np.array(record)
-    
-        return viewname
-        
-    def _create_view_from_expression(self, expression, viewname = None):
-        """
-        Create a new view in the database from an expression.
-
-        Parameters
-        ----------
-
-        expression : str
-            Expression that defines the view to be created
-
-        viewname : str, optional
-            Name to be given to the view at its creation. If not given, a 
-            random name will be generated automatically 
-
-        Returns
-        -------
-        str
-            View name.
-
-        Examples
-        --------
-        TODO
-        """
-        if not isinstance(expression, six.string_types):
-            raise TypeError("expression argument expected to be of string type")
-
-        # Check the viewname
-        if viewname is not None:
-            viewname = nzpyida.utils.check_viewname(viewname)
-        else:
-            viewname = self._get_valid_viewname()
-
-        self._prepare_and_execute("CREATE VIEW \"%s\" AS %s" % (viewname, expression))
-    
-        # Save new view in cache
-        if hasattr(self, "cache_show_tables"):
-            record = (self.current_schema, viewname, self.current_schema, 'V')
-            self.cache_show_tables.loc[len(self.cache_show_tables)] = np.array(record)
-    
-        return viewname
-
-    def _insert_into_database(self, dataframe, schema, tablename, silent=True):
-        """
-        Populate an existing table with data from a dataframe.
-
-        Parameters
-        ----------
-        dataframe: DataFrame
-            Data to be inserted into an existing table, contained in a Pandas 
-            DataFrame. It is assumed that the structure matches.
-        schema: str
-            Schema of the table in which the data is inserted.
-        tablename: str
-            Name of the table in which the data is inserted.
-        silent : bool, default: True
-            If True, the INSERT statement is not printed. Avoids flooding the 
-            console.
-
-        """
-        # TODO : Handle more datatypes
-        if schema is None or schema.strip() == '':
-            schema = self.current_schema
-        tablename = nzpyida.utils.check_tablename(tablename)
-        column_string = '\"%s\"' % '\", \"'.join([str(x).strip() for x in dataframe.columns])
-        row_string = ''
-
-        # Save in a list columns that are booleans
-        boolean_flaglist = []
-        for column in dataframe.columns:
-            if set(dataframe[column].unique()).issubset([True, False, 0, 1, np.nan]):
-                boolean_flaglist.append(1)
-            else:
-                boolean_flaglist.append(0)
-
-        if self._is_netezza_system():
-            sepstr1 = 'SELECT '
-            sepstr2 = ' UNION ALL SELECT '
-            sepstr3 = ' '
-        else:
-            sepstr1 = 'VALUES ('
-            sepstr2 = '), ('
-            sepstr3 = ') '
-
-        row_separator = sepstr1
-        for rows in dataframe.values:
-            value_string = ''
-            for colindex, value in enumerate(rows):
-                if pd.isnull(value): # handles np.nan and None
-                    value_string += "NULL,"  # Handle missing values
-                elif isinstance(value, six.string_types):
-                    ## Handle apostrophe in values
-                    value = value.replace("\\", "'")
-                    value_string += '\'%s\',' % value.replace("'", "''")
-                # REMARK: it is the best way to handle booleans ?
-                elif isinstance(value, bool):
-                    if boolean_flaglist[colindex] == True:
-                        if value in [1, True]:
-                            value_string += '1,'
-                        elif value in [0, False]:
-                            value_string += '0,'
-                    else:
-                        value_string += '\'%s\',' % value
-                # TODO: Handle datetime better than strings
-                elif isinstance(value, datetime.datetime):
-                    value_string += '\'%s\',' % value
-                else:
-                    value_string += '%s,' % value
-            if value_string[-1] == ',':
-                value_string = value_string[:-1]
-            row_string += (row_separator + " %s ") % value_string
-            row_separator = sepstr2
-        row_string = row_string +  sepstr3
-        if row_string[-2:] == '),':
-            row_string = row_string[:-2]
-        if row_string[0] == '(':
-            row_string = row_string[1:]
-        query = ("INSERT INTO \"%s\".\"%s\" (%s) (%s)" % (schema, tablename, column_string, row_string))
-
-        # print(query)
-        # TODO: Good idea : create a savepoint before creating the table
-        # Rollback in to savepoint in case of failure
-        self._prepare_and_execute(query, autocommit=False, silent=silent)
-
-        for idadf in self._idadfs:
-            if idadf._name == tablename:
-                idadf._reset_attributes(["shape", "index"])
-
-    def _prepare_and_execute(self, query, autocommit=True, silent=False):
-        """
-        Prepare and execute a query by using the cursor of an idaobject.
-
-        Parameters
-        ----------
-        idaobject: IdaDataBase or IdaDataFrame
-        query: str
-            Query to be executed.
-        autocommit: bool, default: True
-            If True, the autocommit function is available.
-        silent: bool, default: False
-            If True, the SQL statement is not printed.
-        """
-        self._check_connection()
-        return sql._prepare_and_execute(self, query, autocommit, silent)
-
-    def _check_procedure(self, proc_name, alg_name=None):
-        """
-        Check if a procedure is available in the database.
-
-        Parameters
-        ----------
-        proc_name : str
-            Name of the procedure to be checked as defined in the underlying
-            database management system.
-        alg_name : str
-            Name of the algorithm, human readable.
-
-        Returns
-        -------
-        bool
-
-        Examples
-        --------
-        >>> idadb._check_procedure('KMEANS')
-        True
-        >>> idadb._check_procedure('NOT_EXISTING')
-        IdaDatabaseError: Function 'NOT_EXISTING' is not available.
-        """
-
-        if alg_name is None:
-            alg_name = proc_name
-        if self._is_netezza_system():
-            query = ("SELECT COUNT(*) FROM NZA.._V_PROCEDURE WHERE PROCEDURE='%s'") % proc_name
-        else:
-            query = ("SELECT COUNT(*) FROM SYSCAT.ROUTINES WHERE ROUTINENAME='%s" +
-                     "' AND ROUTINEMODULENAME = 'IDAX'") % proc_name
-        flag = self.ida_scalar_query(query)
-
-        if int(flag) == False:
-            raise IdaDataBaseError("Function '%s' is not available." % alg_name)
-        else:
-            return True
-
-    def _call_stored_procedure(self, sp_name, **kwargs):
-        """
-        Call a specific IDAX/INZA stored procedure and return its result.
-
-        Parameters
-        ----------
-        sp_name : str
-            Name of the stored procedure.
-        **kwargs : ...
-            Additional parameters, specific to the called stored procedure.
-        """
-        tmp = []
-        views = []
-        if self._is_netezza_system():
-            sp_schema = 'NZA.'
-        else:
-            sp_schema = 'IDAX'
-
-        for key, value in six.iteritems(kwargs):
-            if value is None:
-                continue  # Go to next iteration
-            if isinstance(value, nzpyida.frame.IdaDataFrame):
-                tmp_view_name = self._create_view(value)
-                tmp.append("%s=%s" % (key, tmp_view_name))
-                views.append(tmp_view_name)
-            elif isinstance(value, six.string_types) and all([x != " " for x in value]):
-                if key in ("intable", 'model', 'outtable', 'incolumn', 'nametable', 'colPropertiesTable'):
-                    tmp.append("%s=%s"% (key, value))  # no " in the case it is a table name
-                else:
-                    tmp.append("%s=\"%s\"" % (key, value))
-            elif isinstance(value, list):
-                tmp.append("%s=\"%s\"" % (key, " ".join(str(value))))
-            else:
-                tmp.append("%s=%s" % (key, value))
-        try:
-            call = "CALL %s.%s('%s')" % (sp_schema, sp_name, ",".join(tmp))
-            result = self._prepare_and_execute(call)
-        except:
-            if self._is_netezza_system():
-                error_msg = "Error"
-            else:
-                error_msg = self.ida_scalar_query("values idax.last_message")
-            raise IdaDataBaseError(error_msg)
-        finally:
-            for view in views:
-                self.drop_view(view)
-
-        return result
-
-    def _autocommit(self):
-        """
-        Commit changes made to the database in the connection automatically.
-        If the environment variable 'AUTOCOMMIT' is set to True, then commit.
-
-        Notes
-        -----
-        In the case of a commit operation, all changes that are made in the
-        Database after the last commit, including those in the children
-        IdaDataFrames, will be commited.
-
-        If the environment variable 'VERBOSE' is not set to 'True', the
-        autocommit operations will not be notified in the console to the user.
-        """
-        if os.getenv('AUTOCOMMIT') == 'True':
-            self._con.commit()
-            if os.getenv('VERBOSE') == 'True':
-                print("<< AUTOCOMMIT >>")
-
-    def _check_connection(self):
-        """
-        Check if the connection still exists by trying to open a cursor.
-        """
-        if self._con_type == "odbc":
-            try:
-                self._con.cursor()
-            except Exception as e:
-                raise IdaDataBaseError(e.value[-1])
-        elif self._con_type == "nzpy":
-            try:
-                self._con.cursor().execute("select 1")
-            except Exception as e:
-                raise IdaDataBaseError(e)
-        elif self._con_type == "jdbc":
-            # This avoids infinite recursions on Netezza due to reconnect calls in
-            # sql.ida_query, sql.ida_scalar_query and sql._prepare_and_execute
-            if self._con._closed:
-                raise IdaDataBaseError("The connection is closed")
-            try:
-                # Avoid infinite recursion
-                if self._is_netezza_system():
-                    # On Netezza no result sets are returned after the first database error
-                    if read_sql("SELECT OBJID FROM _V_TABLE LIMIT 1", self._con) is None:
-                        raise IdaDataBaseError("The connection is closed")
-                else:
-                    read_sql("SELECT TABLEID FROM SYSCAT.TABLES LIMIT 1", self._con)
-            except Exception as e:
-                raise IdaDataBaseError("The connection is closed")
-
-    def _retrieve_cache(self, cache):
-        """
-        Helper function that retrieve cache if available.
-        Cache are just string type values stored in private attributes.
-        """
-        if not isinstance(cache, six.string_types):
-            raise TypeError("cache is not of string type")
-
-        self._check_connection()
-
-        if hasattr(self, cache):
-            return getattr(self,cache)
-        else:
-            return None
-
-    def _reset_attributes(self, attributes):
-        """
-        Helper function that delete attributes given as parameter if they
-        exists in self. This is used to refresh lazy attributes and caches.
-        """
-        nzpyida.utils._reset_attributes(self, attributes)
-
-
-    def _is_netezza_system(self):
-         """
-         Checks if the underlying database system is Netezza.
-         """
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+# For local installation do "pip install -e" in the ibmdbpy-master directory
+###############################################################################
+
+"""
+An IdaDataBase instance represents a reference to a remote Netezza Warehouse database
+maintaining attributes and methods for administration of the database.
+"""
+
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+from builtins import zip
+from builtins import int
+from builtins import str
+from future import standard_library
+standard_library.install_aliases()
+
+import os
+#from os import path
+import sys
+import math
+import random
+from time import time
+import datetime
+import warnings
+from copy import deepcopy
+
+from collections import OrderedDict
+
+
+import numpy as np
+import pandas as pd
+from pandas.io.sql import read_sql
+
+from lazy import lazy
+import six
+
+import nzpyida
+from nzpyida import sql
+from nzpyida.utils import timed, set_verbose, set_autocommit
+from nzpyida.exceptions import IdaDataBaseError, PrimaryKeyError
+
+class IdaDataBase(object):
+    """
+    An IdaDataBase instance represents a reference to a remote Netezza Warehouse
+    database. This is an abstraction layer for the remote connection. The
+    IdaDataBase interface provides several functions that enable basic database 
+    administration in pythonic syntax.
+
+    You can use either ODBC or JDBC or NZPY to connect to the database. The default
+    connection type is ODBC, which is the standard connection type for Windows 
+    users. To establish an ODBC connection, download an IBM Netezza driver and set
+    up your ODBC connection by specifying your connection protocol, port, and 
+    hostname. An ODBC connection on Linux or Mac might require more settings. 
+    For more information about how to establish an ODBC connection, see the 
+    pypyodbc documentation.
+
+    To connect with JDBC, install the optional external package jaydebeapi, 
+    download the ibm jdbc driver, and save it in your local nzpyida folder.
+    If you put the jdbc driver in the CLASSPATH variable or the folder that
+    contains it, it will work too. A C++ compiler adapted to the current python 
+    version, operating system, and architecture may also be required to install
+    jaydebeapi.
+
+    To connect with NZPY, specify the required parameters in dict format, there is
+    no need to install any drivers.
+
+    The instantiation of an IdaDataBase object is a mandatory step before 
+    creating IdaDataFrame objects because IdaDataFrames require an IdaDataBase 
+    as a parameter to be initialized. By convention, use only one instance of 
+    IdaDataBase per database. However, you can use several instances of 
+    IdaDataFrame per connection.
+    """
+
+    def __init__(self, dsn, uid='', pwd='', autocommit=True, verbose=False):
+        """
+        Open a database connection.
+
+        Parameters
+        ----------
+        dsn : str
+            Data Source Name (as specified in your ODBC settings) or JDBC URL 
+            string or NZPY dict
+
+        uid : str, optional
+            User ID.
+
+        pwd : str, optional
+            User password.
+
+        autocommit : bool, default: True
+            If True, automatically commits all operations.
+
+        verbose : bool, defaukt: True
+            If True, prints all SQL requests that are sent to the database. 
+
+        Attributes
+        ----------
+        data_source_name : str
+            Name of the referring DataBase.
+
+        _con_type : str
+            Type of the connection, either 'odbc' or 'jdbc' or 'nzpy'.
+
+        _connection_string : str
+            Connection string use for connecting via ODBC or JDBC or NZPY.
+
+        _con : connection object
+            Connection object to the remote Database.
+
+        _database_system: str
+            Underlying database system, either 'db2' or 'netezza'
+
+        _database_name: str
+            The name of the database the application is connected to.
+
+        _idadfs : list
+            List of IdaDataFrame objects opened under this connection.
+
+        Returns
+        -------
+        IdaDataBase object
+
+        Raises
+        ------
+        ImportError
+            JayDeBeApi is not installed.
+        IdaDataBaseError
+            * uid and pwd are defined both in uid, pwd parameters and dsn.
+            * The 'db2jcc4.jar' file is not in the ibmdbpy site-package repository.
+
+        Examples
+        --------
+
+        ODBC connection, userID and password are stored in ODBC settings:
+
+        >>> IdaDataBase(dsn="BLUDB") # ODBC Connection
+        <ibmdbpy.base.IdaDataBase at 0x9bec860>
+
+        ODBC connection, userID and password are not stored in ODBC settings:
+
+        >>> IdaDataBase(dsn="BLUDB", uid="<UID>", pwd="<PWD>")
+        <ibmdbpy.base.IdaDataBase at 0x9bec860>
+        
+        JDBC connection, full JDBC string:
+
+        >>> jdbc='jdbc:db2://<HOST>:<PORT>/<DBNAME>:user=<UID>;password=<PWD>'
+        >>> IdaDataBase(dsn=jdbc)
+        <ibmdbpy.base.IdaDataBase at 0x9bec860>
+
+        JDBC connectiom, JDBC string and seperate userID and password:
+
+        >>> jdbc = 'jdbc:db2://<HOST>:<PORT>/<DBNAME>'
+        >>> IdaDataBase(dsn=jdbc, uid="<UID>", pwd="<PWD>")
+        <ibmdbpy.base.IdaDataBase at 0x9bec860>
+
+
+         NZPY connection, userID and password are stored in NZPY dict:
+
+         nzpy_dsn ={
+        "database":"<DBNAME>",
+         "port" :<PORT>,
+        "host" : "<HOST>",
+        "securityLevel":<SECURITYLEVEL>,
+        "logLevel":<LOGLEVEL>,
+        "user":"<UID>",
+        "password":"<PWD>"
+        }
+
+        >>> IdaDataBase(dsn=nzpy_dsn)
+        <ibmdbpy.base.IdaDataBase at 0x9bec860>
+
+
+        NZPY connection, NZPY dict and seperate userID and password
+
+         nzpy_dsn ={
+        "database":"<DBNAME>",
+         "port" :<PORT>,
+        "host" : "<HOST>",
+        "securityLevel":<SECURITYLEVEL>,
+        "logLevel":<LOGLEVEL>
+
+        }
+
+        >>> IdaDataBase(dsn=nzpy_dsn, uid="<UID>", pwd="<PWD>")
+
+
+
+        """
+
+        if isinstance(dsn, dict)==False:
+            for arg,name in zip([dsn, uid, pwd],['dsn','uid','pwd']):
+              if not isinstance(arg, six.string_types):
+                raise TypeError("Argument '%s' of type %, expected : string type."%(name,type(arg)))
+
+        self.data_source_name = dsn
+
+
+        # default value for _database_system is db2
+        self._database_system = 'db2'
+        # first delimiter before the parameters in the jdbc-url
+        url_1stparam_del = ':'
+
+
+        # Detect if user attempt to connection with ODBC or JDBC
+        if isinstance(dsn,str):
+            if dsn.startswith('jdbc:'):
+              self._con_type = "jdbc"
+              if dsn.startswith('jdbc:netezza:'):
+                  self._database_system = 'netezza'
+                  # for Netezza the internal connection is always in autocommit mode
+                  # to allow explicit commits
+                  dsn += ';autocommit=false'
+                  url_1stparam_del = ';'
+              elif dsn.startswith('jdbc:db2:'):
+                  self._database_system = 'db2'
+              else:
+                  raise IdaDataBaseError(("The JDBC connection string is invalid for Db2 and Netezza. " +
+                                          "It has to start either with 'jdbc:db2:' or 'jdbc:netezza:'."))
+
+            else:
+                self._con_type='odbc'
+
+        elif isinstance(dsn, dict):
+            self._con_type = "nzpy"
+
+
+        self._idadfs = []
+
+
+
+        if self._con_type == 'nzpy':
+            #parse dict
+            host=''
+            port=0
+            database=''
+            securityLevel=0
+            logLevel=0
+            user =''
+            password=''
+            for key,value in dsn.items():
+                if(key=='host'):
+                    host=value
+                if (key=='port'):
+                    port=value
+                if (key=='database'):
+                    database=value
+                if (key=='securityLevel') :
+                    securityLevel=value
+                if (key=='logLevel') :
+                    logLevel=value
+                if (key == 'user'):
+                    user = value
+                if (key == 'password'):
+                    password = value
+
+            import nzpy;
+            missingCredentialsMsg = ("Missing credentials to connect via NZPY.")
+            ambiguousDefinitionMsg = ("Ambiguous definition of userID or password: " +
+                                      "Cannot be defined in uid and pwd parameters " +
+                                      "and in nzpy dict at the same time.")
+
+
+            uidSpecified = uid != ''
+            pwdSpecified = pwd != ''
+
+            if not ('user' in dsn):
+                if (uidSpecified):
+                    user= uid
+            elif (uidSpecified):
+                raise IdaDataBaseError(ambiguousDefinitionMsg)
+            uidSpecified = uidSpecified | ('user' in dsn)
+
+            if not ('password' in dsn):
+                if (pwdSpecified):
+                   password= pwd
+            elif (pwdSpecified):
+                raise IdaDataBaseError(ambiguousDefinitionMsg)
+            pwdSpecified = pwdSpecified | ('password' in dsn)
+
+            # throw an exception if either uid or pwd are specified,
+            # i.e. not both or none of the two
+            if (uidSpecified ^ pwdSpecified):
+                raise IdaDataBaseError(missingCredentialsMsg)
+
+
+
+            try:
+                if port <= 0:
+                    port = 5480
+                self._con = nzpy.connect(user=user, password=password,host=host, port=port,
+                                        database=database, securityLevel=securityLevel,logLevel=logLevel)
+            except Exception as e:
+                raise IdaDataBaseError(str(e))
+            self._connection_string ={'user':user,'password':password,'host':host,
+                'port':port, 'database':database, 'securityLevel':securityLevel,'logLevel':logLevel}
+            self._database_system = 'netezza'
+
+        if self._con_type == 'odbc' :
+
+            self._connection_string = "DSN=%s; UID=%s; PWD=%s;LONGDATACOMPAT=1;"%(dsn,uid,pwd)
+            """
+            Workaround for CLOB retrieval: 
+            Set the CLI/ODBC LongDataCompat keyword to 1. 
+            Doing so will force the CLI driver to make the 
+            following data type mappings
+            SQL_CLOB to SQL_LONGVARCHAR
+            SQL_BLOB to SQL_LONGVARBINARY
+            SQL_DBCLOB to SQL_WLONGVARCHAR
+            """
+
+            import pyodbc
+            try:
+                self._con = pyodbc.connect(self._connection_string)
+                self._con.setdecoding(pyodbc.SQL_CHAR, encoding='utf-8')
+                self._con.setdecoding(pyodbc.SQL_WCHAR, encoding='utf-8')
+                self._con.setencoding(encoding='utf-8')
+            except Exception as e:
+                raise IdaDataBaseError(str(e))
+            try:
+                self.ida_query("select count(*) from _V_OBJECT")
+                self._database_system = 'netezza'
+            except  Exception as e1:
+                try:
+                    self.ida_query("select CURRENT_SERVER from SYSIBM.SYSDUMMY1")
+                    self._database_system = 'db2'
+                except Exception as e2:
+                    errorMsg = ("The following errors occurred when trying to determine " +
+                                "if the database system is Netezza or Db2:\n" +
+                                "%s\n%s") % (str(e1), str(e2))
+                    raise IdaDataBaseError(errorMsg)
+
+        if self._con_type == 'jdbc':
+
+            missingCredentialsMsg = ("Missing credentials to connect via JDBC.")
+            ambiguousDefinitionMsg = ("Ambiguous definition of userID or password: " +
+                                      "Cannot be defined in uid and pwd parameters " +
+                                      "and in jdbc_url_string at the same time.")
+
+            # remove trailing ":" or ";" or spaces on dsn, we will replace.
+            dsn = dsn.rstrip(';: ')
+            # find parameters on dsn; if any exist, there will be an equals sign.
+            ix = dsn.find("=")
+
+            # if no parameters exist, then this is the complete dsn
+            if (ix < 0):
+                # nothing needs to be done, if uid and pwd are missing
+                # (uid and pwd are not needed for local database connections)
+                # otherwise both uid and pwd have to be specified
+                if uid and pwd:
+                    dsn = dsn + url_1stparam_del + 'user={};password={};'.format(uid, pwd)
+                elif not uid and not pwd:
+                   # Neither UID nor PWD have to be specified for local connections.
+                   # This assumes that if they're missing, the connection is local.
+                    pass
+                else:
+                    raise IdaDataBaseError(missingCredentialsMsg)
+            else:
+                # if we know there is at least one parameter; we can assume there exists a ":" before the parameter
+                # portion of the string in a correctly formatted dsn.  Therefore, just check for the existence of 
+                # the uid and pwd and add if they are missing.  If they are on the string, IGNORE the string as-is.
+
+                uidSpecified = uid != ''
+                pwdSpecified = pwd != ''
+
+                if not ('user=' in dsn):
+                    if (uidSpecified):
+                       dsn = dsn + ';user=' + uid
+                elif (uidSpecified):
+                    raise IdaDataBaseError(ambiguousDefinitionMsg)
+                uidSpecified = uidSpecified | ('user=' in dsn)
+
+                if not ('password=' in dsn):
+                    if (pwdSpecified):
+                        dsn = dsn + ';password=' + pwd
+                elif (pwdSpecified):
+                    raise IdaDataBaseError(ambiguousDefinitionMsg)
+                pwdSpecified = pwdSpecified | ('password=' in dsn)
+
+                # throw an exception if either uid or pwd are specified,
+                # i.e. not both or none of the two
+                if (uidSpecified^pwdSpecified):
+                    raise IdaDataBaseError(missingCredentialsMsg)
+
+                 # add trailing ";" to dsn.
+                dsn = dsn + ';'
+
+            jdbc_url = dsn
+
+            try:
+                import jaydebeapi
+                import jpype
+            except ImportError:
+                raise ImportError("Please install optional dependency jaydebeapi "+
+                            "to work with JDBC.")
+
+            # check versions
+            if jaydebeapi.__version__.startswith('0'):
+                # Older JaydeBeAPi versions where the connection information is specified through a list
+                # are not supported anymore.
+                # The connection information has to be included now in a single connection string.
+                message = ("Your JayDeBeApi module is not supported anymore.  Please install version 1.x or higher.")
+                raise IdaDataBaseError(message)
+
+            if jpype.__version__ != '0.6.3':
+                message = ("Your JPype1 version is not compatible with JayDeBeApi. Please install version 0.6.3.")
+                raise IdaDataBaseError(message)
+
+            here = os.path.abspath(os.path.dirname(__file__))
+            driver_not_found = ""
+            if self._is_netezza_system():
+                driverlibs = ["nzjdbc3.jar"]
+            else:
+                driverlibs = ["db2jcc4.jar", "db2jcc.jar"]
+
+            if (not jpype.isJVMStarted()):
+                classpath = os.getenv('CLASSPATH','')
+                jarpath = ''
+                platform = sys.platform
+
+                if verbose: print("Trying to find a path to the JDBC driver jar file in CLASSPATH (%s)."%classpath)
+
+                if platform == 'win32':
+                    classpaths = classpath.split(';')
+                else:
+                    classpaths = classpath.split(':')
+
+                if any(dl for dl in driverlibs if dl in classpath):
+                    # A path to the driver exists in the classpath variable
+                    jarpaths =  [jp for jp in classpaths if any([dl in jp for dl in driverlibs])]
+                    jarpath = ""
+                    
+                    while jarpaths: # check if a least one path exists
+                        jarpath = jarpaths.pop(0) # just take the first
+                        if os.path.isfile(jarpath): # Is the path correct?
+                            if platform == 'win32':
+                                jarpath = jarpath.split(':')[1].replace('\\', '/') # get rid of windows style formatting
+                            break
+                        else:
+                            if verbose: print("The path %s does not seem to be correct.\nTrying to recover..."%jarpath)
+                            jarpath = ""
+
+                if not jarpath: # Means the search for a direct path in the CLASSPATH was not successful
+                    def _get_jdbc_driver_from_folders(folders):
+                        jarpath = ''
+                        if platform == 'win32':
+                            jarpaths = [fld.split(':')[1].replace('\\', '/') + "/" + dl
+                                        for fld in folders for dl in driverlibs if os.path.isfile(fld + "/" + dl)]
+                            if jarpaths == []:
+                                # check if classpath contains paths with wildcard "*" at the end
+                                jarpaths = [fld.split(':')[1].replace('\\', '/')[:-1] + dl
+                                            for fld in folders for dl in driverlibs if
+                                            fld[-1:] == '*' and os.path.isfile(fld + "/" + dl)]
+                        else:
+                            jarpaths = [fld + "/" + dl for fld in folders for dl in driverlibs if
+                                        os.path.isfile(fld + "/" + dl)]
+                            if jarpaths == []:
+                                # check if classpath contains paths with wildcard "*" at the end
+                                jarpaths = [fld[:-1] + dl
+                                            for fld in folders for dl in driverlibs if
+                                            fld[-1:] == '*' and os.path.isfile(fld[:-1] + dl)]
+                        if jarpaths:
+                            return jarpaths[0]
+                        
+                    if classpath: # There is at least something in the classpath variable
+                        # Let us see if the jar in a folder of the classpath
+                        if verbose: print("Trying to find the JDBC driver in the folders of CLASSPATH (%s)."%classpath)
+
+                        jarpath = _get_jdbc_driver_from_folders(classpaths)
+                    
+                        if not jarpath: # jarpath is still ''
+                            if verbose: print("Trying to find the JDBC driver in the local folder of ibmdbpy (%s)"%here)
+                            # Try to get the path to the driver from the ibmdbpy folder, the last chance
+                            jarpath = _get_jdbc_driver_from_folders([here])
+
+                if jarpath:
+                    if verbose: print("Found it at %s!\nTrying to connect..."%jarpath)
+
+                jpype.startJVM(jpype.getDefaultJVMPath(), '-Djava.class.path=%s' % jarpath)
+
+
+            if self._is_netezza_system():
+                driver_not_found = ("HELP: The Netezza JDBC driver library 'nzjdbc3.jar' could not be found. "+
+                                    "Please, follow the instructions on "+
+                                    "'https://www.ibm.com/support/knowledgecenter/en/SS5FPD_1.0.0/com.ibm.ips.doc/postgresql/odbc/c_datacon_plg_overview.html' "+
+                                    "for downloading and installing the JDBC driver "+
+                                    "and put the file 'nzjdbc3.jar' in the CLASSPATH variable "+
+                                    "or in a folder that is in the CLASSPATH variable. Alternatively place "+
+                                    "it in the folder '%s'."%here)
+
+            else:
+                driver_not_found = ("HELP: The JDBC driver for IBM Db2 could "+
+                                    "not be found. Please download the latest JDBC Driver at the "+
+                                    "following address: 'https://www.ibm.com/support/pages/node/382667' "+
+                                    "and put the file 'db2jcc.jar' or 'db2jcc4.jar' in the CLASSPATH variable "+
+                                    "or in a folder that is in the CLASSPATH variable. Alternatively place "+
+                                    "it in the folder '%s'."%here)
+
+            self._connection_string = jdbc_url
+
+            try:
+                if self._is_netezza_system():
+                    self._con = jaydebeapi.connect('org.netezza.Driver', self._connection_string)
+                else:
+                    self._con = jaydebeapi.connect('com.ibm.db2.jcc.DB2Driver', self._connection_string)
+            except  Exception as e:
+                print(driver_not_found)
+                raise IdaDataBaseError(e)
+             
+            if verbose: print("Connection successful!")
+                
+            #add DB2GSE to the database FUNCTION PATH            
+            #query = "SET CURRENT FUNCTION PATH = CURRENT FUNCTION PATH, db2gse"
+            #self.ida_query(query)
+            #not anymore, reported problems with ODBC
+            #better mention DB2GSE explicitly when accessing its functions
+
+        # determine name of database
+        if self._is_netezza_system():
+            self._database_name = self.ida_scalar_query('select OBJNAME from _T_OBJECT where OBJID = CURRENT_DB;')
+        else:
+            self._database_name = self.ida_scalar_query('select CURRENT_SERVER from SYSIBM.SYSDUMMY1')
+
+        # Setting Autocommit and verbose environment variables
+        set_autocommit(autocommit)
+        set_verbose(verbose)
+
+    ###########################################################################
+    #### Data Exploration
+    ###########################################################################
+
+    @lazy
+    def current_schema(self):
+        """
+        Get the current user schema name as a string.
+
+        Returns
+        -------
+        str
+            User's schema name.
+
+        Examples
+        --------
+        >>> idadb.current_schema()
+        'DASHXXXXXX'
+        """
+        if self._is_netezza_system():
+          query = 'select TRIM(CURRENT_SCHEMA)'
+        else:
+          query = "SELECT TRIM(CURRENT_SCHEMA) FROM SYSIBM.SYSDUMMY1"
+        return self.ida_scalar_query(query)
+
+    def show_tables(self, show_all=False):
+        """
+        Show tables and views that are available in self. By default, this 
+        function shows only tables that belong to a user’s specific schema.
+
+        Parameters
+        ----------
+        show_all : bool
+            If True, all table and view names in the database are returned, 
+            not only those that belong to the user's schema.
+
+        Returns
+        -------
+        DataFrame
+             A data frame containing tables and views names in self with some 
+             additional information (TABSCHEMA, TABNAME, OWNER, TYPE). 
+
+        Examples
+        --------
+        >>> ida_db.show_tables()
+             TABSCHEMA           TABNAME       OWNER TYPE
+        0    DASHXXXXXX            SWISS  DASHXXXXXX    T
+        1    DASHXXXXXX             IRIS  DASHXXXXXX    T
+        2    DASHXXXXXX     VIEW_TITANIC  DASHXXXXXX    V
+        ...
+        >>> ida_db.show_tables(show_all = True)
+             TABSCHEMA           TABNAME       OWNER TYPE
+        0    DASHXXXXXX            SWISS  DASHXXXXXX    T
+        1    DASHXXXXXX             IRIS  DASHXXXXXX    T
+        2    DASHXXXXXX     VIEW_TITANIC  DASHXXXXXX    V
+        2      SYSTOOLS      IDAX_MODELS  DASH101631    A
+        ...
+
+        Notes
+        -----
+        show_tables implements a cache strategy. The cache is stored when the 
+        user calls the method with the argument show_all set to True. This 
+        improves performance because database table look ups are a very common 
+        operation. The cache gets updated each time a table or view is created 
+        or refreshed, each time a table or view is deleted, or when a new 
+        IdaDataFrame is opened.
+
+        """
+        #### DEVELOPERS FIX: UNCOMMENT WHEN DEAD LOCK
+        #show_all = False
+        ####
+
+        # Try to retrieve the cache
+        if show_all and not self._is_netezza_system():
+            cache = self._retrieve_cache("cache_show_tables")
+            if cache is not None:
+                return cache
+
+        where_part = ""
+        if not show_all:
+            where_part = ("AND TABSCHEMA = '%s' " % self.current_schema)
+
+        if self._is_netezza_system():
+            query = ("SELECT SCHEMA as TABSCHEMA, TABLENAME as TABNAME, OWNER, 'T' as TYPE FROM _V_TABLE " +
+                     "WHERE DATABASE = '" + self._database_name + "' and OBJTYPE = 'TABLE' " + where_part +
+                     "UNION ALL " +
+                     "SELECT SCHEMA as TABSCHEMA, VIEWNAME as TABNAME, OWNER, 'V' as TYPE FROM _V_VIEW " +
+                     "WHERE DATABASE = '" + self._database_name + "' and OBJTYPE = 'VIEW' " + where_part)
+            query = ("SELECT SCHEMA as TABSCHEMA, OBJNAME as TABNAME, OWNER, " +
+                             "CASE WHEN OBJTYPE = 'TABLE' THEN 'T' ELSE 'V' END AS TYPE " +
+                      "FROM _V_OBJECTS  " +
+                      "WHERE OBJTYPE in ('TABLE', 'VIEW') " + where_part +
+                      "ORDER BY TABSCHEMA, TABNAME")
+        else:
+            query = ("SELECT DISTINCT TABSCHEMA, TABNAME, OWNER, TYPE " +
+                     "FROM SYSCAT.TABLES " +
+                     "WHERE OWNERTYPE = 'U' " + where_part +
+                     "ORDER BY TABSCHEMA, TABNAME")
+
+        data = self.ida_query(query)
+        
+        # Workaround for some ODBC version which does not get the entire
+        # string of the column name in the cursor descriptor. 
+        # This is hardcoded, so be careful
+        data.columns = ['TABSCHEMA', 'TABNAME', 'OWNER', 'TYPE']
+        
+        data = self._upper_columns(data)
+
+        # Db2 Warehouse FIX: schema "SAMPLES" and "GOSALES" saved with an extra blank,
+        # By doing so, we delete the extra blank.
+        # Note that this works because all cells are of type string.
+
+        # OLD version, created an unexpected bug in some wrong ODBC version
+        # TypeError: unorderable types: bytes() > int()
+        # less pythonic, do not use anymore 
+        #for col in data:
+        #    for index, val in enumerate(data[col]):
+        #        data[col][index] = val.strip()
+
+        # Can be done with a one liner :) 
+        data = data.apply(lambda x: x.apply(lambda x: x.strip()))
+
+        # Cache the result
+        if show_all is True:
+            self.cache_show_tables = data
+
+        return data
+
+    def show_models(self):
+        """
+        Show models that are available in the database.
+
+        Returns
+        -------
+        DataFrame
+
+        Examples
+        --------
+        >>> idadb.show_models()
+            MODELSCHEMA               MODELNAME       OWNER
+        0   DASHXXXXXX  KMEANS_10857_1434974511  DASHXXXXXX
+        1   DASHXXXXXX  KMEANS_11726_1434977692  DASHXXXXXX
+        2   DASHXXXXXX  KMEANS_11948_1434976568  DASHXXXXXX
+        """
+        if self._is_netezza_system():
+            list_models_stmt = ("SELECT MODELNAME, OWNER, CREATED, STATE, MININGFUNCTION, ALGORITHM, USERCATEGORY " +
+                                "FROM INZA.V_NZA_MODELS")
+            result_columns =  ['modelname', 'owner', 'created', 'state','miningfunction', 'algorithm', 'usercategory']
+        else:
+            list_models_stmt = "call IDAX.LIST_MODELS()"
+            result_columns = ['modelschema', 'modelname', 'owner', 'created', 'state',
+                              'miningfunction', 'algorithm', 'usercategory']
+
+        data = self.ida_query(list_models_stmt)
+
+        data.columns = result_columns
+
+        # Workaround for some ODBC version which does not get the entire
+        # string of the column name in the cursor descriptor. 
+        # This is hardcoded, so be careful
+        data = self._upper_columns(data)
+        return data
+
+    def exists_table_or_view(self, objectname):
+        """
+        Check if a table or view exists in self.
+
+        Parameters
+        ----------
+        objectname : str
+            Name of the table or view to check.
+
+        Returns
+        -------
+        bool
+
+        Raises
+        ------
+        TypeError
+            The object exists but is not of the expected type.
+
+        Examples
+        --------
+        >>> idadb.exists_table_or_view("NOT_EXISTING")
+        False
+        >>> idadb.exists_table_or_view("TABLE_OR_VIEW")
+        True
+        >>> idadb.exists_table_or_view("NO_TABLE_NOR_VIEW")
+        TypeError : "NO_TABLE_NOR_VIEW" exists in schema '?' but of type '?'
+        """
+        return self._exists(objectname,['T', 'V'])
+
+    def exists_table(self, tablename):
+        """
+        Check if a table exists in self.
+
+        Parameters
+        ----------
+        tablename : str
+            Name of the table to check.
+
+        Returns
+        -------
+        bool
+
+        Raises
+        ------
+        TypeError
+            The object exists but is not of the expected type.
+
+        Examples
+        --------
+        >>> idadb.exists_table("NOT_EXISTING")
+        False
+        >>> idadb.exists_table("TABLE")
+        True
+        >>> idadb.exists_table("NO_TABLE")
+        TypeError : "tablename" exists in schema "?" but of type '?'
+        """
+        return self._exists(tablename,['T'])
+
+    def exists_view(self, viewname):
+        """
+        Check if a view exists in self.
+
+        Parameters
+        ----------
+        viewname : str
+            Name of the view to check.
+
+        Returns
+        -------
+        bool
+
+        Raises
+        ------
+        TypeError
+            The object exists but is not of the expected type.
+
+        Examples
+        --------
+        >>> idadb.exists_view("NOT_EXISTING")
+        False
+        >>> idadb.exists_view("VIEW")
+        True
+        >>> idadb.exists_view("NO_VIEW")
+        TypeError : "viewname" exists in schema "?" but of type '?'
+        """
+        return self._exists(viewname,['V'])
+
+    def exists_model(self, modelname):
+        """
+        Check if a model exists in self.
+
+        Parameters
+        ----------
+        modelname : str
+            Name of the model to check. It should contain only alphanumeric
+            characters and underscores. All lower case characters will be
+            converted to upper case characters.
+
+        Returns
+        -------
+        bool
+
+        Raises
+        ------
+        TypeError
+            The object exists but is not of the expected type.
+
+        Examples
+        --------
+        >>> idadb.exists_model("MODEL")
+        True
+        >>> idadb.exists_model("NOT_EXISTING")
+        False
+        >>> idadb.exists_model("NO_MODEL")
+        TypeError : NO_MODEL exists but is not a model (of type '?')
+        """
+        modelname = nzpyida.utils.check_modelname(modelname)
+        if '.' in modelname:
+            modelschema, modelname = modelname.split('.')
+        else:
+            modelschema = self.current_schema
+
+        if self._is_netezza_system():
+            # on Netezza the model schema part of the model name is ignored
+            modelquery = "SELECT count(*) FROM INZA.V_NZA_MODELS WHERE MODELNAME ='%s'"%modelname
+            modelexists = self.ida_scalar_query(modelquery) >= 1
+            if modelexists:
+                return True
+        else:
+            # check if schema exists to avoid exception thrown by idax.list_models
+            schemaquery = "SELECT count(*) FROM SYSCAT.SCHEMATA WHERE SCHEMANAME = '%s'"%modelschema
+            schemaexists = self.ida_scalar_query(schemaquery) >= 1
+            if not schemaexists:
+                return False
+            data = self.ida_query("call idax.list_models('schema=%s, where=MODELNAME=''%s''')" % (modelschema, modelname))
+            if not data.empty:
+                return True
+
+        tablelist = self.show_tables(show_all=True)
+        tablelist = tablelist[(tablelist['TABSCHEMA']==modelschema) & (tablelist['TABNAME']==modelname)]
+        if len(tablelist):
+             tabletype = tablelist['TYPE'].values[0]
+             raise TypeError("%s.%s exists, but is not a model (of type '%s')"
+                             % (modelschema, modelname, tabletype))
+
+        else:
+            return False
+
+    def is_table_or_view(self, objectname):
+        """
+        Check if an object is a table or a view in self.
+
+        Parameters
+        ----------
+        objectname : str
+            Name of the object to check.
+
+        Returns
+        -------
+        bool
+
+        Raises
+        ------
+        ValueError
+            objectname doesn't exist in the database.
+
+
+        Examples
+        --------
+        >>> idadb.is_table_or_view("NO_TABLE")
+        False
+        >>> idadb.is_table_or_view("TABLE")
+        True
+        >>> idadb.is_table_or_view("NOT_EXISTING")
+        ValueError : NO_EXISTING does not exist in database
+        """
+        return self._is(objectname,['T','V'])
+
+    def is_table(self, tablename):
+        """
+        Check if an object is a table in self.
+
+        Parameters
+        ----------
+        tablename : str
+            Name of the table to check.
+
+        Returns
+        -------
+        bool
+
+        Raises
+        ------
+        ValueError
+            The object doesn't exist in the database.
+
+
+        Examples
+        --------
+        >>> idadb.is_table("NO_TABLE")
+        False
+        >>> idadb.is_table("TABLE")
+        True
+        >>> idadb.is_table("NOT_EXISTING")
+        ValueError : NO_EXISTING does not exist in database
+        """
+        return self._is(tablename,['T'])
+
+    def is_view(self, viewname):
+        """
+        Check if an object is a view in self.
+
+        Parameters
+        ----------
+        viewname : str
+            Name of the view to check.
+
+        Returns
+        -------
+        bool
+
+        Raises
+        ------
+        ValueError
+            The object doesn't exist in the database.
+
+        Examples
+        --------
+        >>> idadb.is_view("NO_VIEW")
+        False
+        >>> idadb.is_view("VIEW")
+        True
+        >>> idadb.is_view("NOT_EXISTING")
+        ValueError : NO_EXISTING does not exist in database
+        """
+        return self._is(viewname,['V'])
+
+    def is_model(self, modelname):
+        """
+        Check if an object is a model in self.
+
+        Parameters
+        ----------
+        modelname : str
+            Name of the model to check. It should contain only alphanumeric
+            characters and underscores. All lower case characters will be
+            converted to upper case characters.
+
+        Returns
+        -------
+        bool
+
+        Raises
+        ------
+        ValueError
+            The object doesn't exist in the database.
+
+        Examples
+        --------
+        >>> idadb.is_model("MODEL")
+        True
+        >>> idadb.is_model("NO_MODEL")
+        False
+        >>> idadb.is_model("NOT_EXISTING")
+        ValueError : NOT_EXISTING doesn't exist in database
+        """
+        modelname = nzpyida.utils.check_modelname(modelname)
+
+        if '.' in modelname:
+            modelname_noschema = modelname.split('.')[-1]
+        else:
+            modelname_noschema = modelname
+        data = self.show_models()
+        if not data.empty:
+            if modelname_noschema in data['MODELNAME'].values:
+                return True
+
+        # This part is executed if data is empty or model is not in data
+        try:
+            self.is_table_or_view(modelname)
+        except:
+            raise
+        else:
+            return False
+
+    def ida_query(self, query, silent=False, first_row_only=False, autocommit = False):
+        """
+        Prepare, execute and format the result of a query in a dataframe or
+        in a Tuple. If nothing is expected to be returned for the SQL command,
+        nothing is returned. 
+
+        Parameters
+        ----------
+        query : str
+            Query to be executed.
+        silent: bool, default: False
+             If True, the query is not printed in the python console even if 
+             the verbosity mode is activated (VERBOSE environment variable is 
+             equal to “True”). 
+        first_row_only : bool, default: False
+             If True, only the first row of the result is returned as a Tuple.
+        autocommit: bool, default: False
+            If True, the autocommit function is available.
+
+        Returns
+        -------
+        DataFrame or Tuple (if first_row_only=False)
+
+        Examples
+        --------
+        >>> idadb.ida_query("SELECT * FROM IRIS LIMIT 5")
+           sepal_length  sepal_width  petal_length  petal_width species
+        0           5.1          3.5           1.4          0.2  setosa
+        1           4.9          3.0           1.4          0.2  setosa
+        2           4.7          3.2           1.3          0.2  setosa
+        3           4.6          3.1           1.5          0.2  setosa
+        4           5.0          3.6           1.4          0.2  setosa
+
+        >>> idadb.ida_query("SELECT COUNT(*) FROM IRIS")
+        (150, 150, 150, 150)
+
+        Notes
+        -----
+        If first_row_only argument is True, then even if the actual result of 
+        the query is composed of several rows, only the first row will be 
+        returned.
+        """
+        self._check_connection()
+        return sql.ida_query(self, query, silent, first_row_only, autocommit)
+
+    def ida_scalar_query(self, query, silent=False, autocommit = False):
+        """
+        Prepare and execute a query and return only the first element as a 
+        string. If nothing is returned from the SQL query, an error occurs.
+
+        Parameters
+        ----------
+        query : str
+            Query to be executed.
+        silent: bool, default: False
+            If True, the query will not be printed in python console even if
+            verbosity mode is activated.
+        autocommit: bool, default: False
+            If True, the autocommit function is available.
+
+        Returns
+        -------
+        str or Number
+
+        Examples
+        --------
+        >>> idadb.ida_scalar_query("SELECT TRIM(CURRENT_SCHEMA) from SYSIBM.SYSDUMMY1")
+        'DASHXXXXX'
+
+        Notes
+        -----
+        Even if the actual result of the query is composed of several columns
+        and several rows, only the first element (top-left) will be returned.
+        """
+        self._check_connection()
+        return sql.ida_scalar_query(self, query, silent, autocommit)
+
+    ###############################################################################
+    #### Upload DataFrames
+    ###############################################################################
+
+    @timed
+    def as_idadataframe(self, dataframe, tablename=None, clear_existing=False, primary_key=None, indexer=None):
+        """
+        Upload a dataframe and return its corresponding IdaDataFrame. The target
+        table (tablename) will be created or replaced if the option clear_existing
+        is set to True.
+        
+        To add data to an existing tables, see IdaDataBase.append
+
+        Parameters
+        ----------
+        dataframe : DataFrame
+            Data to be uploaded, contained in a Pandas DataFrame.
+        tablename : str, optional
+            Name to be given to the table created in the database.
+            It should contain only alphanumeric characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If not given, a valid tablename is generated (for example, DATA_FRAME_X
+            where X is a random number).
+        clear_existing : bool
+            If set to True, a table will be replaced when a table with the same 
+            name already exists  in the database.
+        primary_key : str
+            Name of a column to be used as primary key.
+
+        Returns
+        -------
+        IdaDataFrame
+
+        Raises
+        ------
+        TypeError
+            * Argument dataframe is not of type pandas.DataFrame.
+            * The primary key argument is not a string.
+
+        NameError
+            * The name already exists in the database and clear_existing is False.
+            * The primary key argument doesn't correspond to a column.
+
+        PrimaryKeyError
+            The primary key contains non unique values.
+
+        Examples
+        --------
+        >>> from nzpyida.sampledata.iris import iris
+        >>> idadb.as_idadataframe(iris, "IRIS")
+        <ibmdbpy.frame.IdaDataFrame at 0xb34a898>
+        >>> idadb.as_idadataframe(iris, "IRIS")
+        NameError: IRIS already exists, choose a different name or use clear_existing option.
+        >>> idadb.as_idadataframe(iris, "IRIS2")
+        <ibmdbpy.frame.IdaDataFrame at 0xb375940>
+        >>> idadb.as_idadataframe(iris, "IRIS", clear_existing = True)
+        <ibmdbpy.frame.IdaDataFrame at 0xb371cf8>
+        
+        Notes
+        -----
+        This function is not intended to be used to add data to an existing table,
+        rather to create a new table from a dataframe. To add data to an existing
+        table, please consider using IdaDataBase.append 
+        """
+        if not isinstance(dataframe, pd.DataFrame):
+            raise TypeError("Argument dataframe is not of type Pandas.DataFrame")
+
+        if tablename is None:
+                tablename = self._get_valid_tablename(prefix="DATA_FRAME_")
+
+        tablename = nzpyida.utils.check_tablename(tablename)
+
+        if primary_key is not None:
+            if not isinstance(primary_key, six.string_types):
+                raise TypeError("The primary key argument should be a string")
+            if primary_key not in dataframe.columns:
+                raise ValueError("The primary key should be the name of a column" +
+                                 " in the given dataframe")
+            if len(dataframe[primary_key]) != len(set(dataframe[primary_key])):
+                raise PrimaryKeyError(primary_key + " cannot be a primary key for" +
+                                      "table " + tablename + " because it contains" +
+                                      " non unique values")
+
+        if self.exists_table_or_view(tablename):
+            if clear_existing:
+                try:
+                    self.drop_table(tablename)
+                except:
+                    self.drop_view(tablename)
+            else:
+                raise NameError(("%s already exists, choose a different name "+
+                                "or use clear_existing option.")%tablename)
+
+        self._create_table(dataframe, tablename, primary_key=primary_key)
+        idadf = nzpyida.frame.IdaDataFrame(self, tablename, indexer)
+        self.append(idadf, dataframe)
+
+        ############## Experimental ##################
+        # dataframe.to_sql(tablename, self._con, index=False)
+        # idadf = ibmdbpy.frame.IdaDataFrame(self, tablename, flavor='mysql',
+        #                                    schema = self.current_schema)
+
+        self._autocommit()
+
+        if primary_key:
+            idadf._indexer=primary_key
+        return idadf
+        
+    ###########################################################################
+    #### Delete DataBase objects
+    ###########################################################################
+
+    def drop_table(self, tablename):
+        """
+        Drop a table in the database.
+
+        Parameters
+        ----------
+        tablename : str
+            Name of the table to drop.
+
+        Raises
+        ------
+        ValueError
+            If the object does not exist.
+        TypeError
+            If the object is not a table.
+
+        Examples
+        --------
+        >>> idadb.drop_table("TABLE")
+        True
+        >>> idadb.drop_table("NO_TABLE")
+        TypeError : NO_TABLE  exists in schema '?' but of type '?'
+        >>> idadb.drop_table("NOT_EXISTING")
+        ValueError : NO_EXISTING doesn't exist in database
+
+        Notes
+        -----
+            This operation cannot be undone if autocommit mode is activated.
+        """
+        return self._drop(tablename, "T")
+
+    def drop_view(self, viewname):
+        """
+        Drop a view in the database.
+
+        Parameters
+        ----------
+        viewname : str
+            Name of the view to drop.
+
+        Raises
+        ------
+        ValueError
+            If the object does not exist.
+        TypeError
+            If the object is not a view.
+
+        Examples
+        --------
+        >>> idadb.drop_view("VIEW")
+        True
+        >>> idadb.drop_view("NO_VIEW")
+        TypeError : NO_VIEW exists in schema '?' but of type '?'
+        >>> idadb.drop_view("NOT_EXISTING")
+        ValueError : NO_EXISTING doesn't exist in database
+
+        Notes
+        -----
+            This operation cannot be undone if autocommit mode is activated.
+        """
+        return self._drop(viewname, "V")
+
+    def drop_model(self, modelname):
+        """
+        Drop a model in the database.
+
+        Parameters
+        ----------
+        modelname : str
+            Name of the model to drop.
+
+        Raises
+        ------
+        ValueError
+            If the object does not exist.
+        TypeError
+            if the object exists but is not a model.
+
+        Examples
+        --------
+        >>> idadb.drop_model("MODEL")
+        True
+        >>> idadb.drop_model("NO_MODEL")
+        TypeError : NO_MODEL exists in schema '?' but of type '?'
+        >>> idadb.drop_model("NOT_EXISTING")
+        ValueError : NOT_EXISTING does not exist in database
+
+        Notes
+        -----
+            This operation cannot be undone if autocommit mode is activated.
+
+        """
+
+        try:
+            self._call_stored_procedure("DROP_MODEL ", model = modelname)
+        except Exception as e:
+            try:
+                flag = self.exists_table(modelname)
+            except TypeError:
+                # Exists but is not a model (nor a table)
+                raise
+            else:
+                if flag:
+                    # It is a table so make it raise by calling exists_view
+                    self.exists_view(modelname)
+            value_error = ValueError(modelname + " does not exist in database")
+            six.raise_from(value_error, e)
+        else:
+            tables = self.show_tables()
+            if not tables.empty:
+                for table in tables['TABNAME']:
+                    if modelname in table:
+                        self.drop_table(table)
+            return True
+
+    @timed
+    def rename(self, idadf, newname):
+        """
+        Rename a table referenced by an IdaDataFrame in Db2 Warehouse.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+            IdaDataFrame object referencing the table to rename.
+        newname : str
+            Name to be given to self. It should contain only alphanumeric
+            characters and underscores. All lower case characters will be 
+            converted to upper case characters. The new name should not already 
+            exist in the database.
+
+        Raises
+        ------
+        ValueError
+            The new tablename is not valid.
+        TypeError
+            Rename function is supported only for table type.
+        NameError
+            The name of the object to be created is identical to an existing name.
+
+        Notes
+        -----
+            Upper case characters and numbers, optionally separated by 
+            underscores “_”, are valid characters.
+        """
+        # Actually we could support it for views too
+        # Question : Is it better to accept an idadf as argument or rather the name of the table?
+        oldname = idadf._name
+        newname = nzpyida.utils.check_tablename(newname)
+
+        if self.is_table(idadf._name):
+            if self._is_netezza_system():
+                query = "ALTER TABLE %s RENAME TO %s"%(idadf._name, newname)
+            else:
+                query = "RENAME TABLE %s TO %s"%(idadf._name, newname)
+            try:
+                self._prepare_and_execute(query)
+            except Exception as e:
+                if self._con_type == "odbc":
+                    raise NameError(e.value[-1])
+                else:
+                    if self._is_netezza_system():
+                        if "ERROR:  relation does not exist" in str(e.args[0]):
+                            raise ValueError("Object does not exist.")
+                        elif 'ERROR:  ALTER TABLE: object "%s" already exists'%newname in str(e.args[0]):
+                            raise NameError("The new name is identical to the old one")
+                        else:
+                            raise e
+                    else:
+                        sql_code = int(str(e.args[0]).split("SQLCODE=")[-1].split(",")[0])
+                        if sql_code == -601:
+                            raise NameError("The new name is identical to the old one")
+                        else:
+                            raise e
+
+            idadf._name = newname
+            idadf.tablename = newname
+            idadf.internal_state.name = newname
+            self._reset_attributes("cache_show_tables")
+
+            # Update name of all IdaDataFrame that were opened on this table
+            for idadf in self._idadfs:
+                if idadf._name == oldname:
+                    idadf._name = newname
+                    idadf.internal_state.name = newname # to refactor
+        else:
+            raise TypeError("Rename function is supported only for table type")
+
+    def add_column(self, idadf, column = None, colname = None, ncat=10):
+        """
+        Add physically a column to the dataset.
+        If column argument is set to None, a random column is generated which 
+        can take values for 1 to ncat.
+        Used for benchmark purpose. 
+        Note: This method is deprecated. Can probably be removed without impact 
+        """
+        if column is not None:
+            if column not in idadf.columns:
+                raise ValueError("Unknown column %s"%column)
+            dtype = idadf.dtypes['TYPENAME'][column]
+            if dtype == "VARCHAR":
+                dtype = dtype + "(255)"
+        else:
+            dtype = "INTEGER"
+        
+        if colname is None:
+            i = 0
+            colname = "COL%s"%i
+            while colname in idadf.columns:
+                i += 1
+                colname = "COL%s"%i
+        else:
+            if colname in idadf.columns:
+                raise ValueError("Column %s already exists"%colname)
+        
+        self.commit() 
+        query = "ALTER TABLE %s ADD \"%s\" %s"%(idadf._name, colname, dtype) 
+        print(query)         
+        try:
+            self._prepare_and_execute(query)
+        except:
+            print("Failed ! Trying again in 5 seconds")
+            #query2 = "REORG TABLE %s"%idadf._name
+            #print(query2)
+            import time
+            time.sleep(5)
+            self._prepare_and_execute(query)
+        self.commit()
+        
+        # force sleep 2 sec
+        
+        
+        if column is None:
+            # Random column
+            query = "UPDATE %s SET \"%s\" = RAND()* %s + 1"%(idadf._name, colname, ncat)
+        else:
+            query = "UPDATE %s SET \"%s\" = \"%s\""%(idadf._name, colname, column)
+        
+        print(query)
+        
+        try:
+            self._prepare_and_execute(query)
+        except:
+            print("Failed ! Trying again in 5 seconds")
+            import time
+            time.sleep(5)
+            self._prepare_and_execute(query)
+                
+            
+        self.commit()
+        idadf._reset_attributes(["columns", "dtypes", "shape"])
+        
+            
+    
+    @timed
+    def add_column_id(self, idadf, column_id="ID", destructive=False):
+        # TODO: Base the creation of the idea on the sorting of several columns
+        # (or all columns in case there are duplicated rows) so that the ID
+        # can be created in a determinstic and reproducible way
+        """
+        Add an ID column to an IdaDataFrame.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+            IdaDataFrame object to which an ID column will be added
+        column_id : str
+            Name of the ID column to add
+        destructive : bool
+            If set to True, the column will be added phisically in the database.
+            This can take time. If set to False, the column will be added virtually
+            in a view and a new IdaDataFrame is returned.
+
+        Raises
+        ------
+        TypeError
+            idadf is not an IdaDataFrame.
+        ValueError
+            The given column name already exists in the DataBase.
+
+        Notes
+        -----
+            The non-destructive creation of column IDs is not reliable, because row IDs are recalculated on the fly in a non-deterministic 
+            way each time a new view is produced. On the contrary, creating them destructively i.e physically is reliable but can take time. 
+            If no sorting has been done whatsoever before, row IDs will be created at random.
+            Improvement idea: create ID columns in a non-destructive way and base them on the sorting of a set of columns, 
+            defined by the user, or all columns if no column combination results in unique identifiers.
+        """
+        if isinstance(idadf, nzpyida.IdaSeries):
+            raise TypeError("Adding column ID is not supported for IdaSeries")
+        if not isinstance(idadf, nzpyida.IdaDataFrame):
+            raise TypeError("idadf is not an IdaDataFrame, type: %s"%type(idadf))
+        if column_id in idadf.columns:
+            raise ValueError("A column named '"+column_id+"' already exists." +
+                             " Please define a new column name using"+
+                             " column_id argument")
+
+        if destructive is True:
+
+            viewname = self._get_valid_tablename(prefix="VIEW_")
+            if self._is_netezza_system():
+                order_by = "ORDER BY NULL"
+            else:
+                order_by = ""
+            self._prepare_and_execute("CREATE VIEW " + viewname + " AS SELECT ((ROW_NUMBER() OVER("+ order_by +"))-1)" +
+                                      " AS \"" + column_id + "\", \""+
+                                      "\",\"".join(idadf._get_all_columns_in_table()) +
+                                      "\" FROM " + idadf._name)
+
+            # Initiate the modified table under a random name
+            tablename = self._get_valid_tablename(prefix="DATA_FRAME_")
+            if self._is_netezza_system():
+                 self._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s)"%(tablename,viewname))
+            else:
+                self._prepare_and_execute("CREATE TABLE %s LIKE %s"%(tablename,viewname))
+                self._prepare_and_execute("INSERT INTO %s (SELECT * FROM %s)"%(tablename,viewname))
+
+            # Drop the view and old table
+            self.drop_view(viewname)
+            self.drop_table(idadf._name)
+
+            # Give it the original name back
+            self._reset_attributes("cache_show_tables")
+            new_idadf = nzpyida.IdaDataFrame(self, tablename)
+            self.rename(new_idadf, idadf.tablename)
+
+            # Updating internal state
+            # prepend the columndict OrderedDict
+            items = idadf.internal_state.columndict.items()
+            idadf.internal_state.columndict = OrderedDict()
+            idadf.internal_state.columndict[column_id] = "\"" + column_id + "\""
+            for item in items:
+                idadf.internal_state.columndict[item[0]] = item[1]
+
+            idadf.internal_state.update()
+            idadf._reset_attributes(["columns"])
+            idadf.indexer = column_id
+        else:
+            # prepend the columndict OrderedDict
+            items = idadf.internal_state.columndict.items()
+            idadf.internal_state.columndict = OrderedDict()
+            if self._is_netezza_system():
+                order_by = "ORDER BY NULL"
+            else:
+                order_by = ""
+            idadf.internal_state.columndict[column_id] = "((ROW_NUMBER() OVER("+ order_by +"))-1)"
+            for item in items:
+                idadf.internal_state.columndict[item[0]] = item[1]
+            idadf.internal_state.update()
+            idadf._reset_attributes(["columns"])
+            idadf.indexer = column_id
+
+        # Reset attributes
+        idadf._reset_attributes(['shape', 'columns', 'axes', 'dtypes'])
+
+    def delete_column(self, idadf, column_name, destructive=False):
+        """
+        Delete a column in an idaDataFrame.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+            The IdaDataframe in which a column should be deleted.
+        column_name : str
+            Name of the column to delete.
+        destructive : bool
+            If set to True, the column is deleted in the database. Otherwise, 
+            it is deleted virtually, creating a view for the IdaDataFrame.
+
+        Raises
+        ------
+        TypeError
+            column_name should be a string.
+        ValueError
+            column_name refers to a column that doesn't exist in self.
+        """
+        if not isinstance(column_name, six.string_types):
+            raise TypeError("column_name is not of string type")
+        if column_name not in idadf.columns:
+            raise ValueError("%s refers to a columns that doesn't exists in self"%(column_name))
+
+        if destructive is True:
+            if column_name not in idadf._get_all_columns_in_table():
+                # Detect it is a virtual ID, the deletion cannot be destructive
+                return self.delete_column(idadf, column_name, destructive=False)
+
+            viewname = self._get_valid_tablename(prefix="VIEW_")
+            columnlist = list(idadf._get_all_columns_in_table())
+            columnlist.remove(column_name)
+
+            self._prepare_and_execute("CREATE VIEW " + viewname + " AS SELECT \""+
+                                      "\",\"".join(columnlist) +
+                                      "\" FROM " + idadf._name)
+
+            tablename = self._get_valid_tablename(prefix="DATA_FRAME_")
+
+            if self._is_netezza_system():
+                self._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s)"%(tablename,viewname))
+            else:
+                self._prepare_and_execute("CREATE TABLE " + tablename + " LIKE " + viewname)
+                self._prepare_and_execute("INSERT INTO " + tablename + " (SELECT * FROM " + viewname + ")")
+
+            # Drop the view and old table
+            self.drop_view(viewname)
+            self.drop_table(idadf._name)
+
+            # Give it the original name back
+            self._reset_attributes("cache_show_tables") # normally, no needed
+            new_idadf = nzpyida.IdaDataFrame(self, tablename, idadf.indexer)
+            self.rename(new_idadf, idadf.tablename)
+
+            # updating internal state
+            del idadf.internal_state.columndict[column_name]
+            idadf.internal_state.update()
+            self._reset_attributes("cache_show_tables")
+            idadf._reset_attributes(['shape', 'columns', 'dtypes'])
+
+        else:
+            del idadf.internal_state.columndict[column_name]
+            idadf.internal_state.update()
+            idadf._reset_attributes(["columns", "shape", "dtypes"])
+
+        if column_name == idadf.indexer:
+            idadf._reset_attributes(["_indexer"])
+
+
+    def append(self, idadf, df, maxnrow=None):
+        """
+        Append rows of a DataFrame to an IdaDataFrame. The DataFrame must have 
+        the same structure (same column names and datatypes). Optionally, the 
+        DataFrame to be added can be splitted into several chunks. This 
+        improves performance and prevents SQL overflows. By default, chunks are 
+        limited to 100.000 cells.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+            IdaDataFrame that receives data from dataframe df.
+        df : DataFrame
+            Dataframe whose rows are added to IdaDataFrame idadf.
+        maxnrow : int, optional
+            number corresponding to the maximum number of rows for each chunks.
+
+        Raises
+        ------
+        TypeError
+            * maxnrow should be an interger.
+            * Argument idadf should be an IdaDataFrame.
+            * Argument df should be a pandas DataFrame.
+        ValueErrpr
+            * maxnrow should be greater than 1 or nleft blank.
+            * Other should be a Pandas DataFrame.
+            * Other dataframe has not the same number of columns as self.
+            * Some columns in other have different names that are different from the names of the columns in self.
+        """
+        # SANITY CHECK : maxnrow
+        if maxnrow is None:
+            # Note : it has been measured on a big dataset (>1 million rows) that int(100000 / len(df.columns)) 
+            # performs better than the previous empirical value int(8000 / len(df.columns)) 
+            maxnrow = int(100000 / len(df.columns))
+        else:
+            if not isinstance(maxnrow, six.integer_types):
+                raise TypeError("maxnrow is not an integer")
+            if maxnrow < 1:
+                raise ValueError("maxnrow should be stricly positive or omitted")
+            if maxnrow > 15000:
+                warnings.warn("Performance may decrease if maxnrow is bigger than 15000", UserWarning)
+
+        # SANITY CHECK : idadf & other
+        if not isinstance(idadf, nzpyida.frame.IdaDataFrame):
+            raise TypeError("Argument idadf is not an IdaDataFrame")
+        if not isinstance(df, pd.DataFrame):
+            raise TypeError("Argument df is not of type pPandas.DataFrame")
+        if len(df.columns) != len(idadf.columns):
+            raise ValueError("(Ida)DataFrames don't have the same number of columns")
+        if any([column not in idadf.columns for column in [str(x).strip() for x in df.columns]]):
+            raise ValueError("Some column names do not match current object columns: \n" +
+                             "Expected : \t" + str(idadf.columns) + "\n" +
+                             "Found : \t" + str([x.strip() for x in df.columns]) + "\n")
+        if any([str(column_idadf) != str(column_other).strip() for column_idadf, column_other in
+                zip(idadf.columns, df.columns)]):
+            raise ValueError("Order or columns in other and" + idadf._name + "does not match.")
+
+        if df.shape[0] > 1.5 * maxnrow:
+            split_into = math.ceil(df.shape[0] / maxnrow)
+            split = np.array_split(df, split_into)
+            print("DataFrame will be splitted into " + str(split_into) +
+                  " chunks. (" + str(maxnrow) + " rows per chunk)")
+            for i, chunk in enumerate(split, 0):
+                percentage = int(i / split_into * 100)
+                print("Uploaded: " + str(percentage) + "%... ", end="\r")
+                try:
+                    self._insert_into_database(chunk, idadf.schema, idadf.tablename, silent=True)
+                except:
+                    raise
+            print("Uploaded: %s/%s... "%(split_into,split_into), end="")
+            print("[DONE]")
+        else:
+            print("Uploading %s rows (maxnrow was set to %s)"%(df.shape[0], maxnrow))
+            try:
+                self._insert_into_database(df, idadf.schema, idadf.tablename, silent=True)
+            except:
+                raise
+
+        idadf._reset_attributes(['shape', 'axes', 'dtypes', 'index'])
+
+    def merge(self, idadf, other, key):
+        # TODO:
+        pass
+
+    ###############################################################################
+    #### Connection management
+    ###############################################################################
+
+    def commit(self):
+        """
+        Commit operations in the database.
+
+        Notes
+        -----
+
+        All changes that are made in the database after the last commit, 
+        including those in the child IdaDataFrames, are commited.
+
+        If the environment variable ‘VERBOSE’ is set to True, the commit 
+        operations are notified in the console.
+        """
+        self._check_connection()  # Important
+        self._con.commit()
+        if os.getenv('VERBOSE') == 'True':
+            print("<< COMMIT >>")
+        self._reset_attributes("cache_show_tables")
+
+    def rollback(self):
+        """
+        Rollback operations in the database.
+
+        Notes
+        -----
+
+        All changes that are made in the database after the last commit, 
+        including those in the child IdaDataFrames, are discarded.
+        """
+        self._check_connection()  # Important
+        self._con.rollback()
+        if os.getenv('VERBOSE') == 'True':
+            print("<< ROLLBACK >>")
+        self._reset_attributes("cache_show_tables")
+
+    def close(self):
+        """
+        Close the IdaDataBase connection.
+
+        Notes
+        -----
+
+        If the environment variable ‘AUTOCOMMIT’ is set to True, then all 
+        changes after the last commit are committed, otherwise they are 
+        discarded.
+
+        """
+        if os.getenv('AUTOCOMMIT') == 'True':
+            self.commit()
+        else:
+            self.rollback()
+        self._reset_attributes("cache_show_tables")
+        self._con.close()
+        print("Connection closed.")
+
+    def reconnect(self):
+        """
+        Try to reopen the connection.
+        """
+        try:
+            self._check_connection()
+        except IdaDataBaseError:
+            if self._con_type == 'odbc':
+                import pyodbc
+                try:
+                    self._con = pyodbc.connect(self._connection_string)
+                except:
+                    raise
+                else:
+                    print("The connection was successfully restored")
+            elif self._con_type == 'nzpy':
+                import nzpy
+                try:
+                    self._con = nzpy.connect(**(self._connection_string))
+                except:
+                    raise
+                else:
+                    print("The connection was successfully restored")
+            elif self._con_type == 'jdbc':
+                try:
+                    import jaydebeapi
+                    if self._is_netezza_system():
+                        self._con = jaydebeapi.connect('org.netezza.Driver', self._connection_string)
+                    else:
+                        self._con = jaydebeapi.connect('com.ibm.db2.jcc.DB2Driver', self._connection_string)
+                except:
+                    raise
+                else:
+                    print("The connection was successfully restored")
+        else:
+            print("The connection for current IdaDataBase is valid")
+
+        ###############################################################################
+        #### Private methods
+        ###############################################################################
+
+    def __enter__(self):
+        """
+        Allow the object to be used with a "with" statement
+        """
+        return self
+
+    def __exit__(self):
+        """
+        Allow the object to be used with a "with" statement. Make sure the
+        connection is closed when the object get out of scope
+        """
+        self.close()
+
+    def _exists(self, objectname, typelist):
+        """
+        Check if an object of a certain type exists in Db2 Warehouse.
+
+        Notes
+        -----
+        For more information, see exists_table_or_view, exists_table, 
+        exists_view functions.
+        """
+        objectname = nzpyida.utils.check_tablename(objectname)
+
+        tablelist = self.show_tables(show_all=True)
+        schema, name = self._get_name_and_schema(objectname)
+        tablelist = tablelist[tablelist['TABSCHEMA'] == schema]
+
+        if len(tablelist):
+            if name in tablelist['TABNAME'].values:
+                tabletype = tablelist[tablelist['TABNAME'] == name]['TYPE'].values[0]
+                if tabletype in typelist:
+                    return True
+                else:
+                    raise TypeError("%s exists in schema %s but of type \"%s\""
+                                    %(objectname,schema,tabletype))
+            else:
+                return False
+        else:
+            return False
+
+    def _is(self, objectname, typelist):
+        """
+        Check if an existing object is of a certain type or in a list of types.
+
+        Notes
+        -----
+        For more information, see is_table_or_view, is_table, is_view functions.
+        """
+        objectname = nzpyida.utils.check_tablename(objectname)
+
+        tablelist = self.show_tables(show_all=True)
+        schema, name = self._get_name_and_schema(objectname)
+        tablelist = tablelist[tablelist['TABSCHEMA'] == schema]
+
+        if len(tablelist):
+            if name in tablelist['TABNAME'].values:
+                tabletype = tablelist[tablelist['TABNAME'] == name]['TYPE'].values[0]
+                if tabletype in typelist:
+                    return True
+                else:
+                    return False
+
+        raise ValueError("%s does not exist in database"%(objectname))
+
+    def _drop(self, objectname, object_type = "T"):
+        """
+        Drop an object in the table depending on its type.
+        Admissible type values are "T" (table) and "V" (view)
+
+        Notes
+        -----
+        For more information, seedrop_table and drop_view functions.
+
+        """
+        objectname = nzpyida.utils.check_tablename(objectname)
+
+        if object_type == "T":
+            to_drop = "TABLE"
+        elif object_type == "V":
+            to_drop = "VIEW"
+        else:
+            raise ValueError("Unknown type to drop")
+
+        try:
+            self._prepare_and_execute("DROP %s %s"%(to_drop,objectname))
+        except Exception as e:
+            if self._con_type == "odbc":
+                if e.value[0] == "42S02":
+                    raise ValueError(e.value[1])  # does not exist
+                if e.value[0] == "42809":
+                    raise TypeError(e.value[1])  # object is not of expected type
+            else:
+                if self._is_netezza_system():
+                    if "ERROR:  relation does not exist" in str(e.args[0]):
+                        raise ValueError("Object does not exist.")
+                    else:
+                        raise e
+                else:
+                    sql_code = int(str(e.args[0]).split("SQLCODE=")[-1].split(",")[0])
+                    if sql_code == -204:
+                        raise ValueError("Object does not exist.")
+                    elif sql_code == -159:
+                        raise TypeError("Object is not of expected type")
+                    else:
+                        raise e # let the expection raise anyway
+        else:
+            self._reset_attributes("cache_show_tables")
+            return True
+
+    def _upper_columns(self, dataframe):
+        # Could be moved to utils (then move in the test too)
+        """
+        Put every column name of a Pandas DataFrame in upper case.
+
+        Parameters
+        ----------
+        dataframe : DataFrame
+
+        Returns
+        -------
+        DataFrame
+        """
+        data = deepcopy(dataframe)
+        if len(data):
+            data.columns = [x.upper() for x in data.columns]
+        return data
+
+    def _get_name_and_schema(self, objectname):
+        """
+        Helper function that returns the name and the schema from an object 
+        name. Implicitly, if no schema name was given, it is assumed that user 
+        refers to the current schema.
+
+        Parameters
+        ----------
+        objectname : str
+            Name of the object to process. Can be either under the form
+            "SCHEMA.TABLE" or just "TABLE"
+
+        Returns
+        -------
+        tuple
+            A tuple composed of 2 strings containing the schema and the name.
+
+        Examples
+        --------
+        >>> _get_name_and_schema(SCHEMA.TABLE)
+        (SCHEMA, TABLE)
+        >>> _get_name_and_schema(TABLE)
+        (<current schema>, TABLE)
+        """
+        if '.' in objectname:
+            name = objectname.split('.')[-1]
+            schema = objectname.split('.')[0]
+        else:
+            name = objectname
+            schema = self.current_schema
+        return (schema, name)
+
+    def _get_valid_tablename(self, prefix="DATA_FRAME_"):
+        """
+        Generate a valid database table name.
+
+        Parameters
+        ----------
+        prefix : str, default: "DATA_FRAME_"
+            Prefix used to create the table name. The name is constructed using 
+            this pattern : <prefix>_X where <prefix> corresponds to the string 
+            parameter “prefix” capitalized and X corresponds to a pseudo 
+            randomly generated number (0-100000).
+
+        Returns
+        -------
+        str
+
+        Examples
+        --------
+        >>> idadb._get_valid_tablename()
+        'DATA_FRAME_49537_1434978215'
+        >>> idadb._get_valid_tablename("MYDATA_")
+        'MYDATA_65312_1434978215'
+        >>> idadb._get_valid_tablename("mydata_")
+        'MYDATA_78425_1434978215'
+        >>> idadb._get_valid_tablename("mydata$")
+        ValueError: Table name is not valid, only alphanumeric characters and underscores are allowed.
+        """
+        prefix = nzpyida.utils.check_tablename(prefix)
+        # We may assume that the generated name is unlikely to exist
+        name = "%s%s_%s" % (prefix, random.randint(0, 100000), int(time()))
+        return name
+
+    def _get_valid_viewname(self, prefix="VIEW_"):
+        """
+        Convenience function : Alternative name for _get_valid_tablename.
+
+        The parameter prefix has its optional value changed to "VIEW\_"".
+
+        Examples
+        --------
+        >>> idadb._get_valid_viewname()
+        'VIEW_49537_1434978215'
+        >>> idadb._get_valid_viewname("MYVIEW_")
+        'MYVIEW_65312_1434978215'
+        >>> idadb._get_valid_viewname("myview_")
+        'MYVIEW_78425_1434978215'
+        >>> idadb._get_valid_modelname("myview$")
+        ValueError: View name is not valid, only alphanumeric characters and underscores are allowed.
+        """
+        return self._get_valid_tablename(prefix)
+
+    def _get_valid_modelname(self, prefix="MODEL_"):
+        """
+        Convenience function : Alternative name for _get_valid_tablename.
+
+        Parameter prefix has its optional value changed to "MODEL\_".
+
+        Examples
+        --------
+        >>> idadb._get_valid_modelname()
+        'MODEL_49537_1434978215'
+        >>> idadb._get_valid_modelname("TEST_")
+        'TEST_65312_1434996318'
+        >>> idadb._get_valid_modelname("test_")
+        'TEST_78425_1435632423'
+        >>> idadb._get_valid_tablename("mymodel$")
+        ValueError: Table name is not valid, only alphanumeric characters and underscores are allowed.
+        """
+        return self._get_valid_tablename(prefix)
+
+    def _create_table(self, dataframe, tablename, primary_key=None):
+        """
+        Create a new table in the database by declaring its name and columns 
+        based on an existing DataFrame. It is possible declare a column as 
+        primary key.
+
+        Parameters
+        ----------
+        dataframe : DataFrame
+            Pandas DataFrame be used to initiate the table.
+        tablename : str
+            Name to be given to the table at its creation.
+        primary_key: str
+            Name of a column to declare as primary key.
+
+        Notes
+        -----
+        The columns and their data type is deducted from the Pandas DataFrame 
+        given as parameter.
+
+        Examples
+        --------
+        >>> from nzpyida.sampledata.iris import iris
+        >>> idadb._create_table(iris, "IRIS")
+        'IRIS'
+        >>> idadb._create_table(iris)
+        'DATA_FRAME_4956'
+        """
+        # TODO : Handle more types
+        # integer_attributes =
+        # ['int_', 'intc','BIGINT','REAL','DOUBLE','FLOAT','DECIMAL','NUMERIC']
+        # double_attributes =
+        # ['SMALLINT', 'INTEGER','BIGINT','REAL','DOUBLE','FLOAT','DECIMAL','NUMERIC']
+        # self._check_connection()
+
+        if not isinstance(dataframe, pd.DataFrame):
+            raise TypeError("_create_table is valid only for DataFrame objects")
+
+        if primary_key is not None:
+            if not isinstance(primary_key, six.string_types):
+                raise TypeError("primary_key argument should be a string")
+
+        # Check the tablename
+        tablename = nzpyida.utils.check_tablename(tablename)
+
+        # for Netezza we have to check if the schema exists already
+        # otherwise we have to create it
+        if self._is_netezza_system() & ("." in tablename):
+            schemaname, tabname = tablename.split(".")
+            if self.ida_scalar_query("SELECT COUNT(*) FROM _V_SCHEMA WHERE SCHEMA='%s'"%schemaname) == 0:
+                self.ida_query("CREATE SCHEMA " + schemaname)
+
+        column_string = ''
+        for column in dataframe.columns:
+            if dataframe.dtypes[column] in [object,bool]:
+                # Handle boolean type
+                if set(dataframe[column].unique()).issubset([True, False, 0, 1, np.nan]):
+                    column_string += "\"%s\" SMALLINT," % str(column).strip()
+                else:
+                    if column == primary_key:
+                        column_string += "\"%s\" VARCHAR(255) NOT NULL, PRIMARY KEY (\"%s\")," % (str(column).strip(), str(column).strip())
+                    else:
+                        column_string += "\"%s\" VARCHAR(255)," % str(column).strip()
+            elif dataframe.dtypes[column] == np.dtype('datetime64[ns]'):
+                # This is a first patch for handling dates
+                # TODO: Dates as timestamp in the database
+                column_string += "\"%s\" VARCHAR(255)," % str(column).strip()
+            else:
+                if dataframe.dtypes[column] in [np.int64, int, np.int8, np.int32]:
+                    if abs(dataframe[column].max()) < 2147483647/2: # might get bigger 
+                        datatype = "INT"
+                    else:
+                        datatype = "BIGINT"
+                else:
+                    datatype = "DOUBLE"
+                if column == primary_key:
+                    column_string += "\"%s\" %s NOT NULL, PRIMARY KEY (\"%s\")," % (str(column).strip(), datatype, str(column).strip())
+                else:
+                    column_string += "\"%s\" %s," %(str(column.strip()), datatype)
+        if column_string[-1] == ',':
+            column_string = column_string[:-1]
+
+        if '.' in tablename:
+            schema, tablename2 = tablename.split('.')
+        else:
+            schema = self.current_schema
+            tablename2 = tablename
+
+        create_table = "CREATE TABLE \"%s\".\"%s\" (%s)" % (schema, tablename2, column_string)
+
+        self._prepare_and_execute(create_table, autocommit=False)
+
+        # Save new table in cache
+        if hasattr(self, "cache_show_tables"):
+            record = (schema, tablename2, self.current_schema, 'T')
+            self.cache_show_tables.loc[len(self.cache_show_tables)] = np.array(record)
+
+        return "\"%s\".\"%s\"" % (schema, tablename2)
+
+    def _create_view(self, idadf, viewname = None):
+        """
+        Create a new view in the database from an existing table.
+
+        Parameters
+        ----------
+
+        idadf : IdaDataFrame
+            IdaDataFrame to be duplicated as view.
+
+        viewname : str, optional
+            Name to be given to the view at its creation. If not given, a 
+            random name will be generated automatically 
+
+        Returns
+        -------
+        str
+            View name.
+
+        Examples
+        --------
+        >>> idadf = IdaDataFrame(idadb, "IRIS")
+        >>> idadb._create_view(idadf)
+        'IDAR_VIEW_4956'
+        """
+        if not isinstance(idadf, nzpyida.frame.IdaDataFrame):
+            raise TypeError("_create_view is valid only for IdaDataFrame objects")
+
+        # Check the viewname
+        if viewname is not None:
+            viewname = nzpyida.utils.check_viewname(viewname)
+        else:
+            viewname = self._get_valid_viewname()
+
+        self._prepare_and_execute("CREATE VIEW \"%s\" AS SELECT * FROM %s" % (viewname, idadf._name))
+    
+        # Save new view in cache
+        if hasattr(self, "cache_show_tables"):
+            record = (self.current_schema, viewname, self.current_schema, 'V')
+            self.cache_show_tables.loc[len(self.cache_show_tables)] = np.array(record)
+    
+        return viewname
+        
+    def _create_view_from_expression(self, expression, viewname = None):
+        """
+        Create a new view in the database from an expression.
+
+        Parameters
+        ----------
+
+        expression : str
+            Expression that defines the view to be created
+
+        viewname : str, optional
+            Name to be given to the view at its creation. If not given, a 
+            random name will be generated automatically 
+
+        Returns
+        -------
+        str
+            View name.
+
+        Examples
+        --------
+        TODO
+        """
+        if not isinstance(expression, six.string_types):
+            raise TypeError("expression argument expected to be of string type")
+
+        # Check the viewname
+        if viewname is not None:
+            viewname = nzpyida.utils.check_viewname(viewname)
+        else:
+            viewname = self._get_valid_viewname()
+
+        self._prepare_and_execute("CREATE VIEW \"%s\" AS %s" % (viewname, expression))
+    
+        # Save new view in cache
+        if hasattr(self, "cache_show_tables"):
+            record = (self.current_schema, viewname, self.current_schema, 'V')
+            self.cache_show_tables.loc[len(self.cache_show_tables)] = np.array(record)
+    
+        return viewname
+
+    def _insert_into_database(self, dataframe, schema, tablename, silent=True):
+        """
+        Populate an existing table with data from a dataframe.
+
+        Parameters
+        ----------
+        dataframe: DataFrame
+            Data to be inserted into an existing table, contained in a Pandas 
+            DataFrame. It is assumed that the structure matches.
+        schema: str
+            Schema of the table in which the data is inserted.
+        tablename: str
+            Name of the table in which the data is inserted.
+        silent : bool, default: True
+            If True, the INSERT statement is not printed. Avoids flooding the 
+            console.
+
+        """
+        # TODO : Handle more datatypes
+        if schema is None or schema.strip() == '':
+            schema = self.current_schema
+        tablename = nzpyida.utils.check_tablename(tablename)
+        column_string = '\"%s\"' % '\", \"'.join([str(x).strip() for x in dataframe.columns])
+        row_string = ''
+
+        # Save in a list columns that are booleans
+        boolean_flaglist = []
+        for column in dataframe.columns:
+            if set(dataframe[column].unique()).issubset([True, False, 0, 1, np.nan]):
+                boolean_flaglist.append(1)
+            else:
+                boolean_flaglist.append(0)
+
+        if self._is_netezza_system():
+            sepstr1 = 'SELECT '
+            sepstr2 = ' UNION ALL SELECT '
+            sepstr3 = ' '
+        else:
+            sepstr1 = 'VALUES ('
+            sepstr2 = '), ('
+            sepstr3 = ') '
+
+        row_separator = sepstr1
+        for rows in dataframe.values:
+            value_string = ''
+            for colindex, value in enumerate(rows):
+                if pd.isnull(value): # handles np.nan and None
+                    value_string += "NULL,"  # Handle missing values
+                elif isinstance(value, six.string_types):
+                    ## Handle apostrophe in values
+                    value = value.replace("\\", "'")
+                    value_string += '\'%s\',' % value.replace("'", "''")
+                # REMARK: it is the best way to handle booleans ?
+                elif isinstance(value, bool):
+                    if boolean_flaglist[colindex] == True:
+                        if value in [1, True]:
+                            value_string += '1,'
+                        elif value in [0, False]:
+                            value_string += '0,'
+                    else:
+                        value_string += '\'%s\',' % value
+                # TODO: Handle datetime better than strings
+                elif isinstance(value, datetime.datetime):
+                    value_string += '\'%s\',' % value
+                else:
+                    value_string += '%s,' % value
+            if value_string[-1] == ',':
+                value_string = value_string[:-1]
+            row_string += (row_separator + " %s ") % value_string
+            row_separator = sepstr2
+        row_string = row_string +  sepstr3
+        if row_string[-2:] == '),':
+            row_string = row_string[:-2]
+        if row_string[0] == '(':
+            row_string = row_string[1:]
+        query = ("INSERT INTO \"%s\".\"%s\" (%s) (%s)" % (schema, tablename, column_string, row_string))
+
+        # print(query)
+        # TODO: Good idea : create a savepoint before creating the table
+        # Rollback in to savepoint in case of failure
+        self._prepare_and_execute(query, autocommit=False, silent=silent)
+
+        for idadf in self._idadfs:
+            if idadf._name == tablename:
+                idadf._reset_attributes(["shape", "index"])
+
+    def _prepare_and_execute(self, query, autocommit=True, silent=False):
+        """
+        Prepare and execute a query by using the cursor of an idaobject.
+
+        Parameters
+        ----------
+        idaobject: IdaDataBase or IdaDataFrame
+        query: str
+            Query to be executed.
+        autocommit: bool, default: True
+            If True, the autocommit function is available.
+        silent: bool, default: False
+            If True, the SQL statement is not printed.
+        """
+        self._check_connection()
+        return sql._prepare_and_execute(self, query, autocommit, silent)
+
+    def _check_procedure(self, proc_name, alg_name=None):
+        """
+        Check if a procedure is available in the database.
+
+        Parameters
+        ----------
+        proc_name : str
+            Name of the procedure to be checked as defined in the underlying
+            database management system.
+        alg_name : str
+            Name of the algorithm, human readable.
+
+        Returns
+        -------
+        bool
+
+        Examples
+        --------
+        >>> idadb._check_procedure('KMEANS')
+        True
+        >>> idadb._check_procedure('NOT_EXISTING')
+        IdaDatabaseError: Function 'NOT_EXISTING' is not available.
+        """
+
+        if alg_name is None:
+            alg_name = proc_name
+        if self._is_netezza_system():
+            query = ("SELECT COUNT(*) FROM NZA.._V_PROCEDURE WHERE PROCEDURE='%s'") % proc_name
+        else:
+            query = ("SELECT COUNT(*) FROM SYSCAT.ROUTINES WHERE ROUTINENAME='%s" +
+                     "' AND ROUTINEMODULENAME = 'IDAX'") % proc_name
+        flag = self.ida_scalar_query(query)
+
+        if int(flag) == False:
+            raise IdaDataBaseError("Function '%s' is not available." % alg_name)
+        else:
+            return True
+
+    def _call_stored_procedure(self, sp_name, **kwargs):
+        """
+        Call a specific IDAX/INZA stored procedure and return its result.
+
+        Parameters
+        ----------
+        sp_name : str
+            Name of the stored procedure.
+        **kwargs : ...
+            Additional parameters, specific to the called stored procedure.
+        """
+        tmp = []
+        views = []
+        if self._is_netezza_system():
+            sp_schema = 'NZA.'
+        else:
+            sp_schema = 'IDAX'
+
+        for key, value in six.iteritems(kwargs):
+            if value is None:
+                continue  # Go to next iteration
+            if isinstance(value, nzpyida.frame.IdaDataFrame):
+                tmp_view_name = self._create_view(value)
+                tmp.append("%s=%s" % (key, tmp_view_name))
+                views.append(tmp_view_name)
+            elif isinstance(value, six.string_types) and all([x != " " for x in value]):
+                if key in ("intable", 'model', 'outtable', 'incolumn', 'nametable', 'colPropertiesTable'):
+                    tmp.append("%s=%s"% (key, value))  # no " in the case it is a table name
+                else:
+                    tmp.append("%s=\"%s\"" % (key, value))
+            elif isinstance(value, list):
+                tmp.append("%s=\"%s\"" % (key, " ".join(str(value))))
+            else:
+                tmp.append("%s=%s" % (key, value))
+        try:
+            call = "CALL %s.%s('%s')" % (sp_schema, sp_name, ",".join(tmp))
+            result = self._prepare_and_execute(call)
+        except:
+            if self._is_netezza_system():
+                error_msg = "Error"
+            else:
+                error_msg = self.ida_scalar_query("values idax.last_message")
+            raise IdaDataBaseError(error_msg)
+        finally:
+            for view in views:
+                self.drop_view(view)
+
+        return result
+
+    def _autocommit(self):
+        """
+        Commit changes made to the database in the connection automatically.
+        If the environment variable 'AUTOCOMMIT' is set to True, then commit.
+
+        Notes
+        -----
+        In the case of a commit operation, all changes that are made in the
+        Database after the last commit, including those in the children
+        IdaDataFrames, will be commited.
+
+        If the environment variable 'VERBOSE' is not set to 'True', the
+        autocommit operations will not be notified in the console to the user.
+        """
+        if os.getenv('AUTOCOMMIT') == 'True':
+            self._con.commit()
+            if os.getenv('VERBOSE') == 'True':
+                print("<< AUTOCOMMIT >>")
+
+    def _check_connection(self):
+        """
+        Check if the connection still exists by trying to open a cursor.
+        """
+        if self._con_type == "odbc":
+            try:
+                self._con.cursor()
+            except Exception as e:
+                raise IdaDataBaseError(e.value[-1])
+        elif self._con_type == "nzpy":
+            try:
+                self._con.cursor().execute("select 1")
+            except Exception as e:
+                raise IdaDataBaseError(e)
+        elif self._con_type == "jdbc":
+            # This avoids infinite recursions on Netezza due to reconnect calls in
+            # sql.ida_query, sql.ida_scalar_query and sql._prepare_and_execute
+            if self._con._closed:
+                raise IdaDataBaseError("The connection is closed")
+            try:
+                # Avoid infinite recursion
+                if self._is_netezza_system():
+                    # On Netezza no result sets are returned after the first database error
+                    if read_sql("SELECT OBJID FROM _V_TABLE LIMIT 1", self._con) is None:
+                        raise IdaDataBaseError("The connection is closed")
+                else:
+                    read_sql("SELECT TABLEID FROM SYSCAT.TABLES LIMIT 1", self._con)
+            except Exception as e:
+                raise IdaDataBaseError("The connection is closed")
+
+    def _retrieve_cache(self, cache):
+        """
+        Helper function that retrieve cache if available.
+        Cache are just string type values stored in private attributes.
+        """
+        if not isinstance(cache, six.string_types):
+            raise TypeError("cache is not of string type")
+
+        self._check_connection()
+
+        if hasattr(self, cache):
+            return getattr(self,cache)
+        else:
+            return None
+
+    def _reset_attributes(self, attributes):
+        """
+        Helper function that delete attributes given as parameter if they
+        exists in self. This is used to refresh lazy attributes and caches.
+        """
+        nzpyida.utils._reset_attributes(self, attributes)
+
+
+    def _is_netezza_system(self):
+         """
+         Checks if the underlying database system is Netezza.
+         """
          return self._database_system == 'netezza'
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/__init__.py` & `nzpyida-0.3.3/nzpyida/feature_selection/__init__.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,34 +1,34 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-from .correlation import  pearson, spearman
-
-from .entropy import entropy, entropy_stats
-
-from .info_gain import info_gain
-
-from .gain_ratio import gain_ratio
-
-from .symmetric_uncertainty import su
-
-from .gini import gini, gini_pairwise
-from .discretize import discretize
-
-from .chisquared import chisquared
-from .tstats import ttest 
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+from .correlation import  pearson, spearman
+
+from .entropy import entropy, entropy_stats
+
+from .info_gain import info_gain
+
+from .gain_ratio import gain_ratio
+
+from .symmetric_uncertainty import su
+
+from .gini import gini, gini_pairwise
+from .discretize import discretize
+
+from .chisquared import chisquared
+from .tstats import ttest 
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/chisquared.py` & `nzpyida-0.3.3/nzpyida/feature_selection/gini.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,140 +1,178 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Mon Dec 14 11:31:52 2015
-
-@author: efouche
-"""
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import absolute_import
-from builtins import dict
-from future import standard_library
-standard_library.install_aliases()
-
-from collections import OrderedDict
-
-from nzpyida.internals import idadf_state
-from nzpyida.utils import timed
-
-import numpy as np 
-import pandas as pd
-
-from nzpyida.feature_selection.private import _check_input
-
-@idadf_state
-@timed
-def chisquared(idadf, target = None, features = None, ignore_indexer=True):
-    """
-    Compute the Chi-Squared statistics coefficients between a set of features 
-    and a set of target in an IdaDataFrame. 
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    target : str or list of str, optional
-        A column or list of columns against to be used as target. Per default, 
-        consider all columns
-    
-    features : str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-    
-    ignore_indexer : bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.DataFrame or Pandas.Series if only one target
-    
-    Notes
-    -----
-    Input columns as target and features should be categorical, otherwise 
-    this measure does not make much sense. 
-    
-    Chi-squared as defined in 
-    A Comparative Study on Feature Selection and Classification Methods Using 
-    Gene Expression Profiles and Proteomic Patterns. (GIW02F006)
-    
-    The scalability of this approach is not very good. Should not be used on 
-    high dimensional data. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> chisquared(idadf)
-    """
-    # Check input
-    target, features = _check_input(idadf, target, features, ignore_indexer)
-    count_dict = dict()
-    length = len(idadf)
-    
-    values = OrderedDict()
-         
-    for t in target:   
-        if t not in values:
-            values[t] = OrderedDict() 
-        features_notarget = [x for x in features if (x != t)]
-        
-        ### Compute
-        for feature in features_notarget:
-            if feature not in values:
-                values[feature] = OrderedDict()
-            if t not in values[feature]:
-                if t not in count_dict:
-                    count = idadf.count_groupby(t)
-                    count_serie = count["count"]
-                    count_serie.index = count[t]
-                    count_dict[t] = count_serie
-            
-                C = dict(count_dict[t])
-                
-                if feature not in count_dict:
-                    count = idadf.count_groupby(feature)
-                    count_serie = count["count"]
-                    count_serie.index = count[feature]
-                    count_dict[feature] = count_serie
-                    
-                R = dict(count_dict[feature])
-                
-                if (feature, t) not in count_dict:
-                    count_dict[(feature, t)] = idadf.count_groupby([feature , t])
-                
-                count = count_dict[(feature, t)]
-                
-                chi = 0            
-                for target_class in C.keys():
-                    count_target = count[count[t] == target_class][[feature, "count"]]
-                    A_target = count_target['count']
-                    A_target.index = count_target[feature]
-                    
-                    for feature_class in A_target.index:
-                        a = A_target[feature_class]
-                        e = R[feature_class] * C[target_class] / length
-                        chi += ((a - e)**2)/e
-                
-                values[t][feature] = chi   # chisquared is symmetric 
-                if feature in target:
-                    values[feature][t] = chi
-        
-    result = pd.DataFrame(values).fillna(np.nan)
-    result = result.dropna(axis=1, how="all")
-        
-    if len(result.columns) > 1:
-        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
-        result = result.reindex(order)
-    
-    if len(result.columns) == 1:
-        if len(result) == 1:
-            result = result.iloc[0,0]
-        else:
-            result = result[result.columns[0]].copy()
-            result.sort_values(ascending = False)
-        
-
-    
-    
-    return result
-    
-    
+# -*- coding: utf-8 -*-
+"""
+Created on Tue Dec  1 12:29:30 2015
+
+@author: efouche
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+
+from collections import OrderedDict
+
+import pandas as pd
+import numpy as np
+import six
+
+from nzpyida.internals import idadf_state
+from nzpyida.utils import timed
+from nzpyida.feature_selection.private import _check_input
+
+@idadf_state
+@timed
+def gini_pairwise(idadf, target=None, features=None, ignore_indexer=True):
+    """
+    Compute the conditional gini coefficients between a set of features and a 
+    set of target in an IdaDataFrame. 
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    target : str or list of str, optional
+        A column or list of columns against to be used as target. Per default, 
+        consider all columns
+    
+    features : str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+    
+    ignore_indexer : bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.DataFrame or Pandas.Series if only one target
+    
+    Notes
+    -----
+    Input columns as target and features should be categorical, otherwise 
+    this measure does not make much sense. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> gini_pairwise(idadf)
+    """
+    # Check input
+    target, features = _check_input(idadf, target, features, ignore_indexer)
+        
+    gini_dict = OrderedDict()
+    length = len(idadf)
+
+    if idadf._idadb._is_netezza_system():
+        power_function = "POW"
+        div_term = "* POW(c,-1)) * POW(%s,-1)"%length
+    else:
+        power_function = "POWER"
+        div_term = "/c)/%s"%length
+
+    for t in target:
+        gini_dict[t] = OrderedDict() 
+        features_notarget = [x for x in features if (x != t)]
+        
+        for feature in features_notarget:
+            if t not in gini_dict:
+                gini_dict[t] = OrderedDict()
+            
+            query = ("SELECT SUM((%s(c,2) - gini)%s FROM "+
+            "(SELECT SUM(%s(count,2)) as gini, SUM(count) as c FROM "+
+            "(SELECT CAST(COUNT(*) AS FLOAT) AS count, \"%s\" FROM %s GROUP BY \"%s\",\"%s\") AS T1 "+
+            "GROUP BY \"%s\") AS T2 ")
+            query0 = query%(power_function, div_term, power_function, feature, idadf.name, t, feature, feature)
+            gini_dict[t][feature] = idadf.ida_scalar_query(query0)
+            
+    result = pd.DataFrame(gini_dict).fillna(np.nan)
+        
+    if len(result.columns) > 1:
+        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
+        result = result.reindex(order)
+       
+    result = result.dropna(axis=1, how="all")
+    
+    if len(result.columns) == 1:
+        if len(result) == 1:
+            result = result.iloc[0,0]
+        else:
+            result = result[result.columns[0]].copy()
+            result.sort_values(ascending = True)
+    else:
+        result = result.fillna(0)
+    
+    return result
+               
+    
+        
+    
+    
+    
+@idadf_state
+@timed
+def gini(idadf, features=None, ignore_indexer=True):
+    """
+    Compute the gini coefficients for a set of features in an IdaDataFrame. 
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    features : str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+    
+    ignore_indexer : bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.Series
+    
+    Notes
+    -----
+    Input column should be categorical, otherwise this measure does not make 
+    much sense. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> gini(idadf)
+    """
+    if features is None:
+        features = list(idadf.columns)
+    else:
+        if isinstance(features, six.string_types):
+            features = [features]
+
+    if ignore_indexer is True:
+        if idadf.indexer:
+            if idadf.indexer in features:
+                features.remove(idadf.indexer)
+      
+        
+    value_dict = OrderedDict()
+        
+    length = len(idadf)**2
+
+    if idadf._idadb._is_netezza_system():
+      power_function = "POW"
+      div_term = "* POW(%s, -1)"%length
+    else:
+      power_function = "POWER"
+      div_term ="/%s"%length
+
+    for feature in features:
+        
+        subquery = "SELECT COUNT(*) AS count FROM %s GROUP BY \"%s\""%(idadf.name, feature)
+        query = "SELECT (%s - SUM(%s(count,2)))%s FROM (%s) AS T "%(length, power_function, div_term, subquery)
+        value_dict[feature] = idadf.ida_scalar_query(query)
+            
+        if len(features) > 1:
+            result = pd.Series(value_dict) 
+        else:
+            result = value_dict[feature]
+    
+    return result
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/correlation.py` & `nzpyida-0.3.3/nzpyida/feature_selection/correlation.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,221 +1,221 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-from collections import OrderedDict
-
-import nzpyida
-from nzpyida.internals import idadf_state
-from nzpyida.utils import timed, chunklist
-
-import pandas as pd
-
-from nzpyida.feature_selection.private import _check_input
-
-
-@idadf_state
-@timed
-def pearson(idadf, target=None, features=None, ignore_indexer=True):
-    """
-    Compute the pearson correlation coefficients between a set of features and a 
-    set of target in an IdaDataFrame. Provide more granualirity than 
-    IdaDataFrame.corr
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    target : str or list of str, optional
-        A column or list of columns against to be used as target. Per default, 
-        consider all columns
-    
-    features : str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-        
-    ignore_indexer : bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.DataFrame or Pandas.Series if only one target
-    
-    Notes
-    -----
-    Input columns as target and features should be numerical. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> pearson(idadf)
-    """
-    # check if the corr function is installed on the Netezza system
-    if idadf._idadb._is_netezza_system():
-        corr_query = "SELECT count(*) from _V_OBJECT  where OBJNAME like 'CORR#%' AND OBJDB = CURRENT_DB"
-        if idadf._idadb.ida_scalar_query(corr_query) == 0:
-            raise NotImplementedError("The CORR function is not installed on the Netezza database.")
-
-
-    numerical_columns = idadf._get_numerical_columns()
-    if features is None:
-        features = numerical_columns
-        
-    target, features = _check_input(idadf, target, features, ignore_indexer)
-    
-    value_dict = OrderedDict()
-    
-    for feature in features:
-        if feature not in numerical_columns:
-            raise TypeError("Correlation-based measure not available for non-numerical column %s"%feature)
-                    
-    if target == features:
-        return idadf.corr(features = features, ignore_indexer=ignore_indexer)
-    else:
-        for t in target:
-            if feature not in numerical_columns:
-                raise TypeError("Correlation-based measure not available for non-numerical column %s"%t)
-        
-        for t in target:
-            value_dict[t] = OrderedDict()
-            
-            features_notarget = [x for x in features if x != t]
-
-            if idadf._idadb._is_netezza_system():
-                corr_funct = "CORR"
-            else:
-                corr_funct = "CORRELATION"
-
-            if len(features_notarget) < 64:
-                agg_list = ["%s(\"%s\",\"%s\")"%(corr_funct, x, t) for x in features_notarget]
-                agg_string = ', '.join(agg_list)
-                name = idadf.internal_state.current_state
-                data = idadf.ida_query("SELECT %s FROM %s"%(agg_string, name), first_row_only = True)
-            else:
-                chunkgen = chunklist(features_notarget, 100)
-                data = ()
-                for chunk in chunkgen: 
-                    agg_list = ["%s(\"%s\",\"%s\")"%(corr_funct, x, t) for x in chunk]
-                    agg_string = ', '.join(agg_list)
-            
-                    name = idadf.internal_state.current_state
-                    data += idadf.ida_query("SELECT %s FROM %s"%(agg_string, name), first_row_only = True)
-    
-            for i, feature in enumerate(features_notarget):
-                value_dict[t][feature] = data[i]
-        
-        ### Fill the matrix
-        result = pd.DataFrame(value_dict).fillna(1)
-        
-        if len(result.columns) == 1:
-            if len(result) == 1:
-                result = result.iloc[0,0]
-            else:
-                result = result[result.columns[0]].copy()
-                result.sort_values(inplace=True, ascending=False)
-        else:
-            order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
-            result = result.reindex(order)
-        
-        return result 
-  
-@idadf_state
-@timed          
-def spearman(idadf, target=None, features = None, ignore_indexer=True):
-    """
-    Compute the spearman rho correlation coefficients between a set of features 
-    and a set of target in an IdaDataFrame.
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    target : str or list of str, optional
-        A column or list of columns against to be used as target. Per default, 
-        consider all columns
-    
-    features : str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-        
-    ignore_indexer : bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.DataFrame or Pandas.Series if only one target
-    
-    Notes
-    -----
-    Input columns as target and features should be numerical. 
-    This function is a wrapper for pearson. 
-    The scalability of this approach is not very good. Should not be used on 
-    high dimensional data. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> spearman(idadf)
-    """
-    numerical_columns = idadf._get_numerical_columns()
-    if features is None:
-        features = numerical_columns
-        
-    target, features = _check_input(idadf, target, features, ignore_indexer)
-    
-    for feature in features:
-        if feature not in numerical_columns:
-            raise TypeError("Correlation-based measure not available for non-numerical column %s"%feature)
-    
-    if ignore_indexer is True:
-        if idadf.indexer:
-            if idadf.indexer in numerical_columns:
-                features.remove(idadf.indexer)
-    
-    if features is None:
-        features = list(idadf.columns)
-    
-    numerical_features = [x for x in features if x in numerical_columns]
-    numerical_targets = [x for x in target if x in numerical_columns]
-    
-    numerical_features = list(set(numerical_features) | set(numerical_targets))
-    
-    
-    agg_list = ["CAST(RANK() OVER (ORDER BY \"%s\") AS INTEGER) AS \"%s_RANK\""%(x, x) for x in numerical_features]
-    agg_string = ', '.join(agg_list)
-    subselect_stmt = "SELECT %s FROM %s"%(agg_string, idadf.name)
-
-    select_list = ["\"%s_RANK\" AS \"%s\""%(x, x) for x in numerical_features]
-    select_string = ', '.join(select_list)
-    select_stmt = "SELECT " + select_string + " FROM ( " + subselect_stmt + ") AS T"
-
-    viewname = idadf._idadb._create_view_from_expression(select_stmt)
-    
-    try:
-        idadf_rank = nzpyida.IdaDataFrame(idadf._idadb, viewname)
-        return pearson(idadf_rank, target = target, features=numerical_features, ignore_indexer=ignore_indexer)
-    except:
-        raise
-    finally:
-        idadf._idadb.drop_view(viewname)
-    
-    
-    
-    
- 
-        
-        
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+from collections import OrderedDict
+
+import nzpyida
+from nzpyida.internals import idadf_state
+from nzpyida.utils import timed, chunklist
+
+import pandas as pd
+
+from nzpyida.feature_selection.private import _check_input
+
+
+@idadf_state
+@timed
+def pearson(idadf, target=None, features=None, ignore_indexer=True):
+    """
+    Compute the pearson correlation coefficients between a set of features and a 
+    set of target in an IdaDataFrame. Provide more granualirity than 
+    IdaDataFrame.corr
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    target : str or list of str, optional
+        A column or list of columns against to be used as target. Per default, 
+        consider all columns
+    
+    features : str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+        
+    ignore_indexer : bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.DataFrame or Pandas.Series if only one target
+    
+    Notes
+    -----
+    Input columns as target and features should be numerical. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> pearson(idadf)
+    """
+    # check if the corr function is installed on the Netezza system
+    if idadf._idadb._is_netezza_system():
+        corr_query = "SELECT count(*) from _V_OBJECT  where OBJNAME like 'CORR#%' AND OBJDB = CURRENT_DB"
+        if idadf._idadb.ida_scalar_query(corr_query) == 0:
+            raise NotImplementedError("The CORR function is not installed on the Netezza database.")
+
+
+    numerical_columns = idadf._get_numerical_columns()
+    if features is None:
+        features = numerical_columns
+        
+    target, features = _check_input(idadf, target, features, ignore_indexer)
+    
+    value_dict = OrderedDict()
+    
+    for feature in features:
+        if feature not in numerical_columns:
+            raise TypeError("Correlation-based measure not available for non-numerical column %s"%feature)
+                    
+    if target == features:
+        return idadf.corr(features = features, ignore_indexer=ignore_indexer)
+    else:
+        for t in target:
+            if feature not in numerical_columns:
+                raise TypeError("Correlation-based measure not available for non-numerical column %s"%t)
+        
+        for t in target:
+            value_dict[t] = OrderedDict()
+            
+            features_notarget = [x for x in features if x != t]
+
+            if idadf._idadb._is_netezza_system():
+                corr_funct = "CORR"
+            else:
+                corr_funct = "CORRELATION"
+
+            if len(features_notarget) < 64:
+                agg_list = ["%s(\"%s\",\"%s\")"%(corr_funct, x, t) for x in features_notarget]
+                agg_string = ', '.join(agg_list)
+                name = idadf.internal_state.current_state
+                data = idadf.ida_query("SELECT %s FROM %s"%(agg_string, name), first_row_only = True)
+            else:
+                chunkgen = chunklist(features_notarget, 100)
+                data = ()
+                for chunk in chunkgen: 
+                    agg_list = ["%s(\"%s\",\"%s\")"%(corr_funct, x, t) for x in chunk]
+                    agg_string = ', '.join(agg_list)
+            
+                    name = idadf.internal_state.current_state
+                    data += idadf.ida_query("SELECT %s FROM %s"%(agg_string, name), first_row_only = True)
+    
+            for i, feature in enumerate(features_notarget):
+                value_dict[t][feature] = data[i]
+        
+        ### Fill the matrix
+        result = pd.DataFrame(value_dict).fillna(1)
+        
+        if len(result.columns) == 1:
+            if len(result) == 1:
+                result = result.iloc[0,0]
+            else:
+                result = result[result.columns[0]].copy()
+                result.sort_values(inplace=True, ascending=False)
+        else:
+            order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
+            result = result.reindex(order)
+        
+        return result 
+  
+@idadf_state
+@timed          
+def spearman(idadf, target=None, features = None, ignore_indexer=True):
+    """
+    Compute the spearman rho correlation coefficients between a set of features 
+    and a set of target in an IdaDataFrame.
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    target : str or list of str, optional
+        A column or list of columns against to be used as target. Per default, 
+        consider all columns
+    
+    features : str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+        
+    ignore_indexer : bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.DataFrame or Pandas.Series if only one target
+    
+    Notes
+    -----
+    Input columns as target and features should be numerical. 
+    This function is a wrapper for pearson. 
+    The scalability of this approach is not very good. Should not be used on 
+    high dimensional data. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> spearman(idadf)
+    """
+    numerical_columns = idadf._get_numerical_columns()
+    if features is None:
+        features = numerical_columns
+        
+    target, features = _check_input(idadf, target, features, ignore_indexer)
+    
+    for feature in features:
+        if feature not in numerical_columns:
+            raise TypeError("Correlation-based measure not available for non-numerical column %s"%feature)
+    
+    if ignore_indexer is True:
+        if idadf.indexer:
+            if idadf.indexer in numerical_columns:
+                features.remove(idadf.indexer)
+    
+    if features is None:
+        features = list(idadf.columns)
+    
+    numerical_features = [x for x in features if x in numerical_columns]
+    numerical_targets = [x for x in target if x in numerical_columns]
+    
+    numerical_features = list(set(numerical_features) | set(numerical_targets))
+    
+    
+    agg_list = ["CAST(RANK() OVER (ORDER BY \"%s\") AS INTEGER) AS \"%s_RANK\""%(x, x) for x in numerical_features]
+    agg_string = ', '.join(agg_list)
+    subselect_stmt = "SELECT %s FROM %s"%(agg_string, idadf.name)
+
+    select_list = ["\"%s_RANK\" AS \"%s\""%(x, x) for x in numerical_features]
+    select_string = ', '.join(select_list)
+    select_stmt = "SELECT " + select_string + " FROM ( " + subselect_stmt + ") AS T"
+
+    viewname = idadf._idadb._create_view_from_expression(select_stmt)
+    
+    try:
+        idadf_rank = nzpyida.IdaDataFrame(idadf._idadb, viewname)
+        return pearson(idadf_rank, target = target, features=numerical_features, ignore_indexer=ignore_indexer)
+    except:
+        raise
+    finally:
+        idadf._idadb.drop_view(viewname)
+    
+    
+    
+    
+ 
+        
+        
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/discretize.py` & `nzpyida-0.3.3/nzpyida/feature_selection/discretize.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,158 +1,158 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Mon Nov 23 09:02:30 2015
-
-@author: efouche
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-from nzpyida.internals import idadf_state
-import nzpyida
-from nzpyida.utils import timed
-
-import six
-
-@timed
-@idadf_state(force = True)   
-def discretize(idadf, columns = None, disc= "em", target = None, bins = None, outtable = None, clear_existing=False):
-    """
-    Discretize a set of numerical columns from an IdaDataFrame and returns an 
-    IdaDataFrame open on the discretized version of the dataset. 
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    columns : str or list of str, optional
-        A column or list of columns to be discretized
-    
-    disc : "ef", "em", "ew", "ewn" default: "em"
-        Discretization method to be used
-        
-        - ef: Discretization bins of equal frequency 
-        
-        - em: Discretization bins of minimal entropy 
-        
-        - ew: Discretization bins of equal width
-        
-        - ewn: Discretization bins of equal width with human-friendly limits 
-    
-    target : str
-        Target column again which the discretization will be done. Relevant
-        only for "em" discretization. 
-        
-    bins: int, optional
-        Number of bins. Not relevant for "em" discretization. 
-        
-    outtable: str, optional
-        The name of the output table where the assigned clusters are stored.
-        If this parameter is not specified, it is generated automatically.
-        If the parameter corresponds to an existing table in the database,
-        it is replaced.
-    
-    clear_existing: bool, default: False
-        If set to True, a table will be replaced when a table with the same 
-        name already exists  in the database.
-    """
-    if columns is None:
-        columns = idadf._get_numerical_columns()
-        if target is not None:
-            columns = [x for x in columns if columns != target]    
-    else:
-        if isinstance(columns, six.string_types):
-                columns = [columns]
-                
-    stored_proc = _check(idadf, columns, disc, target, bins, outtable)
-
-    bound_outtable = idadf._idadb._get_valid_tablename('DISC_BOUNDS_%s_'%idadf.tablename)
-    intable = idadf.name   # either the table or a view on the top 
-    incolumn = "\";\"".join(columns)
-    
-    
-    # Calculate bounds
-    idadf._idadb._call_stored_procedure(stored_proc,
-                                        outtable=bound_outtable,
-                                        intable=intable,
-                                        incolumn=incolumn,
-                                        target=target,
-                                        bins=bins)
-        
-    # Create discretized dataset
-        
-    if outtable is None:
-        disc_outtable = idadf._idadb._get_valid_tablename('DISC_%s_'%idadf.tablename)
-    else:
-        if clear_existing is True:
-            try:
-                idadf._idadb.drop_table(outtable)
-            except:
-                pass
-        disc_outtable = outtable
-    
-    try:
-        idadf._idadb._call_stored_procedure("APPLY_DISC",
-                                            outtable=disc_outtable,
-                                            intable=intable,
-                                            btable=bound_outtable,
-                                            replace="T")
-    except:
-        raise
-    finally:
-        idadf._idadb.drop_table(bound_outtable)
-    
-    return nzpyida.IdaDataFrame(idadf._idadb, disc_outtable)
-    
-    
-def _check(idadf, columns, disc, target, bins, outtable):
-    """
-    Helper function to handle basic checks for 
-    ibmdbpy.feature_selection.discretize
-    """
-    if outtable is not None:
-        nzpyida.utils.check_tablename(outtable)
-    if bins is not None:
-        if not isinstance(bins, int):
-            raise TypeError("bins argument is not of integer type")
-            
-    if columns is not None:
-        
-        if target is not None:
-            if target in columns:
-                raise ValueError("Target in columns.")
-        unknown = []
-        for column in columns:
-            if column not in idadf.columns:
-                unknown.append(column)
-        if unknown:
-            raise ValueError("Undefined columns: %s"%", ".join(unknown))
-        
-    if disc == "em":
-        if bins is not None:
-            raise ValueError("Number of bins is automatically detected for Entropy Minimization discretization.")
-        if target is None:
-            raise ValueError("Need to define a target for Entropy Minimization discretization.")
-        if target in columns:
-            raise ValueError("Target column %s cannot be discretize too"%target)
-        if target not in idadf.columns:
-            raise ValueError("Undefined target column %s"%target)
-        stored_proc = "EMDISC"
-    else:
-        if target is not None:
-            raise ValueError("Target attribute defined only for Entropy Minimization discretization.")
-        if bins is None:
-            bins = 10      
-        if disc == "ef":
-            stored_proc = "EFDISC"
-        elif disc == "ew":
-            stored_proc = "EWDISC"
-        elif disc == "ewn":
-            stored_proc = "EWDISC_NICE"
-        else:
-            raise ValueError("Unknown discretization method.")
-            
+# -*- coding: utf-8 -*-
+"""
+Created on Mon Nov 23 09:02:30 2015
+
+@author: efouche
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+from nzpyida.internals import idadf_state
+import nzpyida
+from nzpyida.utils import timed
+
+import six
+
+@timed
+@idadf_state(force = True)   
+def discretize(idadf, columns = None, disc= "em", target = None, bins = None, outtable = None, clear_existing=False):
+    """
+    Discretize a set of numerical columns from an IdaDataFrame and returns an 
+    IdaDataFrame open on the discretized version of the dataset. 
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    columns : str or list of str, optional
+        A column or list of columns to be discretized
+    
+    disc : "ef", "em", "ew", "ewn" default: "em"
+        Discretization method to be used
+        
+        - ef: Discretization bins of equal frequency 
+        
+        - em: Discretization bins of minimal entropy 
+        
+        - ew: Discretization bins of equal width
+        
+        - ewn: Discretization bins of equal width with human-friendly limits 
+    
+    target : str
+        Target column again which the discretization will be done. Relevant
+        only for "em" discretization. 
+        
+    bins: int, optional
+        Number of bins. Not relevant for "em" discretization. 
+        
+    outtable: str, optional
+        The name of the output table where the assigned clusters are stored.
+        If this parameter is not specified, it is generated automatically.
+        If the parameter corresponds to an existing table in the database,
+        it is replaced.
+    
+    clear_existing: bool, default: False
+        If set to True, a table will be replaced when a table with the same 
+        name already exists  in the database.
+    """
+    if columns is None:
+        columns = idadf._get_numerical_columns()
+        if target is not None:
+            columns = [x for x in columns if columns != target]    
+    else:
+        if isinstance(columns, six.string_types):
+                columns = [columns]
+                
+    stored_proc = _check(idadf, columns, disc, target, bins, outtable)
+
+    bound_outtable = idadf._idadb._get_valid_tablename('DISC_BOUNDS_%s_'%idadf.tablename)
+    intable = idadf.name   # either the table or a view on the top 
+    incolumn = "\";\"".join(columns)
+    
+    
+    # Calculate bounds
+    idadf._idadb._call_stored_procedure(stored_proc,
+                                        outtable=bound_outtable,
+                                        intable=intable,
+                                        incolumn=incolumn,
+                                        target=target,
+                                        bins=bins)
+        
+    # Create discretized dataset
+        
+    if outtable is None:
+        disc_outtable = idadf._idadb._get_valid_tablename('DISC_%s_'%idadf.tablename)
+    else:
+        if clear_existing is True:
+            try:
+                idadf._idadb.drop_table(outtable)
+            except:
+                pass
+        disc_outtable = outtable
+    
+    try:
+        idadf._idadb._call_stored_procedure("APPLY_DISC",
+                                            outtable=disc_outtable,
+                                            intable=intable,
+                                            btable=bound_outtable,
+                                            replace="T")
+    except:
+        raise
+    finally:
+        idadf._idadb.drop_table(bound_outtable)
+    
+    return nzpyida.IdaDataFrame(idadf._idadb, disc_outtable)
+    
+    
+def _check(idadf, columns, disc, target, bins, outtable):
+    """
+    Helper function to handle basic checks for 
+    ibmdbpy.feature_selection.discretize
+    """
+    if outtable is not None:
+        nzpyida.utils.check_tablename(outtable)
+    if bins is not None:
+        if not isinstance(bins, int):
+            raise TypeError("bins argument is not of integer type")
+            
+    if columns is not None:
+        
+        if target is not None:
+            if target in columns:
+                raise ValueError("Target in columns.")
+        unknown = []
+        for column in columns:
+            if column not in idadf.columns:
+                unknown.append(column)
+        if unknown:
+            raise ValueError("Undefined columns: %s"%", ".join(unknown))
+        
+    if disc == "em":
+        if bins is not None:
+            raise ValueError("Number of bins is automatically detected for Entropy Minimization discretization.")
+        if target is None:
+            raise ValueError("Need to define a target for Entropy Minimization discretization.")
+        if target in columns:
+            raise ValueError("Target column %s cannot be discretize too"%target)
+        if target not in idadf.columns:
+            raise ValueError("Undefined target column %s"%target)
+        stored_proc = "EMDISC"
+    else:
+        if target is not None:
+            raise ValueError("Target attribute defined only for Entropy Minimization discretization.")
+        if bins is None:
+            bins = 10      
+        if disc == "ef":
+            stored_proc = "EFDISC"
+        elif disc == "ew":
+            stored_proc = "EWDISC"
+        elif disc == "ewn":
+            stored_proc = "EWDISC_NICE"
+        else:
+            raise ValueError("Unknown discretization method.")
+            
     return stored_proc
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/entropy.py` & `nzpyida-0.3.3/nzpyida/feature_selection/entropy.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,160 +1,160 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Mon Nov 23 09:05:39 2015
-
-@author: efouche
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-from nzpyida.internals import idadf_state
-from nzpyida.utils import timed
-from collections import OrderedDict
-
-import pandas as pd
-
-import six
-
-@idadf_state
-def entropy(idadf, target=None, mode="normal", execute=True, ignore_indexer=True):
-    """
-    Compute the entropy for a set of features in an IdaDataFrame. 
-    
-    Parameters
-    ----------
-    idadf: IdaDataFrame
-    
-    target: str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-    
-    mode: "normal" or "raw"
-        Experimental
-        
-    execute: bool, default:True
-        Experimental. Execute the request or return the correponding SQL query 
-    
-    ignore_indexer: bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.Series
-    
-    Notes
-    -----
-    Input column should be categorical, otherwise this measure does not make 
-    much sense. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> entropy(idadf)
-    """
-    if target is not None:
-        if isinstance(target, six.string_types):
-            target = [target]
-            
-        targetstr = "\",\"".join(target)
-        subquery = "SELECT COUNT(*) AS a FROM %s GROUP BY \"%s\""%(idadf.name,targetstr)
-        if mode == "normal":
-            length = len(idadf)
-            query = "SELECT (SUM(-a*LOG(a))/%s+LOG(%s))/LOG(2) FROM (%s) AS T"%(length, length, subquery)
-        elif mode == "raw":
-            query = "SELECT SUM(-a*LOG(a)) FROM(%s) AS T "%(subquery)
-        
-        if not execute:
-            query = query[:query.find("FROM")] + ",'%s'"%"\',\'".join(target) + query[query.find("FROM"):]
-            return query
-        return idadf.ida_scalar_query(query)
-    else:
-        entropy_dict = OrderedDict()
-        columns = list(idadf.columns)
-        # Remove indexer
-        if ignore_indexer:
-            if idadf.indexer:
-                if idadf.indexer in columns:
-                    columns.remove(idadf.indexer)
-                    
-        for column in columns:
-           entropy_dict[column] = entropy(idadf, column, mode = mode)
-                    
-        # Output
-        if len(columns) > 1:
-            result = pd.Series(entropy_dict)
-            result.sort_values(ascending = False)
-        else:
-            result = entropy_dict[columns[0]]
-        return result
-    
-def entropy_stats(idadf, target=None, mode="normal", execute = True, ignore_indexer=True):
-    """
-    Similar to ibmdbby.feature_selection.entropy.entrop but use DB2 statistics
-    to speed the computation. Returns an approximate value. Experimental. 
-    
-    Parameters
-    ----------
-    idadf: IdaDataFrame
-    
-    target: str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-    
-    mode: "normal" or "raw"
-        Experimental
-        
-    execute: bool, default:True
-        Experimental. Execute the request or return the correponding SQL query 
-    
-    ignore_indexer: bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.Series
-    
-    Notes
-    -----
-    Input column should be categorical, otherwise this measure does not make 
-    much sense. 
-    
-    Cannot handle columns that are not physically existing in the database, 
-    since no statistics are available for them. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> entropy_stats(idadf)
-    """
-    if target is not None:
-        subquery = "SELECT VALCOUNT as a FROM SYSCAT.COLDIST WHERE TABSCHEMA='%s' AND TABNAME = '%s' AND COLNAME='%s' AND TYPE='F' AND COLVALUE IS NOT NULL"%(idadf.schema, idadf.tablename, target)
-        if mode == "normal":
-            query = "SELECT(SUM(-a*LOG(a))/SUM(a)+LOG(SUM(a)))/LOG(2)FROM(%s)"%(subquery)
-        elif mode == "raw":
-            query = "SELECT SUM(-a*LOG(a)) FROM(%s)"%(subquery)
-        
-        if not execute:
-            return query
-        return idadf.ida_scalar_query(query)
-    else:
-        entropy_dict = OrderedDict()
-        columns = list(idadf.columns)
-        # Remove indexer
-        if ignore_indexer:
-            if idadf.indexer:
-                if idadf.indexer in columns:
-                    columns.remove(idadf.indexer)
-                    
-        for column in columns:
-           entropy_dict[column] = entropy_stats(idadf, column, mode = mode)
-                    
-        # Output
-        if len(columns) > 1:
-            result = pd.Series(entropy_dict)
-            result.sort_values(ascending = False)
-        else:
-            result = entropy_dict[columns[0]]
+# -*- coding: utf-8 -*-
+"""
+Created on Mon Nov 23 09:05:39 2015
+
+@author: efouche
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+from nzpyida.internals import idadf_state
+from nzpyida.utils import timed
+from collections import OrderedDict
+
+import pandas as pd
+
+import six
+
+@idadf_state
+def entropy(idadf, target=None, mode="normal", execute=True, ignore_indexer=True):
+    """
+    Compute the entropy for a set of features in an IdaDataFrame. 
+    
+    Parameters
+    ----------
+    idadf: IdaDataFrame
+    
+    target: str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+    
+    mode: "normal" or "raw"
+        Experimental
+        
+    execute: bool, default:True
+        Experimental. Execute the request or return the correponding SQL query 
+    
+    ignore_indexer: bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.Series
+    
+    Notes
+    -----
+    Input column should be categorical, otherwise this measure does not make 
+    much sense. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> entropy(idadf)
+    """
+    if target is not None:
+        if isinstance(target, six.string_types):
+            target = [target]
+            
+        targetstr = "\",\"".join(target)
+        subquery = "SELECT COUNT(*) AS a FROM %s GROUP BY \"%s\""%(idadf.name,targetstr)
+        if mode == "normal":
+            length = len(idadf)
+            query = "SELECT (SUM(-a*LOG(a))/%s+LOG(%s))/LOG(2) FROM (%s) AS T"%(length, length, subquery)
+        elif mode == "raw":
+            query = "SELECT SUM(-a*LOG(a)) FROM(%s) AS T "%(subquery)
+        
+        if not execute:
+            query = query[:query.find("FROM")] + ",'%s'"%"\',\'".join(target) + query[query.find("FROM"):]
+            return query
+        return idadf.ida_scalar_query(query)
+    else:
+        entropy_dict = OrderedDict()
+        columns = list(idadf.columns)
+        # Remove indexer
+        if ignore_indexer:
+            if idadf.indexer:
+                if idadf.indexer in columns:
+                    columns.remove(idadf.indexer)
+                    
+        for column in columns:
+           entropy_dict[column] = entropy(idadf, column, mode = mode)
+                    
+        # Output
+        if len(columns) > 1:
+            result = pd.Series(entropy_dict)
+            result.sort_values(ascending = False)
+        else:
+            result = entropy_dict[columns[0]]
+        return result
+    
+def entropy_stats(idadf, target=None, mode="normal", execute = True, ignore_indexer=True):
+    """
+    Similar to ibmdbby.feature_selection.entropy.entrop but use DB2 statistics
+    to speed the computation. Returns an approximate value. Experimental. 
+    
+    Parameters
+    ----------
+    idadf: IdaDataFrame
+    
+    target: str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+    
+    mode: "normal" or "raw"
+        Experimental
+        
+    execute: bool, default:True
+        Experimental. Execute the request or return the correponding SQL query 
+    
+    ignore_indexer: bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.Series
+    
+    Notes
+    -----
+    Input column should be categorical, otherwise this measure does not make 
+    much sense. 
+    
+    Cannot handle columns that are not physically existing in the database, 
+    since no statistics are available for them. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> entropy_stats(idadf)
+    """
+    if target is not None:
+        subquery = "SELECT VALCOUNT as a FROM SYSCAT.COLDIST WHERE TABSCHEMA='%s' AND TABNAME = '%s' AND COLNAME='%s' AND TYPE='F' AND COLVALUE IS NOT NULL"%(idadf.schema, idadf.tablename, target)
+        if mode == "normal":
+            query = "SELECT(SUM(-a*LOG(a))/SUM(a)+LOG(SUM(a)))/LOG(2)FROM(%s)"%(subquery)
+        elif mode == "raw":
+            query = "SELECT SUM(-a*LOG(a)) FROM(%s)"%(subquery)
+        
+        if not execute:
+            return query
+        return idadf.ida_scalar_query(query)
+    else:
+        entropy_dict = OrderedDict()
+        columns = list(idadf.columns)
+        # Remove indexer
+        if ignore_indexer:
+            if idadf.indexer:
+                if idadf.indexer in columns:
+                    columns.remove(idadf.indexer)
+                    
+        for column in columns:
+           entropy_dict[column] = entropy_stats(idadf, column, mode = mode)
+                    
+        # Output
+        if len(columns) > 1:
+            result = pd.Series(entropy_dict)
+            result.sort_values(ascending = False)
+        else:
+            result = entropy_dict[columns[0]]
         return result
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/gain_ratio.py` & `nzpyida-0.3.3/nzpyida/feature_selection/gain_ratio.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,124 +1,124 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Mon Nov 23 09:48:18 2015
-
-@author: efouche
-"""
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import absolute_import
-from builtins import dict
-from future import standard_library
-standard_library.install_aliases()
-
-from collections import OrderedDict
-
-from nzpyida.feature_selection.entropy import entropy
-
-from nzpyida.internals import idadf_state
-from nzpyida.utils import timed
-
-import numpy as np 
-import pandas as pd
-
-from nzpyida.feature_selection.private import _check_input
-
-
-@idadf_state
-@timed
-def gain_ratio(idadf, target = None, features = None, symmetry=True, ignore_indexer=True):
-    """
-    Compute the gain ratio coefficients between a set of features and a 
-    set of target in an IdaDataFrame. 
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    target : str or list of str, optional
-        A column or list of columns against to be used as target. Per default, 
-        consider all columns
-    
-    features : str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-        
-    symmetry : bool, default: True
-        If True, compute the symmetric gain ratio as defined by
-        [Lopez de Mantaras 1991]. Otherwise, the asymmetric gain ratio. 
-    
-    ignore_indexer : bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.DataFrame or Pandas.Series if only one target
-    
-    Notes
-    -----
-    Input columns as target and features should be categorical, otherwise 
-    this measure does not make much sense. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> gain_ratio(idadf)
-    """
-    # Check input 
-    target, features = _check_input(idadf, target, features, ignore_indexer)
-    
-    entropy_dict = dict()
-    length = len(idadf)
-    values = OrderedDict()
-    corrector = length*np.log(length)
-        
-    for t in target:
-        if t not in values:
-            values[t] = OrderedDict()
-        features_notarget = [x for x in features if (x != t)]
-        
-        for feature in features_notarget:
-            if feature not in values:
-                values[feature] = OrderedDict()      
-                
-            if t not in values[feature]:    # i.e. it was not already computed 
-                if t not in entropy_dict:
-                    entropy_dict[t] = entropy(idadf, t, mode = "raw")
-                if feature not in entropy_dict:
-                    entropy_dict[feature] = entropy(idadf, feature, mode = "raw")
-                    
-                join_entropy = entropy(idadf,  [t] + [feature], mode = "raw")     
-                disjoin_entropy = entropy_dict[t] + entropy_dict[feature]
-                info_gain = (disjoin_entropy - join_entropy)
-            
-                if symmetry:
-                    gain_ratio = (info_gain + corrector)/(disjoin_entropy + 2*corrector) # 2* because symmetric
-                    values[t][feature] = gain_ratio
-                    if feature in target:
-                        values[feature][t] = gain_ratio
-                else:
-                    gain_ratio_1 = (info_gain + corrector)/(entropy_dict[t] + corrector)
-                    values[t][feature] = gain_ratio_1
-                    if feature in target:
-                        gain_ratio_2 = (info_gain + corrector)/(entropy_dict[feature] + corrector)
-                        values[feature][t] = gain_ratio_2
-             
-    ### Fill the matrix
-    result = pd.DataFrame(values).fillna(np.nan)
-    result = result.dropna(axis=1, how="all")
-        
-    if len(result.columns) > 1:
-        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
-        result = result.reindex(order)
-       
-    if len(result.columns) == 1:
-        if len(result) == 1:
-            result = result.iloc[0,0]
-        else:
-            result = result[result.columns[0]].copy()
-            result.sort_values(ascending = True)
-    else:
-        result = result.fillna(1)
-            
+# -*- coding: utf-8 -*-
+"""
+Created on Mon Nov 23 09:48:18 2015
+
+@author: efouche
+"""
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import absolute_import
+from builtins import dict
+from future import standard_library
+standard_library.install_aliases()
+
+from collections import OrderedDict
+
+from nzpyida.feature_selection.entropy import entropy
+
+from nzpyida.internals import idadf_state
+from nzpyida.utils import timed
+
+import numpy as np 
+import pandas as pd
+
+from nzpyida.feature_selection.private import _check_input
+
+
+@idadf_state
+@timed
+def gain_ratio(idadf, target = None, features = None, symmetry=True, ignore_indexer=True):
+    """
+    Compute the gain ratio coefficients between a set of features and a 
+    set of target in an IdaDataFrame. 
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    target : str or list of str, optional
+        A column or list of columns against to be used as target. Per default, 
+        consider all columns
+    
+    features : str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+        
+    symmetry : bool, default: True
+        If True, compute the symmetric gain ratio as defined by
+        [Lopez de Mantaras 1991]. Otherwise, the asymmetric gain ratio. 
+    
+    ignore_indexer : bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.DataFrame or Pandas.Series if only one target
+    
+    Notes
+    -----
+    Input columns as target and features should be categorical, otherwise 
+    this measure does not make much sense. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> gain_ratio(idadf)
+    """
+    # Check input 
+    target, features = _check_input(idadf, target, features, ignore_indexer)
+    
+    entropy_dict = dict()
+    length = len(idadf)
+    values = OrderedDict()
+    corrector = length*np.log(length)
+        
+    for t in target:
+        if t not in values:
+            values[t] = OrderedDict()
+        features_notarget = [x for x in features if (x != t)]
+        
+        for feature in features_notarget:
+            if feature not in values:
+                values[feature] = OrderedDict()      
+                
+            if t not in values[feature]:    # i.e. it was not already computed 
+                if t not in entropy_dict:
+                    entropy_dict[t] = entropy(idadf, t, mode = "raw")
+                if feature not in entropy_dict:
+                    entropy_dict[feature] = entropy(idadf, feature, mode = "raw")
+                    
+                join_entropy = entropy(idadf,  [t] + [feature], mode = "raw")     
+                disjoin_entropy = entropy_dict[t] + entropy_dict[feature]
+                info_gain = (disjoin_entropy - join_entropy)
+            
+                if symmetry:
+                    gain_ratio = (info_gain + corrector)/(disjoin_entropy + 2*corrector) # 2* because symmetric
+                    values[t][feature] = gain_ratio
+                    if feature in target:
+                        values[feature][t] = gain_ratio
+                else:
+                    gain_ratio_1 = (info_gain + corrector)/(entropy_dict[t] + corrector)
+                    values[t][feature] = gain_ratio_1
+                    if feature in target:
+                        gain_ratio_2 = (info_gain + corrector)/(entropy_dict[feature] + corrector)
+                        values[feature][t] = gain_ratio_2
+             
+    ### Fill the matrix
+    result = pd.DataFrame(values).fillna(np.nan)
+    result = result.dropna(axis=1, how="all")
+        
+    if len(result.columns) > 1:
+        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
+        result = result.reindex(order)
+       
+    if len(result.columns) == 1:
+        if len(result) == 1:
+            result = result.iloc[0,0]
+        else:
+            result = result[result.columns[0]].copy()
+            result.sort_values(ascending = True)
+    else:
+        result = result.fillna(1)
+            
     return result
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/gini.py` & `nzpyida-0.3.3/nzpyida/feature_selection/symmetric_uncertainty.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,178 +1,157 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Tue Dec  1 12:29:30 2015
-
-@author: efouche
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-
-from collections import OrderedDict
-
-import pandas as pd
-import numpy as np
-import six
-
-from nzpyida.internals import idadf_state
-from nzpyida.utils import timed
-from nzpyida.feature_selection.private import _check_input
-
-@idadf_state
-@timed
-def gini_pairwise(idadf, target=None, features=None, ignore_indexer=True):
-    """
-    Compute the conditional gini coefficients between a set of features and a 
-    set of target in an IdaDataFrame. 
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    target : str or list of str, optional
-        A column or list of columns against to be used as target. Per default, 
-        consider all columns
-    
-    features : str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-    
-    ignore_indexer : bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.DataFrame or Pandas.Series if only one target
-    
-    Notes
-    -----
-    Input columns as target and features should be categorical, otherwise 
-    this measure does not make much sense. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> gini_pairwise(idadf)
-    """
-    # Check input
-    target, features = _check_input(idadf, target, features, ignore_indexer)
-        
-    gini_dict = OrderedDict()
-    length = len(idadf)
-
-    if idadf._idadb._is_netezza_system():
-        power_function = "POW"
-        div_term = "* POW(c,-1)) * POW(%s,-1)"%length
-    else:
-        power_function = "POWER"
-        div_term = "/c)/%s"%length
-
-    for t in target:
-        gini_dict[t] = OrderedDict() 
-        features_notarget = [x for x in features if (x != t)]
-        
-        for feature in features_notarget:
-            if t not in gini_dict:
-                gini_dict[t] = OrderedDict()
-            
-            query = ("SELECT SUM((%s(c,2) - gini)%s FROM "+
-            "(SELECT SUM(%s(count,2)) as gini, SUM(count) as c FROM "+
-            "(SELECT CAST(COUNT(*) AS FLOAT) AS count, \"%s\" FROM %s GROUP BY \"%s\",\"%s\") AS T1 "+
-            "GROUP BY \"%s\") AS T2 ")
-            query0 = query%(power_function, div_term, power_function, feature, idadf.name, t, feature, feature)
-            gini_dict[t][feature] = idadf.ida_scalar_query(query0)
-            
-    result = pd.DataFrame(gini_dict).fillna(np.nan)
-        
-    if len(result.columns) > 1:
-        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
-        result = result.reindex(order)
-       
-    result = result.dropna(axis=1, how="all")
-    
-    if len(result.columns) == 1:
-        if len(result) == 1:
-            result = result.iloc[0,0]
-        else:
-            result = result[result.columns[0]].copy()
-            result.sort_values(ascending = True)
-    else:
-        result = result.fillna(0)
-    
-    return result
-               
-    
-        
-    
-    
-    
-@idadf_state
-@timed
-def gini(idadf, features=None, ignore_indexer=True):
-    """
-    Compute the gini coefficients for a set of features in an IdaDataFrame. 
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    features : str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-    
-    ignore_indexer : bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.Series
-    
-    Notes
-    -----
-    Input column should be categorical, otherwise this measure does not make 
-    much sense. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> gini(idadf)
-    """
-    if features is None:
-        features = list(idadf.columns)
-    else:
-        if isinstance(features, six.string_types):
-            features = [features]
-
-    if ignore_indexer is True:
-        if idadf.indexer:
-            if idadf.indexer in features:
-                features.remove(idadf.indexer)
-      
-        
-    value_dict = OrderedDict()
-        
-    length = len(idadf)**2
-
-    if idadf._idadb._is_netezza_system():
-      power_function = "POW"
-      div_term = "* POW(%s, -1)"%length
-    else:
-      power_function = "POWER"
-      div_term ="/%s"%length
-
-    for feature in features:
-        
-        subquery = "SELECT COUNT(*) AS count FROM %s GROUP BY \"%s\""%(idadf.name, feature)
-        query = "SELECT (%s - SUM(%s(count,2)))%s FROM (%s) AS T "%(length, power_function, div_term, subquery)
-        value_dict[feature] = idadf.ida_scalar_query(query)
-            
-        if len(features) > 1:
-            result = pd.Series(value_dict) 
-        else:
-            result = value_dict[feature]
-    
-    return result
+# -*- coding: utf-8 -*-
+"""
+Created on Mon Nov 23 09:53:19 2015
+
+@author: efouche
+"""
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import absolute_import
+from builtins import dict
+from future import standard_library
+standard_library.install_aliases()
+
+from collections import OrderedDict
+
+import nzpyida
+
+from nzpyida.feature_selection import entropy
+from nzpyida.feature_selection.private import _check_input
+
+from nzpyida.internals import idadf_state
+from nzpyida.utils import timed
+
+import pandas as pd
+import numpy as np
+
+@idadf_state
+@timed
+def su(idadf, target = None, features = None, ignore_indexer=True):
+    """
+    Compute the symmetric uncertainty coefficients between a set of features
+    and a set of target in an IdaDataFrame. 
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    target : str or list of str, optional
+        A column or list of columns against to be used as target. Per default, 
+        consider all columns
+    
+    features : str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+    
+    ignore_indexer : bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.DataFrame or Pandas.Series if only one target
+    
+    Notes
+    -----
+    Input columns as target and features should be categorical, otherwise 
+    this measure does not make much sense. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> su(idadf)
+    """
+    # Check input
+    target, features = _check_input(idadf, target, features, ignore_indexer)
+                
+    entropy_dict = dict()
+    length = len(idadf)
+    corrector = np.log(length)*length
+    values = OrderedDict()
+        
+    for t in target:
+        if t not in values:
+            values[t] = OrderedDict() 
+        features_notarget = [x for x in features if (x != t)]
+        
+        for feature in features_notarget:
+            if feature not in values:
+                values[feature] = OrderedDict()
+            if t not in values[feature]:
+                if t not in entropy_dict:
+                    entropy_dict[t] = entropy(idadf, t, mode = "raw")
+                if feature not in entropy_dict:
+                    entropy_dict[feature] = entropy(idadf, feature, mode = "raw")
+                join_entropy = entropy(idadf, [t] + [feature], mode = "raw")     
+                disjoin_entropy = entropy_dict[t] + entropy_dict[feature]
+                value = (2.0*(disjoin_entropy - join_entropy + corrector)/(disjoin_entropy + corrector*2))
+                values[t][feature] = value
+                if feature in target:
+                    values[feature][t] = value
+    
+    result = pd.DataFrame(values).fillna(np.nan)
+    result = result.dropna(axis=1, how="all")
+        
+    if len(result.columns) > 1:
+        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
+        result = result.reindex(order)
+    
+    if len(result.columns) == 1:
+        if len(result) == 1:
+            result = result.iloc[0,0]
+        else:
+            result = result[result.columns[0]].copy()
+            result.sort_values(ascending = True)
+    else:
+        result = result.fillna(1)
+   
+    return result
+
+@idadf_state
+@timed
+def outer_su(idadf1, key1, idadf2, key2, target = None, features1 = None, features2 = None):
+    """
+    Compute the symmetric uncertainty coefficients between a set of features
+    and a set of target from two different IdaDataFrames on a particular key. 
+    
+    This is experimental 
+    """
+    target1, features1 = _check_input(idadf1, target, features1)
+    target2, features2 = _check_input(idadf2, None, features2)
+    
+    if key1 not in idadf1.columns:
+        raise ValueError("%s is not a column in idadf1")
+    if key2 not in idadf2.columns:
+        raise ValueError("%s is not a column in idadf2")
+       
+    condition = "a.\"%s\" = b.\"%s\""%(key1,key2)
+    
+    if key2 in features2:
+        features2.remove(key2)
+    
+    afeaturesas = ", ".join(["a.\"%s\" as \"a.%s\" "%(feature, feature) for feature in features1])
+    bfeaturesas = ", ".join(["b.\"%s\" as \"b.%s\" "%(feature, feature) for feature in features2])
+    
+    selectlist = [afeaturesas, bfeaturesas]
+    
+    if target1 is not None:
+        atargetas = ", ".join(["a.\"%s\" as \"a.%s\" "%(tar, tar) for tar in [target1]])
+        selectlist.append(atargetas)
+        atarget = "a." + target1
+    else:
+        atarget = None
+        
+    abfeatures = ["a." + feature for feature in features1] + ["b." + feature for feature in features2]
+    selectstr = ", ".join(selectlist)
+    
+    expression = "SELECT %s FROM %s as a FULL OUTER JOIN %s as b ON %s"%(selectstr, idadf1.name, idadf2.name, condition)
+    
+    viewname = idadf1._idadb._create_view_from_expression(expression)
+    
+    try:
+        idadf_join = nzpyida.IdaDataFrame(idadf1._idadb, viewname)
+        return su(idadf_join, target = atarget, features = abfeatures)
+    except:
+        raise
+    finally:
+        idadf1._idadb.drop_view(viewname)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/info_gain.py` & `nzpyida-0.3.3/nzpyida/feature_selection/info_gain.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,107 +1,107 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Mon Nov 23 08:59:00 2015
-
-@author: efouche
-"""
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-from collections import OrderedDict
-
-from nzpyida.internals import idadf_state
-from nzpyida.utils import timed
-
-import pandas as pd
-from numpy import log
-import numpy as np
-
-from nzpyida.feature_selection.entropy import entropy
-
-from nzpyida.feature_selection.private import _check_input
-
-@idadf_state
-@timed
-def info_gain(idadf, target = None, features = None, ignore_indexer=True):
-    """
-    Compute the information gain / mutual information coefficients between a 
-    set of features and a set of target in an IdaDataFrame. 
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    target : str or list of str, optional
-        A column or list of columns against to be used as target. Per default, 
-        consider all columns
-    
-    features : str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-    
-    ignore_indexer : bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.DataFrame or Pandas.Series if only one target
-    
-    Notes
-    -----
-    Input columns as target and features should be categorical, otherwise 
-    this measure does not make much sense. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> info_gain(idadf)
-    """
-    # Check input
-    target, features = _check_input(idadf, target, features, ignore_indexer)
-    
-    entropy_dict = OrderedDict()
-    length = len(idadf)
-    loglength = log(length)
-    
-    values = OrderedDict()
-    
-    for t in target:
-        if t not in values:
-            values[t] = OrderedDict() 
-        features_notarget = [x for x in features if (x != t)]
-        
-        for feature in features_notarget:
-            if feature not in values:
-                values[feature] = OrderedDict()
-            if t not in values[feature]:
-                if t not in entropy_dict:
-                    entropy_dict[t] = entropy(idadf, t, mode = "raw")
-                if feature not in entropy_dict:
-                    entropy_dict[feature] = entropy(idadf, feature, mode = "raw")
-                join_entropy = entropy(idadf, [t] + [feature], mode = "raw")            
-                
-                value = ((entropy_dict[t] + entropy_dict[feature] - join_entropy)/length + loglength)/log(2)
-                values[t][feature] = value
-                if feature in target:
-                    values[feature][t] = value
-    
-    result = pd.DataFrame(values).fillna(np.nan)
-    result = result.dropna(axis=1, how="all")
-    
-    if len(result.columns) > 1:
-        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
-        result = result.reindex(order)
-    
-    if len(result.columns) == 1:
-        if len(result) == 1:
-            result = result.iloc[0,0]
-        else:
-            result = result[result.columns[0]].copy()
-            result.sort_values(inplace=True, ascending=False)
-
-    return result        
-
+# -*- coding: utf-8 -*-
+"""
+Created on Mon Nov 23 08:59:00 2015
+
+@author: efouche
+"""
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+from collections import OrderedDict
+
+from nzpyida.internals import idadf_state
+from nzpyida.utils import timed
+
+import pandas as pd
+from numpy import log
+import numpy as np
+
+from nzpyida.feature_selection.entropy import entropy
+
+from nzpyida.feature_selection.private import _check_input
+
+@idadf_state
+@timed
+def info_gain(idadf, target = None, features = None, ignore_indexer=True):
+    """
+    Compute the information gain / mutual information coefficients between a 
+    set of features and a set of target in an IdaDataFrame. 
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    target : str or list of str, optional
+        A column or list of columns against to be used as target. Per default, 
+        consider all columns
+    
+    features : str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+    
+    ignore_indexer : bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.DataFrame or Pandas.Series if only one target
+    
+    Notes
+    -----
+    Input columns as target and features should be categorical, otherwise 
+    this measure does not make much sense. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> info_gain(idadf)
+    """
+    # Check input
+    target, features = _check_input(idadf, target, features, ignore_indexer)
+    
+    entropy_dict = OrderedDict()
+    length = len(idadf)
+    loglength = log(length)
+    
+    values = OrderedDict()
+    
+    for t in target:
+        if t not in values:
+            values[t] = OrderedDict() 
+        features_notarget = [x for x in features if (x != t)]
+        
+        for feature in features_notarget:
+            if feature not in values:
+                values[feature] = OrderedDict()
+            if t not in values[feature]:
+                if t not in entropy_dict:
+                    entropy_dict[t] = entropy(idadf, t, mode = "raw")
+                if feature not in entropy_dict:
+                    entropy_dict[feature] = entropy(idadf, feature, mode = "raw")
+                join_entropy = entropy(idadf, [t] + [feature], mode = "raw")            
+                
+                value = ((entropy_dict[t] + entropy_dict[feature] - join_entropy)/length + loglength)/log(2)
+                values[t][feature] = value
+                if feature in target:
+                    values[feature][t] = value
+    
+    result = pd.DataFrame(values).fillna(np.nan)
+    result = result.dropna(axis=1, how="all")
+    
+    if len(result.columns) > 1:
+        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
+        result = result.reindex(order)
+    
+    if len(result.columns) == 1:
+        if len(result) == 1:
+            result = result.iloc[0,0]
+        else:
+            result = result[result.columns[0]].copy()
+            result.sort_values(inplace=True, ascending=False)
+
+    return result        
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/private.py` & `nzpyida-0.3.3/nzpyida/feature_selection/private.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,89 +1,89 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Mon Nov 23 10:46:25 2015
-
-@author: efouche
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-import six
-
-def _check_input(idadf, target, features, ignore_indexer=True):
-    """
-    Check if the input is valid, i.e. if each column in target and features
-    exists in idadf. 
-    
-    Parameters
-    ----------
-    target: str or list of str
-        A column or list of columns to be used as target
-        
-    features: str or list of str
-        A column or list of columns to be used as feature
-    
-    ignore_indexer: bool, default: True
-        If True, remove the indexer from the features set, as long as an
-        indexer is defined in idadf
-    """
-    #import pdb ; pdb.set_trace()
-    if target is not None:
-        if isinstance(target, six.string_types):
-            if target not in idadf.columns:
-                raise ValueError("Unknown target column %s"%target)
-            target = [target]
-        else:
-            if hasattr(target, '__iter__'):
-                target = list(target)
-            for x in target:
-                if x not in idadf.columns:
-                    raise ValueError("Unknown target column %s"%x)
-            
-    if features is not None:
-        if isinstance(features, six.string_types):
-            if features not in idadf.columns:
-                raise ValueError("Unknown feature column %s"%features)
-            features = [features]
-        else:
-            if hasattr(features, '__iter__'):
-                features = list(features)
-            for x in features:
-                if x not in idadf.columns:
-                    raise ValueError("Unknown feature column %s"%x)
-        if target is None:
-            if len(features) == 1:
-                raise ValueError("Cannot compute correlation coefficients of only one"+
-                                 " column (%s), need at least 2"%features[0])
-    else:
-        if target is not None:
-            if len(target) == 1:
-                features = [x for x in idadf.columns if x not in target]
-            else:
-                features = list(idadf.columns)
-        else:
-            features = list(idadf.columns)
-            
-        ## Remove indexer from feature list
-        # This is useless and expensive to compute with a primary key 
-        if ignore_indexer is True:
-            if idadf.indexer:
-                if idadf.indexer in features:
-                    features.remove(idadf.indexer)
-    
-    # Catch the case where users ask for the correlation between the two same columns
-    #import pdb ; pdb.set_trace()
-    if target == features:
-        if len(target) == 1:
-            raise ValueError("The correlation value of two same columns is always maximal")
-            
-    if target is None:
-        if features is None:
-            target = list(idadf.columns) 
-        else:
-            target = features
-            
+# -*- coding: utf-8 -*-
+"""
+Created on Mon Nov 23 10:46:25 2015
+
+@author: efouche
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+import six
+
+def _check_input(idadf, target, features, ignore_indexer=True):
+    """
+    Check if the input is valid, i.e. if each column in target and features
+    exists in idadf. 
+    
+    Parameters
+    ----------
+    target: str or list of str
+        A column or list of columns to be used as target
+        
+    features: str or list of str
+        A column or list of columns to be used as feature
+    
+    ignore_indexer: bool, default: True
+        If True, remove the indexer from the features set, as long as an
+        indexer is defined in idadf
+    """
+    #import pdb ; pdb.set_trace()
+    if target is not None:
+        if isinstance(target, six.string_types):
+            if target not in idadf.columns:
+                raise ValueError("Unknown target column %s"%target)
+            target = [target]
+        else:
+            if hasattr(target, '__iter__'):
+                target = list(target)
+            for x in target:
+                if x not in idadf.columns:
+                    raise ValueError("Unknown target column %s"%x)
+            
+    if features is not None:
+        if isinstance(features, six.string_types):
+            if features not in idadf.columns:
+                raise ValueError("Unknown feature column %s"%features)
+            features = [features]
+        else:
+            if hasattr(features, '__iter__'):
+                features = list(features)
+            for x in features:
+                if x not in idadf.columns:
+                    raise ValueError("Unknown feature column %s"%x)
+        if target is None:
+            if len(features) == 1:
+                raise ValueError("Cannot compute correlation coefficients of only one"+
+                                 " column (%s), need at least 2"%features[0])
+    else:
+        if target is not None:
+            if len(target) == 1:
+                features = [x for x in idadf.columns if x not in target]
+            else:
+                features = list(idadf.columns)
+        else:
+            features = list(idadf.columns)
+            
+        ## Remove indexer from feature list
+        # This is useless and expensive to compute with a primary key 
+        if ignore_indexer is True:
+            if idadf.indexer:
+                if idadf.indexer in features:
+                    features.remove(idadf.indexer)
+    
+    # Catch the case where users ask for the correlation between the two same columns
+    #import pdb ; pdb.set_trace()
+    if target == features:
+        if len(target) == 1:
+            raise ValueError("The correlation value of two same columns is always maximal")
+            
+    if target is None:
+        if features is None:
+            target = list(idadf.columns) 
+        else:
+            target = features
+            
     return target, features
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/symmetric_uncertainty.py` & `nzpyida-0.3.3/nzpyida/feature_selection/chisquared.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,157 +1,140 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Mon Nov 23 09:53:19 2015
-
-@author: efouche
-"""
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import absolute_import
-from builtins import dict
-from future import standard_library
-standard_library.install_aliases()
-
-from collections import OrderedDict
-
-import nzpyida
-
-from nzpyida.feature_selection import entropy
-from nzpyida.feature_selection.private import _check_input
-
-from nzpyida.internals import idadf_state
-from nzpyida.utils import timed
-
-import pandas as pd
-import numpy as np
-
-@idadf_state
-@timed
-def su(idadf, target = None, features = None, ignore_indexer=True):
-    """
-    Compute the symmetric uncertainty coefficients between a set of features
-    and a set of target in an IdaDataFrame. 
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    target : str or list of str, optional
-        A column or list of columns against to be used as target. Per default, 
-        consider all columns
-    
-    features : str or list of str, optional
-        A column or list of columns to be used as features. Per default, 
-        consider all columns. 
-    
-    ignore_indexer : bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.DataFrame or Pandas.Series if only one target
-    
-    Notes
-    -----
-    Input columns as target and features should be categorical, otherwise 
-    this measure does not make much sense. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> su(idadf)
-    """
-    # Check input
-    target, features = _check_input(idadf, target, features, ignore_indexer)
-                
-    entropy_dict = dict()
-    length = len(idadf)
-    corrector = np.log(length)*length
-    values = OrderedDict()
-        
-    for t in target:
-        if t not in values:
-            values[t] = OrderedDict() 
-        features_notarget = [x for x in features if (x != t)]
-        
-        for feature in features_notarget:
-            if feature not in values:
-                values[feature] = OrderedDict()
-            if t not in values[feature]:
-                if t not in entropy_dict:
-                    entropy_dict[t] = entropy(idadf, t, mode = "raw")
-                if feature not in entropy_dict:
-                    entropy_dict[feature] = entropy(idadf, feature, mode = "raw")
-                join_entropy = entropy(idadf, [t] + [feature], mode = "raw")     
-                disjoin_entropy = entropy_dict[t] + entropy_dict[feature]
-                value = (2.0*(disjoin_entropy - join_entropy + corrector)/(disjoin_entropy + corrector*2))
-                values[t][feature] = value
-                if feature in target:
-                    values[feature][t] = value
-    
-    result = pd.DataFrame(values).fillna(np.nan)
-    result = result.dropna(axis=1, how="all")
-        
-    if len(result.columns) > 1:
-        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
-        result = result.reindex(order)
-    
-    if len(result.columns) == 1:
-        if len(result) == 1:
-            result = result.iloc[0,0]
-        else:
-            result = result[result.columns[0]].copy()
-            result.sort_values(ascending = True)
-    else:
-        result = result.fillna(1)
-   
-    return result
-
-@idadf_state
-@timed
-def outer_su(idadf1, key1, idadf2, key2, target = None, features1 = None, features2 = None):
-    """
-    Compute the symmetric uncertainty coefficients between a set of features
-    and a set of target from two different IdaDataFrames on a particular key. 
-    
-    This is experimental 
-    """
-    target1, features1 = _check_input(idadf1, target, features1)
-    target2, features2 = _check_input(idadf2, None, features2)
-    
-    if key1 not in idadf1.columns:
-        raise ValueError("%s is not a column in idadf1")
-    if key2 not in idadf2.columns:
-        raise ValueError("%s is not a column in idadf2")
-       
-    condition = "a.\"%s\" = b.\"%s\""%(key1,key2)
-    
-    if key2 in features2:
-        features2.remove(key2)
-    
-    afeaturesas = ", ".join(["a.\"%s\" as \"a.%s\" "%(feature, feature) for feature in features1])
-    bfeaturesas = ", ".join(["b.\"%s\" as \"b.%s\" "%(feature, feature) for feature in features2])
-    
-    selectlist = [afeaturesas, bfeaturesas]
-    
-    if target1 is not None:
-        atargetas = ", ".join(["a.\"%s\" as \"a.%s\" "%(tar, tar) for tar in [target1]])
-        selectlist.append(atargetas)
-        atarget = "a." + target1
-    else:
-        atarget = None
-        
-    abfeatures = ["a." + feature for feature in features1] + ["b." + feature for feature in features2]
-    selectstr = ", ".join(selectlist)
-    
-    expression = "SELECT %s FROM %s as a FULL OUTER JOIN %s as b ON %s"%(selectstr, idadf1.name, idadf2.name, condition)
-    
-    viewname = idadf1._idadb._create_view_from_expression(expression)
-    
-    try:
-        idadf_join = nzpyida.IdaDataFrame(idadf1._idadb, viewname)
-        return su(idadf_join, target = atarget, features = abfeatures)
-    except:
-        raise
-    finally:
-        idadf1._idadb.drop_view(viewname)
+# -*- coding: utf-8 -*-
+"""
+Created on Mon Dec 14 11:31:52 2015
+
+@author: efouche
+"""
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import absolute_import
+from builtins import dict
+from future import standard_library
+standard_library.install_aliases()
+
+from collections import OrderedDict
+
+from nzpyida.internals import idadf_state
+from nzpyida.utils import timed
+
+import numpy as np 
+import pandas as pd
+
+from nzpyida.feature_selection.private import _check_input
+
+@idadf_state
+@timed
+def chisquared(idadf, target = None, features = None, ignore_indexer=True):
+    """
+    Compute the Chi-Squared statistics coefficients between a set of features 
+    and a set of target in an IdaDataFrame. 
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    target : str or list of str, optional
+        A column or list of columns against to be used as target. Per default, 
+        consider all columns
+    
+    features : str or list of str, optional
+        A column or list of columns to be used as features. Per default, 
+        consider all columns. 
+    
+    ignore_indexer : bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.DataFrame or Pandas.Series if only one target
+    
+    Notes
+    -----
+    Input columns as target and features should be categorical, otherwise 
+    this measure does not make much sense. 
+    
+    Chi-squared as defined in 
+    A Comparative Study on Feature Selection and Classification Methods Using 
+    Gene Expression Profiles and Proteomic Patterns. (GIW02F006)
+    
+    The scalability of this approach is not very good. Should not be used on 
+    high dimensional data. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> chisquared(idadf)
+    """
+    # Check input
+    target, features = _check_input(idadf, target, features, ignore_indexer)
+    count_dict = dict()
+    length = len(idadf)
+    
+    values = OrderedDict()
+         
+    for t in target:   
+        if t not in values:
+            values[t] = OrderedDict() 
+        features_notarget = [x for x in features if (x != t)]
+        
+        ### Compute
+        for feature in features_notarget:
+            if feature not in values:
+                values[feature] = OrderedDict()
+            if t not in values[feature]:
+                if t not in count_dict:
+                    count = idadf.count_groupby(t)
+                    count_serie = count["count"]
+                    count_serie.index = count[t]
+                    count_dict[t] = count_serie
+            
+                C = dict(count_dict[t])
+                
+                if feature not in count_dict:
+                    count = idadf.count_groupby(feature)
+                    count_serie = count["count"]
+                    count_serie.index = count[feature]
+                    count_dict[feature] = count_serie
+                    
+                R = dict(count_dict[feature])
+                
+                if (feature, t) not in count_dict:
+                    count_dict[(feature, t)] = idadf.count_groupby([feature , t])
+                
+                count = count_dict[(feature, t)]
+                
+                chi = 0            
+                for target_class in C.keys():
+                    count_target = count[count[t] == target_class][[feature, "count"]]
+                    A_target = count_target['count']
+                    A_target.index = count_target[feature]
+                    
+                    for feature_class in A_target.index:
+                        a = A_target[feature_class]
+                        e = R[feature_class] * C[target_class] / length
+                        chi += ((a - e)**2)/e
+                
+                values[t][feature] = chi   # chisquared is symmetric 
+                if feature in target:
+                    values[feature][t] = chi
+        
+    result = pd.DataFrame(values).fillna(np.nan)
+    result = result.dropna(axis=1, how="all")
+        
+    if len(result.columns) > 1:
+        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
+        result = result.reindex(order)
+    
+    if len(result.columns) == 1:
+        if len(result) == 1:
+            result = result.iloc[0,0]
+        else:
+            result = result[result.columns[0]].copy()
+            result.sort_values(ascending = False)
+        
+
+    
+    
+    return result
+    
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/feature_selection/tstats.py` & `nzpyida-0.3.3/nzpyida/feature_selection/tstats.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,137 +1,137 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Mon Dec 14 13:05:27 2015
-
-@author: efouche
-"""
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import absolute_import
-from builtins import dict
-from future import standard_library
-standard_library.install_aliases()
-
-from collections import OrderedDict
-
-import numpy as np 
-import pandas as pd
-
-#from ibmdbpy.internals import idadf_state
-from nzpyida.feature_selection.private import _check_input
-
-#@idadf_state#(force=True)
-def ttest(idadf, target=None, features=None, ignore_indexer=True):
-    """
-    Compute the t-statistics values of a set of features against a set of 
-    target attributes. 
-    
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    
-    target : str or list of str, optional
-        A column or list of columns against which the t-statistcs values will 
-        be computed. Per default, consider all columns
-    
-    features : str or list of str, optional
-        A column or list of columns for which the t-statistics values will be 
-        computed against each target attributes. Per default, consider all 
-        columns, except non numerical columns. 
-    
-    ignore_indexer : bool, default: True
-        Per default, ignore the column declared as indexer in idadf
-        
-    Returns
-    -------
-    Pandas.DataFrame or Pandas.Series if only one target
-    
-    Raises
-    ------
-    TypeError
-        If the features argument or the data set does not contains any 
-        numerical features. Raise TypeError. 
-        
-    Notes
-    -----
-    This implements the "modified" ttest as defined in the paper
-    A Modified T-test feature Selection Method and Its Application on
-    the HapMap Genotype Data (Zhou et al.)
-    
-    The target columns should be categorical, while the feature columns should
-    be numerical.
-    
-    The scalability of this approach is not very good. Should not be used on 
-    high dimensional data. 
-    
-    Examples
-    --------
-    >>> idadf = IdaDataFrame(idadb, "IRIS")
-    >>> ttest(idadf,"CLASS")
-    """
-    # Check input
-    target, features = _check_input(idadf, target, features, ignore_indexer)
-    ttest_dict = OrderedDict()
-    length = len(idadf)
-    
-    S_dict = dict()
-    M_dict = dict()
-    class_mean_dict = dict()
-    
-    numerical_columns = idadf._get_numerical_columns()
-    
-    # Filter out non numerical columns
-    features = [feature for feature in features if feature in numerical_columns]
-    if not features:
-        raise TypeError("No numerical features.")
-        
-    #mean = idadf[features].mean() # This is broken
-    mean = idadf.mean()
-    
-    if target is None:
-        target = list(idadf.columns)
-            
-    for t in target:
-        features_notarget = [x for x in features if (x != t)]
-    
-        if t not in M_dict:
-            count = idadf.count_groupby(t)    
-            target_count = count["count"]
-            target_count.index = count[t]
-            M_dict[t] = np.sqrt(1/target_count + 1/length)     
-        
-        if t not in S_dict:
-            S_dict[t] = idadf.within_class_std(target = t, features = features_notarget)
-        
-        if t not in class_mean_dict:
-            class_mean_dict[t] = idadf.mean_groupby(t, features = features_notarget)
-            
-        M = M_dict[t]
-        S = S_dict[t]
-        class_mean = class_mean_dict[t]
-        
-        ttest_dict[t] = OrderedDict()
-        for feature in features_notarget:
-            ttest_dict[t][feature] = OrderedDict()
-            for target_class in class_mean.index:
-                numerator = abs(class_mean.loc[target_class][feature] - mean[feature])
-                denominator = M[target_class] * S[feature]
-                
-                ttest_dict[t][feature][target_class] = numerator / denominator
-                    
-        for feature in features_notarget:
-            ttest_dict[t][feature] = max(ttest_dict[t][feature].values())
-        
-    result = pd.DataFrame(ttest_dict)
-    
-    if len(result.columns) == 1:
-        if len(result) == 1:
-            result = result.iloc[0,0]
-        else:
-            result = result[result.columns[0]].copy()
-            result.sort_values(ascending = False)
-    else:
-        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
-        result = result.reindex(order)
-        
+# -*- coding: utf-8 -*-
+"""
+Created on Mon Dec 14 13:05:27 2015
+
+@author: efouche
+"""
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import absolute_import
+from builtins import dict
+from future import standard_library
+standard_library.install_aliases()
+
+from collections import OrderedDict
+
+import numpy as np 
+import pandas as pd
+
+#from ibmdbpy.internals import idadf_state
+from nzpyida.feature_selection.private import _check_input
+
+#@idadf_state#(force=True)
+def ttest(idadf, target=None, features=None, ignore_indexer=True):
+    """
+    Compute the t-statistics values of a set of features against a set of 
+    target attributes. 
+    
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    
+    target : str or list of str, optional
+        A column or list of columns against which the t-statistcs values will 
+        be computed. Per default, consider all columns
+    
+    features : str or list of str, optional
+        A column or list of columns for which the t-statistics values will be 
+        computed against each target attributes. Per default, consider all 
+        columns, except non numerical columns. 
+    
+    ignore_indexer : bool, default: True
+        Per default, ignore the column declared as indexer in idadf
+        
+    Returns
+    -------
+    Pandas.DataFrame or Pandas.Series if only one target
+    
+    Raises
+    ------
+    TypeError
+        If the features argument or the data set does not contains any 
+        numerical features. Raise TypeError. 
+        
+    Notes
+    -----
+    This implements the "modified" ttest as defined in the paper
+    A Modified T-test feature Selection Method and Its Application on
+    the HapMap Genotype Data (Zhou et al.)
+    
+    The target columns should be categorical, while the feature columns should
+    be numerical.
+    
+    The scalability of this approach is not very good. Should not be used on 
+    high dimensional data. 
+    
+    Examples
+    --------
+    >>> idadf = IdaDataFrame(idadb, "IRIS")
+    >>> ttest(idadf,"CLASS")
+    """
+    # Check input
+    target, features = _check_input(idadf, target, features, ignore_indexer)
+    ttest_dict = OrderedDict()
+    length = len(idadf)
+    
+    S_dict = dict()
+    M_dict = dict()
+    class_mean_dict = dict()
+    
+    numerical_columns = idadf._get_numerical_columns()
+    
+    # Filter out non numerical columns
+    features = [feature for feature in features if feature in numerical_columns]
+    if not features:
+        raise TypeError("No numerical features.")
+        
+    #mean = idadf[features].mean() # This is broken
+    mean = idadf.mean()
+    
+    if target is None:
+        target = list(idadf.columns)
+            
+    for t in target:
+        features_notarget = [x for x in features if (x != t)]
+    
+        if t not in M_dict:
+            count = idadf.count_groupby(t)    
+            target_count = count["count"]
+            target_count.index = count[t]
+            M_dict[t] = np.sqrt(1/target_count + 1/length)     
+        
+        if t not in S_dict:
+            S_dict[t] = idadf.within_class_std(target = t, features = features_notarget)
+        
+        if t not in class_mean_dict:
+            class_mean_dict[t] = idadf.mean_groupby(t, features = features_notarget)
+            
+        M = M_dict[t]
+        S = S_dict[t]
+        class_mean = class_mean_dict[t]
+        
+        ttest_dict[t] = OrderedDict()
+        for feature in features_notarget:
+            ttest_dict[t][feature] = OrderedDict()
+            for target_class in class_mean.index:
+                numerator = abs(class_mean.loc[target_class][feature] - mean[feature])
+                denominator = M[target_class] * S[feature]
+                
+                ttest_dict[t][feature][target_class] = numerator / denominator
+                    
+        for feature in features_notarget:
+            ttest_dict[t][feature] = max(ttest_dict[t][feature].values())
+        
+    result = pd.DataFrame(ttest_dict)
+    
+    if len(result.columns) == 1:
+        if len(result) == 1:
+            result = result.iloc[0,0]
+        else:
+            result = result[result.columns[0]].copy()
+            result.sort_values(ascending = False)
+    else:
+        order = [x for x in result.columns if x in features] + [x for x in features if x not in result.columns]
+        result = result.reindex(order)
+        
     return result
```

### Comparing `nzpyida-0.2.2.6/nzpyida/filtering.py` & `nzpyida-0.3.3/nzpyida/filtering.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,189 +1,189 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""The module implement objects and functions that are used to filter IdaDataFrame objects"""
-
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-from builtins import str
-from future import standard_library
-standard_library.install_aliases()
-
-from copy import deepcopy
-from numbers import Number
-from nzpyida.exceptions import IdaDataBaseError
-import six
-
-class FilterQuery(object):
-    """
-    FilterQueries are used to represent the filtering of an IdaDataFrame
-    in ibmdbpy. The use of comparison operators, such as <, <=, ==, >=, >
-    on an IdaDataFrame instance produces a FilterQuery instance which acts as a
-    container for the where clause of the corresponding SQL request.
-
-    Filtering is possible using a Pandas-like syntax. Applying comparison
-    operators to IdaDataFrames produces a FilterQuery instance which
-    contains the string embedding the corresponding where clause in the 
-    “wherestr” attribute.
-
-    FilterQuery objects also contain a logic that allows them to be
-    combined, thus allowing complex filtering.
-
-      You can combine the following operators: \|, \&, \^ (OR, AND and XOR)
-
-    Examples
-    --------
-    >>> idadf[['sepal_length', 'petal_width'] < 5]
-    >>> <ibmdbpy.filtering.FilterQuery at 0xa65ba90>
-    >>> _.wherestr
-    '("sepal_length" < 5 AND "petal_width" < 5)'
-
-    >>> idadf[idadf[['sepal_length', 'petal_width']] < 5]
-    <ibmdbpy.frame.IdaDataFrame at 0xa73a860>
-    >>> _.head() # filtered IdaDataFrame
-        sepal_length  sepal_width  petal_length  petal_width     species
-    0           4.4          2.9           1.4          0.2      setosa
-    1           4.7          3.2           1.6          0.2      setosa
-    2           4.9          2.5           4.5          1.7   virginica
-    3           4.9          2.4           3.3          1.0  versicolor
-    4           4.6          3.2           1.4          0.2      setosa
-
-    >>> idadf[(idadf['sepal_length'] < 5) & (idadf[petal_width'] > 1.5)]
-    <ibmdbpy.frame.IdaDataFrame at 0xa74b9b0>
-    >>> _.head()
-       sepal_length  sepal_width  petal_length  petal_width    species
-    0           4.9          2.5           4.5          1.7  virginica
-
-    Notes
-    -----
-    It is not possible to filter an IdaDataFrame by using an IdaDataFrame that 
-    is opened in a different data source in the database. This is due to the 
-    fact that, using a Pandas-like syntax, “idadf[‘petal_width’] < 5” will 
-    return a Boolean array that is used to subset the original DataFrame. This 
-    is a fundamental restriction of ibmdbpy: we cannot afford to compute and 
-    download such an array because we cannot assume that the result will fit 
-    into user’s memory. Download time can also be a performance issue.
-
-    """
-    def __init__(self, columns, tablename, method, value):
-        """
-        Constructor for filterquery objects.
-
-        Parameters
-        ----------
-        columns : str or list
-            columns on which the filter operation should be applied
-        tablename : str
-            Name of the table to which columns belong
-        method : str
-            value representing the comparision operator.
-            Admissible value:  ["lt","le","eq","ne","ge,"gt"]
-        value: str or number
-            Value to use to filter.
-
-        Attributes
-        ----------
-        columns : str or list
-            columns on which the filter operation should be applied
-        tablename : str
-            Name of the table to which columns belong
-        method : str
-            value representing the comparision operator.
-            Admissible value:  ["lt","le","eq","ne","ge,"gt"]
-        value: str or number
-            Value to use to filter.
-        wherestr : str
-            SQL where clause to be used for filtering
-
-        Raises
-        ------
-        IdaDataBaseError
-            * The value for filtering is not a string or a number
-            * The filtering method is not suppoted
-        Notes
-        -----
-        This object is considered as private, and should be called internally
-        by IdaDataFrame instances.
-        """
-        # Sanity checks
-        if isinstance(columns, six.string_types):
-            columns = [columns]
-        if isinstance(value, six.string_types):
-            value = "'" + value + "'"
-        if not ((isinstance(value,six.string_types)|isinstance(value,Number))):
-            raise IdaDataBaseError("Value for Filterquery is expected to be a string or a number. Type %s"%str(type(value)))
-        dictmethod = {"lt": " < ", "le": " <= ",  "eq": " = ", "ne": " != ", "ge": " >= ", "gt": " > "}
-        if method not in dictmethod.keys():
-            raise IdaDataBaseError("The filtering method is not suppoted. Admissible values for method argument are %s"%list(dictmethod.keys()))
-
-        self.columns = columns     # TO DEPRECATE
-        self.tablename = tablename
-        self.method = method       # TO DEPRECATE
-        self.value = value         # TO DEPRECATE
-        self.wherestr = ("(\"" + ("\"" + str(dictmethod[method]) + str(value) + " AND \"").join(columns) +
-                        "\"" +  str(dictmethod[method]) + str(value) + ")")
-
-    # TO DEPRECATE
-    @property
-    def query(self):
-        """
-        Return an SQL query like "SELECT * FROM %s WHERE <WHERESTR>", where
-        <WHERESTR> is the value of the attribute "wherestr".
-        """
-        return "SELECT * FROM %s WHERE " + self.wherestr
-
-    def __and__(self, other):
-        """
-        Combine two FilterQuery instances with the operator "&" (AND)
-        For example : (idadf['sepal_length'] < 5) & (idadf[petal_width'] > 3)
-        """
-        self._combine_check(other)
-        newquery = deepcopy(self)
-        newquery.wherestr = "(%s AND %s)"%(self.wherestr,other.wherestr)
-        return newquery
-
-    def __or__(self, other):
-        """
-        Combine two FilterQuery instances with the operator "|" (OR)
-        For example : (idadf['sepal_length'] < 5) | (idadf[petal_width'] > 3)
-        """
-        self._combine_check(other)
-        newquery = deepcopy(self)
-        newquery.wherestr =  "(%s OR %s)"%(self.wherestr,other.wherestr)
-        return newquery
-
-    def __xor__(self, other):
-        """
-        Combine two FilterQuery instances with the operator "^" (XOR)
-        For example : (idadf['sepal_length'] < 5) ^ (idadf[petal_width'] > 3)
-        """
-        self._combine_check(other)
-        newquery = deepcopy(self)
-        newquery.wherestr = ("((NOT %s AND %s) OR (%s AND NOT %s))"%(self.wherestr,other.wherestr,self.wherestr,other.wherestr))
-        return newquery
-
-    def _combine_check(self, other):
-        """
-        Check if the name of two  FilterQuery is the same. Raise an
-        IdaDataBaseError in case it is different.
-        This function is used before performing logical operations between
-        FilterQuery instances.
-
-        Raises
-        ------
-        IdaDataBaseError
-        """
-        if self.tablename != other.tablename:
-            raise IdaDataBaseError("Combining filtering criterions from "+
-                                   "columns belongings to different tables "+
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""The module implement objects and functions that are used to filter IdaDataFrame objects"""
+
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+from builtins import str
+from future import standard_library
+standard_library.install_aliases()
+
+from copy import deepcopy
+from numbers import Number
+from nzpyida.exceptions import IdaDataBaseError
+import six
+
+class FilterQuery(object):
+    """
+    FilterQueries are used to represent the filtering of an IdaDataFrame
+    in ibmdbpy. The use of comparison operators, such as <, <=, ==, >=, >
+    on an IdaDataFrame instance produces a FilterQuery instance which acts as a
+    container for the where clause of the corresponding SQL request.
+
+    Filtering is possible using a Pandas-like syntax. Applying comparison
+    operators to IdaDataFrames produces a FilterQuery instance which
+    contains the string embedding the corresponding where clause in the 
+    “wherestr” attribute.
+
+    FilterQuery objects also contain a logic that allows them to be
+    combined, thus allowing complex filtering.
+
+      You can combine the following operators: \|, \&, \^ (OR, AND and XOR)
+
+    Examples
+    --------
+    >>> idadf[['sepal_length', 'petal_width'] < 5]
+    >>> <ibmdbpy.filtering.FilterQuery at 0xa65ba90>
+    >>> _.wherestr
+    '("sepal_length" < 5 AND "petal_width" < 5)'
+
+    >>> idadf[idadf[['sepal_length', 'petal_width']] < 5]
+    <ibmdbpy.frame.IdaDataFrame at 0xa73a860>
+    >>> _.head() # filtered IdaDataFrame
+        sepal_length  sepal_width  petal_length  petal_width     species
+    0           4.4          2.9           1.4          0.2      setosa
+    1           4.7          3.2           1.6          0.2      setosa
+    2           4.9          2.5           4.5          1.7   virginica
+    3           4.9          2.4           3.3          1.0  versicolor
+    4           4.6          3.2           1.4          0.2      setosa
+
+    >>> idadf[(idadf['sepal_length'] < 5) & (idadf[petal_width'] > 1.5)]
+    <ibmdbpy.frame.IdaDataFrame at 0xa74b9b0>
+    >>> _.head()
+       sepal_length  sepal_width  petal_length  petal_width    species
+    0           4.9          2.5           4.5          1.7  virginica
+
+    Notes
+    -----
+    It is not possible to filter an IdaDataFrame by using an IdaDataFrame that 
+    is opened in a different data source in the database. This is due to the 
+    fact that, using a Pandas-like syntax, “idadf[‘petal_width’] < 5” will 
+    return a Boolean array that is used to subset the original DataFrame. This 
+    is a fundamental restriction of ibmdbpy: we cannot afford to compute and 
+    download such an array because we cannot assume that the result will fit 
+    into user’s memory. Download time can also be a performance issue.
+
+    """
+    def __init__(self, columns, tablename, method, value):
+        """
+        Constructor for filterquery objects.
+
+        Parameters
+        ----------
+        columns : str or list
+            columns on which the filter operation should be applied
+        tablename : str
+            Name of the table to which columns belong
+        method : str
+            value representing the comparision operator.
+            Admissible value:  ["lt","le","eq","ne","ge,"gt"]
+        value: str or number
+            Value to use to filter.
+
+        Attributes
+        ----------
+        columns : str or list
+            columns on which the filter operation should be applied
+        tablename : str
+            Name of the table to which columns belong
+        method : str
+            value representing the comparision operator.
+            Admissible value:  ["lt","le","eq","ne","ge,"gt"]
+        value: str or number
+            Value to use to filter.
+        wherestr : str
+            SQL where clause to be used for filtering
+
+        Raises
+        ------
+        IdaDataBaseError
+            * The value for filtering is not a string or a number
+            * The filtering method is not suppoted
+        Notes
+        -----
+        This object is considered as private, and should be called internally
+        by IdaDataFrame instances.
+        """
+        # Sanity checks
+        if isinstance(columns, six.string_types):
+            columns = [columns]
+        if isinstance(value, six.string_types):
+            value = "'" + value + "'"
+        if not ((isinstance(value,six.string_types)|isinstance(value,Number))):
+            raise IdaDataBaseError("Value for Filterquery is expected to be a string or a number. Type %s"%str(type(value)))
+        dictmethod = {"lt": " < ", "le": " <= ",  "eq": " = ", "ne": " != ", "ge": " >= ", "gt": " > "}
+        if method not in dictmethod.keys():
+            raise IdaDataBaseError("The filtering method is not suppoted. Admissible values for method argument are %s"%list(dictmethod.keys()))
+
+        self.columns = columns     # TO DEPRECATE
+        self.tablename = tablename
+        self.method = method       # TO DEPRECATE
+        self.value = value         # TO DEPRECATE
+        self.wherestr = ("(\"" + ("\"" + str(dictmethod[method]) + str(value) + " AND \"").join(columns) +
+                        "\"" +  str(dictmethod[method]) + str(value) + ")")
+
+    # TO DEPRECATE
+    @property
+    def query(self):
+        """
+        Return an SQL query like "SELECT * FROM %s WHERE <WHERESTR>", where
+        <WHERESTR> is the value of the attribute "wherestr".
+        """
+        return "SELECT * FROM %s WHERE " + self.wherestr
+
+    def __and__(self, other):
+        """
+        Combine two FilterQuery instances with the operator "&" (AND)
+        For example : (idadf['sepal_length'] < 5) & (idadf[petal_width'] > 3)
+        """
+        self._combine_check(other)
+        newquery = deepcopy(self)
+        newquery.wherestr = "(%s AND %s)"%(self.wherestr,other.wherestr)
+        return newquery
+
+    def __or__(self, other):
+        """
+        Combine two FilterQuery instances with the operator "|" (OR)
+        For example : (idadf['sepal_length'] < 5) | (idadf[petal_width'] > 3)
+        """
+        self._combine_check(other)
+        newquery = deepcopy(self)
+        newquery.wherestr =  "(%s OR %s)"%(self.wherestr,other.wherestr)
+        return newquery
+
+    def __xor__(self, other):
+        """
+        Combine two FilterQuery instances with the operator "^" (XOR)
+        For example : (idadf['sepal_length'] < 5) ^ (idadf[petal_width'] > 3)
+        """
+        self._combine_check(other)
+        newquery = deepcopy(self)
+        newquery.wherestr = ("((NOT %s AND %s) OR (%s AND NOT %s))"%(self.wherestr,other.wherestr,self.wherestr,other.wherestr))
+        return newquery
+
+    def _combine_check(self, other):
+        """
+        Check if the name of two  FilterQuery is the same. Raise an
+        IdaDataBaseError in case it is different.
+        This function is used before performing logical operations between
+        FilterQuery instances.
+
+        Raises
+        ------
+        IdaDataBaseError
+        """
+        if self.tablename != other.tablename:
+            raise IdaDataBaseError("Combining filtering criterions from "+
+                                   "columns belongings to different tables "+
                                    "is not possible.")
```

### Comparing `nzpyida-0.2.2.6/nzpyida/frame.py` & `nzpyida-0.3.3/nzpyida/frame.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,2511 +1,2511 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-idaDataFrame
----------
-An efficient 2D container that looks like a panda's DataFrame and behave the
-same, the only difference is that it uses only a reference to a remote database
-instead of having the data loaded into memory
-
-Also similar to its R counterpart, data.frame, except providing automatic data
-alignment and a host of useful data manipulation methods having to do with the
-labeling information
-"""
-
-# Ensure Python 2 compatibility
-from __future__ import print_function
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import absolute_import
-from builtins import dict
-from builtins import zip
-from builtins import str
-from builtins import int
-from future import standard_library
-standard_library.install_aliases()
-
-import sys
-import os
-from copy import deepcopy
-import warnings
-from numbers import Number
-from collections import OrderedDict
-
-import numpy as np
-import pandas as pd
-from pandas.core.index import Index, RangeIndex
-
-from lazy import lazy
-import six
-
-import nzpyida
-import nzpyida.statistics
-import nzpyida.indexing
-import nzpyida.aggregation
-import nzpyida.filtering
-import nzpyida.utils
-
-from nzpyida.utils import timed, chunklist
-from nzpyida.internals import InternalState
-from nzpyida.exceptions import IdaDataFrameError
-from nzpyida.internals import idadf_state
-
-
-class IdaDataFrame(object):
-    """
-    An IdaDataFrame object is a reference to a table in a remote Db2 Warehouse
-    database. IDA stands for In-DataBase Analytics. IdaDataFrame copies the
-    Pandas interface for DataFrame objects to ensure intuitive interaction for
-    end-users.
-
-    Examples
-    --------
-    >>> idadb = IdaDataBase('BLUDB') # See documentation for IdaDataBase
-    >>> ida_iris = IdaDataFrame(idadb, 'IRIS')
-    >>> ida_iris.cov()
-                      sepal_length  sepal_width  petal_length  petal_width
-    sepal_length      0.685694    -0.042434      1.274315     0.516271
-    sepal_width      -0.042434     0.189979     -0.329656    -0.121639
-    petal_length      1.274315    -0.329656      3.116278     1.295609
-    petal_width       0.516271    -0.121639      1.295609     0.581006
-    """
-
-    # TODO: Check if everything is ok when selecting AND projecting with loc
-    # TODO: BUG: Filtering after selection/projection
-
-    def __init__(self, idadb, tablename, indexer = None):
-        """
-        Constructor for IdaDataFrame objects.
-
-        Parameters
-        ----------
-        idadb : IdaDataBase
-            IdaDataBase instance which contains the connection to be used.
-        tablename : str
-            Name of the table to be opened in the database.
-            It should contain only alphanumeric characters and underscores.
-            All lower case characters will be converted to upper case characters.
-        indexer : str, optional
-            Name of the column that should be used as an index. This is
-            optional. However, if no indexer is given, the order of rows issued
-            by the head and tail functions is not guaranteed. Also, several
-            in-database machine learning algorithms need an indexer as a
-            parameter to be executed.
-
-        Attributes
-        ----------
-        _idadb : IdaDataBase
-            IdaDataBase object parent of the current instance.
-        tablename : str
-            Name of the table self references.
-        name : str
-            Full name of the table self references, including schema.
-        schema : str
-            Name of the schema the table belongs to.
-        indexer : str
-            Name of the column used as an index. "None" if no indexer.
-        loc : str
-            Indexer that enables the selection and projection of IdaDataFrame
-            instances. For more information, see the loc class documentation.
-        internal_state : InternalState
-            Object used to internally store the state of the IdaDataFrame. It
-            also allows several non-destructive manipulation methods.
-        type : str
-            Type of the IdaDataFrame : “Table”, “View”, or “Unknown”.
-        dtypes : DataFrame
-            Data type in the database for each column.
-        index : pandas.core.index
-            Index containing the row names in this table.
-        columns : pandas.core.index
-            Index containing the columns names in this table.
-        axes : list
-            List containing columns and index attributes.
-        shape : Tuple
-            Number of rows and number of columns.
-
-        Notes
-        -----
-        Attributes "type", "dtypes", "index", "columns", "axes", and "shape"
-        are evaluated in a lazy way to avoid an overhead when creating an
-        IdaDataFrame. Sometimes the index may be too big to be downloaded.
-
-        Examples
-        --------
-        >>> idadb = IdaDataBase('BLUDB')
-        >>> ida_iris = IdaDataFrame(idadb, "IRIS")
-        """
-        #TODO: Implement equality comparision between two IdaDataFrames
-
-        if not idadb.__class__.__name__ == "IdaDataBase":
-            idadb_class = idadb.__class__.__name__
-            raise TypeError("Argument 'idadb' is of type %s, expected : IdaDataBase"%idadb_class)
-        tablename = nzpyida.utils.check_tablename(tablename)
-
-        #idadb._reset_attributes("cache_show_tables")
-
-        # TODO: Test what kind of error append when a table in use and cached
-        # is suddently deleted
-
-        if idadb.exists_table_or_view(tablename) is False:
-            # Try again after refreshing the cache
-            idadb._reset_attributes("cache_show_tables")
-            # Refresh the show table cache in parent IdaDataBase, because a table
-            # could have been created by other mean / user and we have to make sure
-            # the lookup is done and is actual.
-            if idadb.exists_table_or_view(tablename) is False:
-                raise NameError("Table %s does not exist in the database %s."
-                                %(tablename, idadb.data_source_name))
-
-
-        self._idadb = idadb
-        self._indexer = indexer
-
-        # Initialise indexer object
-        self.loc = nzpyida.indexing.Loc(self)
-
-        if "." in tablename:
-            self.schema = tablename.split('.')[0]
-            #self.name = tablename.split('.')[-1]
-            self._name = tablename
-            self.tablename = tablename.split('.')[-1]
-        else:
-            self.schema = idadb.current_schema
-            self._name = idadb.current_schema + '.' + tablename
-            self.tablename = tablename
-
-        # self._name is the original name, this is a "final" variable
-
-        # Push a reference to itself in its parent IdaDataBase
-        self._idadb._idadfs.append(self)
-        # TODO : self.size
-
-        # A cache for unique value of each column
-        self._unique = dict()
-
-###############################################################################
-### Attributes & Metadata computation
-###############################################################################
-
-    @lazy
-    def internal_state(self):
-        """
-         InternalState instances manage the state of an IdaDataFrame instance
-         and allow several non-destructive data manipulation methods, such as
-         the selection, projection, filtering, and aggregation of columns.
-
-        """
-        return InternalState(self)
-
-
-    @property
-    @idadf_state
-    def name(self):
-        return self.internal_state.current_state
-
-
-    @property
-    def indexer(self):
-        """
-        The indexer attribute refers to the name of a column that should be
-        used to index the table. This makes sense because Db2 Warehouse is a
-        column-based database, so row IDs do not make sense and are not
-        deterministic. As a consequence, the only way to address a particular
-        row is to refer to it by its index. If no indexer is provided, ibmdbpy
-        still works but a correct row order is not guaranteed as far as the
-        dataset is not sorted. Also, note that the indexer column is not taken
-        into account in data mining algorithms.
-        """
-        if hasattr(self, "_indexer"):
-            return self._indexer
-        else:
-            None    
-
-        
-    @indexer.setter
-    def indexer(self, value):
-        """
-        Basic checks for indexer :
-        * The column exists in the table.
-        * All values are unique.
-        """
-        if value is None:
-            return
-
-        if value not in self.columns:
-            raise IdaDataFrameError("'%s' cannot be used as indexer "%value +
-                                    " because this is not a column in '%s'"%self._name)
-
-        del self.columns
-        #count = self[value].count_distinct() ## TODO: TO FIX, should return directly just a number
-        count = self.levels(value)
-        if count < self.shape[0]:
-            raise IdaDataFrameError("'%s' cannot be used as indexer "%value +
-                                    " because it contains non unique values.")
-        self._indexer = value
-
-
-    @lazy
-    def type(self):
-        """
-        Type of self: 'Table', 'View'  or 'Unknown'.
-
-        Returns
-        -------
-        str
-            idaDataFrame type.
-
-        Examples
-        --------
-        >>> ida_iris.type
-        'Table'
-        """
-        return self._get_type()
-
-    @lazy
-    @idadf_state(force = True)
-    def dtypes(self):
-        """
-        Data type in database for each column in self.
-
-        Returns
-        -------
-        DataFrame
-            In-Database type for each columns.
-
-        Examples
-        --------
-        >>> ida_iris.dtypes
-                     TYPENAME
-        sepal_length   DOUBLE
-        sepal_width    DOUBLE
-        petal_length   DOUBLE
-        petal_width    DOUBLE
-        species       VARCHAR
-        """
-        #import pdb ; pdb.set_trace()
-        return self._get_columns_dtypes()
-
-    @lazy
-    @idadf_state
-    # to deprecate
-    def index(self):
-        """
-        Index containing the row names in self.
-
-        Returns
-        -------
-        Index
-
-        Examples
-        --------
-        >>> ida_iris.index
-        Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,
-            ...
-            140, 141, 142, 143, 144, 145, 146, 147, 148, 149],
-           dtype='int64', length=150)
-
-        Notes
-        -----
-        Because indexes in a database can be only numeric, it is not that
-        interesting for an IdaDataFrame but can still be useful sometimes. The
-        function can break if the table is too large. Ask for the user’s
-        approval before downloading an index which has more than 10000 values.
-        """
-        return self._get_index()
-
-    @lazy
-    def columns(self):
-        """
-        Index containing the column names in self.
-
-        Returns
-        -------
-        Index
-
-        Examples
-        --------
-        >>> ida_iris.columns
-        Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',
-        'species'],
-        dtype='object')
-        """
-        if hasattr(self, "internal_state"):
-            self.internal_state._create_view()
-            cols = self._get_columns()
-            self.internal_state._delete_view()
-            return cols
-        else:
-            return self._get_columns()
-
-    @lazy
-    @idadf_state
-    # to deprecate (no index)
-    def axes(self):
-        """
-        List containing IdaDataFrame.columns and IdaDataFrame.index attributes.
-
-        Returns
-        -------
-        list
-            List containing two indexes (indexes and column attributes).
-
-        Examples
-        --------
-        >>> ida_iris.axes
-        [Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,
-             ...
-             140, 141, 142, 143, 144, 145, 146, 147, 148, 149],
-            dtype='int64', length=150),
-        Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',
-        'species'],
-        dtype='object')]
-        """
-        return [self.index, self.columns]
-
-    @lazy
-    @idadf_state
-    def shape(self):
-        """
-        Tuple containing number of rows and number of columns.
-
-        Returns
-        -------
-        tuple
-
-        Examples
-        --------
-        >>> ida_iris.shape
-        (150, 5)
-        """
-        return self._get_shape()
-
-    @property
-    @idadf_state
-    def empty(self):
-        """
-        Boolean that is True if the table is empty (no rows).
-
-        Returns
-        -------
-        Boolean
-        """
-        if self.shape[0] == 0:
-            return True
-        else:
-            return False
-
-    def __len__(self):
-        """
-        Number of records.
-
-        Returns
-        -------
-        int
-
-        Examples
-        --------
-        >>> len(idadf)
-        150
-        """
-        return self.shape[0]
-
-    def __iter__(self):
-        """
-        Iterate over columns.
-        """
-        return iter(self.columns)
-
-    def __getitem__(self, item):
-        """
-        Enable label based projection (selection of columns) in IdaDataFrames.
-
-        Enable slice based selection of rows in IdaDataFrames.
-
-        Enable row filtering.
-
-        The syntax is similar to Pandas.
-
-        Examples
-        --------
-
-        >>> idadf['col1'] # return an IdaSeries
-
-        >>> idadf[['col1']] # return an IdaDataFrame with one column
-
-        >>> idadf[['col1', 'col2', 'col3']] # return an IdaDataFrame with 3 columns
-
-        >>> idadf[0:9] # Select the 10 first rows
-
-        >>> idadf[idadf['col1'] = "test"]
-        # select of rows for which attribute col1 is equal to "test"
-
-        Notes
-        -----
-        The row order is not guaranteed if no indexer is given and the dataset
-        is not sorted
-        """
-
-        if isinstance(item, nzpyida.filtering.FilterQuery):
-            newidadf = self._clone()
-            newidadf.internal_state.update(item)
-            newidadf._reset_attributes(["shape"])
-        else:
-            if isinstance(item, slice):
-                return self.loc[item]
-            if not (isinstance(item,six.string_types)|isinstance(item, list)):
-                raise KeyError(item)
-            if isinstance(item, six.string_types):
-                # Case when only one column was selected
-                if item not in self.columns:
-                    raise KeyError(item)
-                newidaseries = self._clone_as_serie(item)
-
-                # Form the new columndict
-                for column in list(newidaseries.internal_state.columndict):
-                    if column != item:
-                        del newidaseries.internal_state.columndict[column]
-                newColumndict = newidaseries.internal_state.columndict
-
-                # Erase attributes
-                newidaseries._reset_attributes(["columns", "shape", "dtypes"])
-                # Set columns and columndict attributes
-                newidaseries.internal_state.columns = ["\"%s\""%col for col in item]
-                newidaseries.internal_state.columndict = newColumndict
-                # Update, i.e. appends an entry to internal_state._cumulative
-                newidaseries.internal_state.update()
-
-                # Performance improvement
-                # avoid, caused wrong dtypes for the result
-                newidaseries.dtypes = self.dtypes.loc[[item]]
-
-                return newidaseries
-
-            # Case of multiple columns
-            not_a_column = [x for x in item if x not in self.columns]
-            if not_a_column:
-                raise KeyError("%s"%not_a_column)
-
-            newidadf = self._clone()
-
-            # Form the new columndict
-            newColumndict = OrderedDict()
-            for col in item:
-                # Column name as key, its definition as value
-                newColumndict[col] = self.internal_state.columndict[col]
-
-            # Erase attributes
-            newidadf._reset_attributes(["columns", "shape", "dtypes"])
-            # Set columns and columndict attributes
-            newidadf.internal_state.columns = ["\"%s\""%col for col in item]
-            newidadf.internal_state.columndict = newColumndict
-            # Update, i.e. appends an entry to internal_state._cumulative
-            newidadf.internal_state.update()
-
-            # Performance improvement
-            # avoid, caused wrong dtypes for the result
-            newidadf.dtypes = self.dtypes.loc[item]
-
-        return newidadf
-
-    def __setitem__(self, key, item):
-        """
-        Enable the creation and aggregation of columns.
-
-        Examples
-        --------
-
-        >>> idadf['new'] = idadf['sepal_length'] * idadf['sepalwidth']
-        # select a new column as the product of two existing columns
-
-        >>> idadf['sepal_length'] = idadf['sepal_length'] / idadf['sepal_length'].mean()
-        # modify an existing column
-        """
-        if not (isinstance(item, IdaDataFrame)):
-            raise TypeError("Modifying columns is supported only using "+
-                            "IdaDataFrames.")
-        if isinstance(key, six.string_types):
-            key = [key]
-        if len(key) != len(item.columns):
-                raise ValueError("Wrong number of items passed %s, placement implies %s"%(len(item.columns),len(key)))
-
-        #form the new columndict
-        for newname, oldname in zip(key, item.columns):
-            self.internal_state.columndict[newname] = item.internal_state.columndict[oldname]
-        newColumndict = self.internal_state.columndict
-
-        #erase attributes
-        self._reset_attributes(["columns", "shape", "dtypes"])
-        #set columns and columndict attributes
-        self.internal_state.columndict = newColumndict
-        self.internal_state.columns = ["\"%s\""%col for col in newColumndict.keys()]
-        #update, i.e. appends an entry to internal_state._cumulative
-        self.internal_state.update()
-
-        # Flush the "unique" cache
-        for column in key:
-            if column in self._unique:
-                del self._unique[column]
-
-    def __delitem__(self, item):
-        """
-        Enable non-destructive deletion of columns using a Pandas style syntax.
-        This happens inplace, which means that the current IdaDataFrame is
-        modified.
-
-        Examples
-        --------
-        >>> idadf = IdaDataFrame(idadb, "IRIS")
-        >>> idadf.columns
-        Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'], dtype='object')
-        >>> del idadf['sepal_length']
-        >>> idadf.columns
-        Index(['sepal_width', 'petal_length', 'petal_width', 'species'], dtype='object')
-        """
-        if not (isinstance(item,six.string_types)):
-            raise TypeError
-        if item not in self.columns:
-            raise KeyError(item)
-
-        # Flush the "unique" cache
-        if item in self._unique:
-            del self._unique[item]
-
-        self._idadb.delete_column(self, item, destructive = False)
-        return
-
-    def __enter__(self):
-        """
-        Allow the object to be used with a "with" statement
-        """
-        return self
-
-    def __exit__(self):
-        """
-        Allow the object to be used with a "with" statement. Make sure that
-        allow possible views related to the IdaDataFrame with be deleted when
-        the object goes out of scope
-        """
-        while self.internal_state.viewstack:
-            try :
-                view = self.internal_state.viewstack.pop()
-                # Just a last check to make sure not to drop user's db
-                if view != self.tablename:
-                    drop = "DROP VIEW \"%s\"" %view
-                    self._prepare_and_execute(drop, autocommit = True)
-            except: pass
-
-    #We decided not to allow columns access idadf.columnname like this for now.
-    #We could decide to allow it but for this we may have to switch all
-    #existing attributes to private ("_" as first character) so to avoid
-    #conflicts between attributes and columns because for example IdaDataFrame
-    #has an attribute "name" -> if a column is labelled "name" this will make
-    #it unavailable. Pandas do also poorly manage this issue, for example :
-    #DataFrame attribute "size".
-
-    #May be implemented in the future
-
-    #def __getattr__(self, name):
-    #    """
-    #    After regular attribute access, try look up the name
-    #    This allows simpler access to columns for interactive use.
-    #    """
-        # Note: obj.x will always call obj.__getattribute__('x') prior to
-        # calling obj.__getattr__('x').
-        #if hasattr(self, name):
-        #    return object.__getattribute__(self,name)
-        #else:
-    #    if name not in self.__dict__["columns"]:
-            #return self[name]
-    #        return object.__getattribute__(self, name)
-    #    raise AttributeError("'%s' object has no attribute '%s'" %
-    #                         (type(self).__name__, name))
-
-    #def __setattr__(self, name, value):
-    #    if name not in self.__dict__["columns"]:
-    #        self.__dict__[name] = value
-    #    else:
-    #        raise ValueError("It is not allowed to set the value of a column in an IdaDataFrame.")
-
-    def __lt__(self, value):
-        """
-        ibmdbpy.filtering.FilterQuery object when comparing self using "<".
-        """
-        return nzpyida.filtering.FilterQuery(self.columns, self._name, "lt", value)
-
-    def __le__(self, value):
-        """
-        ibmdbpy.filtering.FilterQuery object when comparing self using "<=".
-        """
-        return nzpyida.filtering.FilterQuery(self.columns, self._name, "le", value)
-
-    def __eq__(self, value):
-        """
-        ibmdbpy.filtering.FilterQuery object when comparing self using "==".
-        """
-        return nzpyida.filtering.FilterQuery(self.columns, self._name, "eq", value)
-
-    def __ne__(self, value):
-        """
-        ibmdbpy.filtering.FilterQuery object when comparing self using "!=".
-        """
-        return nzpyida.filtering.FilterQuery(self.columns, self._name, "ne", value)
-
-    def __ge__(self, value):
-        """
-        ibmdbpy.filtering.FilterQuery object when comparing self using ">=".
-        """
-        return nzpyida.filtering.FilterQuery(self.columns, self._name, "ge", value)
-
-    def __gt__(self, value):
-        """
-        ibmdbpy.filtering.FilterQuery object when comparing self using ">".
-        """
-        return nzpyida.filtering.FilterQuery(self.columns, self._name, "gt", value)
-
-    ################ Arithmetic operations
-
-    def __add__(self, other):
-        """
-        Perform an addition between self and another IdaDataFrame or number.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] + 3
-
-        Notes
-        -----
-        Arithmetic operations only make sense if self contains only numeric columns.
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(self, "add", other)
-
-    def __radd__(self, other):
-        """
-        Enable the reflexivity of the addtion operation.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] + 3
-
-        >>> ida = 3 + idadf['sepal_length']
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(other, "add", self, swap = True)
-
-    def __div__(self, other):
-        """
-        Perform a division between self and another IdaDataFrame or number.
-
-        When __future__.division is not in effect.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] / 3
-
-        Notes
-        -----
-        Arithmetic operations only make sense if self contains only numeric columns.
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(self, "div", other)
-
-    def __rdiv__(self, other):
-        """
-        Enable the reflexivity of the division operation.
-
-        When __future__.division is not in effect.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] / 3
-
-        >>> ida = 3 / idadf['sepal_length']
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(other, "div", self, swap = True)
-
-    def __truediv__(self, other):
-        """
-        Perform a division between self and another IdaDataFrame or number.
-
-        When __future__.division is in effect.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] / 3
-
-        Notes
-        -----
-        Arithmetic operations only make sense if self contains only numeric columns.
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(self, "div", other)
-
-    def __rtruediv__(self, other):
-        """
-        Enable the reflexivity of the division operation.
-
-        When __future__.division is in effect.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] / 3
-
-        >>> ida = 3 / idadf['sepal_length']
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(other, "div", self, swap = True)
-
-    def __floordiv__(self,other):
-        """
-        Perform an integer division between self and another IdaDataFrame or number.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] // 3
-
-        Notes
-        -----
-        Arithmetic operations only make sense if self contains only numeric columns.
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(self, "floordiv", other)
-
-    def __rfloordiv__(self, other):
-        """
-        Enable the reflexivity of the integer division operation.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] // 3
-
-        >>> ida = 3 // idadf['sepal_length']
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(other, "floordiv", self, swap = True)
-
-    def __mod__(self,other):
-        """
-        Perform a modulo operation between self and another IdaDataFrame or number.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] % 3
-
-        Notes
-        -----
-        Arithmetic operations make sense if self has only numeric columns.
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(self, "mod", other)
-
-    def __rmod__(self, other):
-        """
-        Enable the reflexivity of the modulo operation.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] % 3
-
-        >>> ida = 3 % idadf['sepal_length']
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(other, "mod", self, swap = True)
-
-    def __mul__(self,other):
-        """
-        Perform a multiplication between self and another IdaDataFrame or number.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] * 3
-
-        Notes
-        -----
-        Arithmetic operations only make sense if self contains only numeric columns.
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(self, "mul", other)
-
-    def __rmul__(self, other):
-        """
-        Enable the reflexivity of the multiplication operation.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] % 3
-
-        >>> ida = 3 % idadf['sepal_length']
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(other, "mul", self, swap = True)
-
-    def __neg__(self):
-        """
-        Calculate the absolute negative of all columns in self.
-
-        Notes
-        -----
-        Arithmetic operations only make sense if self contains only numeric columns.
-        """
-        other = None
-        return nzpyida.aggregation.aggregate_idadf(self, "neg", other)
-
-    def __rpos__(self,other):
-        """
-        Calculate the absolute positive. No operation required.
-        """
-        return self
-
-    def __pow__(self,other):
-        """
-        Perform a power operation between self and another IdaDataFrame or number.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] ** 3
-
-        Notes
-        -----
-        Arithmetic operations only make sense if self contains only numeric columns.
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(self, "pow", other)
-
-    def __rpow__(self, other):
-        """
-        Enable the reflexivity of the power operation.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] ** 3
-
-        >>> ida = 3 ** idadf['sepal_length']
-
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(other, "pow", self, swap = True)
-
-    def __sub__(self,other):
-        """
-        Perform a substraction between self and another IdaDataFrame or number.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] - 3
-
-        Notes
-        -----
-        Arithmetic operations only make sense if self contains only numeric columns.
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(self, "sub", other)
-
-    def __rsub__(self, other):
-        """
-        Enable the reflexivity of the substraction operation.
-
-        Examples
-        --------
-        >>> ida = idadf['sepal_length'] - 3
-
-        >>> ida = 3 - idadf['sepal_length']
-        """
-        self._combine_check(other)
-        return nzpyida.aggregation.aggregate_idadf(other, "sub", self, swap = True)
-
-    #def __truediv__(self,value): ########
-    #    pass
-
-    #def __concat__(self,value): ####
-    #    pass
-
-    #def __contains__(self,value):
-    #    pass
-
-    # TODO: Do the inplace versions (with "i" in front)
-
-###############################################################################
-### Database Features
-###############################################################################
-
-    ##############################
-    ## Delegation from IdaDataBase
-    ##############################
-
-    def exists(self):
-        """
-        Convenience function delegated from IdaDataBase.
-
-        Check if the data still exists in the database.
-        """
-        return self._idadb.exists_table_or_view(self._name)
-
-    def is_view(self):
-        """
-        Convenience function delegated from IdaDataBase.
-
-        Check if the IdaDataFrame corresponds to a view in the database.
-        """
-        if self.type == 'View':
-            return True
-        else:
-            return False
-
-    def is_table(self):
-        """
-        Convenience function delegated from IdaDataBase.
-
-        Check if the IdaDataFrame corresponds to a table in the database.
-        """
-        if self.type == 'Table':
-            return True
-        else:
-            return False
-
-    def get_primary_key(self):
-        # TODO: What happen if the primary key is composed of several columns ?
-        """
-        Get the name of the primary key of self, if there is one. Otherwise,
-        this function returns 0. This function may be deprecated in future
-        versions because it is not very useful.
-        """
-        name = self.internal_state.current_state
-        pk = NULL
-        # on Netezza primary keys are not supported
-        if not self._idadb._is_netezza_system():
-            pk = self.ida_query("SELECT NAME FROM SYSIBM.SYSCOLUMNS WHERE TBNAME = '" +
-                                name + "' AND KEYSEQ > 0 ORDER BY KEYSEQ ASC", first_row_only = True)
-        if pk:
-            return pk[0]
-        else:
-            return False
-
-    # Should we maybe allow this only in IdaDataBase object ?
-    #@timed
-    def ida_query(self, query, silent = False, first_row_only = False, autocommit = False):
-        """
-        Convenience function delegated from IdaDataBase.
-
-        Prepare, execute and format the result of a query in a data frame or in
-        a tuple. See the IdaDataBase.ida_query documentation.
-        """
-        return self._idadb.ida_query(query, silent, first_row_only, autocommit)
-
-    def ida_scalar_query(self, query, silent = False, autocommit = False):
-        """
-        Convenience function delegated from IdaDataBase.
-
-        Prepare and execute a query and return only the first element as a
-        string. See the IdaDataBase.ida_scalar_query documentation.
-
-        """
-        return self._idadb.ida_scalar_query(query, silent, autocommit)
-
-
-
-###############################################################################
-### Data Exploration
-###############################################################################
-
-    def print(self):
-        print(self.internal_state.get_state())
-
-    @idadf_state
-    def head(self, nrow=5, sort=True):
-        """
-        Print the n first rows of the instance, n is set to 5 by default.
-
-        Parameters
-        ----------
-        nrow : int > 0
-            Number of rows to be included in the result.
-
-        sort: default is True
-            If set to True and no indexer is set the data will be
-            sorted by the first numeric column or if no numeric column
-            is available by the first column of the dataframe.
-            If set to False and no indexer is set the row order is not
-            guaranteed and can vary with each execution. For big tables
-            this option might save query processing time.
-
-
-        Returns
-        -------
-        DataFrame or Series
-            The index of the corresponding row number and the columns are all
-            columns of self. If the IdaDataFrame has only one column, it
-            returns a Series.
-
-
-        Examples
-        --------
-        >>> ida_iris.head()
-           sepal_length  sepal_width  petal_length  petal_width species
-        0           5.1          3.5           1.4          0.2  setosa
-        1           4.9          3.0           1.4          0.2  setosa
-        2           4.7          3.2           1.3          0.2  setosa
-        3           4.6          3.1           1.5          0.2  setosa
-        4           5.0          3.6           1.4          0.2  setosa
-        """
-        if (nrow < 1) | (not isinstance(nrow, int)):
-            raise ValueError("Parameter nrow should be an int greater than 0.")
-        else:
-            name = self.internal_state.current_state
-            order = self.internal_state.get_order()
-            if not " ORDER BY " in self.internal_state.get_state():
-                if (self.indexer is not None)&(self.indexer in self.columns):
-                    order = " ORDER BY \"" + str(self.indexer) + "\" ASC"
-                elif self.indexer is None:
-                    if sort:
-                        column = self.columns[0]
-                        if self._get_numerical_columns():
-                            column = self._get_numerical_columns()[0]
-                        order = " ORDER BY \"" + column + "\" ASC"
-                    else:
-                        order = ''
-            data = self.ida_query("SELECT * FROM %s %s LIMIT %s "%(name, order, nrow))
-
-            if data.shape[0] != 0:
-                # otherwise column sort order is reverted
-                if not 'SELECT ' in name:
-                    columns = self.columns
-                    data.columns = columns
-#                data = ibmdbpy.utils._convert_dtypes(self, data)
-                if isinstance(self, nzpyida.IdaSeries):
-                    data = pd.Series(data)
-            return data
-
-    # TODO : There is a warning in anaconda when there are missing values -> why ?
-    @idadf_state
-    def tail(self, nrow=5, sort=True):
-        """
-        Print the n last rows of the instance, n is set to 5 by default.
-
-        Parameters
-        ----------
-        nrow : int > 0
-            The number of rows to be included in the result.
-
-        sort: default is True
-            If set to True and no indexer is set the data will be
-            sorted by the first numeric column or if no numeric column
-            is available by the first column of the dataframe.
-            If set to False and no indexer is set the row order is not
-            guaranteed and can vary with each execution. For big tables
-            this option might save query processing time.
-
-        Returns
-        -------
-        DataFrame
-            The index of the corresponding row number and the columns are all
-            columns of self.
-
-
-        Examples
-        --------
-        >>> ida_iris.tail()
-             sepal_length  sepal_width  petal_length  petal_width    species
-        145           6.7          3.0           5.2          2.3  virginica
-        146           6.3          2.5           5.0          1.9  virginica
-        147           6.5          3.0           5.2          2.0  virginica
-        148           6.2          3.4           5.4          2.3  virginica
-        149           5.9          3.0           5.1          1.8  virginica
-        """
-        if (nrow < 1) | (not isinstance(nrow, int)):
-            raise ValueError("Parameter nrow should be an int greater than 0.")
-        else:
-            column_string = '\"' + '\", \"'.join(self.columns) + '\"'
-
-            name = self.internal_state.current_state
-
-            if " ORDER BY " in self.internal_state.get_state():
-                query = "SELECT * FROM %s LIMIT %s "%(name, nrow)
-                data = self.ida_query(query)
-                data.columns = self.columns
-                data.set_index(data[self.indexer], inplace=True)
-            else:
-                if self._idadb._is_netezza_system():
-                    order = "ORDER BY NULL"
-                else:
-                    order = ""
-                if self.indexer:
-                    sortkey = str(self.indexer)
-                    order = "ORDER BY \"" + sortkey + "\""
-                else:
-                    if sort:
-                        sortkey = self.columns[0]
-                        if self._get_numerical_columns():
-                            sortkey = self._get_numerical_columns()[0]
-                        order = "ORDER BY \"" + sortkey + "\""
-                query = ("SELECT * FROM (SELECT * FROM (SELECT " + column_string +
-                         ", ((ROW_NUMBER() OVER(" + order +
-                         "))-1) AS ROWNUMBER FROM " + name +
-                         ") AS TEMP1 ORDER BY ROWNUMBER DESC LIMIT " + str(nrow) +
-                         " ) AS TEMP2 ORDER BY ROWNUMBER ASC")
-                data = self.ida_query(query)
-                data.set_index(data.columns[-1], inplace=True)  # Set the index
-                data.columns = self.columns
-
-            # del data.index.name
-            #            data = ibmdbpy.utils._convert_dtypes(self, data)
-            if isinstance(self, nzpyida.IdaSeries):
-                data = pd.Series(data[data.columns[0]])
-            return data
-
-    @idadf_state
-    def pivot_table(self, values=None, columns=None, max_entries=1000,
-                    sort=None, factor_threshold=20, interactive=False,
-                    aggfunc='count'):
-        """
-        Compute an aggregation function over all rows of each column that is
-        specified as a value on the dataset. The result grouped by the columns
-        defined in “columns”.
-
-        Parameters
-        ----------
-        values: str or list or str optional
-            List of columns on which “aggfunc” is computed.
-        columns: str or list or str optional
-            List of columns that is used as an index and by which the
-            dataframe is grouped.
-        max_entries: int, default=1000
-            The maximum number of cells to be part of the output. By default,
-            set to 1000.
-        sort: str, optional
-            Admissible values are: “alpha” and “factors”.
-                * If “alpha”, the index of the output is sorted according to the alphabetical order.
-                * If “factors”, the index of the output will be sorted according to increasing number of the distinct values.
-
-            By default, the index will be sorted in the same order that is specified in “columns” argument.
-
-        factor_threshold: int, default: 20
-            Number of distinct values above which a categorical column should
-            not be considered categorical anymore and under which a numerical
-            column column should not be considered numerical anymore.
-        interactive: bool
-            If True, the user is asked if he wants to display the output, given
-            its size.
-        aggfunc: str
-            Aggregation function to be computed on each column specified in the
-            argument “values”. Admissible values are: “count”, “sum”, “avg”.
-            This entry is not case-sensitive.
-
-        Returns
-        -------
-        Pandas Series with Multi-index (columns)
-
-        Examples
-        --------
-        >>> val = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
-        >>> ida_iris.pivot_table(values= val, aggfunc="avg")
-                      species
-        sepal_length  setosa        5.006
-                      versicolor    5.936
-                      virginica     6.588
-        sepal_width   setosa        3.428
-                      versicolor    2.770
-                      virginica     2.974
-        petal_length  setosa        1.462
-                      versicolor    4.260
-                      virginica     5.552
-        petal_width   setosa        0.246
-                      versicolor    1.326
-                      virginica     2.026
-        dtype: float64
-        """
-        # TODO : Support index
-        from nzpyida.statistics import pivot_table
-        return pivot_table(idadf=self, values=values, columns=columns,
-                           max_entries=max_entries, sort=sort,
-                           factor_threshold=factor_threshold,
-                           interactive=interactive, aggfunc=aggfunc)
-
-    @idadf_state
-    def groupby(self, by):
-        # TODO: create an IdadataFrame groupby
-        raise NotImplementedError()
-
-    @idadf_state
-    def merge(self):
-        raise NotImplementedError()
-
-    @idadf_state
-    def concat(self):
-        raise NotImplementedError()
-
-    @idadf_state
-    def join(self):
-        raise NotImplementedError()
-
-    # TODO : implement NULL FIRST option
-    @idadf_state
-    def sort(self, columns=None, axis=0, ascending=True, inplace=False):
-        """
-        Sort the IdaDataFrame row wise or column wise.
-
-        Parameters
-        ----------
-        columns : str or list of str
-            Columns that should be used to sort the rows in the IdaDataFrame.
-            If columns is set to None and axis to 0, then the IdaDataFrame
-            columns are sorted in lexicographical order.
-        axis : int (0/1)
-             Axis that is sorted. 0 for sorting row wise, 1 for sorting column
-             wise.
-        ascending : bool, default: True
-            Sorting order, True : ascending, False : descending
-        inplace : bool, default: False
-            The current object is modified or creates a modified copy. If
-            False, the function creates a modified copy of the current
-            dataframe. If True,  the function modifies the current dataframe.
-
-        Raises
-        ------
-        ValueError
-            * When sorting by column (column not None), the axis value must be 0 (rows).
-            * A column does not belong to self.
-            * The axis argument has a value other than 0 or 1.
-
-        Notes
-        -----
-        If columns is set to None and axis to 0, this undoes all sorting the
-        IdaDataFrame and returns the original sorting in the Db2 Warehouse
-        database.
-
-        No actual changes are made in Db2 Warehouse, only the querying changes.
-        Everything is registered in an InternalState object. Changes can be
-        observed by using  head and tail function.
-        """
-        if isinstance(columns, six.string_types):
-            columns = [columns]
-
-        # Sanitiy check
-        if columns:
-            if axis != 0:
-                raise ValueError("When sorting by column, axis must be 0 (rows)")
-            for column in columns:
-                if column not in self.columns:
-                    raise ValueError("Column "+column+" is not in "+self._name)
-        if axis not in [0,1]:
-            raise ValueError("Allowed values for axis is 0 (rows) or 1 (columns)")
-
-        if inplace:
-            idadf = self
-        else:
-            idadf = self._clone()
-
-        if axis:
-            # Sort the columns in ascending or descending lexicographic order
-            idadf.internal_state.columndict = OrderedDict(sorted(idadf.internal_state.columndict.items(), reverse = not ascending))
-            idadf.internal_state.update()
-        else:
-            if columns:
-                idadf.internal_state.set_order(columns, ascending)
-                idadf.internal_state.update()
-            else:
-                if isinstance(idadf, nzpyida.IdaSeries):
-                    idadf.internal_state.set_order([idadf.column], ascending)
-                    idadf.internal_state.update()
-                else:
-                    idadf.internal_state.reset_order()
-
-        if not inplace:
-            return idadf
-
-    @idadf_state
-    def levels(self, columns = None):
-        # TODO: Test, doc, name?
-        """
-        Return the numbers of distinct values
-        """
-        if columns is not None:
-            if isinstance(columns, six.string_types):
-                columns = [columns]
-            for column in columns:
-                message = ''
-                if column not in self.columns:
-                    message += "Column %s does not belong to current idadataframe. \n"%column
-                if message:
-                    raise ValueError(message)
-        else:
-            columns = self.columns
-
-        agglist = []
-        for column in columns:
-            agglist.append("COUNT(DISTINCT \"%s\")"%column)
-
-        aggstr = ",".join(agglist)
-
-        query = "SELECT %s FROM %s"%(aggstr, self.name)
-        levels_tuple = self.ida_query(query, first_row_only = True)
-
-        if len(levels_tuple) == 1:
-            return levels_tuple[0]
-
-        result = pd.Series(levels_tuple)
-        result.index = columns
-        return result
-
-
-    #@timed
-    @idadf_state
-    def count_groupby(self, columns = None, count_only = False, having = None):
-        """
-        Count the occurence of the values of a column or group of columns
-        """
-        # TODO: Document, test
-        if columns is not None:
-            if isinstance(columns, six.string_types):
-                columns = [columns]
-            for column in columns:
-                message = ''
-                if column not in self.columns:
-                    message += "Column %s does not belong to current idadataframe. \n"
-                if message:
-                    raise ValueError(message)
-        else:
-            columns = list(self.columns)
-        if having:
-            if not isinstance(having, Number):
-                raise TypeError("having argument should be a number")
-
-        select_columnstr = "\"" + "\",\"".join(columns) + "\", COUNT(*)"
-        if count_only:
-            select_columnstr = "COUNT(*)"
-        groupby_columnstr = "\"" + "\",\"".join(columns) + "\""
-        if having:
-            groupby_columnstr = groupby_columnstr + " HAVING count >= %s"%having
-        data = self.ida_query("SELECT %s AS count FROM %s GROUP BY %s"%(select_columnstr,self.name,groupby_columnstr))
-
-        data.columns = columns + ["count"]
-        return data
-
-    def mean_freq_of_instance(self, columns = None):
-        """
-        Return the average occurence of the values of a column or group of columns
-        """
-        # TODO: Document, test
-        if columns is not None:
-            if isinstance(columns, six.string_types):
-                columns = [columns]
-            for column in columns:
-                message = ''
-                if column not in self.columns:
-                    message += "Column %s does not belong to current idadataframe. \n"
-                if message:
-                    raise ValueError(message)
-        else:
-            columns = list(self.columns)
-
-        select_columnstr = "COUNT(*)"
-        groupby_columnstr = "\"" + "\",\"".join(columns) + "\""
-
-        subquery = "SELECT %s AS count FROM %s GROUP BY %s"%(select_columnstr,self.name,groupby_columnstr)
-        data = self.ida_scalar_query("SELECT AVG(DISTINCT count) FROM (%s)"%subquery)
-
-        return int(data)
-
-#    @timed
-    @idadf_state
-    def unique(self, column):
-        """
-        Return the unique values of a column
-        """
-        # TODO: Document, test
-        if column in self._unique:
-            return self._unique[column]
-
-        #name = self.internal_state.current_state
-
-        if not isinstance(column, six.string_types):
-            raise TypeError("column argument not of string type")
-
-        if column not in self.columns:
-            # idadf.name somewhat false in case of modification
-            raise ValueError("Undefined column \"%s\" in table %s"%(column, self._name))
-
-        result = self.ida_query("SELECT DISTINCT \"%s\" FROM %s"%(column, self.name))
-
-        self._unique[column] = result
-        return result
-
-    # TODO: to implement
-    @timed
-    @idadf_state
-    def info(self, buf=None):
-        """Some information about current IdaDataFrame. NOTIMPLEMENTED"""
-        # There is a lot more
-        #from pandas.core.format import _put_lines
-        raise IdaDataFrameError("NOT IMPLEMENTED")
-
-        if buf is None:  # pragma: no cover
-            buf = sys.stdout
-
-        lines = []
-
-        lines.append(str(type(self)))
-        lines.append(self.index.summary())
-
-        if len(self.columns) == 0:
-            lines.append('Empty %s' % type(self).__name__)
-            _put_lines(buf, lines)
-            return
-
-###############################################################################
-### Descriptive statistics
-###############################################################################
-
-    # TODO: to implement for categorical attributes
-    @timed
-    @idadf_state
-    def describe(self, percentiles=[0.25, 0.50, 0.75]):
-        """
-        A basic statistical summary about current IdaDataFrame. If at least one
-        numerical column exists, the summary includes:
-
-            * The count of non-missing values for each numerical column.
-            * The mean for each numerical column.
-            * The standart deviation for each numerical column.
-            * The minimum and maximum for each numerical column.
-            * A list of percentiles set by the user (default : the quartiles).
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-        percentiles : Float or list of floats, default: [0.25, 0.50, 0.75].
-            percentiles to be computed on numerical columns.
-            All values in percentiles must be > 0  and < 1.
-
-        Returns
-        -------
-        summary: DataFrame, where
-            * Index is the name of the computed values.
-            * Columns are either numerical or categorical columns of self.
-
-        """
-        from nzpyida.statistics import describe
-        return describe(idadf=self, percentiles=percentiles)
-
-
-    @timed
-    @idadf_state
-    def summary(self):
-        """
-        A basic statistical summary about current IdaDataFrame.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-
-
-        Returns
-        -------
-        summary: DataFrame, where
-            * Index is the name of the computed values.
-            * Columns are either numerical or categorical columns of self.
-
-        """
-        from nzpyida.statistics import summary
-        return summary(idadf=self)
-
-    @timed
-    @idadf_state
-    def cov(self):
-        """
-        Compute the covariance matrix, composed of covariance coefficients
-        between all pairs of columns in self.
-        It must have at least two numeric columns.
-
-        Returns
-        -------
-        covariance matrix: DataFrame
-            The axes are the columns of self and the values are the covariance
-            coefficients.
-        """
-        from nzpyida.statistics import cov
-        return cov(idadf=self)
-
-    @timed
-    @idadf_state
-    def corr(self, method="pearson", features=None, ignore_indexer=True):
-        """
-        Compute the correlation matrix, composed of correlation coefficients
-        between all pairs of columns in self.
-        It must have at least two numeric columns.
-
-        Parameters
-        ----------
-        method : str, default: pearson
-            Method to be used to compute the correlation. By default, compute
-            the pearson correlation coefficient. The Spearman rank correlation
-            is also available. Admissible values are: "pearson", "spearman".
-
-        Returns
-        -------
-        correlation matrix: DataFrame
-            The axes are the columns of self and the values are the correlation
-            coefficients.
-
-        Notes
-        -----
-        For the Spearman rank correlation, the ordinal rank of columns is
-        computed. For performance reasons this is easier to compute than the
-        fractional rank traditionally computed for the Spearman rank
-        correlation method. This strategy has the property that the sum of the
-        ranking numbers is the same as under ordinal ranking. We then apply
-        the pearson correlation coefficient method to these ranks.
-        """
-        from nzpyida.statistics import corr
-        #return corr(idadf=self, features=features, ignore_indexer=ignore_indexer)
-        return corr(idadf=self)
-
-    @timed
-
-    def train_test_split(self, train_table, test_table, id, fraction, seed):
-        """
-        Split the table into train and test sets
-
-        Parameters
-        ----------
-        train_table : str
-            the output table that will contain the given fraction of the input records
-
-        test_table :str
-            the output table that will contain the rest (1-<fraction>) of the input records
-
-        id : str
-            the input table column identifying a unique instance id
-
-        fraction: float
-             the fraction of the data to split
-
-        seed: float
-            the seed of the random function
-
-        Returns
-        -------
-        number of records in train table: float
-
-
-
-        """
-        from nzpyida.statistics import train_test_split
-
-        return train_test_split(idadf=self, train_table=train_table, test_table=test_table, id=id, fraction=fraction, seed=seed)
-
-    # TODO: to implement
-    @timed
-    @idadf_state
-    def corrwith(self, other):
-        """
-        Compute the correlation matrix, composed of correlation coefficients
-        between the columns of self and the columns of another IdaDataFrame.
-
-        Parameters
-        ----------
-        other : DataFrame
-
-        Returns
-        -------
-        correlation matrix: DataFrame
-            The columns are the columns of self and the index the columns
-            of other. The values are the covariance coefficients.
-        """
-        raise NotImplementedError("TODO")
-
-    # TODO: to implement
-    @timed
-    @idadf_state
-    def mode(self):
-        """
-        Compute the most common value for each non numeric column self. NOTIMPLEMENTED
-
-        Returns
-        -------
-        mode: Series
-        """
-        raise NotImplementedError("TODO")
-
-    @timed
-    @idadf_state
-    def quantile(self, q=0.5):
-        """
-        Compute row wise quantiles for each numeric column.
-
-        Parameters
-        ----------
-        q : float or array-like, default 0.5 (50% quantile)
-            0 <= q <= 1, the quantile(s) to compute
-
-        Returns
-        -------
-        quantiles: Series or DataFrame
-            If q is an array, the function returns a DataFrame in which the
-            index is q. The columns are the columns of sel, and the values are
-            the quantiles. If q is a float, a Series is returned where the
-            index is the columns of self and the values are the quantiles.
-
-        """
-        from nzpyida.statistics import quantile
-        return quantile(idadf=self, q=q)
-
-    @timed
-    @idadf_state
-    def mad(self):
-        """
-        Compute the mean absolute distance for all numeric columns of self.
-        It must have at least two numeric columns.
-
-        Returns
-        -------
-        mad: Series
-            The index consists of the columns of self and the values are the mean absolute distance.
-        """
-        from nzpyida.statistics import mad
-        return mad(idadf=self)
-
-    @timed
-    @idadf_state
-    def min(self):
-        """
-        Compute the minimum value for all numerics column of self.
-
-        Returns
-        -------
-        min: Series
-            The index consists of the columns of self and the values are the minimum.
-        """
-        from nzpyida.statistics import ida_min
-        return ida_min(idadf=self)
-
-    @timed
-    @idadf_state
-    def max(self):
-        """
-        Compute the maximum value over for all numeric columns of self.
-
-        Returns
-        -------
-        max: Series.
-            The index consists of the columns of self and the values are the maximum.
-        """
-        from nzpyida.statistics import ida_max
-        return ida_max(idadf=self)
-
-    @timed
-    @idadf_state
-    def count(self):
-        """
-        Compute the count of non-missing values for all columns of self.
-
-        Returns
-        -------
-        count: Series.
-            The index consists of the columns of self and the values are the number of non-missing values.
-        """
-        from nzpyida.statistics import count
-        return count(idadf=self)
-
-    @timed
-    @idadf_state
-    def count_distinct(self):
-        # deprecated, use levels instead
-        """
-        Compute the count of distinct values for all numeric columns of self.
-
-        Returns
-        -------
-        disctinct count: Series
-            The index consists of the columns of self and values are the number of distinct values.
-        """
-        from nzpyida.statistics import count_distinct
-        return count_distinct(idadf=self)
-
-    @timed
-    @idadf_state
-    def std(self):
-        """
-        Compute the standard deviation for all numeric columns of self.
-
-        Returns
-        -------
-        std: Series
-            The index consists of the columns of self and the values are the standard deviation.
-        """
-        from nzpyida.statistics import std
-        return std(idadf=self)
-
-    @timed
-    @idadf_state
-    def within_class_var(self, target, features = None, ignore_indexer=True):
-        if features is None:
-            numerical_columns = self._get_numerical_columns()
-            features = [x for x in numerical_columns if x != target]
-        else:
-            if isinstance(features, six.string_types):
-                features = [features]
-
-        if ignore_indexer is True:
-            if self.indexer:
-                if self.indexer in features:
-                    features.remove(self.indexer)
-
-        result = pd.Series()
-
-        #C = self.levels(target)
-        N = len(self)
-        if self._idadb._is_netezza_system():
-            power_function = "POW"
-        else:
-            power_function = "POWER"
-
-        if len(features) < 5:
-
-            avglist = ["AVG(\"%s\") as \"average%s\""%(feature, index) for index, feature in enumerate(features)]
-            sumlist = ["SUM(CAST(%s(\"%s\" - \"average%s\", 2) as DOUBLE))"%(power_function, feature, index) for index, feature in enumerate(features)]
-
-            avgstr = ", ".join(avglist)
-            sumstr = ", ".join(sumlist)
-
-            subquery = "(SELECT \"%s\", %s FROM %s GROUP BY \"%s\") AS B"%(target, avgstr, self.name, target)
-            condition = "ON A.\"%s\" = B.\"%s\""%(target, target)
-            groupby = "GROUP BY A.\"%s\""%target
-            query = "SELECT %s FROM %s AS A INNER JOIN %s %s %s"%(sumstr, self.name, subquery, condition, groupby)
-
-            classvar = self.ida_query(query)
-            if len(features) == 1:
-                classvar = pd.DataFrame(classvar)
-
-            C = len(classvar)
-            if N == C:
-                N += 1
-
-
-            classvar.columns = pd.Index(features)
-            result = pd.Series()
-            for attr in classvar:
-                result[attr] = classvar[attr].sum()/(N -C)
-
-        else:
-            chunkgen = chunklist(features, 5)
-            result = pd.Series()
-            for chunk in chunkgen:
-                avglist = ["AVG(\"%s\") as \"average%s\""%(feature, index) for index, feature in enumerate(chunk)]
-                sumlist = ["SUM(CAST(%s(\"%s\" - \"average%s\", 2) as DOUBLE))"%(power_function, feature, index) for index, feature in enumerate(chunk)]
-
-                avgstr = ", ".join(avglist)
-                sumstr = ", ".join(sumlist)
-
-                subquery = "(SELECT \"%s\", %s FROM %s GROUP BY \"%s\") AS B"%(target, avgstr, self.name, target)
-                condition = "ON A.\"%s\" = B.\"%s\""%(target, target)
-                groupby = "GROUP BY A.\"%s\""%target
-                query = "SELECT %s FROM %s AS A INNER JOIN %s %s %s"%(sumstr, self.name, subquery, condition, groupby)
-
-                classvar = self.ida_query(query)
-
-                if len(chunk) == 1:
-                    classvar = pd.DataFrame(classvar)
-
-                C = len(classvar)
-                if N == C:
-                    N += 1
-
-                classvar.columns = pd.Index(chunk)
-
-                for attr in classvar:
-                    result[attr] = classvar[attr].sum()/(N -C)
-
-        return result
-
-    @timed
-    @idadf_state
-    def within_class_std(self, target, features = None, ignore_indexer= True):
-        return np.sqrt(self.within_class_var(target, features, ignore_indexer))
-
-    @timed
-    @idadf_state
-    def var(self):
-        """
-        Compute the variance for all numeric columns of self.
-
-        Returns
-        -------
-        var: Series
-            The index consists of the columns of self and the values are the variance.
-        """
-        from nzpyida.statistics import var
-        return var(idadf=self)
-
-    @timed
-    @idadf_state
-    def mean(self):
-        """
-        Compute the mean for each numeric columns of self.
-
-        Returns
-        -------
-        mean: Series
-            The index consists of the columns of self and the values are the mean.
-        """
-        from nzpyida.statistics import mean
-        return mean(idadf=self)
-
-    @timed
-    @idadf_state
-    def mean_groupby(self, groupby, features = None):
-        if features is None:
-            features = [x for x in self.columns if x != groupby]
-        else:
-            if isinstance(features, six.string_types):
-                features = [features]
-
-        avglist = ["CAST(AVG(\"%s\") as FLOAT) AS \"%s_AVG\""%(feature, feature) for feature in features]
-        avgstr = ", ".join(avglist)
-
-        query = "SELECT \"%s\", %s FROM %s GROUP BY \"%s\""%(groupby, avgstr, self.name, groupby)
-        result = self.ida_query(query)
-        result = result.pivot_table(index = result.columns[0])
-        result.columns = pd.Index(features)
-        return result
-
-
-    @timed
-    @idadf_state
-    def sum(self):
-        """
-        Compute the sum of values for all numeric columns of self.
-
-        Returns
-        -------
-        sum: Series
-            The index consists of the columns of self and the values are the sum.
-        """
-        # Behave like having the option "numeric only" to true
-        # TODO: Implement the option
-        from nzpyida.statistics import ida_sum
-        return ida_sum(idadf=self)
-
-    @timed
-    @idadf_state
-    def median(self):
-        """
-        Compute the median for all numeric columns of self.
-
-        Returns
-        -------
-        median: Series
-            The index consists of the columns of self and the values are the median.
-        """
-        # Behave like having the option "numeric only" to true
-        # TODO: Implement the option
-        from nzpyida.statistics import median
-        return median(idadf=self)
-
-    # Maybe not be possible
-    @idadf_state
-    def rank(self):
-        """Compute the rank over all entries for all columns of self."""
-        raise NotImplementedError("TODO")
-
-    # TODO : cumsum, cummean, cummcountm cummax, cumprod
-
-###############################################################################
-### Save current IdaDataFrame to Db2 Warehouse as a table
-###############################################################################
-
-    # TODO: Should this function be in IdaDataBase ?
-    # To my mind, it is more intuitive to let it here, but it is "destructive"
-    @timed
-    @idadf_state
-    def save_as(self, tablename, clear_existing = False):
-        """
-        Save self as a table name in the remote database with the name
-        tablename. This function might erase an existing table if tablename
-        already exists and clear_existing is True.
-
-        """
-        # TODO: to test !
-        if tablename == self.tablename:
-            if clear_existing is False:
-                raise ValueError("Cannot overwrite current IdaDataFrame if "+
-                                " clear_existing option set to False.")
-            message = "Table %s already exists."%(tablename)
-            warnings.warn(message, UserWarning)
-            question = "Are you sure that you want to overwrite %s"%(tablename)
-            display_yes = nzpyida.utils.query_yes_no(question)
-            if not display_yes:
-                return
-            tempname = self._idadb._get_valid_tablename()
-            self._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s) WITH DATA"%(tempname, tablename))
-            try:
-                self._idadb.drop_table(tablename)
-            except:
-                self._idadb.drop_view(tablename)
-
-            newidadf = IdaDataFrame(self._idadb, tempname)
-            self._idadb.rename(newidadf, tablename)
-            self = newidadf
-
-        if self._idadb.exists_table_or_view(tablename):
-            if clear_existing:
-                message = "Table %s already exists."%(tablename)
-                warnings.warn(message, UserWarning)
-                question = "Are you sure that you want to overwrite %s"%(tablename)
-                display_yes = nzpyida.utils.query_yes_no(question)
-                if not display_yes:
-                    return
-                try:
-                    self._idadb.drop_table(tablename)
-                except:
-                    self._idadb.drop_view(tablename)
-            else:
-                raise NameError(("%s already exists, choose a different name "+
-                                "or use clear_existing option.")%tablename)
-
-        name = self.internal_state.current_state
-
-        self._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s) WITH DATA"%(tablename, name))
-
-        # Reset the cache
-        self._idadb._reset_attributes("cache_show_tables")
-
-###############################################################################
-### Import as DataFrame
-###############################################################################
-
-    @timed
-    @idadf_state
-    def as_dataframe(self):
-        """
-        Download and return an in-memory representation of the dataset as
-        a Pandas DataFrame.
-
-        Returns
-        -------
-        DataFrame
-            Columns and records are the same as in self.
-
-
-        Examples
-        --------
-        >>> iris = ida_iris.as_dataframe()
-        >>> iris.head()
-           sepal_length  sepal_width  petal_length  petal_width species
-        0           5.1          3.5           1.4          0.2  setosa
-        1           4.9          3.0           1.4          0.2  setosa
-        2           4.7          3.2           1.3          0.2  setosa
-        3           4.6          3.1           1.5          0.2  setosa
-        4           5.0          3.6           1.4          0.2  setosa
-        """
-        if os.environ['VERBOSE'] == 'True':
-            # We use an empirical estimation
-            # Experimental results :
-            # 1M row, 12 columns => 550 secs
-            # 3.77M, 12 columns => 2256 secs
-            raw_estimation = ((self.shape[0]*self.shape[1]) / 4000000) * 3
-            if raw_estimation < 0.01:
-                # There is a small offset, which is negligeable for higher measurements
-                estimation = str(raw_estimation*60 + 0.717) + " seconds."
-            elif raw_estimation > 60:
-                estimation = str(raw_estimation/60) + " hours."
-            else:
-                estimation = "%s minutes."%raw_estimation
-            print("Your IdaDataFrame has %s rows and %s columns." % (self.shape[0], self.shape[1]))
-            print("Estimated download time : ~ " + estimation)
-            if raw_estimation > 30:
-                message = ("Estimated Download time is greater than" +
-                           " %s minutes.")%raw_estimation
-                warnings.warn(message, UserWarning)
-                question = "Do you want to download the dataset?"
-                display_yes = nzpyida.utils.query_yes_no(question)
-                if not display_yes:
-                    return
-
-        data = self.ida_query(self.internal_state.get_state())
-        data.columns = self.columns
-        data.name = self.tablename
-        # Handle datatypes
-#        data = ibmdbpy.utils._convert_dtypes(self, data)
-        return data
-
-###############################################################################
-### Connection Management
-###############################################################################
-
-# Connection is a bit different, this is made to allow user to work on several
-# IdaDataFrame and close them, without closing the connection. This is why
-# function "close" is overwritten and a function reopen is provided
-
-    # Not sure commit and rollback should be here ?
-
-    def commit(self):
-        """
-        Commit operations in the database.
-
-        Notes
-        -----
-
-        All changes that are made in the database after the last commit,
-        including those in the child IdaDataFrames, are commited.
-
-        If the environment variable ‘VERBOSE’ is set to True, the commit
-        operations are notified in the console.
-        """
-        self._idadb.commit()
-
-    def rollback(self):
-        """
-        Rollback operations in the database.
-
-        Notes
-        -----
-
-        All changes that are made in the database after the last commit,
-        including those in the child IdaDataFrames, are discarded.
-        """
-        self._idadb.rollback()
-
-
-###############################################################################
-### Private functions
-###############################################################################
-
-    def _clone(self):
-        """
-        Clone the actual object.
-        """
-        newida = IdaDataFrame(self._idadb, self._name, self.indexer)
-        newida.columns = self.columns
-        newida.dtypes = self.dtypes     # avoid recomputing it
-        # otherwise risk of infinite loop between
-        # idadf.columns and internalstate.columndict
-
-        # This is not possible to use deepcopy on an IdaDataFrame object
-        # because the reference to the parents IdaDataBase with the connection
-        # object is not pickleable. As a consequence, we create a new
-        # IdaDataFrame and copy all the relevant attributes
-
-        newida.internal_state.name = deepcopy(self.internal_state.name)
-        newida.internal_state.ascending = deepcopy(self.internal_state.ascending)
-        #newida.internal_state.views = deepcopy(self.internal_state.views)
-        newida.internal_state._views = deepcopy(self.internal_state._views)
-        newida.internal_state._cumulative = deepcopy(self.internal_state._cumulative)
-        newida.internal_state.order = deepcopy(self.internal_state.order)
-        newida.internal_state.columndict = deepcopy(self.internal_state.columndict)
-        return newida
-
-    def _clone_as_serie(self, column):
-        """
-        Clone the actual object as an IdaSeries and select one of its columns.
-        """
-        newida = nzpyida.IdaSeries(idadb = self._idadb, tablename = self._name,
-                                   indexer = self.indexer, column = column)
-
-        newida.internal_state.name = deepcopy(self.internal_state.name)
-        newida.internal_state.ascending = deepcopy(self.internal_state.ascending)
-        #newida.internal_state.views = deepcopy(self.internal_state.views)
-        newida.internal_state._views = deepcopy(self.internal_state._views)
-        newida.internal_state._cumulative = deepcopy(self.internal_state._cumulative)
-        newida.internal_state.order = deepcopy(self.internal_state.order)
-        newida.internal_state.columndict = deepcopy(self.internal_state.columndict)
-        return newida
-
-    def _get_type(self):
-        """
-        Type of the IdaDataFrame : "Table", "View" or "Unknown".
-        """
-        if self._idadb.is_table(self._name):
-            return "Table"
-        elif self._idadb.is_view(self._name):
-            return "View"
-        else:
-            return "Unkown"
-
-    def _get_columns(self):
-        """
-        Index containing a list of the columns in self.
-        """
-        tablename = self.internal_state.current_state
-        if self._idadb._con_type == 'odbc':
-            if '.' in tablename:
-                schema = tablename.split('.')[-2]
-                tablename = tablename.split('.')[-1]
-            else:
-                schema = self._idadb.current_schema
-            columns = self._idadb._con.cursor().columns(table=tablename, schema=schema)
-            columnlist = [column[3] for column in columns]
-            return Index(columnlist)
-        elif self._idadb._con_type == 'nzpy':
-            cursor = self._idadb._con.cursor()
-            cursor.execute("SELECT * FROM %s LIMIT 1"%tablename)
-            columnlist = []
-            if type(cursor.description[0][0]) == bytes:
-                columnlist = [column[0].decode() for column in cursor.description]
-            else:
-                columnlist = [column[0] for column in cursor.description]
-            cursor.close()
-            return Index(columnlist)
-        elif self._idadb._con_type == 'jdbc':
-            cursor = self._idadb._con.cursor()
-            cursor.execute("SELECT * FROM %s LIMIT 1"%tablename)
-            columnlist = [column[0] for column in cursor.description]
-            cursor.close()
-            return Index(columnlist)
-
-    def _get_all_columns_in_table(self):
-        """Get all columns that exists in the physical table."""
-        return self._get_columns()
-
-    # TODO: To deprecate
-    def _get_index(self, force=False):
-        """
-        Index containing a list of the row names in self.
-        """
-
-        rows = self.shape[0]
-
-        return RangeIndex(0, rows, 1)
-
-    def _get_shape(self):
-        """
-        Tuple containing the number of rows and the number of columns in self.
-        """
-        name = self.internal_state.current_state
-
-        nrow = self.ida_scalar_query("SELECT CAST(COUNT(*) AS INTEGER) FROM %s"%name)
-        ncol = len(self.columns)
-        return (nrow, ncol)
-
-    def _get_columns_dtypes(self):
-        """
-        DataFrame containing the column names and database types in self.
-        """
-        name = self.internal_state.current_state
-
-        # In case the name is composed the following way : SCHEMA.TABLENAME
-        # We need to separate it to keep only the TABLENAME for this query.
-        if '.' in name :
-            name = name.split('.')[-1]
-
-        if self._idadb._is_netezza_system():
-            nz_hidden_columns =  "('ROWID', 'DATASLICEID', 'CREATEXID', 'DELETEXID', '_PAGEID', '_EXTENTID') "
-            query_select = ("SELECT COLUMN_NAME AS COLNAME, " +
-                            "CASE WHEN strpos(TYPE_NAME, '(') = 0 THEN TYPE_NAME " +
-                            "ELSE substr(TYPE_NAME, 1, strpos(TYPE_NAME,'(')-1) END AS TYPENAME " +
-                            "FROM _V_SYS_COLUMNS ")
-            # hidden Netezza columns are not included in the set of columns of this data frame
-            query_where1 = "WHERE TABLE_NAME = \'%s\' AND COLUMN_NAME NOT IN " + nz_hidden_columns
-            query_where2 = "AND SCHEMA = \'%s\' "
-            query_order_by = "ORDER BY ORDINAL_POSITION "
-        else:
-            query_select = "SELECT COLNAME, TYPENAME FROM SYSCAT.COLUMNS "
-            query_where1 = "WHERE TABNAME = \'%s\' "
-            query_where2 = "AND TABSCHEMA = \'%s\' "
-            query_order_by = "ORDER BY COLNO"
-
-        if name.find("TEMP_VIEW_") == 0:
-            #When the column names are going to be retrieved from a temporary
-            #view that was created with the definition of the current state of
-            #the IdaDataFrame, the schema name cannot be assumed as the same of
-            #the IdaDataFrame. Also mind that the name of the temporary view
-            #is thought to be random enough to avoid collisions
-            data = self.ida_query((query_select + query_where1 + query_order_by)%(name))
-        else:
-            data = self.ida_query((query_select + query_where1 + query_where2 + query_order_by)%(name, self.schema))
-
-        # Workaround for some ODBC version which does not get the entire
-        # string of the column name in the cursor descriptor.
-        # This is hardcoded, so be careful
-        data.columns = ["COLNAME", "TYPENAME"]
-        data.columns = [x.upper() for x in data.columns]
-        data.set_index(keys='COLNAME', inplace=True)
-        # del data.index.name
-        return data
-
-    def _reset_attributes(self, attributes):
-        """
-        Delete an attribute of self to force its refreshing at the next call.
-        """
-        if isinstance(attributes, six.string_types):
-            attributes = [attributes]
-
-        # Special case : resetting columns
-        if "columns" in attributes:
-             try:
-                 del self.internal_state.columndict
-             except:
-                 pass
-
-        nzpyida.utils._reset_attributes(self, attributes)
-
-
-###############################################################################
-### DB2 Warehouse to pandas type mapping
-###############################################################################
-
-    def _table_def(self, factor_threshold=None):
-        """
-        Classify columns in the idaDataFrame into 4 classes: CATEGORICAL,
-        STRING, NUMERIC or NONE. Use the database data type and a
-        user-threshold “factor_threshold”:
-
-            * CATEGORICAL columns that have a number of distinct values that is greater
-              than the factor_threshold should be considered a STRING.
-            * NUMERIC columns that have a number of distinct values that is smaller or equal
-              to the factor_threshold should be considered CATEGORICAL.
-
-        Returns
-        -------
-        DataFrame
-
-            * Index is the columns of self.
-            * Column "FACTORS" contains the number of distinct values.
-            * Column "VALTYPE" contains the resulting class.
-
-        Examples
-        --------
-        >>> ida_iris._table_def()
-                     TYPENAME FACTORS      VALTYPE
-        sepal_length   DOUBLE      35      NUMERIC
-        sepal_width    DOUBLE      23      NUMERIC
-        petal_length   DOUBLE      43      NUMERIC
-        petal_width    DOUBLE      22      NUMERIC
-        species       VARCHAR       3  CATEGORICAL
-        """
-        # We don't want to change the value of the attribute
-        data = deepcopy(self.dtypes)
-
-        def _valtype_from_dbtype(tup):
-            """
-            Decides if a column should be considered categorical or numerical
-            """
-            categorical_attributes = ['VARCHAR', 'CHARACTER VARYING', 'CHARACTER', 'VARGRAPHIC',
-                                      'GRAPHIC', 'CLOB']
-            numerical_attributes = ['SMALLINT', 'INTEGER', 'BIGINT', 'REAL',
-                                    'DOUBLE', 'DOUBLE PRECISION', 'FLOAT', 'DECIMAL', 'NUMERIC']
-            if tup[0] in categorical_attributes:
-                if factor_threshold is None:
-                    return "CATEGORICAL"
-                elif tup[1] <= factor_threshold:
-                    return "CATEGORICAL"
-                else:
-                    return "STRING"
-            elif tup[0] in numerical_attributes:
-                if factor_threshold is None:
-                    return "NUMERIC"
-                elif tup[1] > factor_threshold:
-                    return "NUMERIC"
-                else:
-                    return "CATEGORICAL"
-            else:
-                return "NONE"
-
-        data['FACTORS'] = nzpyida.statistics._count_level(self, data.index.values)
-        data['VALTYPE'] = [_valtype_from_dbtype(x) for x in
-                           data[['TYPENAME', 'FACTORS']].to_records(index=0)]
-        return data
-
-    def _get_numerical_columns(self):
-        """
-        Get the columns of self that are considered as numerical. Their data
-        type in the database determines whether these columns are numerical.
-        The following data types are considered numerical:
-
-            SMALLINT, INTEGER, BIGINT, REAL, DOUBLE, FLOAT, DECIMAL, NUMERIC.
-
-        Returns
-        -------
-        list
-            List of numerical column names.
-
-        Examples
-        --------
-        >>> ida_iris._get_numerical_columns()
-        ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
-        """
-        num = ['SMALLINT', 'INTEGER', 'BIGINT', 'REAL',
-                            'DOUBLE', 'DOUBLE PRECISION', 'FLOAT', 'DECIMAL', 'NUMERIC']
-        return list(self.dtypes.loc[self.dtypes['TYPENAME'].isin(num)].index)
-
-    def _get_categorical_columns(self):
-        """
-        Get the columns of self that are considered as categorical. Their data
-        type in the database determines whether these columns are categorical.
-        The following data types are considered categorical:
-
-            VARCHAR,CHARACTER, VARGRAPHIC, GRAPHIC, CLOB.
-
-        Returns
-        -------
-        list
-            List of categorical column names.
-
-        Examples
-        --------
-        >>> ida_iris._get_categorical_columns()
-        ['species']
-        """
-        cat = ['VARCHAR', 'CHARACTER', 'VARGRAPHIC', 'GRAPHIC', 'CLOB']
-        return list(self.dtypes.loc[self.dtypes['TYPENAME'].isin(cat)].index)
-
-    def _prepare_and_execute(self, query, autocommit = True, silent = False):
-        """
-        Prepare and execute a query.
-
-        Parameters
-        ----------
-        query : str
-            Query to be executed.
-        autocommit : bool
-             If set to true, the autocommit function is available.
-        """
-        return self._idadb._prepare_and_execute(query, autocommit, silent)
-
-    def _autocommit(self):
-        """
-        Autocommit the connection. If the environment variable ‘AUTOCOMMIT’ is
-        set to True, the function commits the changes.
-
-        Notes
-        -----
-
-        If you commit, all changes that are made in the database after the last
-        commit, including those in the child IdaDataFrames, are commited.
-
-        If the environment variable ‘VERBOSE’ is set to True, the autocommit
-        operations are notified in the console.
-        """
-        self._idadb._autocommit()
-
-    def _combine_check(self, other):
-        """
-        Check if self and other refer to the same table and if all columns in
-        self and other are numeric. This sanity check is used before performing
-        aggregation operations between IdaDataFrame/IdaSeries.
-        """
-        def check_numeric_columns(idaobject):
-            not_valid = []
-            numeric_columns = idaobject._get_numerical_columns()
-            for column in idaobject.columns:
-                if column not in numeric_columns:
-                    not_valid.append(column)
-            if not_valid:
-                raise TypeError("Arithmetic operation are not defined for %s"%not_valid)
-
-        if isinstance(other, IdaDataFrame) | isinstance(other, nzpyida.IdaSeries):
-            if self._name != other._name:
-                raise IdaDataFrameError("It is not possible to aggregate columns using columns of a different table.")
-
-        if not(isinstance(other, IdaDataFrame) | isinstance(other, nzpyida.IdaSeries) | isinstance(other, Number)):
-            if other is not None:
-                raise TypeError("Aggregation makes only sense with numbers, "+
-                                "or IdaDataFrames refering to the same table.")
-
-        check_numeric_columns(self)
-        if isinstance(other, nzpyida.IdaSeries)|isinstance(other, IdaDataFrame):
-            check_numeric_columns(other)
-
-    
-    def delete_na(self, columns, logic="any", inplace=False):
-        """
-        Filter rows containing NULL values. Can be done in a destructive way (rows are deleted in the physical table)
-        or in a non destructive way (a new IdaDataFrame is defined, original table and IdaDataFrame are preserved). 
-        
-        Parameters
-        -----------
-        columns : list of strings
-            list of eligible column names
-        logic : str, optional
-            "any" by default. 
-            If logic is set to "any" then all rows which contain a NaN value in any 
-            (id est at least one) of the cited columns is deleted, this is a union condition;
-            if logic is set to "all", then only rows containing null values in all the fields are deleted, this is an intersection condition.         
-        inplace : bool, optional 
-            False by default. 
-            If True, then the underlying table is physically modified,
-            if False, then the original objects remain unaffected, a copy of the IdaDataFrame is made and modified. 
-        
-        Returns
-        --------
-        If inplace is True, the original table is modified and the IdaDataFrame will be modified accordingly. No return.
-        If inplace is False, a new IdaDataFrame is returned, the original table is not affected. This new IdaDataFrame points to a 
-        new table which has been created according to the user defined criterions on columns.
-        
-        Examples
-        --------
-        >>> idadf.delete_na(["COL_1", "COL_2"], inplace = True)
-        >>> #each row of the original table is physically deleted if there is a NULL value in one of the listed columns
-        >>> new_idadf = idadf.delete_na(["COL_1", "COL_2"], logic = 'all')
-        >>> # a new IdaDataFrame is created, rows will be selected only if all the listed columns have a NULL value.
-        """
-        
-        # Check if list columns is not empty
-        if len(columns)<1:
-            raise IdaDataFrameError("You must specify the columns as a list of eligible names")
-        # Check if column names are eligible
-        for column_name in columns:
-            if not isinstance(column_name, six.string_types):
-                raise TypeError("%s is not of string type"%(column_name))
-            if column_name not in self.columns:
-                raise ValueError("%s refers to a columns that doesn't exists in self"%(column_name))
-        
-        idadb = self._idadb
-        
-        if inplace == True:
-            # Explain
-            print("The table %s will be physically modified." %self.tablename)
-            print("Any IdaDataFrame pointing this table might be modified accordingly.")
-            # Write DELETE query on original table
-            tablename = self.tablename
-            query = 'DELETE FROM %s WHERE "%s" IS NULL'%(tablename, columns[0])
-            
-            if len(columns)>1:
-                if logic == "any":
-                    for col in columns[1:]:
-                        query = query + ' OR "%s" IS NULL'%col
-                if logic == "all":
-                    for col in columns[1:]:
-                        query = query + ' AND "%s" IS NULL'%col                        
-            idadb.ida_query(query)
-
-        else:            
-            # SELECT statement on the original table to create a view
-            tablename0 = self.tablename 
-            query = 'SELECT * FROM %s WHERE "%s" IS NOT NULL'%(tablename0, columns[0])
-            
-            if len(columns)>1:
-                if logic == "any":
-                    for col in columns[1:]:
-                        query = query + ' AND "%s" IS NOT NULL'%col
-                if logic == "all":
-                    for col in columns[1:]:
-                        query = query + ' OR "%s" IS NOT NULL'%col                                               
-            # Create view with this select statement
-            viewname = idadb._get_valid_tablename(prefix="VIEW_")
-            self._prepare_and_execute("CREATE VIEW " + viewname + " AS "+ query)
-            # Initiate the modified table under a random name
-            tablename = idadb._get_valid_tablename(prefix="DATA_FRAME_")
-            idadb._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s) WITH DATA"%(tablename,viewname))
-            print('A new table with filtered rows is available under the name %s.' %tablename)
-            print('The newly created IdaDataFrame refers to this new table.')
-            # Drop the view 
-            idadb.drop_view(viewname)            
-            # Define a new IdaDataFrame pointing to the new table
-            idadf = IdaDataFrame(idadb, tablename, indexer = self.indexer)
-            return idadf
-    
-            
-
-
-            
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+idaDataFrame
+---------
+An efficient 2D container that looks like a panda's DataFrame and behave the
+same, the only difference is that it uses only a reference to a remote database
+instead of having the data loaded into memory
+
+Also similar to its R counterpart, data.frame, except providing automatic data
+alignment and a host of useful data manipulation methods having to do with the
+labeling information
+"""
+
+# Ensure Python 2 compatibility
+from __future__ import print_function
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import absolute_import
+from builtins import dict
+from builtins import zip
+from builtins import str
+from builtins import int
+from future import standard_library
+standard_library.install_aliases()
+
+import sys
+import os
+from copy import deepcopy
+import warnings
+from numbers import Number
+from collections import OrderedDict
+
+import numpy as np
+import pandas as pd
+from pandas import Index, RangeIndex
+
+from lazy import lazy
+import six
+
+import nzpyida
+import nzpyida.statistics
+import nzpyida.indexing
+import nzpyida.aggregation
+import nzpyida.filtering
+import nzpyida.utils
+
+from nzpyida.utils import timed, chunklist
+from nzpyida.internals import InternalState
+from nzpyida.exceptions import IdaDataFrameError
+from nzpyida.internals import idadf_state
+
+
+class IdaDataFrame(object):
+    """
+    An IdaDataFrame object is a reference to a table in a remote Db2 Warehouse
+    database. IDA stands for In-DataBase Analytics. IdaDataFrame copies the
+    Pandas interface for DataFrame objects to ensure intuitive interaction for
+    end-users.
+
+    Examples
+    --------
+    >>> idadb = IdaDataBase('BLUDB') # See documentation for IdaDataBase
+    >>> ida_iris = IdaDataFrame(idadb, 'IRIS')
+    >>> ida_iris.cov()
+                      sepal_length  sepal_width  petal_length  petal_width
+    sepal_length      0.685694    -0.042434      1.274315     0.516271
+    sepal_width      -0.042434     0.189979     -0.329656    -0.121639
+    petal_length      1.274315    -0.329656      3.116278     1.295609
+    petal_width       0.516271    -0.121639      1.295609     0.581006
+    """
+
+    # TODO: Check if everything is ok when selecting AND projecting with loc
+    # TODO: BUG: Filtering after selection/projection
+
+    def __init__(self, idadb, tablename, indexer = None):
+        """
+        Constructor for IdaDataFrame objects.
+
+        Parameters
+        ----------
+        idadb : IdaDataBase
+            IdaDataBase instance which contains the connection to be used.
+        tablename : str
+            Name of the table to be opened in the database.
+            It should contain only alphanumeric characters and underscores.
+            All lower case characters will be converted to upper case characters.
+        indexer : str, optional
+            Name of the column that should be used as an index. This is
+            optional. However, if no indexer is given, the order of rows issued
+            by the head and tail functions is not guaranteed. Also, several
+            in-database machine learning algorithms need an indexer as a
+            parameter to be executed.
+
+        Attributes
+        ----------
+        _idadb : IdaDataBase
+            IdaDataBase object parent of the current instance.
+        tablename : str
+            Name of the table self references.
+        name : str
+            Full name of the table self references, including schema.
+        schema : str
+            Name of the schema the table belongs to.
+        indexer : str
+            Name of the column used as an index. "None" if no indexer.
+        loc : str
+            Indexer that enables the selection and projection of IdaDataFrame
+            instances. For more information, see the loc class documentation.
+        internal_state : InternalState
+            Object used to internally store the state of the IdaDataFrame. It
+            also allows several non-destructive manipulation methods.
+        type : str
+            Type of the IdaDataFrame : “Table”, “View”, or “Unknown”.
+        dtypes : DataFrame
+            Data type in the database for each column.
+        index : pandas.core.index
+            Index containing the row names in this table.
+        columns : pandas.core.index
+            Index containing the columns names in this table.
+        axes : list
+            List containing columns and index attributes.
+        shape : Tuple
+            Number of rows and number of columns.
+
+        Notes
+        -----
+        Attributes "type", "dtypes", "index", "columns", "axes", and "shape"
+        are evaluated in a lazy way to avoid an overhead when creating an
+        IdaDataFrame. Sometimes the index may be too big to be downloaded.
+
+        Examples
+        --------
+        >>> idadb = IdaDataBase('BLUDB')
+        >>> ida_iris = IdaDataFrame(idadb, "IRIS")
+        """
+        #TODO: Implement equality comparision between two IdaDataFrames
+
+        if not idadb.__class__.__name__ == "IdaDataBase":
+            idadb_class = idadb.__class__.__name__
+            raise TypeError("Argument 'idadb' is of type %s, expected : IdaDataBase"%idadb_class)
+        tablename = nzpyida.utils.check_tablename(tablename)
+
+        #idadb._reset_attributes("cache_show_tables")
+
+        # TODO: Test what kind of error append when a table in use and cached
+        # is suddently deleted
+
+        if idadb.exists_table_or_view(tablename) is False:
+            # Try again after refreshing the cache
+            idadb._reset_attributes("cache_show_tables")
+            # Refresh the show table cache in parent IdaDataBase, because a table
+            # could have been created by other mean / user and we have to make sure
+            # the lookup is done and is actual.
+            if idadb.exists_table_or_view(tablename) is False:
+                raise NameError("Table %s does not exist in the database %s."
+                                %(tablename, idadb.data_source_name))
+
+
+        self._idadb = idadb
+        self._indexer = indexer
+
+        # Initialise indexer object
+        self.loc = nzpyida.indexing.Loc(self)
+
+        if "." in tablename:
+            self.schema = tablename.split('.')[0]
+            #self.name = tablename.split('.')[-1]
+            self._name = tablename
+            self.tablename = tablename.split('.')[-1]
+        else:
+            self.schema = idadb.current_schema
+            self._name = idadb.current_schema + '.' + tablename
+            self.tablename = tablename
+
+        # self._name is the original name, this is a "final" variable
+
+        # Push a reference to itself in its parent IdaDataBase
+        self._idadb._idadfs.append(self)
+        # TODO : self.size
+
+        # A cache for unique value of each column
+        self._unique = dict()
+
+###############################################################################
+### Attributes & Metadata computation
+###############################################################################
+
+    @lazy
+    def internal_state(self):
+        """
+         InternalState instances manage the state of an IdaDataFrame instance
+         and allow several non-destructive data manipulation methods, such as
+         the selection, projection, filtering, and aggregation of columns.
+
+        """
+        return InternalState(self)
+
+
+    @property
+    @idadf_state
+    def name(self):
+        return self.internal_state.current_state
+
+
+    @property
+    def indexer(self):
+        """
+        The indexer attribute refers to the name of a column that should be
+        used to index the table. This makes sense because Db2 Warehouse is a
+        column-based database, so row IDs do not make sense and are not
+        deterministic. As a consequence, the only way to address a particular
+        row is to refer to it by its index. If no indexer is provided, ibmdbpy
+        still works but a correct row order is not guaranteed as far as the
+        dataset is not sorted. Also, note that the indexer column is not taken
+        into account in data mining algorithms.
+        """
+        if hasattr(self, "_indexer"):
+            return self._indexer
+        else:
+            None    
+
+        
+    @indexer.setter
+    def indexer(self, value):
+        """
+        Basic checks for indexer :
+        * The column exists in the table.
+        * All values are unique.
+        """
+        if value is None:
+            return
+
+        if value not in self.columns:
+            raise IdaDataFrameError("'%s' cannot be used as indexer "%value +
+                                    " because this is not a column in '%s'"%self._name)
+
+        del self.columns
+        #count = self[value].count_distinct() ## TODO: TO FIX, should return directly just a number
+        count = self.levels(value)
+        if count < self.shape[0]:
+            raise IdaDataFrameError("'%s' cannot be used as indexer "%value +
+                                    " because it contains non unique values.")
+        self._indexer = value
+
+
+    @lazy
+    def type(self):
+        """
+        Type of self: 'Table', 'View'  or 'Unknown'.
+
+        Returns
+        -------
+        str
+            idaDataFrame type.
+
+        Examples
+        --------
+        >>> ida_iris.type
+        'Table'
+        """
+        return self._get_type()
+
+    @lazy
+    @idadf_state(force = True)
+    def dtypes(self):
+        """
+        Data type in database for each column in self.
+
+        Returns
+        -------
+        DataFrame
+            In-Database type for each columns.
+
+        Examples
+        --------
+        >>> ida_iris.dtypes
+                     TYPENAME
+        sepal_length   DOUBLE
+        sepal_width    DOUBLE
+        petal_length   DOUBLE
+        petal_width    DOUBLE
+        species       VARCHAR
+        """
+        #import pdb ; pdb.set_trace()
+        return self._get_columns_dtypes()
+
+    @lazy
+    @idadf_state
+    # to deprecate
+    def index(self):
+        """
+        Index containing the row names in self.
+
+        Returns
+        -------
+        Index
+
+        Examples
+        --------
+        >>> ida_iris.index
+        Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,
+            ...
+            140, 141, 142, 143, 144, 145, 146, 147, 148, 149],
+           dtype='int64', length=150)
+
+        Notes
+        -----
+        Because indexes in a database can be only numeric, it is not that
+        interesting for an IdaDataFrame but can still be useful sometimes. The
+        function can break if the table is too large. Ask for the user’s
+        approval before downloading an index which has more than 10000 values.
+        """
+        return self._get_index()
+
+    @lazy
+    def columns(self):
+        """
+        Index containing the column names in self.
+
+        Returns
+        -------
+        Index
+
+        Examples
+        --------
+        >>> ida_iris.columns
+        Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',
+        'species'],
+        dtype='object')
+        """
+        if hasattr(self, "internal_state"):
+            self.internal_state._create_view()
+            cols = self._get_columns()
+            self.internal_state._delete_view()
+            return cols
+        else:
+            return self._get_columns()
+
+    @lazy
+    @idadf_state
+    # to deprecate (no index)
+    def axes(self):
+        """
+        List containing IdaDataFrame.columns and IdaDataFrame.index attributes.
+
+        Returns
+        -------
+        list
+            List containing two indexes (indexes and column attributes).
+
+        Examples
+        --------
+        >>> ida_iris.axes
+        [Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,
+             ...
+             140, 141, 142, 143, 144, 145, 146, 147, 148, 149],
+            dtype='int64', length=150),
+        Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',
+        'species'],
+        dtype='object')]
+        """
+        return [self.index, self.columns]
+
+    @lazy
+    @idadf_state
+    def shape(self):
+        """
+        Tuple containing number of rows and number of columns.
+
+        Returns
+        -------
+        tuple
+
+        Examples
+        --------
+        >>> ida_iris.shape
+        (150, 5)
+        """
+        return self._get_shape()
+
+    @property
+    @idadf_state
+    def empty(self):
+        """
+        Boolean that is True if the table is empty (no rows).
+
+        Returns
+        -------
+        Boolean
+        """
+        if self.shape[0] == 0:
+            return True
+        else:
+            return False
+
+    def __len__(self):
+        """
+        Number of records.
+
+        Returns
+        -------
+        int
+
+        Examples
+        --------
+        >>> len(idadf)
+        150
+        """
+        return self.shape[0]
+
+    def __iter__(self):
+        """
+        Iterate over columns.
+        """
+        return iter(self.columns)
+
+    def __getitem__(self, item):
+        """
+        Enable label based projection (selection of columns) in IdaDataFrames.
+
+        Enable slice based selection of rows in IdaDataFrames.
+
+        Enable row filtering.
+
+        The syntax is similar to Pandas.
+
+        Examples
+        --------
+
+        >>> idadf['col1'] # return an IdaSeries
+
+        >>> idadf[['col1']] # return an IdaDataFrame with one column
+
+        >>> idadf[['col1', 'col2', 'col3']] # return an IdaDataFrame with 3 columns
+
+        >>> idadf[0:9] # Select the 10 first rows
+
+        >>> idadf[idadf['col1'] = "test"]
+        # select of rows for which attribute col1 is equal to "test"
+
+        Notes
+        -----
+        The row order is not guaranteed if no indexer is given and the dataset
+        is not sorted
+        """
+
+        if isinstance(item, nzpyida.filtering.FilterQuery):
+            newidadf = self._clone()
+            newidadf.internal_state.update(item)
+            newidadf._reset_attributes(["shape"])
+        else:
+            if isinstance(item, slice):
+                return self.loc[item]
+            if not (isinstance(item,six.string_types)|isinstance(item, list)):
+                raise KeyError(item)
+            if isinstance(item, six.string_types):
+                # Case when only one column was selected
+                if item not in self.columns:
+                    raise KeyError(item)
+                newidaseries = self._clone_as_serie(item)
+
+                # Form the new columndict
+                for column in list(newidaseries.internal_state.columndict):
+                    if column != item:
+                        del newidaseries.internal_state.columndict[column]
+                newColumndict = newidaseries.internal_state.columndict
+
+                # Erase attributes
+                newidaseries._reset_attributes(["columns", "shape", "dtypes"])
+                # Set columns and columndict attributes
+                newidaseries.internal_state.columns = ["\"%s\""%col for col in item]
+                newidaseries.internal_state.columndict = newColumndict
+                # Update, i.e. appends an entry to internal_state._cumulative
+                newidaseries.internal_state.update()
+
+                # Performance improvement
+                # avoid, caused wrong dtypes for the result
+                newidaseries.dtypes = self.dtypes.loc[[item]]
+
+                return newidaseries
+
+            # Case of multiple columns
+            not_a_column = [x for x in item if x not in self.columns]
+            if not_a_column:
+                raise KeyError("%s"%not_a_column)
+
+            newidadf = self._clone()
+
+            # Form the new columndict
+            newColumndict = OrderedDict()
+            for col in item:
+                # Column name as key, its definition as value
+                newColumndict[col] = self.internal_state.columndict[col]
+
+            # Erase attributes
+            newidadf._reset_attributes(["columns", "shape", "dtypes"])
+            # Set columns and columndict attributes
+            newidadf.internal_state.columns = ["\"%s\""%col for col in item]
+            newidadf.internal_state.columndict = newColumndict
+            # Update, i.e. appends an entry to internal_state._cumulative
+            newidadf.internal_state.update()
+
+            # Performance improvement
+            # avoid, caused wrong dtypes for the result
+            newidadf.dtypes = self.dtypes.loc[item]
+
+        return newidadf
+
+    def __setitem__(self, key, item):
+        """
+        Enable the creation and aggregation of columns.
+
+        Examples
+        --------
+
+        >>> idadf['new'] = idadf['sepal_length'] * idadf['sepalwidth']
+        # select a new column as the product of two existing columns
+
+        >>> idadf['sepal_length'] = idadf['sepal_length'] / idadf['sepal_length'].mean()
+        # modify an existing column
+        """
+        if not (isinstance(item, IdaDataFrame)):
+            raise TypeError("Modifying columns is supported only using "+
+                            "IdaDataFrames.")
+        if isinstance(key, six.string_types):
+            key = [key]
+        if len(key) != len(item.columns):
+                raise ValueError("Wrong number of items passed %s, placement implies %s"%(len(item.columns),len(key)))
+
+        #form the new columndict
+        for newname, oldname in zip(key, item.columns):
+            self.internal_state.columndict[newname] = item.internal_state.columndict[oldname]
+        newColumndict = self.internal_state.columndict
+
+        #erase attributes
+        self._reset_attributes(["columns", "shape", "dtypes"])
+        #set columns and columndict attributes
+        self.internal_state.columndict = newColumndict
+        self.internal_state.columns = ["\"%s\""%col for col in newColumndict.keys()]
+        #update, i.e. appends an entry to internal_state._cumulative
+        self.internal_state.update()
+
+        # Flush the "unique" cache
+        for column in key:
+            if column in self._unique:
+                del self._unique[column]
+
+    def __delitem__(self, item):
+        """
+        Enable non-destructive deletion of columns using a Pandas style syntax.
+        This happens inplace, which means that the current IdaDataFrame is
+        modified.
+
+        Examples
+        --------
+        >>> idadf = IdaDataFrame(idadb, "IRIS")
+        >>> idadf.columns
+        Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'], dtype='object')
+        >>> del idadf['sepal_length']
+        >>> idadf.columns
+        Index(['sepal_width', 'petal_length', 'petal_width', 'species'], dtype='object')
+        """
+        if not (isinstance(item,six.string_types)):
+            raise TypeError
+        if item not in self.columns:
+            raise KeyError(item)
+
+        # Flush the "unique" cache
+        if item in self._unique:
+            del self._unique[item]
+
+        self._idadb.delete_column(self, item, destructive = False)
+        return
+
+    def __enter__(self):
+        """
+        Allow the object to be used with a "with" statement
+        """
+        return self
+
+    def __exit__(self):
+        """
+        Allow the object to be used with a "with" statement. Make sure that
+        allow possible views related to the IdaDataFrame with be deleted when
+        the object goes out of scope
+        """
+        while self.internal_state.viewstack:
+            try :
+                view = self.internal_state.viewstack.pop()
+                # Just a last check to make sure not to drop user's db
+                if view != self.tablename:
+                    drop = "DROP VIEW \"%s\"" %view
+                    self._prepare_and_execute(drop, autocommit = True)
+            except: pass
+
+    #We decided not to allow columns access idadf.columnname like this for now.
+    #We could decide to allow it but for this we may have to switch all
+    #existing attributes to private ("_" as first character) so to avoid
+    #conflicts between attributes and columns because for example IdaDataFrame
+    #has an attribute "name" -> if a column is labelled "name" this will make
+    #it unavailable. Pandas do also poorly manage this issue, for example :
+    #DataFrame attribute "size".
+
+    #May be implemented in the future
+
+    #def __getattr__(self, name):
+    #    """
+    #    After regular attribute access, try look up the name
+    #    This allows simpler access to columns for interactive use.
+    #    """
+        # Note: obj.x will always call obj.__getattribute__('x') prior to
+        # calling obj.__getattr__('x').
+        #if hasattr(self, name):
+        #    return object.__getattribute__(self,name)
+        #else:
+    #    if name not in self.__dict__["columns"]:
+            #return self[name]
+    #        return object.__getattribute__(self, name)
+    #    raise AttributeError("'%s' object has no attribute '%s'" %
+    #                         (type(self).__name__, name))
+
+    #def __setattr__(self, name, value):
+    #    if name not in self.__dict__["columns"]:
+    #        self.__dict__[name] = value
+    #    else:
+    #        raise ValueError("It is not allowed to set the value of a column in an IdaDataFrame.")
+
+    def __lt__(self, value):
+        """
+        ibmdbpy.filtering.FilterQuery object when comparing self using "<".
+        """
+        return nzpyida.filtering.FilterQuery(self.columns, self._name, "lt", value)
+
+    def __le__(self, value):
+        """
+        ibmdbpy.filtering.FilterQuery object when comparing self using "<=".
+        """
+        return nzpyida.filtering.FilterQuery(self.columns, self._name, "le", value)
+
+    def __eq__(self, value):
+        """
+        ibmdbpy.filtering.FilterQuery object when comparing self using "==".
+        """
+        return nzpyida.filtering.FilterQuery(self.columns, self._name, "eq", value)
+
+    def __ne__(self, value):
+        """
+        ibmdbpy.filtering.FilterQuery object when comparing self using "!=".
+        """
+        return nzpyida.filtering.FilterQuery(self.columns, self._name, "ne", value)
+
+    def __ge__(self, value):
+        """
+        ibmdbpy.filtering.FilterQuery object when comparing self using ">=".
+        """
+        return nzpyida.filtering.FilterQuery(self.columns, self._name, "ge", value)
+
+    def __gt__(self, value):
+        """
+        ibmdbpy.filtering.FilterQuery object when comparing self using ">".
+        """
+        return nzpyida.filtering.FilterQuery(self.columns, self._name, "gt", value)
+
+    ################ Arithmetic operations
+
+    def __add__(self, other):
+        """
+        Perform an addition between self and another IdaDataFrame or number.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] + 3
+
+        Notes
+        -----
+        Arithmetic operations only make sense if self contains only numeric columns.
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(self, "add", other)
+
+    def __radd__(self, other):
+        """
+        Enable the reflexivity of the addtion operation.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] + 3
+
+        >>> ida = 3 + idadf['sepal_length']
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(other, "add", self, swap = True)
+
+    def __div__(self, other):
+        """
+        Perform a division between self and another IdaDataFrame or number.
+
+        When __future__.division is not in effect.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] / 3
+
+        Notes
+        -----
+        Arithmetic operations only make sense if self contains only numeric columns.
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(self, "div", other)
+
+    def __rdiv__(self, other):
+        """
+        Enable the reflexivity of the division operation.
+
+        When __future__.division is not in effect.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] / 3
+
+        >>> ida = 3 / idadf['sepal_length']
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(other, "div", self, swap = True)
+
+    def __truediv__(self, other):
+        """
+        Perform a division between self and another IdaDataFrame or number.
+
+        When __future__.division is in effect.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] / 3
+
+        Notes
+        -----
+        Arithmetic operations only make sense if self contains only numeric columns.
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(self, "div", other)
+
+    def __rtruediv__(self, other):
+        """
+        Enable the reflexivity of the division operation.
+
+        When __future__.division is in effect.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] / 3
+
+        >>> ida = 3 / idadf['sepal_length']
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(other, "div", self, swap = True)
+
+    def __floordiv__(self,other):
+        """
+        Perform an integer division between self and another IdaDataFrame or number.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] // 3
+
+        Notes
+        -----
+        Arithmetic operations only make sense if self contains only numeric columns.
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(self, "floordiv", other)
+
+    def __rfloordiv__(self, other):
+        """
+        Enable the reflexivity of the integer division operation.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] // 3
+
+        >>> ida = 3 // idadf['sepal_length']
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(other, "floordiv", self, swap = True)
+
+    def __mod__(self,other):
+        """
+        Perform a modulo operation between self and another IdaDataFrame or number.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] % 3
+
+        Notes
+        -----
+        Arithmetic operations make sense if self has only numeric columns.
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(self, "mod", other)
+
+    def __rmod__(self, other):
+        """
+        Enable the reflexivity of the modulo operation.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] % 3
+
+        >>> ida = 3 % idadf['sepal_length']
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(other, "mod", self, swap = True)
+
+    def __mul__(self,other):
+        """
+        Perform a multiplication between self and another IdaDataFrame or number.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] * 3
+
+        Notes
+        -----
+        Arithmetic operations only make sense if self contains only numeric columns.
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(self, "mul", other)
+
+    def __rmul__(self, other):
+        """
+        Enable the reflexivity of the multiplication operation.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] % 3
+
+        >>> ida = 3 % idadf['sepal_length']
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(other, "mul", self, swap = True)
+
+    def __neg__(self):
+        """
+        Calculate the absolute negative of all columns in self.
+
+        Notes
+        -----
+        Arithmetic operations only make sense if self contains only numeric columns.
+        """
+        other = None
+        return nzpyida.aggregation.aggregate_idadf(self, "neg", other)
+
+    def __rpos__(self,other):
+        """
+        Calculate the absolute positive. No operation required.
+        """
+        return self
+
+    def __pow__(self,other):
+        """
+        Perform a power operation between self and another IdaDataFrame or number.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] ** 3
+
+        Notes
+        -----
+        Arithmetic operations only make sense if self contains only numeric columns.
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(self, "pow", other)
+
+    def __rpow__(self, other):
+        """
+        Enable the reflexivity of the power operation.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] ** 3
+
+        >>> ida = 3 ** idadf['sepal_length']
+
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(other, "pow", self, swap = True)
+
+    def __sub__(self,other):
+        """
+        Perform a substraction between self and another IdaDataFrame or number.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] - 3
+
+        Notes
+        -----
+        Arithmetic operations only make sense if self contains only numeric columns.
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(self, "sub", other)
+
+    def __rsub__(self, other):
+        """
+        Enable the reflexivity of the substraction operation.
+
+        Examples
+        --------
+        >>> ida = idadf['sepal_length'] - 3
+
+        >>> ida = 3 - idadf['sepal_length']
+        """
+        self._combine_check(other)
+        return nzpyida.aggregation.aggregate_idadf(other, "sub", self, swap = True)
+
+    #def __truediv__(self,value): ########
+    #    pass
+
+    #def __concat__(self,value): ####
+    #    pass
+
+    #def __contains__(self,value):
+    #    pass
+
+    # TODO: Do the inplace versions (with "i" in front)
+
+###############################################################################
+### Database Features
+###############################################################################
+
+    ##############################
+    ## Delegation from IdaDataBase
+    ##############################
+
+    def exists(self):
+        """
+        Convenience function delegated from IdaDataBase.
+
+        Check if the data still exists in the database.
+        """
+        return self._idadb.exists_table_or_view(self._name)
+
+    def is_view(self):
+        """
+        Convenience function delegated from IdaDataBase.
+
+        Check if the IdaDataFrame corresponds to a view in the database.
+        """
+        if self.type == 'View':
+            return True
+        else:
+            return False
+
+    def is_table(self):
+        """
+        Convenience function delegated from IdaDataBase.
+
+        Check if the IdaDataFrame corresponds to a table in the database.
+        """
+        if self.type == 'Table':
+            return True
+        else:
+            return False
+
+    def get_primary_key(self):
+        # TODO: What happen if the primary key is composed of several columns ?
+        """
+        Get the name of the primary key of self, if there is one. Otherwise,
+        this function returns 0. This function may be deprecated in future
+        versions because it is not very useful.
+        """
+        name = self.internal_state.current_state
+        pk = NULL
+        # on Netezza primary keys are not supported
+        if not self._idadb._is_netezza_system():
+            pk = self.ida_query("SELECT NAME FROM SYSIBM.SYSCOLUMNS WHERE TBNAME = '" +
+                                name + "' AND KEYSEQ > 0 ORDER BY KEYSEQ ASC", first_row_only = True)
+        if pk:
+            return pk[0]
+        else:
+            return False
+
+    # Should we maybe allow this only in IdaDataBase object ?
+    #@timed
+    def ida_query(self, query, silent = False, first_row_only = False, autocommit = False):
+        """
+        Convenience function delegated from IdaDataBase.
+
+        Prepare, execute and format the result of a query in a data frame or in
+        a tuple. See the IdaDataBase.ida_query documentation.
+        """
+        return self._idadb.ida_query(query, silent, first_row_only, autocommit)
+
+    def ida_scalar_query(self, query, silent = False, autocommit = False):
+        """
+        Convenience function delegated from IdaDataBase.
+
+        Prepare and execute a query and return only the first element as a
+        string. See the IdaDataBase.ida_scalar_query documentation.
+
+        """
+        return self._idadb.ida_scalar_query(query, silent, autocommit)
+
+
+
+###############################################################################
+### Data Exploration
+###############################################################################
+
+    def print(self):
+        print(self.internal_state.get_state())
+
+    @idadf_state
+    def head(self, nrow=5, sort=True):
+        """
+        Print the n first rows of the instance, n is set to 5 by default.
+
+        Parameters
+        ----------
+        nrow : int > 0
+            Number of rows to be included in the result.
+
+        sort: default is True
+            If set to True and no indexer is set the data will be
+            sorted by the first numeric column or if no numeric column
+            is available by the first column of the dataframe.
+            If set to False and no indexer is set the row order is not
+            guaranteed and can vary with each execution. For big tables
+            this option might save query processing time.
+
+
+        Returns
+        -------
+        DataFrame or Series
+            The index of the corresponding row number and the columns are all
+            columns of self. If the IdaDataFrame has only one column, it
+            returns a Series.
+
+
+        Examples
+        --------
+        >>> ida_iris.head()
+           sepal_length  sepal_width  petal_length  petal_width species
+        0           5.1          3.5           1.4          0.2  setosa
+        1           4.9          3.0           1.4          0.2  setosa
+        2           4.7          3.2           1.3          0.2  setosa
+        3           4.6          3.1           1.5          0.2  setosa
+        4           5.0          3.6           1.4          0.2  setosa
+        """
+        if (nrow < 1) | (not isinstance(nrow, int)):
+            raise ValueError("Parameter nrow should be an int greater than 0.")
+        else:
+            name = self.internal_state.current_state
+            order = self.internal_state.get_order()
+            if not " ORDER BY " in self.internal_state.get_state():
+                if (self.indexer is not None)&(self.indexer in self.columns):
+                    order = " ORDER BY \"" + str(self.indexer) + "\" ASC"
+                elif self.indexer is None:
+                    if sort:
+                        column = self.columns[0]
+                        if self._get_numerical_columns():
+                            column = self._get_numerical_columns()[0]
+                        order = " ORDER BY \"" + column + "\" ASC"
+                    else:
+                        order = ''
+            data = self.ida_query("SELECT * FROM %s %s LIMIT %s "%(name, order, nrow))
+
+            if data.shape[0] != 0:
+                # otherwise column sort order is reverted
+                if not 'SELECT ' in name:
+                    columns = self.columns
+                    data.columns = columns
+#                data = ibmdbpy.utils._convert_dtypes(self, data)
+                if isinstance(self, nzpyida.IdaSeries):
+                    data = pd.Series(data)
+            return data
+
+    # TODO : There is a warning in anaconda when there are missing values -> why ?
+    @idadf_state
+    def tail(self, nrow=5, sort=True):
+        """
+        Print the n last rows of the instance, n is set to 5 by default.
+
+        Parameters
+        ----------
+        nrow : int > 0
+            The number of rows to be included in the result.
+
+        sort: default is True
+            If set to True and no indexer is set the data will be
+            sorted by the first numeric column or if no numeric column
+            is available by the first column of the dataframe.
+            If set to False and no indexer is set the row order is not
+            guaranteed and can vary with each execution. For big tables
+            this option might save query processing time.
+
+        Returns
+        -------
+        DataFrame
+            The index of the corresponding row number and the columns are all
+            columns of self.
+
+
+        Examples
+        --------
+        >>> ida_iris.tail()
+             sepal_length  sepal_width  petal_length  petal_width    species
+        145           6.7          3.0           5.2          2.3  virginica
+        146           6.3          2.5           5.0          1.9  virginica
+        147           6.5          3.0           5.2          2.0  virginica
+        148           6.2          3.4           5.4          2.3  virginica
+        149           5.9          3.0           5.1          1.8  virginica
+        """
+        if (nrow < 1) | (not isinstance(nrow, int)):
+            raise ValueError("Parameter nrow should be an int greater than 0.")
+        else:
+            column_string = '\"' + '\", \"'.join(self.columns) + '\"'
+
+            name = self.internal_state.current_state
+
+            if " ORDER BY " in self.internal_state.get_state():
+                query = "SELECT * FROM %s LIMIT %s "%(name, nrow)
+                data = self.ida_query(query)
+                data.columns = self.columns
+                data.set_index(data[self.indexer], inplace=True)
+            else:
+                if self._idadb._is_netezza_system():
+                    order = "ORDER BY NULL"
+                else:
+                    order = ""
+                if self.indexer:
+                    sortkey = str(self.indexer)
+                    order = "ORDER BY \"" + sortkey + "\""
+                else:
+                    if sort:
+                        sortkey = self.columns[0]
+                        if self._get_numerical_columns():
+                            sortkey = self._get_numerical_columns()[0]
+                        order = "ORDER BY \"" + sortkey + "\""
+                query = ("SELECT * FROM (SELECT * FROM (SELECT " + column_string +
+                         ", ((ROW_NUMBER() OVER(" + order +
+                         "))-1) AS ROWNUMBER FROM " + name +
+                         ") AS TEMP1 ORDER BY ROWNUMBER DESC LIMIT " + str(nrow) +
+                         " ) AS TEMP2 ORDER BY ROWNUMBER ASC")
+                data = self.ida_query(query)
+                data.set_index(data.columns[-1], inplace=True)  # Set the index
+                data.columns = self.columns
+
+            # del data.index.name
+            #            data = ibmdbpy.utils._convert_dtypes(self, data)
+            if isinstance(self, nzpyida.IdaSeries):
+                data = pd.Series(data[data.columns[0]])
+            return data
+
+    @idadf_state
+    def pivot_table(self, values=None, columns=None, max_entries=1000,
+                    sort=None, factor_threshold=20, interactive=False,
+                    aggfunc='count'):
+        """
+        Compute an aggregation function over all rows of each column that is
+        specified as a value on the dataset. The result grouped by the columns
+        defined in “columns”.
+
+        Parameters
+        ----------
+        values: str or list or str optional
+            List of columns on which “aggfunc” is computed.
+        columns: str or list or str optional
+            List of columns that is used as an index and by which the
+            dataframe is grouped.
+        max_entries: int, default=1000
+            The maximum number of cells to be part of the output. By default,
+            set to 1000.
+        sort: str, optional
+            Admissible values are: “alpha” and “factors”.
+                * If “alpha”, the index of the output is sorted according to the alphabetical order.
+                * If “factors”, the index of the output will be sorted according to increasing number of the distinct values.
+
+            By default, the index will be sorted in the same order that is specified in “columns” argument.
+
+        factor_threshold: int, default: 20
+            Number of distinct values above which a categorical column should
+            not be considered categorical anymore and under which a numerical
+            column column should not be considered numerical anymore.
+        interactive: bool
+            If True, the user is asked if he wants to display the output, given
+            its size.
+        aggfunc: str
+            Aggregation function to be computed on each column specified in the
+            argument “values”. Admissible values are: “count”, “sum”, “avg”.
+            This entry is not case-sensitive.
+
+        Returns
+        -------
+        Pandas Series with Multi-index (columns)
+
+        Examples
+        --------
+        >>> val = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
+        >>> ida_iris.pivot_table(values= val, aggfunc="avg")
+                      species
+        sepal_length  setosa        5.006
+                      versicolor    5.936
+                      virginica     6.588
+        sepal_width   setosa        3.428
+                      versicolor    2.770
+                      virginica     2.974
+        petal_length  setosa        1.462
+                      versicolor    4.260
+                      virginica     5.552
+        petal_width   setosa        0.246
+                      versicolor    1.326
+                      virginica     2.026
+        dtype: float64
+        """
+        # TODO : Support index
+        from nzpyida.statistics import pivot_table
+        return pivot_table(idadf=self, values=values, columns=columns,
+                           max_entries=max_entries, sort=sort,
+                           factor_threshold=factor_threshold,
+                           interactive=interactive, aggfunc=aggfunc)
+
+    @idadf_state
+    def groupby(self, by):
+        # TODO: create an IdadataFrame groupby
+        raise NotImplementedError()
+
+    @idadf_state
+    def merge(self):
+        raise NotImplementedError()
+
+    @idadf_state
+    def concat(self):
+        raise NotImplementedError()
+
+    @idadf_state
+    def join(self):
+        raise NotImplementedError()
+
+    # TODO : implement NULL FIRST option
+    @idadf_state
+    def sort(self, columns=None, axis=0, ascending=True, inplace=False):
+        """
+        Sort the IdaDataFrame row wise or column wise.
+
+        Parameters
+        ----------
+        columns : str or list of str
+            Columns that should be used to sort the rows in the IdaDataFrame.
+            If columns is set to None and axis to 0, then the IdaDataFrame
+            columns are sorted in lexicographical order.
+        axis : int (0/1)
+             Axis that is sorted. 0 for sorting row wise, 1 for sorting column
+             wise.
+        ascending : bool, default: True
+            Sorting order, True : ascending, False : descending
+        inplace : bool, default: False
+            The current object is modified or creates a modified copy. If
+            False, the function creates a modified copy of the current
+            dataframe. If True,  the function modifies the current dataframe.
+
+        Raises
+        ------
+        ValueError
+            * When sorting by column (column not None), the axis value must be 0 (rows).
+            * A column does not belong to self.
+            * The axis argument has a value other than 0 or 1.
+
+        Notes
+        -----
+        If columns is set to None and axis to 0, this undoes all sorting the
+        IdaDataFrame and returns the original sorting in the Db2 Warehouse
+        database.
+
+        No actual changes are made in Db2 Warehouse, only the querying changes.
+        Everything is registered in an InternalState object. Changes can be
+        observed by using  head and tail function.
+        """
+        if isinstance(columns, six.string_types):
+            columns = [columns]
+
+        # Sanitiy check
+        if columns:
+            if axis != 0:
+                raise ValueError("When sorting by column, axis must be 0 (rows)")
+            for column in columns:
+                if column not in self.columns:
+                    raise ValueError("Column "+column+" is not in "+self._name)
+        if axis not in [0,1]:
+            raise ValueError("Allowed values for axis is 0 (rows) or 1 (columns)")
+
+        if inplace:
+            idadf = self
+        else:
+            idadf = self._clone()
+
+        if axis:
+            # Sort the columns in ascending or descending lexicographic order
+            idadf.internal_state.columndict = OrderedDict(sorted(idadf.internal_state.columndict.items(), reverse = not ascending))
+            idadf.internal_state.update()
+        else:
+            if columns:
+                idadf.internal_state.set_order(columns, ascending)
+                idadf.internal_state.update()
+            else:
+                if isinstance(idadf, nzpyida.IdaSeries):
+                    idadf.internal_state.set_order([idadf.column], ascending)
+                    idadf.internal_state.update()
+                else:
+                    idadf.internal_state.reset_order()
+
+        if not inplace:
+            return idadf
+
+    @idadf_state
+    def levels(self, columns = None):
+        # TODO: Test, doc, name?
+        """
+        Return the numbers of distinct values
+        """
+        if columns is not None:
+            if isinstance(columns, six.string_types):
+                columns = [columns]
+            for column in columns:
+                message = ''
+                if column not in self.columns:
+                    message += "Column %s does not belong to current idadataframe. \n"%column
+                if message:
+                    raise ValueError(message)
+        else:
+            columns = self.columns
+
+        agglist = []
+        for column in columns:
+            agglist.append("COUNT(DISTINCT \"%s\")"%column)
+
+        aggstr = ",".join(agglist)
+
+        query = "SELECT %s FROM %s"%(aggstr, self.name)
+        levels_tuple = self.ida_query(query, first_row_only = True)
+
+        if len(levels_tuple) == 1:
+            return levels_tuple[0]
+
+        result = pd.Series(levels_tuple)
+        result.index = columns
+        return result
+
+
+    #@timed
+    @idadf_state
+    def count_groupby(self, columns = None, count_only = False, having = None):
+        """
+        Count the occurence of the values of a column or group of columns
+        """
+        # TODO: Document, test
+        if columns is not None:
+            if isinstance(columns, six.string_types):
+                columns = [columns]
+            for column in columns:
+                message = ''
+                if column not in self.columns:
+                    message += "Column %s does not belong to current idadataframe. \n"
+                if message:
+                    raise ValueError(message)
+        else:
+            columns = list(self.columns)
+        if having:
+            if not isinstance(having, Number):
+                raise TypeError("having argument should be a number")
+
+        select_columnstr = "\"" + "\",\"".join(columns) + "\", COUNT(*)"
+        if count_only:
+            select_columnstr = "COUNT(*)"
+        groupby_columnstr = "\"" + "\",\"".join(columns) + "\""
+        if having:
+            groupby_columnstr = groupby_columnstr + " HAVING count >= %s"%having
+        data = self.ida_query("SELECT %s AS count FROM %s GROUP BY %s"%(select_columnstr,self.name,groupby_columnstr))
+
+        data.columns = columns + ["count"]
+        return data
+
+    def mean_freq_of_instance(self, columns = None):
+        """
+        Return the average occurence of the values of a column or group of columns
+        """
+        # TODO: Document, test
+        if columns is not None:
+            if isinstance(columns, six.string_types):
+                columns = [columns]
+            for column in columns:
+                message = ''
+                if column not in self.columns:
+                    message += "Column %s does not belong to current idadataframe. \n"
+                if message:
+                    raise ValueError(message)
+        else:
+            columns = list(self.columns)
+
+        select_columnstr = "COUNT(*)"
+        groupby_columnstr = "\"" + "\",\"".join(columns) + "\""
+
+        subquery = "SELECT %s AS count FROM %s GROUP BY %s"%(select_columnstr,self.name,groupby_columnstr)
+        data = self.ida_scalar_query("SELECT AVG(DISTINCT count) FROM (%s)"%subquery)
+
+        return int(data)
+
+#    @timed
+    @idadf_state
+    def unique(self, column):
+        """
+        Return the unique values of a column
+        """
+        # TODO: Document, test
+        if column in self._unique:
+            return self._unique[column]
+
+        #name = self.internal_state.current_state
+
+        if not isinstance(column, six.string_types):
+            raise TypeError("column argument not of string type")
+
+        if column not in self.columns:
+            # idadf.name somewhat false in case of modification
+            raise ValueError("Undefined column \"%s\" in table %s"%(column, self._name))
+
+        result = self.ida_query("SELECT DISTINCT \"%s\" FROM %s"%(column, self.name))
+
+        self._unique[column] = result
+        return result
+
+    # TODO: to implement
+    @timed
+    @idadf_state
+    def info(self, buf=None):
+        """Some information about current IdaDataFrame. NOTIMPLEMENTED"""
+        # There is a lot more
+        #from pandas.core.format import _put_lines
+        raise IdaDataFrameError("NOT IMPLEMENTED")
+
+        if buf is None:  # pragma: no cover
+            buf = sys.stdout
+
+        lines = []
+
+        lines.append(str(type(self)))
+        lines.append(self.index.summary())
+
+        if len(self.columns) == 0:
+            lines.append('Empty %s' % type(self).__name__)
+            _put_lines(buf, lines)
+            return
+
+###############################################################################
+### Descriptive statistics
+###############################################################################
+
+    # TODO: to implement for categorical attributes
+    @timed
+    @idadf_state
+    def describe(self, percentiles=[0.25, 0.50, 0.75]):
+        """
+        A basic statistical summary about current IdaDataFrame. If at least one
+        numerical column exists, the summary includes:
+
+            * The count of non-missing values for each numerical column.
+            * The mean for each numerical column.
+            * The standart deviation for each numerical column.
+            * The minimum and maximum for each numerical column.
+            * A list of percentiles set by the user (default : the quartiles).
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+        percentiles : Float or list of floats, default: [0.25, 0.50, 0.75].
+            percentiles to be computed on numerical columns.
+            All values in percentiles must be > 0  and < 1.
+
+        Returns
+        -------
+        summary: DataFrame, where
+            * Index is the name of the computed values.
+            * Columns are either numerical or categorical columns of self.
+
+        """
+        from nzpyida.statistics import describe
+        return describe(idadf=self, percentiles=percentiles)
+
+
+    @timed
+    @idadf_state
+    def summary(self):
+        """
+        A basic statistical summary about current IdaDataFrame.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+
+
+        Returns
+        -------
+        summary: DataFrame, where
+            * Index is the name of the computed values.
+            * Columns are either numerical or categorical columns of self.
+
+        """
+        from nzpyida.statistics import summary
+        return summary(idadf=self)
+
+    @timed
+    @idadf_state
+    def cov(self):
+        """
+        Compute the covariance matrix, composed of covariance coefficients
+        between all pairs of columns in self.
+        It must have at least two numeric columns.
+
+        Returns
+        -------
+        covariance matrix: DataFrame
+            The axes are the columns of self and the values are the covariance
+            coefficients.
+        """
+        from nzpyida.statistics import cov
+        return cov(idadf=self)
+
+    @timed
+    @idadf_state
+    def corr(self, method="pearson", features=None, ignore_indexer=True):
+        """
+        Compute the correlation matrix, composed of correlation coefficients
+        between all pairs of columns in self.
+        It must have at least two numeric columns.
+
+        Parameters
+        ----------
+        method : str, default: pearson
+            Method to be used to compute the correlation. By default, compute
+            the pearson correlation coefficient. The Spearman rank correlation
+            is also available. Admissible values are: "pearson", "spearman".
+
+        Returns
+        -------
+        correlation matrix: DataFrame
+            The axes are the columns of self and the values are the correlation
+            coefficients.
+
+        Notes
+        -----
+        For the Spearman rank correlation, the ordinal rank of columns is
+        computed. For performance reasons this is easier to compute than the
+        fractional rank traditionally computed for the Spearman rank
+        correlation method. This strategy has the property that the sum of the
+        ranking numbers is the same as under ordinal ranking. We then apply
+        the pearson correlation coefficient method to these ranks.
+        """
+        from nzpyida.statistics import corr
+        #return corr(idadf=self, features=features, ignore_indexer=ignore_indexer)
+        return corr(idadf=self)
+
+    @timed
+
+    def train_test_split(self, train_table, test_table, id, fraction, seed):
+        """
+        Split the table into train and test sets
+
+        Parameters
+        ----------
+        train_table : str
+            the output table that will contain the given fraction of the input records
+
+        test_table :str
+            the output table that will contain the rest (1-<fraction>) of the input records
+
+        id : str
+            the input table column identifying a unique instance id
+
+        fraction: float
+             the fraction of the data to split
+
+        seed: float
+            the seed of the random function
+
+        Returns
+        -------
+        number of records in train table: float
+
+
+
+        """
+        from nzpyida.statistics import train_test_split
+
+        return train_test_split(idadf=self, train_table=train_table, test_table=test_table, id=id, fraction=fraction, seed=seed)
+
+    # TODO: to implement
+    @timed
+    @idadf_state
+    def corrwith(self, other):
+        """
+        Compute the correlation matrix, composed of correlation coefficients
+        between the columns of self and the columns of another IdaDataFrame.
+
+        Parameters
+        ----------
+        other : DataFrame
+
+        Returns
+        -------
+        correlation matrix: DataFrame
+            The columns are the columns of self and the index the columns
+            of other. The values are the covariance coefficients.
+        """
+        raise NotImplementedError("TODO")
+
+    # TODO: to implement
+    @timed
+    @idadf_state
+    def mode(self):
+        """
+        Compute the most common value for each non numeric column self. NOTIMPLEMENTED
+
+        Returns
+        -------
+        mode: Series
+        """
+        raise NotImplementedError("TODO")
+
+    @timed
+    @idadf_state
+    def quantile(self, q=0.5):
+        """
+        Compute row wise quantiles for each numeric column.
+
+        Parameters
+        ----------
+        q : float or array-like, default 0.5 (50% quantile)
+            0 <= q <= 1, the quantile(s) to compute
+
+        Returns
+        -------
+        quantiles: Series or DataFrame
+            If q is an array, the function returns a DataFrame in which the
+            index is q. The columns are the columns of sel, and the values are
+            the quantiles. If q is a float, a Series is returned where the
+            index is the columns of self and the values are the quantiles.
+
+        """
+        from nzpyida.statistics import quantile
+        return quantile(idadf=self, q=q)
+
+    @timed
+    @idadf_state
+    def mad(self):
+        """
+        Compute the mean absolute distance for all numeric columns of self.
+        It must have at least two numeric columns.
+
+        Returns
+        -------
+        mad: Series
+            The index consists of the columns of self and the values are the mean absolute distance.
+        """
+        from nzpyida.statistics import mad
+        return mad(idadf=self)
+
+    @timed
+    @idadf_state
+    def min(self):
+        """
+        Compute the minimum value for all numerics column of self.
+
+        Returns
+        -------
+        min: Series
+            The index consists of the columns of self and the values are the minimum.
+        """
+        from nzpyida.statistics import ida_min
+        return ida_min(idadf=self)
+
+    @timed
+    @idadf_state
+    def max(self):
+        """
+        Compute the maximum value over for all numeric columns of self.
+
+        Returns
+        -------
+        max: Series.
+            The index consists of the columns of self and the values are the maximum.
+        """
+        from nzpyida.statistics import ida_max
+        return ida_max(idadf=self)
+
+    @timed
+    @idadf_state
+    def count(self):
+        """
+        Compute the count of non-missing values for all columns of self.
+
+        Returns
+        -------
+        count: Series.
+            The index consists of the columns of self and the values are the number of non-missing values.
+        """
+        from nzpyida.statistics import count
+        return count(idadf=self)
+
+    @timed
+    @idadf_state
+    def count_distinct(self):
+        # deprecated, use levels instead
+        """
+        Compute the count of distinct values for all numeric columns of self.
+
+        Returns
+        -------
+        disctinct count: Series
+            The index consists of the columns of self and values are the number of distinct values.
+        """
+        from nzpyida.statistics import count_distinct
+        return count_distinct(idadf=self)
+
+    @timed
+    @idadf_state
+    def std(self):
+        """
+        Compute the standard deviation for all numeric columns of self.
+
+        Returns
+        -------
+        std: Series
+            The index consists of the columns of self and the values are the standard deviation.
+        """
+        from nzpyida.statistics import std
+        return std(idadf=self)
+
+    @timed
+    @idadf_state
+    def within_class_var(self, target, features = None, ignore_indexer=True):
+        if features is None:
+            numerical_columns = self._get_numerical_columns()
+            features = [x for x in numerical_columns if x != target]
+        else:
+            if isinstance(features, six.string_types):
+                features = [features]
+
+        if ignore_indexer is True:
+            if self.indexer:
+                if self.indexer in features:
+                    features.remove(self.indexer)
+
+        result = pd.Series()
+
+        #C = self.levels(target)
+        N = len(self)
+        if self._idadb._is_netezza_system():
+            power_function = "POW"
+        else:
+            power_function = "POWER"
+
+        if len(features) < 5:
+
+            avglist = ["AVG(\"%s\") as \"average%s\""%(feature, index) for index, feature in enumerate(features)]
+            sumlist = ["SUM(CAST(%s(\"%s\" - \"average%s\", 2) as DOUBLE))"%(power_function, feature, index) for index, feature in enumerate(features)]
+
+            avgstr = ", ".join(avglist)
+            sumstr = ", ".join(sumlist)
+
+            subquery = "(SELECT \"%s\", %s FROM %s GROUP BY \"%s\") AS B"%(target, avgstr, self.name, target)
+            condition = "ON A.\"%s\" = B.\"%s\""%(target, target)
+            groupby = "GROUP BY A.\"%s\""%target
+            query = "SELECT %s FROM %s AS A INNER JOIN %s %s %s"%(sumstr, self.name, subquery, condition, groupby)
+
+            classvar = self.ida_query(query)
+            if len(features) == 1:
+                classvar = pd.DataFrame(classvar)
+
+            C = len(classvar)
+            if N == C:
+                N += 1
+
+
+            classvar.columns = pd.Index(features)
+            result = pd.Series()
+            for attr in classvar:
+                result[attr] = classvar[attr].sum()/(N -C)
+
+        else:
+            chunkgen = chunklist(features, 5)
+            result = pd.Series()
+            for chunk in chunkgen:
+                avglist = ["AVG(\"%s\") as \"average%s\""%(feature, index) for index, feature in enumerate(chunk)]
+                sumlist = ["SUM(CAST(%s(\"%s\" - \"average%s\", 2) as DOUBLE))"%(power_function, feature, index) for index, feature in enumerate(chunk)]
+
+                avgstr = ", ".join(avglist)
+                sumstr = ", ".join(sumlist)
+
+                subquery = "(SELECT \"%s\", %s FROM %s GROUP BY \"%s\") AS B"%(target, avgstr, self.name, target)
+                condition = "ON A.\"%s\" = B.\"%s\""%(target, target)
+                groupby = "GROUP BY A.\"%s\""%target
+                query = "SELECT %s FROM %s AS A INNER JOIN %s %s %s"%(sumstr, self.name, subquery, condition, groupby)
+
+                classvar = self.ida_query(query)
+
+                if len(chunk) == 1:
+                    classvar = pd.DataFrame(classvar)
+
+                C = len(classvar)
+                if N == C:
+                    N += 1
+
+                classvar.columns = pd.Index(chunk)
+
+                for attr in classvar:
+                    result[attr] = classvar[attr].sum()/(N -C)
+
+        return result
+
+    @timed
+    @idadf_state
+    def within_class_std(self, target, features = None, ignore_indexer= True):
+        return np.sqrt(self.within_class_var(target, features, ignore_indexer))
+
+    @timed
+    @idadf_state
+    def var(self):
+        """
+        Compute the variance for all numeric columns of self.
+
+        Returns
+        -------
+        var: Series
+            The index consists of the columns of self and the values are the variance.
+        """
+        from nzpyida.statistics import var
+        return var(idadf=self)
+
+    @timed
+    @idadf_state
+    def mean(self):
+        """
+        Compute the mean for each numeric columns of self.
+
+        Returns
+        -------
+        mean: Series
+            The index consists of the columns of self and the values are the mean.
+        """
+        from nzpyida.statistics import mean
+        return mean(idadf=self)
+
+    @timed
+    @idadf_state
+    def mean_groupby(self, groupby, features = None):
+        if features is None:
+            features = [x for x in self.columns if x != groupby]
+        else:
+            if isinstance(features, six.string_types):
+                features = [features]
+
+        avglist = ["CAST(AVG(\"%s\") as FLOAT) AS \"%s_AVG\""%(feature, feature) for feature in features]
+        avgstr = ", ".join(avglist)
+
+        query = "SELECT \"%s\", %s FROM %s GROUP BY \"%s\""%(groupby, avgstr, self.name, groupby)
+        result = self.ida_query(query)
+        result = result.pivot_table(index = result.columns[0])
+        result.columns = pd.Index(features)
+        return result
+
+
+    @timed
+    @idadf_state
+    def sum(self):
+        """
+        Compute the sum of values for all numeric columns of self.
+
+        Returns
+        -------
+        sum: Series
+            The index consists of the columns of self and the values are the sum.
+        """
+        # Behave like having the option "numeric only" to true
+        # TODO: Implement the option
+        from nzpyida.statistics import ida_sum
+        return ida_sum(idadf=self)
+
+    @timed
+    @idadf_state
+    def median(self):
+        """
+        Compute the median for all numeric columns of self.
+
+        Returns
+        -------
+        median: Series
+            The index consists of the columns of self and the values are the median.
+        """
+        # Behave like having the option "numeric only" to true
+        # TODO: Implement the option
+        from nzpyida.statistics import median
+        return median(idadf=self)
+
+    # Maybe not be possible
+    @idadf_state
+    def rank(self):
+        """Compute the rank over all entries for all columns of self."""
+        raise NotImplementedError("TODO")
+
+    # TODO : cumsum, cummean, cummcountm cummax, cumprod
+
+###############################################################################
+### Save current IdaDataFrame to Db2 Warehouse as a table
+###############################################################################
+
+    # TODO: Should this function be in IdaDataBase ?
+    # To my mind, it is more intuitive to let it here, but it is "destructive"
+    @timed
+    @idadf_state
+    def save_as(self, tablename, clear_existing = False):
+        """
+        Save self as a table name in the remote database with the name
+        tablename. This function might erase an existing table if tablename
+        already exists and clear_existing is True.
+
+        """
+        # TODO: to test !
+        if tablename == self.tablename:
+            if clear_existing is False:
+                raise ValueError("Cannot overwrite current IdaDataFrame if "+
+                                " clear_existing option set to False.")
+            message = "Table %s already exists."%(tablename)
+            warnings.warn(message, UserWarning)
+            question = "Are you sure that you want to overwrite %s"%(tablename)
+            display_yes = nzpyida.utils.query_yes_no(question)
+            if not display_yes:
+                return
+            tempname = self._idadb._get_valid_tablename()
+            self._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s) WITH DATA"%(tempname, tablename))
+            try:
+                self._idadb.drop_table(tablename)
+            except:
+                self._idadb.drop_view(tablename)
+
+            newidadf = IdaDataFrame(self._idadb, tempname)
+            self._idadb.rename(newidadf, tablename)
+            self = newidadf
+
+        if self._idadb.exists_table_or_view(tablename):
+            if clear_existing:
+                message = "Table %s already exists."%(tablename)
+                warnings.warn(message, UserWarning)
+                question = "Are you sure that you want to overwrite %s"%(tablename)
+                display_yes = nzpyida.utils.query_yes_no(question)
+                if not display_yes:
+                    return
+                try:
+                    self._idadb.drop_table(tablename)
+                except:
+                    self._idadb.drop_view(tablename)
+            else:
+                raise NameError(("%s already exists, choose a different name "+
+                                "or use clear_existing option.")%tablename)
+
+        name = self.internal_state.current_state
+
+        self._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s) WITH DATA"%(tablename, name))
+
+        # Reset the cache
+        self._idadb._reset_attributes("cache_show_tables")
+
+###############################################################################
+### Import as DataFrame
+###############################################################################
+
+    @timed
+    @idadf_state
+    def as_dataframe(self):
+        """
+        Download and return an in-memory representation of the dataset as
+        a Pandas DataFrame.
+
+        Returns
+        -------
+        DataFrame
+            Columns and records are the same as in self.
+
+
+        Examples
+        --------
+        >>> iris = ida_iris.as_dataframe()
+        >>> iris.head()
+           sepal_length  sepal_width  petal_length  petal_width species
+        0           5.1          3.5           1.4          0.2  setosa
+        1           4.9          3.0           1.4          0.2  setosa
+        2           4.7          3.2           1.3          0.2  setosa
+        3           4.6          3.1           1.5          0.2  setosa
+        4           5.0          3.6           1.4          0.2  setosa
+        """
+        if os.environ['VERBOSE'] == 'True':
+            # We use an empirical estimation
+            # Experimental results :
+            # 1M row, 12 columns => 550 secs
+            # 3.77M, 12 columns => 2256 secs
+            raw_estimation = ((self.shape[0]*self.shape[1]) / 4000000) * 3
+            if raw_estimation < 0.01:
+                # There is a small offset, which is negligeable for higher measurements
+                estimation = str(raw_estimation*60 + 0.717) + " seconds."
+            elif raw_estimation > 60:
+                estimation = str(raw_estimation/60) + " hours."
+            else:
+                estimation = "%s minutes."%raw_estimation
+            print("Your IdaDataFrame has %s rows and %s columns." % (self.shape[0], self.shape[1]))
+            print("Estimated download time : ~ " + estimation)
+            if raw_estimation > 30:
+                message = ("Estimated Download time is greater than" +
+                           " %s minutes.")%raw_estimation
+                warnings.warn(message, UserWarning)
+                question = "Do you want to download the dataset?"
+                display_yes = nzpyida.utils.query_yes_no(question)
+                if not display_yes:
+                    return
+
+        data = self.ida_query(self.internal_state.get_state())
+        data.columns = self.columns
+        data.name = self.tablename
+        # Handle datatypes
+#        data = ibmdbpy.utils._convert_dtypes(self, data)
+        return data
+
+###############################################################################
+### Connection Management
+###############################################################################
+
+# Connection is a bit different, this is made to allow user to work on several
+# IdaDataFrame and close them, without closing the connection. This is why
+# function "close" is overwritten and a function reopen is provided
+
+    # Not sure commit and rollback should be here ?
+
+    def commit(self):
+        """
+        Commit operations in the database.
+
+        Notes
+        -----
+
+        All changes that are made in the database after the last commit,
+        including those in the child IdaDataFrames, are commited.
+
+        If the environment variable ‘VERBOSE’ is set to True, the commit
+        operations are notified in the console.
+        """
+        self._idadb.commit()
+
+    def rollback(self):
+        """
+        Rollback operations in the database.
+
+        Notes
+        -----
+
+        All changes that are made in the database after the last commit,
+        including those in the child IdaDataFrames, are discarded.
+        """
+        self._idadb.rollback()
+
+
+###############################################################################
+### Private functions
+###############################################################################
+
+    def _clone(self):
+        """
+        Clone the actual object.
+        """
+        newida = IdaDataFrame(self._idadb, self._name, self.indexer)
+        newida.columns = self.columns
+        newida.dtypes = self.dtypes     # avoid recomputing it
+        # otherwise risk of infinite loop between
+        # idadf.columns and internalstate.columndict
+
+        # This is not possible to use deepcopy on an IdaDataFrame object
+        # because the reference to the parents IdaDataBase with the connection
+        # object is not pickleable. As a consequence, we create a new
+        # IdaDataFrame and copy all the relevant attributes
+
+        newida.internal_state.name = deepcopy(self.internal_state.name)
+        newida.internal_state.ascending = deepcopy(self.internal_state.ascending)
+        #newida.internal_state.views = deepcopy(self.internal_state.views)
+        newida.internal_state._views = deepcopy(self.internal_state._views)
+        newida.internal_state._cumulative = deepcopy(self.internal_state._cumulative)
+        newida.internal_state.order = deepcopy(self.internal_state.order)
+        newida.internal_state.columndict = deepcopy(self.internal_state.columndict)
+        return newida
+
+    def _clone_as_serie(self, column):
+        """
+        Clone the actual object as an IdaSeries and select one of its columns.
+        """
+        newida = nzpyida.IdaSeries(idadb = self._idadb, tablename = self._name,
+                                   indexer = self.indexer, column = column)
+
+        newida.internal_state.name = deepcopy(self.internal_state.name)
+        newida.internal_state.ascending = deepcopy(self.internal_state.ascending)
+        #newida.internal_state.views = deepcopy(self.internal_state.views)
+        newida.internal_state._views = deepcopy(self.internal_state._views)
+        newida.internal_state._cumulative = deepcopy(self.internal_state._cumulative)
+        newida.internal_state.order = deepcopy(self.internal_state.order)
+        newida.internal_state.columndict = deepcopy(self.internal_state.columndict)
+        return newida
+
+    def _get_type(self):
+        """
+        Type of the IdaDataFrame : "Table", "View" or "Unknown".
+        """
+        if self._idadb.is_table(self._name):
+            return "Table"
+        elif self._idadb.is_view(self._name):
+            return "View"
+        else:
+            return "Unkown"
+
+    def _get_columns(self):
+        """
+        Index containing a list of the columns in self.
+        """
+        tablename = self.internal_state.current_state
+        if self._idadb._con_type == 'odbc':
+            if '.' in tablename:
+                schema = tablename.split('.')[-2]
+                tablename = tablename.split('.')[-1]
+            else:
+                schema = self._idadb.current_schema
+            columns = self._idadb._con.cursor().columns(table=tablename, schema=schema)
+            columnlist = [column[3] for column in columns]
+            return Index(columnlist)
+        elif self._idadb._con_type == 'nzpy':
+            cursor = self._idadb._con.cursor()
+            cursor.execute("SELECT * FROM %s LIMIT 1"%tablename)
+            columnlist = []
+            if type(cursor.description[0][0]) == bytes:
+                columnlist = [column[0].decode() for column in cursor.description]
+            else:
+                columnlist = [column[0] for column in cursor.description]
+            cursor.close()
+            return Index(columnlist)
+        elif self._idadb._con_type == 'jdbc':
+            cursor = self._idadb._con.cursor()
+            cursor.execute("SELECT * FROM %s LIMIT 1"%tablename)
+            columnlist = [column[0] for column in cursor.description]
+            cursor.close()
+            return Index(columnlist)
+
+    def _get_all_columns_in_table(self):
+        """Get all columns that exists in the physical table."""
+        return self._get_columns()
+
+    # TODO: To deprecate
+    def _get_index(self, force=False):
+        """
+        Index containing a list of the row names in self.
+        """
+
+        rows = self.shape[0]
+
+        return RangeIndex(0, rows, 1)
+
+    def _get_shape(self):
+        """
+        Tuple containing the number of rows and the number of columns in self.
+        """
+        name = self.internal_state.current_state
+
+        nrow = self.ida_scalar_query("SELECT CAST(COUNT(*) AS INTEGER) FROM %s"%name)
+        ncol = len(self.columns)
+        return (nrow, ncol)
+
+    def _get_columns_dtypes(self):
+        """
+        DataFrame containing the column names and database types in self.
+        """
+        name = self.internal_state.current_state
+
+        # In case the name is composed the following way : SCHEMA.TABLENAME
+        # We need to separate it to keep only the TABLENAME for this query.
+        if '.' in name :
+            name = name.split('.')[-1]
+
+        if self._idadb._is_netezza_system():
+            nz_hidden_columns =  "('ROWID', 'DATASLICEID', 'CREATEXID', 'DELETEXID', '_PAGEID', '_EXTENTID') "
+            query_select = ("SELECT COLUMN_NAME AS COLNAME, " +
+                            "CASE WHEN strpos(TYPE_NAME, '(') = 0 THEN TYPE_NAME " +
+                            "ELSE substr(TYPE_NAME, 1, strpos(TYPE_NAME,'(')-1) END AS TYPENAME " +
+                            "FROM _V_SYS_COLUMNS ")
+            # hidden Netezza columns are not included in the set of columns of this data frame
+            query_where1 = "WHERE TABLE_NAME = \'%s\' AND COLUMN_NAME NOT IN " + nz_hidden_columns
+            query_where2 = "AND SCHEMA = \'%s\' "
+            query_order_by = "ORDER BY ORDINAL_POSITION "
+        else:
+            query_select = "SELECT COLNAME, TYPENAME FROM SYSCAT.COLUMNS "
+            query_where1 = "WHERE TABNAME = \'%s\' "
+            query_where2 = "AND TABSCHEMA = \'%s\' "
+            query_order_by = "ORDER BY COLNO"
+
+        if name.find("TEMP_VIEW_") == 0:
+            #When the column names are going to be retrieved from a temporary
+            #view that was created with the definition of the current state of
+            #the IdaDataFrame, the schema name cannot be assumed as the same of
+            #the IdaDataFrame. Also mind that the name of the temporary view
+            #is thought to be random enough to avoid collisions
+            data = self.ida_query((query_select + query_where1 + query_order_by)%(name))
+        else:
+            data = self.ida_query((query_select + query_where1 + query_where2 + query_order_by)%(name, self.schema))
+
+        # Workaround for some ODBC version which does not get the entire
+        # string of the column name in the cursor descriptor.
+        # This is hardcoded, so be careful
+        data.columns = ["COLNAME", "TYPENAME"]
+        data.columns = [x.upper() for x in data.columns]
+        data.set_index(keys='COLNAME', inplace=True)
+        # del data.index.name
+        return data
+
+    def _reset_attributes(self, attributes):
+        """
+        Delete an attribute of self to force its refreshing at the next call.
+        """
+        if isinstance(attributes, six.string_types):
+            attributes = [attributes]
+
+        # Special case : resetting columns
+        if "columns" in attributes:
+             try:
+                 del self.internal_state.columndict
+             except:
+                 pass
+
+        nzpyida.utils._reset_attributes(self, attributes)
+
+
+###############################################################################
+### DB2 Warehouse to pandas type mapping
+###############################################################################
+
+    def _table_def(self, factor_threshold=None):
+        """
+        Classify columns in the idaDataFrame into 4 classes: CATEGORICAL,
+        STRING, NUMERIC or NONE. Use the database data type and a
+        user-threshold “factor_threshold”:
+
+            * CATEGORICAL columns that have a number of distinct values that is greater
+              than the factor_threshold should be considered a STRING.
+            * NUMERIC columns that have a number of distinct values that is smaller or equal
+              to the factor_threshold should be considered CATEGORICAL.
+
+        Returns
+        -------
+        DataFrame
+
+            * Index is the columns of self.
+            * Column "FACTORS" contains the number of distinct values.
+            * Column "VALTYPE" contains the resulting class.
+
+        Examples
+        --------
+        >>> ida_iris._table_def()
+                     TYPENAME FACTORS      VALTYPE
+        sepal_length   DOUBLE      35      NUMERIC
+        sepal_width    DOUBLE      23      NUMERIC
+        petal_length   DOUBLE      43      NUMERIC
+        petal_width    DOUBLE      22      NUMERIC
+        species       VARCHAR       3  CATEGORICAL
+        """
+        # We don't want to change the value of the attribute
+        data = deepcopy(self.dtypes)
+
+        def _valtype_from_dbtype(tup):
+            """
+            Decides if a column should be considered categorical or numerical
+            """
+            categorical_attributes = ['VARCHAR', 'CHARACTER VARYING', 'CHARACTER', 'VARGRAPHIC',
+                                      'GRAPHIC', 'CLOB']
+            numerical_attributes = ['SMALLINT', 'INTEGER', 'BIGINT', 'REAL',
+                                    'DOUBLE', 'DOUBLE PRECISION', 'FLOAT', 'DECIMAL', 'NUMERIC']
+            if tup[0] in categorical_attributes:
+                if factor_threshold is None:
+                    return "CATEGORICAL"
+                elif tup[1] <= factor_threshold:
+                    return "CATEGORICAL"
+                else:
+                    return "STRING"
+            elif tup[0] in numerical_attributes:
+                if factor_threshold is None:
+                    return "NUMERIC"
+                elif tup[1] > factor_threshold:
+                    return "NUMERIC"
+                else:
+                    return "CATEGORICAL"
+            else:
+                return "NONE"
+
+        data['FACTORS'] = nzpyida.statistics._count_level(self, data.index.values)
+        data['VALTYPE'] = [_valtype_from_dbtype(x) for x in
+                           data[['TYPENAME', 'FACTORS']].to_records(index=0)]
+        return data
+
+    def _get_numerical_columns(self):
+        """
+        Get the columns of self that are considered as numerical. Their data
+        type in the database determines whether these columns are numerical.
+        The following data types are considered numerical:
+
+            SMALLINT, INTEGER, BIGINT, REAL, DOUBLE, FLOAT, DECIMAL, NUMERIC.
+
+        Returns
+        -------
+        list
+            List of numerical column names.
+
+        Examples
+        --------
+        >>> ida_iris._get_numerical_columns()
+        ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
+        """
+        num = ['SMALLINT', 'INTEGER', 'BIGINT', 'REAL',
+                            'DOUBLE', 'DOUBLE PRECISION', 'FLOAT', 'DECIMAL', 'NUMERIC']
+        return list(self.dtypes.loc[self.dtypes['TYPENAME'].isin(num)].index)
+
+    def _get_categorical_columns(self):
+        """
+        Get the columns of self that are considered as categorical. Their data
+        type in the database determines whether these columns are categorical.
+        The following data types are considered categorical:
+
+            VARCHAR,CHARACTER, VARGRAPHIC, GRAPHIC, CLOB.
+
+        Returns
+        -------
+        list
+            List of categorical column names.
+
+        Examples
+        --------
+        >>> ida_iris._get_categorical_columns()
+        ['species']
+        """
+        cat = ['VARCHAR', 'CHARACTER', 'VARGRAPHIC', 'GRAPHIC', 'CLOB']
+        return list(self.dtypes.loc[self.dtypes['TYPENAME'].isin(cat)].index)
+
+    def _prepare_and_execute(self, query, autocommit = True, silent = False):
+        """
+        Prepare and execute a query.
+
+        Parameters
+        ----------
+        query : str
+            Query to be executed.
+        autocommit : bool
+             If set to true, the autocommit function is available.
+        """
+        return self._idadb._prepare_and_execute(query, autocommit, silent)
+
+    def _autocommit(self):
+        """
+        Autocommit the connection. If the environment variable ‘AUTOCOMMIT’ is
+        set to True, the function commits the changes.
+
+        Notes
+        -----
+
+        If you commit, all changes that are made in the database after the last
+        commit, including those in the child IdaDataFrames, are commited.
+
+        If the environment variable ‘VERBOSE’ is set to True, the autocommit
+        operations are notified in the console.
+        """
+        self._idadb._autocommit()
+
+    def _combine_check(self, other):
+        """
+        Check if self and other refer to the same table and if all columns in
+        self and other are numeric. This sanity check is used before performing
+        aggregation operations between IdaDataFrame/IdaSeries.
+        """
+        def check_numeric_columns(idaobject):
+            not_valid = []
+            numeric_columns = idaobject._get_numerical_columns()
+            for column in idaobject.columns:
+                if column not in numeric_columns:
+                    not_valid.append(column)
+            if not_valid:
+                raise TypeError("Arithmetic operation are not defined for %s"%not_valid)
+
+        if isinstance(other, IdaDataFrame) | isinstance(other, nzpyida.IdaSeries):
+            if self._name != other._name:
+                raise IdaDataFrameError("It is not possible to aggregate columns using columns of a different table.")
+
+        if not(isinstance(other, IdaDataFrame) | isinstance(other, nzpyida.IdaSeries) | isinstance(other, Number)):
+            if other is not None:
+                raise TypeError("Aggregation makes only sense with numbers, "+
+                                "or IdaDataFrames refering to the same table.")
+
+        check_numeric_columns(self)
+        if isinstance(other, nzpyida.IdaSeries)|isinstance(other, IdaDataFrame):
+            check_numeric_columns(other)
+
+    
+    def delete_na(self, columns, logic="any", inplace=False):
+        """
+        Filter rows containing NULL values. Can be done in a destructive way (rows are deleted in the physical table)
+        or in a non destructive way (a new IdaDataFrame is defined, original table and IdaDataFrame are preserved). 
+        
+        Parameters
+        -----------
+        columns : list of strings
+            list of eligible column names
+        logic : str, optional
+            "any" by default. 
+            If logic is set to "any" then all rows which contain a NaN value in any 
+            (id est at least one) of the cited columns is deleted, this is a union condition;
+            if logic is set to "all", then only rows containing null values in all the fields are deleted, this is an intersection condition.         
+        inplace : bool, optional 
+            False by default. 
+            If True, then the underlying table is physically modified,
+            if False, then the original objects remain unaffected, a copy of the IdaDataFrame is made and modified. 
+        
+        Returns
+        --------
+        If inplace is True, the original table is modified and the IdaDataFrame will be modified accordingly. No return.
+        If inplace is False, a new IdaDataFrame is returned, the original table is not affected. This new IdaDataFrame points to a 
+        new table which has been created according to the user defined criterions on columns.
+        
+        Examples
+        --------
+        >>> idadf.delete_na(["COL_1", "COL_2"], inplace = True)
+        >>> #each row of the original table is physically deleted if there is a NULL value in one of the listed columns
+        >>> new_idadf = idadf.delete_na(["COL_1", "COL_2"], logic = 'all')
+        >>> # a new IdaDataFrame is created, rows will be selected only if all the listed columns have a NULL value.
+        """
+        
+        # Check if list columns is not empty
+        if len(columns)<1:
+            raise IdaDataFrameError("You must specify the columns as a list of eligible names")
+        # Check if column names are eligible
+        for column_name in columns:
+            if not isinstance(column_name, six.string_types):
+                raise TypeError("%s is not of string type"%(column_name))
+            if column_name not in self.columns:
+                raise ValueError("%s refers to a columns that doesn't exists in self"%(column_name))
+        
+        idadb = self._idadb
+        
+        if inplace == True:
+            # Explain
+            print("The table %s will be physically modified." %self.tablename)
+            print("Any IdaDataFrame pointing this table might be modified accordingly.")
+            # Write DELETE query on original table
+            tablename = self.tablename
+            query = 'DELETE FROM %s WHERE "%s" IS NULL'%(tablename, columns[0])
+            
+            if len(columns)>1:
+                if logic == "any":
+                    for col in columns[1:]:
+                        query = query + ' OR "%s" IS NULL'%col
+                if logic == "all":
+                    for col in columns[1:]:
+                        query = query + ' AND "%s" IS NULL'%col                        
+            idadb.ida_query(query)
+
+        else:            
+            # SELECT statement on the original table to create a view
+            tablename0 = self.tablename 
+            query = 'SELECT * FROM %s WHERE "%s" IS NOT NULL'%(tablename0, columns[0])
+            
+            if len(columns)>1:
+                if logic == "any":
+                    for col in columns[1:]:
+                        query = query + ' AND "%s" IS NOT NULL'%col
+                if logic == "all":
+                    for col in columns[1:]:
+                        query = query + ' OR "%s" IS NOT NULL'%col                                               
+            # Create view with this select statement
+            viewname = idadb._get_valid_tablename(prefix="VIEW_")
+            self._prepare_and_execute("CREATE VIEW " + viewname + " AS "+ query)
+            # Initiate the modified table under a random name
+            tablename = idadb._get_valid_tablename(prefix="DATA_FRAME_")
+            idadb._prepare_and_execute("CREATE TABLE %s AS (SELECT * FROM %s) WITH DATA"%(tablename,viewname))
+            print('A new table with filtered rows is available under the name %s.' %tablename)
+            print('The newly created IdaDataFrame refers to this new table.')
+            # Drop the view 
+            idadb.drop_view(viewname)            
+            # Define a new IdaDataFrame pointing to the new table
+            idadf = IdaDataFrame(idadb, tablename, indexer = self.indexer)
+            return idadf
+    
+            
+
+
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/geoFrame.py` & `nzpyida-0.3.3/nzpyida/geoFrame.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,1081 +1,1081 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-IdaGeoDataFrame
-"""
-
-# Ensure Python 2 compatibility
-from __future__ import print_function
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import absolute_import
-from builtins import super
-from builtins import zip
-from builtins import str
-from builtins import int
-from future import standard_library
-standard_library.install_aliases()
-
-import nzpyida
-from nzpyida.frame import IdaDataFrame
-from nzpyida.geoSeries import IdaGeoSeries
-from nzpyida.exceptions import IdaGeoDataFrameError
-
-from copy import deepcopy
-
-import six
-
-
-class IdaGeoDataFrame(IdaDataFrame):
-    """  
-    An IdaGeoDataFrame container inherits from IdaDataFrame.
-
-    It has a property called "geometry" which refers to a column with
-    geometry type. It is set as a string with a column name, either at
-    instantiation time or with the set_geometry() method.
-
-    If the "geometry" property is set, when calling a geospatial method from
-    IdaDataFrame the method will be carried on the column this property refers
-    to.
-
-    The property "geometry" returns an IdaGeoSeries.
-
-    See IdaDataFrame.
-    See IdaGeoSeries.
-
-    Notes
-    -----
-    IdaGeoDataFrame objects are not supported on Netezza.
-
-    Examples
-    --------
-    >>> idageodf = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY',
-    indexer='OBJECTID')
-    >>> idageodf.dtypes
-                     TYPENAME
-    OBJECTID          INTEGER
-    SHAPE     ST_MULTIPOLYGON
-    STATEFP           VARCHAR
-    COUNTYFP          VARCHAR
-    COUNTYNS          VARCHAR
-    NAME              VARCHAR
-    GEOID             VARCHAR
-    NAMELSAD          VARCHAR
-    LSAD              VARCHAR
-    CLASSFP           VARCHAR
-    MTFCC             VARCHAR
-    CSAFP             VARCHAR
-    CBSAFP            VARCHAR
-    METDIVFP          VARCHAR
-    FUNCSTAT          VARCHAR
-    ALAND             DECIMAL
-    AWATER            DECIMAL
-    INTPTLAT          VARCHAR
-    INTPTLON          VARCHAR
-
-    >>> idageodf[['NAME', 'SHAPE']].head()
-           NAME                                              SHAPE
-    0    Becker  MULTIPOLYGON (((-95.1637185512 46.7176480983, ...
-    1  Jim Hogg  MULTIPOLYGON (((-98.9542377853 26.7856984795, ...
-    2     Henry  MULTIPOLYGON (((-88.0532984194 36.4970648458, ...
-    3     Keith  MULTIPOLYGON (((-102.0517705602 41.0038968011,...
-    4   Clinton  MULTIPOLYGON (((-94.2059683962 39.7458481141, ...
-
-    >>> idageodf.geometry
-    AttributeError: Geometry property has not been set yet. Use set_geometry
-    method to set it.
-
-    >>> idageodf.set_geometry('SHAPE')
-    >>> idageodf.geometry.column
-    'SHAPE'
-
-    >>> type(idageodf.geometry)
-    <class 'ibmdbpy.geoSeries.IdaGeoSeries'>
-
-    >>> idageoseries = idageodf.geometry    
-    >>> idageoseries.head()
-    0    MULTIPOLYGON (((-95.1637185512 46.7176480983, ...
-    1    MULTIPOLYGON (((-98.9542377853 26.7856984795, ...
-    2    MULTIPOLYGON (((-88.0532984194 36.4970648458, ...
-    3    MULTIPOLYGON (((-102.0517705602 41.0038968011,...
-    4    MULTIPOLYGON (((-94.2059683962 39.7458481141, ...
-    Name: SHAPE, dtype: object
-
-    >>> idageodf['County area'] = idageodf.area(unit='mile')
-
-    >>> counties_with_areas = idageodf[['NAME', 'SHAPE', 'County area']]
-    >>> counties_with_areas.dtypes
-                        TYPENAME
-    NAME                 VARCHAR
-    SHAPE        ST_MULTIPOLYGON
-    County area           DOUBLE
-
-    >>> counties_with_areas.head()
-            NAME                                             SHAPE  County area
-    0     Menard  MULTIPOLYGON (((-99.4847630885 30.940610279, ...   902.281540
-    1      Boone  MULTIPOLYGON (((-88.7764991497 42.491919892, ...   282.045087
-    2  Ochiltree  MULTIPOLYGON (((-100.5467326897 36.056542135,...   918.188142
-    3    Sharkey  MULTIPOLYGON (((-90.9143429922 33.007703026, ...   435.548518
-    4    Audubon  MULTIPOLYGON (((-94.7006367168 41.504155369, ...   444.827726
-    """
-
-    def __init__(self, idadb, tablename, indexer = None, geometry = None):
-        """
-        Constructor for IdaGeoDataFrame objects.
-        See IdaDataFrame.__init__ documentation.
-
-        Parameters
-        ----------
-        geometry : str, optional
-            Column name to set the "geometry" property of the IdaGeoDataFrame.
-            The column must have geometry type.
-
-        Attributes
-        ----------
-        _geometry_colname : str
-            Name of the column that "geometry" property refers to.
-            This attribute must be set through the set_geometry() method.
-        geometry : IdaGeoSeries
-            The column referenced by _geometry_colname attribute.
-        """
-        # TODO: Add support for receiving either a string or an IdaGeoSeries as 
-        # geometry parameter.        
-
-        if (idadb.__class__.__name__ == "IdaDataBase") & idadb._is_netezza_system():
-            raise IdaGeoDataFrameError("IdaGeoDataFrame objects are not supported on Netezza.")
-
-        if geometry is not None and not isinstance(geometry, six.string_types):
-            raise TypeError("geometry must be a string")
-        super(IdaGeoDataFrame, self).__init__(idadb, tablename, indexer)
-        self._geometry_colname = None
-        if geometry is not None:
-            self.set_geometry(geometry)
-
-    def __getitem__(self, item):
-        """
-        Returns an IdaDataFrame, IdaSeries, IdaGeoDataFrame or IdaGeoSeries
-        as appropriate.
-        
-        Returns
-        --------
-        IdaGeoSeries
-            When the projection has only one column and it has geometry type.
-        IdaGeoDataFrame
-            When the projection has more than one column, and the "geometry"
-            column of the IdaGeoDataFrame is included in them.
-        IdaDataFrame
-            When the projection has more than one column, and the "geometry"
-            column of the IdaGeoDataFrame is not included in them.
-        IdaSeries
-            When the projection has only one column and it doesn't have 
-            geometry type.
-        """
-        ida = super(IdaGeoDataFrame, self).__getitem__(item)
-        if isinstance(ida, nzpyida.IdaSeries):
-            if ida.dtypes['TYPENAME'][ida.column].find('ST_') == 0:
-                idageoseries = IdaGeoSeries.from_IdaSeries(ida)
-                # Return IdaGeoSeries
-                return idageoseries
-            else:
-                # Return IdaSeries
-                return ida
-        elif isinstance(ida, nzpyida.IdaDataFrame):
-            if self._geometry_colname in ida.dtypes.index:
-                # Return IdaGeoDataFrame
-                idageodf = IdaGeoDataFrame.from_IdaDataFrame(ida)
-                idageodf._geometry_colname = self._geometry_colname
-                return idageodf
-            else:
-                # Return IdaDataFrame
-                return ida
-
-    def __delitem__(self, item):
-        """
-        Erases the "geometry" property if the column it refers to is deleted.
-        """
-        super(IdaGeoDataFrame, self).__delitem__(item)
-        if item == self._geometry_colname:
-            self._geometry_colname = None
-
-    def __getattr__(self, name):
-        """
-        Carry geospatial method calls on the "geometry" column of the
-        IdaGeoDataFrame, if it was set.
-        
-        Notes
-        -----
-        This method gets called only when an attribute lookup on
-        IdaGeoDataFrame is not resolved, i.e. it is not an instance attribute
-        and it's not found in the class tree.
-        """        
-        
-        if name == 'geometry':
-            # When .geometry is accessed and _geometry_colname is None
-            return self.__getattribute__('geometry')
-
-        if hasattr(IdaGeoSeries, name):
-            # Geospatial method call
-            if self._geometry_colname is None:
-                raise AttributeError("Geometry column has not been set yet.")
-            else:
-                # Get a IdaGeoSeries and carry the operation on it
-                idageoseries = self.__getitem__(item = self._geometry_colname)
-                return idageoseries.__getattribute__(name)
-        else:
-            raise AttributeError
-    
-    @property
-    def geometry(self):
-        """
-        Returns an IdaGeoSeries with the column whose name is stored in 
-        _geometry_colname attribute.
-
-        The setter calls the set_geometry() method.
-
-        Returns
-        -------
-        IdaGeoSeries
-
-        Raises
-        ------
-        AttributeError
-            If the property has not been set yet.
-        
-        """
-        if self._geometry_colname is None:
-            raise AttributeError(
-                "Geometry property has not been set yet. "
-                "Use set_geometry method to set it.")
-        else:
-            return self.__getitem__(self._geometry_colname)
-    
-    @geometry.setter
-    def geometry(self, value):
-        """
-        See set_geometry() method.
-        """
-        self.set_geometry(value)
-
-    @classmethod
-    def from_IdaDataFrame(cls, idadf, geometry = None):
-        """ 
-        Creates an IdaGeoDataFrame from an IdaDataFrame.
-        
-        Parameters
-        ----------
-        geometry : str, optional
-            Column name to set the "geometry" property of the IdaGeoDataFrame.
-            The column must have geometry type.
-
-        Raises
-        ------
-        TypeError
-            If idadf is not an IdaDataFrame.
-        """
-        
-        if not isinstance(idadf, IdaDataFrame):
-            raise TypeError("Expected IdaDataFrame")
-        else:
-            # TODO: check if it's better to only change the .__base__ attribute
-
-            #behavior based on _clone() method of IdaDataFrame
-            newida = IdaGeoDataFrame(
-                    idadf._idadb, idadf._name, idadf.indexer, geometry)
-            newida.columns = idadf.columns 
-            newida.dtypes = idadf.dtypes
-            
-            newida.internal_state.name = deepcopy(idadf.internal_state.name)
-            newida.internal_state.ascending = deepcopy(idadf.internal_state.ascending)
-            #newida.internal_state.views = deepcopy(idadf.internal_state.views)
-            newida.internal_state._views = deepcopy(idadf.internal_state._views)
-            newida.internal_state._cumulative = deepcopy(idadf.internal_state._cumulative)
-            newida.internal_state.order = deepcopy(idadf.internal_state.order)
-            newida.internal_state.columndict = deepcopy(idadf.internal_state.columndict)
-            return newida
-
-    def set_geometry(self, column_name):
-        """
-        Receives a column name to set as the "geometry" column of the
-        IdaDataFrame.
-
-        Parameters
-        -----------
-        column_name : str
-            Name of the column to be set as geometry column of the 
-            IdaDataFrame. It must have geometry type.
-
-        Raises
-        ------
-        KeyError
-            If the column is not present in the IdaGeoDataFrame.
-        TypeError
-            If the column doesn't have geometry type.
-        """
-        if not isinstance(column_name, six.string_types):
-            raise TypeError("column_name must be a string")
-        if column_name not in self.columns:
-            raise KeyError(
-                "'" + column_name + "' cannot be set as geometry column: "
-                "not a column in the IdaGeoDataFrame."
-            )
-        elif self.dtypes.TYPENAME[column_name].find('ST_') != 0:
-            raise TypeError(
-                "'" + column_name + "' cannot be set as geometry column: "
-                "column doesn't have geometry type."
-            )
-        else:
-            self._geometry_colname = column_name
-
-    # ==============================================================================
-    ### Binary geospatial methods
-    # ==============================================================================
-    def equals(self, ida2):
-        """
-        Valid types for the column in the calling IdaGeoDataFrame:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoDataFrame of indices of the two input
-        IdaGeoDataFrames and a result column with 1 or 0 depending
-        upon whether the geometry of the first IdaGeoDataFrame
-        crosses the second.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-
-        References
-        ----------
-        DB2 Spatial Extender ST_CROSSES() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.equals(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          0
-        2            1840         0
-        2            109          0
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_EQUALS',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def distance(self, ida2, unit=None):
-        """
-        Valid types for the column in the calling IdaGeoDataFrame:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoDataFrame of indices of the two input
-        IdaGeoDataFrames and a result column with a numeric value
-        which is the geographic distance measured between the
-        geometries of the input IdaGeoDataFrames.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-        unit : str, optional
-            Name of the unit, it is case-insensitive.
-            If omitted, the following rules are used:
-
-                * If geometry is in a projected or geocentric coordinate
-                  system, the linear unit associated with this coordinate system
-                  is used.
-                * If geometry is in a geographic coordinate system, the angular
-                  unit associated with this coordinate system is used.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_DISTANCE() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.distance(ida2,unit = 'KILOMETER')
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          26.918942
-        2            1840         4.868971
-        2            109          16.387094
-        """
-        add_args = None
-        if unit is not None:
-            unit = self._check_linear_unit(unit)  # Can raise exceptions
-            add_args = []
-            add_args.append(unit)
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_DISTANCE',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'],
-            additional_args = add_args)
-
-    def crosses(self, ida2):
-        """
-        Valid types for the column in the calling IdaGeoDataFrame:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoDataFrame of indices of the two input
-        IdaGeoDataFrames and a result column with 1 or 0 depending
-        upon whether the geometry of the first IdaGeoDataFrame
-        crosses the second.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_CROSSES() function.
-
-        See also
-        --------
-        linear_units : list of valid units.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.crosses(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          0
-        2            1840         0
-        2            109          0
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_CROSSES',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def intersects(self, ida2):
-        """
-        Valid types for the column in the calling IdaGeoDataFrame:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoDataFrame of indices of the two input
-        IdaGeoDataFrames and a result column with 1 or 0 depending
-        upon whether the geometries of the input IdaGeoDataFrames
-        intersect each other.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_INTERSECTS() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.intersects(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          0
-        2            1840         0
-        2            109          0
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_INTERSECTS',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def overlaps(self, ida2):
-        """
-        Valid types for the column in the calling IdaGeoDataFrame:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoDataFrame of indices of the two input
-        IdaGeoDataFrames and a result column with 1 or 0 depending
-        upon whether the geometries of the input IdaGeoDataFrames
-        overlap each other.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_OVERLAPS() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.overlaps(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          0
-        2            1840         0
-        2            109          0
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_OVERLAPS',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def touches(self, ida2):
-        """
-        Valid types for the column in the calling IdaGeoDataFrame:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoDataFrame of indices of the two input
-        IdaGeoDataFrames and a result column with 1 or 0 depending
-        upon whether the boundary of the first geometry touches
-        the second.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_TOUCHES() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.touches(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          0
-        2            1840         0
-        2            109          0
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_TOUCHES',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def disjoint(self, ida2):
-        """
-        Valid types for the column in the calling IdaGeoDataFrame:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoDataFrame of indices of the two input
-        IdaGeoDataFrames and a result column with 1 or 0 depending
-        upon whether the geometries in the input dataframes are
-        disjoint.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_DISJOINT() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.disjoint(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          1
-        2            1840         1
-        2            109          1
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_DISJOINT',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def contains(self, ida2):
-        """
-        Valid types for the column in the calling IdaGeoDataFrame:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoDataFrame of indices of the two input
-        IdaGeoDataFrames and a result column with 1 or 0 depending
-        upon whether the second geometry contains the first.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_CONTAINS() function.
-
-        Examples
-        --------
-        >>> idageodf_customer = IdaGeoDataFrame(idadb,'SAMPLES.GEO_CUSTOMER',indexer='OBJECTID')
-        >>> idageodf_customer.set_geometry('SHAPE')
-        >>> idageodf_county = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> idageodf_county.set_geometry('SHAPE')
-        >>> ida1 = idageodf_customer[idageodf_customer['INSURANCE_VALUE']>250000]
-        >>> ida2 = idageodf_county[idageodf_county['NAME']=='Madison']
-        >>> result = ida2.contains(ida1)
-        >>> result[result['RESULT']==1].head()
-        INDEXERIDA1    INDEXERIDA2    RESULT
-        21473          134            1
-        21413          134            1
-        21414          134            1
-        21417          134            1
-        21419          134            1
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_CONTAINS',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def within(self, ida2):
-        """
-        Valid types for the column in the calling IdaGeoDataFrame:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoDataFrame of indices of the two input
-        IdaGeoDataFrames and a result column with 1 or 0 depending
-        upon whether the first geometry is inside the second.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_WITHIN() function.
-
-        Examples
-        --------
-        >>> idageodf_customer = IdaGeoDataFrame(idadb,'SAMPLES.GEO_CUSTOMER',indexer='OBJECTID')
-        >>> idageodf_customer.set_geometry('SHAPE')
-        >>> idageodf_county = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> idageodf_county.set_geometry('SHAPE')
-        >>> ida1 = idageodf_customer[idageodf_customer['INSURANCE_VALUE']>250000]
-        >>> ida2 = idageodf_county[idageodf_county['NAME']=='Madison']
-        >>> result = ida1.within(ida2)
-        >>> result[result['RESULT']==1].head()
-        INDEXERIDA1    INDEXERIDA2    RESULT
-        134            21473          1
-        134            21413          1
-        134            21414          1
-        134            21417          1
-        134            21419          1
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_WITHIN',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def mbr_intersects(self, ida2):
-        """
-        This method takes a second IdaGeoDataFrame an an input
-        and checks if the Minimum Bounding rectangles of the
-        geometries from both IdaGeoDataFrames intersect and
-        stores the result as 0 or 1 in the RESULT column of
-        the resulting IdaGeoDataFrame.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MBRIntersects() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.difference(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          0
-        2            1840         0
-        2            109          0
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_MBRINTERSECTS',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def difference(self, ida2):
-        """
-        This method takes a second IdaGeoDataFrame an an input
-        and returns the difference of the geometries from both
-        IdaGeoDataFrames as a new geometry stored in the RESULT
-        column of the resulting IdaGeoDataFrame.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_Difference() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.difference(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          POLYGON ((-96.6219873342 30.0442882117, -96.61...
-        2            1840         POLYGON ((-96.6219873342 30.0442882117, -96.61...
-        2            109          POLYGON ((-96.6219873342 30.0442882117, -96.61...
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_DIFFERENCE',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def intersection(self, ida2):
-        """
-        This method takes a second IdaGeoDataFrame an an input
-        and returns the intersection of the geometries from both
-        IdaGeoDataFrames as a new geometry stored in the RESULT
-        column of the resulting IdaGeoDataFrame.
-
-        For None geometries the output is None.
-        For empty geometries the output is POINT EMPTY.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_Intersection() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.intersection(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          POINT EMPTY
-        2            1840         POINT EMPTY
-        2            109          POINT EMPTY
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_INTERSECTION',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def union(self, ida2):
-        """
-        This method takes a second IdaGeoDataFrame an an input
-        and returns the union of the geometries from both
-        IdaGeoDataFrames as a new geometry stored in the RESULT
-        column of the resulting IdaGeoDataFrame.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        Returns an IdaGeoDataFrame with three columns:
-
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-
-        Parameters
-        ----------
-        ida2 : IdaGeoDataFrame
-            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
-            will be invoked.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_Union() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> ida1 = counties[counties['NAME'] == 'Austin']
-        >>> ida2 = counties[counties['NAME'] == 'Kent']
-        >>> result = ida1.union(ida2)
-        >>> result.head()
-        INDEXERIDA1  INDEXERIDA2  RESULT
-        2            163          MULTIPOLYGON (((-96.6219873342 30.0442882117, ...
-        2            1840         MULTIPOLYGON (((-96.6219873342 30.0442882117, ...
-        2            109          MULTIPOLYGON (((-96.6219873342 30.0442882117, ..
-        """
-        return self._binary_operation_handler(
-            ida2,
-            db2gse_function='DB2GSE.ST_UNION',
-            valid_types_ida1=['ST_GEOMETRY'],
-            valid_types_ida2=['ST_GEOMETRY'])
-
-    def _binary_operation_handler(self, ida2, db2gse_function,
-                                          valid_types_ida1, valid_types_ida2,
-                                          additional_args=None):
-
-
-        """
-        Returns an IdaGeoDataFrame with three columns:
-        [
-        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
-        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
-        RESULT : the result of the operation
-        ]
-
-
-        Parameters
-        ----------
-        db2gse_function : str
-                Name of the corresponding DB2GSE function.
-        valid_types_ida1 : list of str
-                Valid input typenames for the first IdaGeoSeries.
-        valid_types_ida2 : list of str
-                Valid input typenames for the second IdaGeoSeries.
-        additional_args : list of str, optional
-                Additional arguments for the DB2GSE function.
-
-        Returns
-        -------
-        IdaGeoDataFrame
-        """
-        ida1 = self
-        
-        # Check if allowed data type
-        if not (ida1.dtypes.TYPENAME[0] in valid_types_ida1 or
-                        valid_types_ida1[0] == 'ST_GEOMETRY'):
-            raise TypeError("Column " + ida1.column +
-                            " has incompatible type.")
-        if not (ida2.dtypes.TYPENAME[0] in valid_types_ida2 or
-                        valid_types_ida2[0] == 'ST_GEOMETRY'):
-            raise TypeError("Column " + ida2.column +
-                            " has incompatible type.")
-
-        # Get the definitions of the columns, which will be the arguments for
-        # the DB2GSE function
-        column1_for_db2gse = 'IDA1.\"%s\"' %(ida1.geometry.column)
-        column2_for_db2gse = 'IDA2.\"%s\"' %(ida2.geometry.column)
-        arguments_for_db2gse_function = [column1_for_db2gse, column2_for_db2gse]
-        if additional_args is not None:
-            for arg in additional_args:
-                arguments_for_db2gse_function.append(arg)
-
-        # SELECT statement
-        select_columns=[]
-        if hasattr(ida1, '_indexer') and ida1._indexer is not None:
-            select_columns.append('IDA1.\"%s\" AS \"INDEXERIDA1\"' %(ida1.indexer))
-        else:
-            message = (ida1+"has no indexer defined. Please assign index column with set_indexer and retry.")
-            raise IdaGeoDataFrameError(message)
-        if hasattr(ida2, '_indexer') and ida2._indexer is not None:
-            select_columns.append('IDA2.\"%s\" AS \"INDEXERIDA2\"' %(ida2.indexer))
-        else:
-            message = (ida2+"has no indexer defined. Please assign index column with set_indexer and retry.")
-            raise IdaGeoDataFrameError(message)
-        result_column = (
-            db2gse_function+
-            '('+
-            ','.join(map(str, arguments_for_db2gse_function))+
-            ')'
-        )
-        select_columns.append('%s AS \"RESULT\"' %(result_column))
-        select_statement = 'SELECT '+','.join(select_columns)+' '        
-        
-        # FROM clause
-        from_clause=(
-            'FROM '+
-            ida1.name+' AS IDA1, '+
-            ida2.name+' AS IDA2 '
-        )
-
-        # Create a view
-        view_creation_query='('+select_statement+from_clause+')'
-        viewname=self._idadb._create_view_from_expression(view_creation_query)
-
-        idageodf=nzpyida.IdaGeoDataFrame(self._idadb, viewname, indexer='INDEXERIDA1')
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+IdaGeoDataFrame
+"""
+
+# Ensure Python 2 compatibility
+from __future__ import print_function
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import absolute_import
+from builtins import super
+from builtins import zip
+from builtins import str
+from builtins import int
+from future import standard_library
+standard_library.install_aliases()
+
+import nzpyida
+from nzpyida.frame import IdaDataFrame
+from nzpyida.geoSeries import IdaGeoSeries
+from nzpyida.exceptions import IdaGeoDataFrameError
+
+from copy import deepcopy
+
+import six
+
+
+class IdaGeoDataFrame(IdaDataFrame):
+    """  
+    An IdaGeoDataFrame container inherits from IdaDataFrame.
+
+    It has a property called "geometry" which refers to a column with
+    geometry type. It is set as a string with a column name, either at
+    instantiation time or with the set_geometry() method.
+
+    If the "geometry" property is set, when calling a geospatial method from
+    IdaDataFrame the method will be carried on the column this property refers
+    to.
+
+    The property "geometry" returns an IdaGeoSeries.
+
+    See IdaDataFrame.
+    See IdaGeoSeries.
+
+    Notes
+    -----
+    IdaGeoDataFrame objects are not supported on Netezza.
+
+    Examples
+    --------
+    >>> idageodf = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY',
+    indexer='OBJECTID')
+    >>> idageodf.dtypes
+                     TYPENAME
+    OBJECTID          INTEGER
+    SHAPE     ST_MULTIPOLYGON
+    STATEFP           VARCHAR
+    COUNTYFP          VARCHAR
+    COUNTYNS          VARCHAR
+    NAME              VARCHAR
+    GEOID             VARCHAR
+    NAMELSAD          VARCHAR
+    LSAD              VARCHAR
+    CLASSFP           VARCHAR
+    MTFCC             VARCHAR
+    CSAFP             VARCHAR
+    CBSAFP            VARCHAR
+    METDIVFP          VARCHAR
+    FUNCSTAT          VARCHAR
+    ALAND             DECIMAL
+    AWATER            DECIMAL
+    INTPTLAT          VARCHAR
+    INTPTLON          VARCHAR
+
+    >>> idageodf[['NAME', 'SHAPE']].head()
+           NAME                                              SHAPE
+    0    Becker  MULTIPOLYGON (((-95.1637185512 46.7176480983, ...
+    1  Jim Hogg  MULTIPOLYGON (((-98.9542377853 26.7856984795, ...
+    2     Henry  MULTIPOLYGON (((-88.0532984194 36.4970648458, ...
+    3     Keith  MULTIPOLYGON (((-102.0517705602 41.0038968011,...
+    4   Clinton  MULTIPOLYGON (((-94.2059683962 39.7458481141, ...
+
+    >>> idageodf.geometry
+    AttributeError: Geometry property has not been set yet. Use set_geometry
+    method to set it.
+
+    >>> idageodf.set_geometry('SHAPE')
+    >>> idageodf.geometry.column
+    'SHAPE'
+
+    >>> type(idageodf.geometry)
+    <class 'ibmdbpy.geoSeries.IdaGeoSeries'>
+
+    >>> idageoseries = idageodf.geometry    
+    >>> idageoseries.head()
+    0    MULTIPOLYGON (((-95.1637185512 46.7176480983, ...
+    1    MULTIPOLYGON (((-98.9542377853 26.7856984795, ...
+    2    MULTIPOLYGON (((-88.0532984194 36.4970648458, ...
+    3    MULTIPOLYGON (((-102.0517705602 41.0038968011,...
+    4    MULTIPOLYGON (((-94.2059683962 39.7458481141, ...
+    Name: SHAPE, dtype: object
+
+    >>> idageodf['County area'] = idageodf.area(unit='mile')
+
+    >>> counties_with_areas = idageodf[['NAME', 'SHAPE', 'County area']]
+    >>> counties_with_areas.dtypes
+                        TYPENAME
+    NAME                 VARCHAR
+    SHAPE        ST_MULTIPOLYGON
+    County area           DOUBLE
+
+    >>> counties_with_areas.head()
+            NAME                                             SHAPE  County area
+    0     Menard  MULTIPOLYGON (((-99.4847630885 30.940610279, ...   902.281540
+    1      Boone  MULTIPOLYGON (((-88.7764991497 42.491919892, ...   282.045087
+    2  Ochiltree  MULTIPOLYGON (((-100.5467326897 36.056542135,...   918.188142
+    3    Sharkey  MULTIPOLYGON (((-90.9143429922 33.007703026, ...   435.548518
+    4    Audubon  MULTIPOLYGON (((-94.7006367168 41.504155369, ...   444.827726
+    """
+
+    def __init__(self, idadb, tablename, indexer = None, geometry = None):
+        """
+        Constructor for IdaGeoDataFrame objects.
+        See IdaDataFrame.__init__ documentation.
+
+        Parameters
+        ----------
+        geometry : str, optional
+            Column name to set the "geometry" property of the IdaGeoDataFrame.
+            The column must have geometry type.
+
+        Attributes
+        ----------
+        _geometry_colname : str
+            Name of the column that "geometry" property refers to.
+            This attribute must be set through the set_geometry() method.
+        geometry : IdaGeoSeries
+            The column referenced by _geometry_colname attribute.
+        """
+        # TODO: Add support for receiving either a string or an IdaGeoSeries as 
+        # geometry parameter.        
+
+        if (idadb.__class__.__name__ == "IdaDataBase") & idadb._is_netezza_system():
+            raise IdaGeoDataFrameError("IdaGeoDataFrame objects are not supported on Netezza.")
+
+        if geometry is not None and not isinstance(geometry, six.string_types):
+            raise TypeError("geometry must be a string")
+        super(IdaGeoDataFrame, self).__init__(idadb, tablename, indexer)
+        self._geometry_colname = None
+        if geometry is not None:
+            self.set_geometry(geometry)
+
+    def __getitem__(self, item):
+        """
+        Returns an IdaDataFrame, IdaSeries, IdaGeoDataFrame or IdaGeoSeries
+        as appropriate.
+        
+        Returns
+        --------
+        IdaGeoSeries
+            When the projection has only one column and it has geometry type.
+        IdaGeoDataFrame
+            When the projection has more than one column, and the "geometry"
+            column of the IdaGeoDataFrame is included in them.
+        IdaDataFrame
+            When the projection has more than one column, and the "geometry"
+            column of the IdaGeoDataFrame is not included in them.
+        IdaSeries
+            When the projection has only one column and it doesn't have 
+            geometry type.
+        """
+        ida = super(IdaGeoDataFrame, self).__getitem__(item)
+        if isinstance(ida, nzpyida.IdaSeries):
+            if ida.dtypes['TYPENAME'][ida.column].find('ST_') == 0:
+                idageoseries = IdaGeoSeries.from_IdaSeries(ida)
+                # Return IdaGeoSeries
+                return idageoseries
+            else:
+                # Return IdaSeries
+                return ida
+        elif isinstance(ida, nzpyida.IdaDataFrame):
+            if self._geometry_colname in ida.dtypes.index:
+                # Return IdaGeoDataFrame
+                idageodf = IdaGeoDataFrame.from_IdaDataFrame(ida)
+                idageodf._geometry_colname = self._geometry_colname
+                return idageodf
+            else:
+                # Return IdaDataFrame
+                return ida
+
+    def __delitem__(self, item):
+        """
+        Erases the "geometry" property if the column it refers to is deleted.
+        """
+        super(IdaGeoDataFrame, self).__delitem__(item)
+        if item == self._geometry_colname:
+            self._geometry_colname = None
+
+    def __getattr__(self, name):
+        """
+        Carry geospatial method calls on the "geometry" column of the
+        IdaGeoDataFrame, if it was set.
+        
+        Notes
+        -----
+        This method gets called only when an attribute lookup on
+        IdaGeoDataFrame is not resolved, i.e. it is not an instance attribute
+        and it's not found in the class tree.
+        """        
+        
+        if name == 'geometry':
+            # When .geometry is accessed and _geometry_colname is None
+            return self.__getattribute__('geometry')
+
+        if hasattr(IdaGeoSeries, name):
+            # Geospatial method call
+            if self._geometry_colname is None:
+                raise AttributeError("Geometry column has not been set yet.")
+            else:
+                # Get a IdaGeoSeries and carry the operation on it
+                idageoseries = self.__getitem__(item = self._geometry_colname)
+                return idageoseries.__getattribute__(name)
+        else:
+            raise AttributeError
+    
+    @property
+    def geometry(self):
+        """
+        Returns an IdaGeoSeries with the column whose name is stored in 
+        _geometry_colname attribute.
+
+        The setter calls the set_geometry() method.
+
+        Returns
+        -------
+        IdaGeoSeries
+
+        Raises
+        ------
+        AttributeError
+            If the property has not been set yet.
+        
+        """
+        if self._geometry_colname is None:
+            raise AttributeError(
+                "Geometry property has not been set yet. "
+                "Use set_geometry method to set it.")
+        else:
+            return self.__getitem__(self._geometry_colname)
+    
+    @geometry.setter
+    def geometry(self, value):
+        """
+        See set_geometry() method.
+        """
+        self.set_geometry(value)
+
+    @classmethod
+    def from_IdaDataFrame(cls, idadf, geometry = None):
+        """ 
+        Creates an IdaGeoDataFrame from an IdaDataFrame.
+        
+        Parameters
+        ----------
+        geometry : str, optional
+            Column name to set the "geometry" property of the IdaGeoDataFrame.
+            The column must have geometry type.
+
+        Raises
+        ------
+        TypeError
+            If idadf is not an IdaDataFrame.
+        """
+        
+        if not isinstance(idadf, IdaDataFrame):
+            raise TypeError("Expected IdaDataFrame")
+        else:
+            # TODO: check if it's better to only change the .__base__ attribute
+
+            #behavior based on _clone() method of IdaDataFrame
+            newida = IdaGeoDataFrame(
+                    idadf._idadb, idadf._name, idadf.indexer, geometry)
+            newida.columns = idadf.columns 
+            newida.dtypes = idadf.dtypes
+            
+            newida.internal_state.name = deepcopy(idadf.internal_state.name)
+            newida.internal_state.ascending = deepcopy(idadf.internal_state.ascending)
+            #newida.internal_state.views = deepcopy(idadf.internal_state.views)
+            newida.internal_state._views = deepcopy(idadf.internal_state._views)
+            newida.internal_state._cumulative = deepcopy(idadf.internal_state._cumulative)
+            newida.internal_state.order = deepcopy(idadf.internal_state.order)
+            newida.internal_state.columndict = deepcopy(idadf.internal_state.columndict)
+            return newida
+
+    def set_geometry(self, column_name):
+        """
+        Receives a column name to set as the "geometry" column of the
+        IdaDataFrame.
+
+        Parameters
+        -----------
+        column_name : str
+            Name of the column to be set as geometry column of the 
+            IdaDataFrame. It must have geometry type.
+
+        Raises
+        ------
+        KeyError
+            If the column is not present in the IdaGeoDataFrame.
+        TypeError
+            If the column doesn't have geometry type.
+        """
+        if not isinstance(column_name, six.string_types):
+            raise TypeError("column_name must be a string")
+        if column_name not in self.columns:
+            raise KeyError(
+                "'" + column_name + "' cannot be set as geometry column: "
+                "not a column in the IdaGeoDataFrame."
+            )
+        elif self.dtypes.TYPENAME[column_name].find('ST_') != 0:
+            raise TypeError(
+                "'" + column_name + "' cannot be set as geometry column: "
+                "column doesn't have geometry type."
+            )
+        else:
+            self._geometry_colname = column_name
+
+    # ==============================================================================
+    ### Binary geospatial methods
+    # ==============================================================================
+    def equals(self, ida2):
+        """
+        Valid types for the column in the calling IdaGeoDataFrame:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoDataFrame of indices of the two input
+        IdaGeoDataFrames and a result column with 1 or 0 depending
+        upon whether the geometry of the first IdaGeoDataFrame
+        crosses the second.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+
+        References
+        ----------
+        DB2 Spatial Extender ST_CROSSES() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.equals(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          0
+        2            1840         0
+        2            109          0
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_EQUALS',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def distance(self, ida2, unit=None):
+        """
+        Valid types for the column in the calling IdaGeoDataFrame:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoDataFrame of indices of the two input
+        IdaGeoDataFrames and a result column with a numeric value
+        which is the geographic distance measured between the
+        geometries of the input IdaGeoDataFrames.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+        unit : str, optional
+            Name of the unit, it is case-insensitive.
+            If omitted, the following rules are used:
+
+                * If geometry is in a projected or geocentric coordinate
+                  system, the linear unit associated with this coordinate system
+                  is used.
+                * If geometry is in a geographic coordinate system, the angular
+                  unit associated with this coordinate system is used.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_DISTANCE() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.distance(ida2,unit = 'KILOMETER')
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          26.918942
+        2            1840         4.868971
+        2            109          16.387094
+        """
+        add_args = None
+        if unit is not None:
+            unit = self._check_linear_unit(unit)  # Can raise exceptions
+            add_args = []
+            add_args.append(unit)
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_DISTANCE',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'],
+            additional_args = add_args)
+
+    def crosses(self, ida2):
+        """
+        Valid types for the column in the calling IdaGeoDataFrame:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoDataFrame of indices of the two input
+        IdaGeoDataFrames and a result column with 1 or 0 depending
+        upon whether the geometry of the first IdaGeoDataFrame
+        crosses the second.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_CROSSES() function.
+
+        See also
+        --------
+        linear_units : list of valid units.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.crosses(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          0
+        2            1840         0
+        2            109          0
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_CROSSES',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def intersects(self, ida2):
+        """
+        Valid types for the column in the calling IdaGeoDataFrame:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoDataFrame of indices of the two input
+        IdaGeoDataFrames and a result column with 1 or 0 depending
+        upon whether the geometries of the input IdaGeoDataFrames
+        intersect each other.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_INTERSECTS() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.intersects(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          0
+        2            1840         0
+        2            109          0
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_INTERSECTS',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def overlaps(self, ida2):
+        """
+        Valid types for the column in the calling IdaGeoDataFrame:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoDataFrame of indices of the two input
+        IdaGeoDataFrames and a result column with 1 or 0 depending
+        upon whether the geometries of the input IdaGeoDataFrames
+        overlap each other.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_OVERLAPS() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.overlaps(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          0
+        2            1840         0
+        2            109          0
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_OVERLAPS',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def touches(self, ida2):
+        """
+        Valid types for the column in the calling IdaGeoDataFrame:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoDataFrame of indices of the two input
+        IdaGeoDataFrames and a result column with 1 or 0 depending
+        upon whether the boundary of the first geometry touches
+        the second.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_TOUCHES() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.touches(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          0
+        2            1840         0
+        2            109          0
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_TOUCHES',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def disjoint(self, ida2):
+        """
+        Valid types for the column in the calling IdaGeoDataFrame:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoDataFrame of indices of the two input
+        IdaGeoDataFrames and a result column with 1 or 0 depending
+        upon whether the geometries in the input dataframes are
+        disjoint.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_DISJOINT() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.disjoint(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          1
+        2            1840         1
+        2            109          1
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_DISJOINT',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def contains(self, ida2):
+        """
+        Valid types for the column in the calling IdaGeoDataFrame:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoDataFrame of indices of the two input
+        IdaGeoDataFrames and a result column with 1 or 0 depending
+        upon whether the second geometry contains the first.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_CONTAINS() function.
+
+        Examples
+        --------
+        >>> idageodf_customer = IdaGeoDataFrame(idadb,'SAMPLES.GEO_CUSTOMER',indexer='OBJECTID')
+        >>> idageodf_customer.set_geometry('SHAPE')
+        >>> idageodf_county = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> idageodf_county.set_geometry('SHAPE')
+        >>> ida1 = idageodf_customer[idageodf_customer['INSURANCE_VALUE']>250000]
+        >>> ida2 = idageodf_county[idageodf_county['NAME']=='Madison']
+        >>> result = ida2.contains(ida1)
+        >>> result[result['RESULT']==1].head()
+        INDEXERIDA1    INDEXERIDA2    RESULT
+        21473          134            1
+        21413          134            1
+        21414          134            1
+        21417          134            1
+        21419          134            1
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_CONTAINS',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def within(self, ida2):
+        """
+        Valid types for the column in the calling IdaGeoDataFrame:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoDataFrame of indices of the two input
+        IdaGeoDataFrames and a result column with 1 or 0 depending
+        upon whether the first geometry is inside the second.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_WITHIN() function.
+
+        Examples
+        --------
+        >>> idageodf_customer = IdaGeoDataFrame(idadb,'SAMPLES.GEO_CUSTOMER',indexer='OBJECTID')
+        >>> idageodf_customer.set_geometry('SHAPE')
+        >>> idageodf_county = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> idageodf_county.set_geometry('SHAPE')
+        >>> ida1 = idageodf_customer[idageodf_customer['INSURANCE_VALUE']>250000]
+        >>> ida2 = idageodf_county[idageodf_county['NAME']=='Madison']
+        >>> result = ida1.within(ida2)
+        >>> result[result['RESULT']==1].head()
+        INDEXERIDA1    INDEXERIDA2    RESULT
+        134            21473          1
+        134            21413          1
+        134            21414          1
+        134            21417          1
+        134            21419          1
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_WITHIN',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def mbr_intersects(self, ida2):
+        """
+        This method takes a second IdaGeoDataFrame an an input
+        and checks if the Minimum Bounding rectangles of the
+        geometries from both IdaGeoDataFrames intersect and
+        stores the result as 0 or 1 in the RESULT column of
+        the resulting IdaGeoDataFrame.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MBRIntersects() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.difference(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          0
+        2            1840         0
+        2            109          0
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_MBRINTERSECTS',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def difference(self, ida2):
+        """
+        This method takes a second IdaGeoDataFrame an an input
+        and returns the difference of the geometries from both
+        IdaGeoDataFrames as a new geometry stored in the RESULT
+        column of the resulting IdaGeoDataFrame.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_Difference() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.difference(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          POLYGON ((-96.6219873342 30.0442882117, -96.61...
+        2            1840         POLYGON ((-96.6219873342 30.0442882117, -96.61...
+        2            109          POLYGON ((-96.6219873342 30.0442882117, -96.61...
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_DIFFERENCE',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def intersection(self, ida2):
+        """
+        This method takes a second IdaGeoDataFrame an an input
+        and returns the intersection of the geometries from both
+        IdaGeoDataFrames as a new geometry stored in the RESULT
+        column of the resulting IdaGeoDataFrame.
+
+        For None geometries the output is None.
+        For empty geometries the output is POINT EMPTY.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_Intersection() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.intersection(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          POINT EMPTY
+        2            1840         POINT EMPTY
+        2            109          POINT EMPTY
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_INTERSECTION',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def union(self, ida2):
+        """
+        This method takes a second IdaGeoDataFrame an an input
+        and returns the union of the geometries from both
+        IdaGeoDataFrames as a new geometry stored in the RESULT
+        column of the resulting IdaGeoDataFrame.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        Returns an IdaGeoDataFrame with three columns:
+
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+
+        Parameters
+        ----------
+        ida2 : IdaGeoDataFrame
+            Name of the second IdaGeoDataFrame on which the function ST_EQUALS()
+            will be invoked.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_Union() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> ida1 = counties[counties['NAME'] == 'Austin']
+        >>> ida2 = counties[counties['NAME'] == 'Kent']
+        >>> result = ida1.union(ida2)
+        >>> result.head()
+        INDEXERIDA1  INDEXERIDA2  RESULT
+        2            163          MULTIPOLYGON (((-96.6219873342 30.0442882117, ...
+        2            1840         MULTIPOLYGON (((-96.6219873342 30.0442882117, ...
+        2            109          MULTIPOLYGON (((-96.6219873342 30.0442882117, ..
+        """
+        return self._binary_operation_handler(
+            ida2,
+            db2gse_function='DB2GSE.ST_UNION',
+            valid_types_ida1=['ST_GEOMETRY'],
+            valid_types_ida2=['ST_GEOMETRY'])
+
+    def _binary_operation_handler(self, ida2, db2gse_function,
+                                          valid_types_ida1, valid_types_ida2,
+                                          additional_args=None):
+
+
+        """
+        Returns an IdaGeoDataFrame with three columns:
+        [
+        INDEXERIDA1 : indexer of the first IdaGeoSeries (None if not set),
+        INDEXERIDA2 : indexer of the second IdaGeoSeries (None if not set),
+        RESULT : the result of the operation
+        ]
+
+
+        Parameters
+        ----------
+        db2gse_function : str
+                Name of the corresponding DB2GSE function.
+        valid_types_ida1 : list of str
+                Valid input typenames for the first IdaGeoSeries.
+        valid_types_ida2 : list of str
+                Valid input typenames for the second IdaGeoSeries.
+        additional_args : list of str, optional
+                Additional arguments for the DB2GSE function.
+
+        Returns
+        -------
+        IdaGeoDataFrame
+        """
+        ida1 = self
+        
+        # Check if allowed data type
+        if not (ida1.dtypes.TYPENAME[0] in valid_types_ida1 or
+                        valid_types_ida1[0] == 'ST_GEOMETRY'):
+            raise TypeError("Column " + ida1.column +
+                            " has incompatible type.")
+        if not (ida2.dtypes.TYPENAME[0] in valid_types_ida2 or
+                        valid_types_ida2[0] == 'ST_GEOMETRY'):
+            raise TypeError("Column " + ida2.column +
+                            " has incompatible type.")
+
+        # Get the definitions of the columns, which will be the arguments for
+        # the DB2GSE function
+        column1_for_db2gse = 'IDA1.\"%s\"' %(ida1.geometry.column)
+        column2_for_db2gse = 'IDA2.\"%s\"' %(ida2.geometry.column)
+        arguments_for_db2gse_function = [column1_for_db2gse, column2_for_db2gse]
+        if additional_args is not None:
+            for arg in additional_args:
+                arguments_for_db2gse_function.append(arg)
+
+        # SELECT statement
+        select_columns=[]
+        if hasattr(ida1, '_indexer') and ida1._indexer is not None:
+            select_columns.append('IDA1.\"%s\" AS \"INDEXERIDA1\"' %(ida1.indexer))
+        else:
+            message = (ida1+"has no indexer defined. Please assign index column with set_indexer and retry.")
+            raise IdaGeoDataFrameError(message)
+        if hasattr(ida2, '_indexer') and ida2._indexer is not None:
+            select_columns.append('IDA2.\"%s\" AS \"INDEXERIDA2\"' %(ida2.indexer))
+        else:
+            message = (ida2+"has no indexer defined. Please assign index column with set_indexer and retry.")
+            raise IdaGeoDataFrameError(message)
+        result_column = (
+            db2gse_function+
+            '('+
+            ','.join(map(str, arguments_for_db2gse_function))+
+            ')'
+        )
+        select_columns.append('%s AS \"RESULT\"' %(result_column))
+        select_statement = 'SELECT '+','.join(select_columns)+' '        
+        
+        # FROM clause
+        from_clause=(
+            'FROM '+
+            ida1.name+' AS IDA1, '+
+            ida2.name+' AS IDA2 '
+        )
+
+        # Create a view
+        view_creation_query='('+select_statement+from_clause+')'
+        viewname=self._idadb._create_view_from_expression(view_creation_query)
+
+        idageodf=nzpyida.IdaGeoDataFrame(self._idadb, viewname, indexer='INDEXERIDA1')
         return idageodf
```

### Comparing `nzpyida-0.2.2.6/nzpyida/geoSeries.py` & `nzpyida-0.3.3/nzpyida/geoSeries.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,2121 +1,2121 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-IdaGeoSeries
-"""
-
-# Ensure Python 2 compatibility
-from __future__ import print_function
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import absolute_import
-from builtins import map
-from builtins import super
-from future import standard_library
-standard_library.install_aliases()
-
-from numbers import Number
-from collections import OrderedDict
-
-from lazy import lazy
-import six
-
-import nzpyida
-from nzpyida.series import IdaSeries
-from nzpyida.exceptions import IdaGeoDataFrameError
-
-class IdaGeoSeries(nzpyida.IdaSeries):
-    """
-    An IdaSeries whose column must have geometry type.
-    It has geospatial methods based on Db2 Warehouse Spatial Extender (DB2GSE).
-    
-    Note on sample data used for the examples:
-
-        * Sample tables available out of the box in Db2 Warehouse:
-
-          GEO_TORNADO, GEO_COUNTY
-
-        * Sample tables which you can create by executing the SQL statements in
-          https://github.com/ibmdbanalytics/ibmdbpy/blob/master/ibmdbpy/sampledata/sql_script:
-
-          SAMPLE_POLYGONS, SAMPLE_LINES, SAMPLE_GEOMETRIES, SAMPLE_MLINES, SAMPLE_POINTS
-
-    Notes
-    -----
-    IdaGeoDataSeries objects are not supported on Netezza.
-
-    An IdaGeoSeries doesn't have an indexer attribute because geometries are
-    unorderable in DB2 Spatial Extender.
-
-    Examples
-    --------
-    >>> idageodf = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer='OBJECTID', geometry = "SHAPE")
-    >>> idageoseries = idageodf["SHAPE"]
-    >>> idageoseries.dtypes
-                 -------------------
-                | TYPE_NAME         |
-         ----------------------------
-        | SHAPE | ST_MULTIPOLYGON   |
-         ----------------------------
-
-    """
-    def __init__(self, idadb, tablename, indexer, column):
-        """
-        Ensures that the specified column has geometry type.
-        See __init__ of IdaSeries.
-
-        Parameters
-        ----------
-        column : str
-            Column name. It must have geometry type.
-
-        Notes
-        -----
-        Even though geometry types are unorderable in DB2GSE, the IdaGeoSeries
-        might have as indexer another column of the table whose column the
-        IdaGeoSeries refers to.
-        """
-
-        if (idadb.__class__.__name__ == "IdaDataBase") & idadb._is_netezza_system():
-                    raise IdaGeoDataFrameError("IdaGeoDataSeries objects are not supported on Netezza.")
-
-        super(IdaGeoSeries, self).__init__(idadb, tablename, indexer, column)
-        if self.dtypes.TYPENAME[self.column].find('ST_') != 0:
-            raise TypeError("Specified column doesn't have geometry type. "
-                            "Cannot create IdaGeoSeries object")
-
-    @classmethod
-    def from_IdaSeries(cls, idaseries):
-        """
-        Creates an IdaGeoSeries from an IdaSeries, ensuring that the column
-        of the given IdaSeries has geometry type.
-        """
-        if not isinstance(idaseries, IdaSeries):
-            raise TypeError("Expected IdaSeries")
-        else:
-            # Mind that the given IdaSeries might have non-destructive
-            # columns that were added by the user. That's why __init__ is not
-            # used for this purpose.
-            if idaseries.dtypes.TYPENAME[idaseries.column].find('ST_') != 0:
-                raise TypeError(
-                    "The column of the IdaSeries doesn't have geometry type. "
-                    "Cannot create IdaGeoSeries object")
-            else:
-                idageoseries = idaseries
-                idageoseries.__class__ = IdaGeoSeries
-                return idageoseries
-
-#==============================================================================
-### Methods whose behavior is not defined for geometry types in DB2GSE.
-#==============================================================================
-
-    # TODO: Override all the methods of IdaSeries (and those of its parent,
-    # i.e. IdaDataFrame, which are not defined in DB2GSE for geometry columnns,
-    # like min(), max(), etc.)
-
-    def min(self):
-        raise TypeError("Unorderable geometries")
-        pass
-
-    def max(self):
-        raise TypeError("Unorderable geometries")
-        pass
-
-#==============================================================================
-### Unary geospatial methods
-#==============================================================================
-
-    def generalize(self, threshold):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoSeries of geometries which represent each of the
-        geometries in the calling IdaGeoSeries, but with a reduced number of
-        points, while preserving the general characteristics of the geometry.
-
-        The Douglas-Peucker line-simplification algorithm is used, by which the
-        sequence of points that define the geometry is recursively subdivided
-        until a run of the points can be replaced by a straight line segment.
-        In this line segment, none of the defining points deviates from the
-        straight line segment by more than the given threshold. Z and M
-        coordinates are not considered for the simplification. The resulting
-        geometry is in the spatial reference system of the given geometry.
-
-        For empty geometries, the output is an empty geometry of type ST_Point.
-        For None geometries the output is None.
-
-        Parameters
-        ----------
-        threshold : float
-            Threshold to be used for the line-simplification algorithm.
-            The threshold must be greater than or equal to 0.
-            The larger the threshold, the smaller the number of points that
-            will be used to represent the generalized geometry.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_GENERALIZE() function.
-
-        Examples
-        --------
-        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
-        >>> tornadoes.set_geometry('SHAPE')
-        >>> tornadoes['generalize'] = tornadoes.generalize(threshold = 4)
-        >>> tornadoes[['OBJECTID','generalize']].head()
-        OBJECTID  generalize
-        1         MULTILINESTRING ((-90.2200062071 38.7700071663...
-        2         MULTILINESTRING ((-89.3000059755 39.1000072739...
-        3         MULTILINESTRING ((-84.5800047496 40.8800078382...
-        4         MULTILINESTRING ((-94.3700070010 34.4000061520...
-        5         MULTILINESTRING ((-90.6800062393 37.6000069289...
-        """
-        try:
-            threshold = float(threshold)
-        except:
-            raise TypeError("threshold must be float")        
-        if threshold < 0:
-            raise ValueError("threshold must be greater than or equal to 0")
-        additional_args = [threshold]
-        return self._unary_operation_handler(
-            db2gse_function = 'DB2GSE.ST_GENERALIZE',
-            valid_types = ['ST_GEOMETRY'],
-            additional_args = additional_args)
-
-    def buffer(self, distance, unit = None):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoSeries of geometries in which each point is the
-        specified distance away from the geometries in the calling
-        IdaGeoSeries, measured in the given unit.
-
-        Parameters
-        ----------
-        distance : float
-            Distance, can be positive or negative.
-        unit : str, optional
-            Name of the unit, it is case-insensitive.
-            If omitted, the following rules are used:
-
-                * If geometry is in a projected or geocentric coordinate
-                  system, the linear unit associated with this coordinate system
-                  is the default.
-                * If geometry is in a geographic coordinate system, the angular
-                  unit associated with this coordinate system is the default.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        See also
-        ---------
-        linear_units
-
-        Notes
-        -----
-        Restrictions on unit conversions: An error (SQLSTATE 38SU4) is returned
-        if any of the following conditions occur:
-
-            * The geometry is in an unspecified coordinate system and the unit
-              parameter is specified.
-            * The geometry is in a projected coordinate system and an angular
-              unit is specified.
-            * The geometry is in a geographic coordinate system, but is not an
-              ST_Point value , and a linear unit is specified.
-
-        # TODO: handle this SQLSTATE error
-
-        References
-        ----------
-        DB2 Spatial Extender ST_BUFFER() function.
-
-        Examples
-        --------
-        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
-        >>> tornadoes.set_geometry('SHAPE')
-        >>> tornadoes['buffer_20_km'] = tornadoes.buffer(distance = 20, unit = 'KILOMETER')
-        >>> tornadoes[['OBJECTID','SHAPE','buffer_20_km']].head()
-        OBJECTID  SHAPE                                        buffer_20_km
-        1         MULTILINESTRING ((-90.2200062071 38.770....  POLYGON ((-90.3065519651 38.9369737029, -90.32..
-        2         MULTILINESTRING ((-89.3000059755 39.100....  POLYGON ((-89.3798853739 39.2690904737, -89.39.
-        3         MULTILINESTRING ((-84.5800047496 40.880....  POLYGON ((-84.7257488606 41.0222185578, -84.73...
-        4         MULTILINESTRING ((-94.3700070010 34.400....  POLYGON ((-94.5212609425 34.5296645617, -94.53...
-        5         MULTILINESTRING ((-90.6800062393 37.600....  POLYGON ((-90.8575378881 37.7120296620, -90.86...
-        """
-        if not isinstance(distance, Number):
-            # distance can be positive or negative
-            raise TypeError("Distance must be numerical")
-        additional_args = []
-        additional_args.append(distance)
-        if unit is not None:
-            unit = self._check_linear_unit(unit)  # Can raise exceptions
-            additional_args.append(unit)
-        return self._unary_operation_handler(
-            db2gse_function = 'DB2GSE.ST_BUFFER',
-            valid_types = ['ST_GEOMETRY'],
-            additional_args = additional_args)
-
-    def centroid(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoSeries of points which represent the geometric center
-        of each of the geometries in the calling IdaGeoSeries.
-
-        The geometric center is the center of the minimum bounding rectangle of
-        the given geometry, as a point.
-
-        The resulting point is represented in the spatial reference system of
-        the given geometry.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_CENTROID() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> counties['centroid'] = counties.centroid()
-        >>> counties[['NAME','centroid']].head()
-        NAME         centroid
-        Wood         POINT (-83.6490410160 41.3923524865)
-        Cass         POINT (-94.3483719161 33.0944709011)
-        Washington   POINT (-89.4241634562 38.3657576429)
-        Fulton       POINT (-74.4337987380 43.1359187016)
-        Clay         POINT (-96.5066339619 46.8908550036)
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_CENTROID',
-                valid_types = ['ST_GEOMETRY'])
-
-    def convex_hull(self):
-        """
-        The convex hull of a shape, also called convex envelope or convex closure, is the smallest convex set that contains it.
-        For example, if you have a bounded subset of points in the Euclidean space, the convex hull may be visualized as 
-        the shape enclosed by an elastic band stretched around the outside points of the subset. 
-        If vertices of the geometry do not form a convex, convexhull returns a null.
-        
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        If possible, the specific type of the returned geometry will be ST_Point, ST_LineString, or ST_Polygon.
-        The convex hull of a convex polygon with no holes is a single linestring, represented as ST_LineString. 
-        The convex hull of a non convex polygon does not exit. 
-        
-        Returns
-        -------
-        IdaGeoSeries
-
-            Returns an IdaGeoSeries containing geometries which are the convex hull of each
-            of the geometries in the calling IdaGeoSeries.
-            The resulting geometry is represented in the spatial reference system
-            of the given geometry.
-            For None geometries, for empty geometries and for non convex geometries the output is None.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_CONVEXHULL() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> counties['convex_envelope'] = counties["SHAPE"].convex_hull()
-        >>> counties[['OBJECTID','SHAPE','convex_envelope']].head()
-                OBJECTID 	SHAPE 	convex_envelope
-        0 	1 	MULTIPOLYGON (((-99.4756582604 33.8340108094, ... 	POLYGON ((-99.4756582604 33.8340108094, -99.47...
-        1 	2 	MULTIPOLYGON (((-96.6219873342 30.0442882117, ... 	POLYGON ((-96.6219873342 30.0442882117, -96.55...
-        2 	3 	MULTIPOLYGON (((-99.4497297204 46.6316377481, ... 	POLYGON ((-99.9174847900 46.3122496703, -99.91...
-        3 	4 	MULTIPOLYGON (((-107.4817473750 37.0000108736,... 	POLYGON ((-108.3792135685 36.9995188176, -108....
-        4 	5 	MULTIPOLYGON (((-91.2589262966 36.2578866492, ... 	POLYGON ((-91.4074433538 36.4871686853, -91.24...
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_CONVEXHULL',
-                valid_types = ['ST_GEOMETRY'])
-
-    def boundary(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoSeries of geometries which are the boundary of each
-        of the geometries in the calling IdaGeoSeries.
-
-        The resulting geometry is represented in the spatial reference system
-        of the given geometry.
-
-        If the given geometry is a point, multipoint, closed curve, or closed
-        multicurve, or if it is empty, then the result is an empty geometry of
-        type ST_Point. For curves or multicurves that are not closed, the start
-        points and end points of the curves are returned as an ST_MultiPoint
-        value, unless such a point is the start or end point of an even number
-        of curves. For surfaces and multisurfaces, the curve defining the
-        boundary of the given geometry is returned, either as an ST_Curve or an
-        ST_MultiCurve value.
-
-        If possible, the specific type of the returned geometry will be
-        ST_Point, ST_LineString, or ST_Polygon. For example, the boundary of a
-        polygon with no holes is a single linestring, represented as
-        ST_LineString. The boundary of a polygon with one or more holes
-        consists of multiple linestrings, represented as ST_MultiLineString.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_BOUNDARY() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> counties['boundary'] = counties.boundary()
-        >>> counties[['NAME','boundary']].head()
-        NAME         boundary
-        Madison      LINESTRING (-90.4500428418 32.5737889565, -90....
-        Lake         LINESTRING (-114.6043395348 47.7897504535, -11...
-        Broward      LINESTRING (-80.8798118938 26.2594597939, -80....
-        Buena Vista  LINESTRING (-95.3880180283 42.5617494883, -95....
-        Jones        LINESTRING (-77.0903250894 34.8027619185, -77..
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_BOUNDARY',
-                valid_types = ['ST_GEOMETRY'])
-
-    def envelope(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry.
-
-        Returns an IdaGeoSeries of polygons which are an envelope around each
-        of the geometries in the calling IdaGeoSeries. The envelope is a
-        rectangle that is represented as a polygon.
-
-        If the given geometry is a point, a horizontal linestring, or a
-        vertical linestring, then a rectangle, which is slightly larger than
-        the given geometry, is returned. Otherwise, the minimum bounding
-        rectangle of the geometry is returned as the envelope.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        See also
-        --------
-        mbr
-
-        References
-        ----------
-        DB2 Spatial Extender ST_ENVELOPE() function.
-
-        Examples
-        --------
-        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
-        >>> tornadoes.set_geometry('SHAPE')
-        >>> tornadoes['envelope'] = tornadoes.envelope()
-        >>> tornadoes[['OBJECTID', 'SHAPE', 'envelope']].head()
-        OBJECTID   SHAPE                                      envelope
-        1          MULTILINESTRING ((-90.2200062071 38.77..   POLYGON ((-90.2200062071 38.77..
-        2          MULTILINESTRING ((-89.3000059755 39.10..   POLYGON ((-89.3000059755 39.10..
-        3          MULTILINESTRING ((-84.5800047496 40.88..   POLYGON ((-84.5800047496 40.88..
-        4          MULTILINESTRING ((-94.3700070010 34.40..   POLYGON ((-94.3700070010 34.40..
-        5          MULTILINESTRING ((-90.6800062393 37.60..   POLYGON ((-90.6800062393 37.60..
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_ENVELOPE',
-                valid_types = ['ST_GEOMETRY'])
-
-    def exterior_ring(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Polygon.
-
-        Returns an IdaGeoSeries of curves which are the exterior ring of each
-        of the geometries in the calling IdaGeoSeries.
-
-        The resulting curve is represented in the spatial reference system of
-        the given polygon.
-
-        If the polygon does not have any interior rings, the returned exterior
-        ring is identical to the boundary of the polygon.
-
-        For None polygons the output is None.
-        For empty polygons the output is None.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_EXTERIORRING() function.
-
-        Examples
-        --------
-        >>> sample_polygons["ext_ring"] = sample_polygons.exterior_ring()
-        >>> sample_polygons.head()
-        ID 	GEOMETRY 	ext_ring
-        0 	1101 	POLYGON ((110.000000 120.000000, 120.000000 13... 	LINESTRING (110.000000 120.000000, 120.000000 ...
-        1 	1102 	POLYGON ((110.000000 120.000000, 130.000000 12... 	LINESTRING (110.000000 120.000000, 130.000000 ...
-
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_EXTERIORRING',
-                valid_types = ['ST_POLYGON'])
-
-    def mbr(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaGeoSeries of geometries which are the minimum bounding
-        rectangle of each of the geometries in the calling IdaGeoSeries.
-
-        If the given geometry is a point, then the point itself is returned.
-        If the geometry is a horizontal linestring or a vertical linestring,
-        the horizontal or vertical linestring itself is returned.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MBR() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> counties["MBR"] = counties.mbr()
-        >>> counties[["NAME", "SHAPE", "MBR"]].head()
-                NAME 	    SHAPE 	                                          MBR
-        0 	  Lafayette MULTIPOLYGON (((-90.4263836312 42.5071807967, ... 	POLYGON ((-90.4269086653 42.5056648248, -89.83...
-        1 	  Sanilac 	MULTIPOLYGON (((-82.1455052616 43.6955954588, ... 	POLYGON ((-83.1204005291 43.1541073218, -82.12...
-        2 	  Taylor 	MULTIPOLYGON (((-84.0691810519 32.5918031946, ... 	POLYGON ((-84.4532361602 32.3720591397, -84.00...
-        3 	  Ohio 	    MULTIPOLYGON (((-80.5191234475 40.0164178652, ... 	POLYGON ((-80.7338065145 40.0164178652, -80.51...
-        4 	  Houston 	MULTIPOLYGON (((-83.7877562454 32.5016909466, ... 	POLYGON ((-83.8568549803 32.2825891390, -83.48...
-
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MBR',
-                valid_types = ['ST_GEOMETRY'])
-
-    def end_point(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_LINESTRING.
-
-        Returns an IdaGeoSeries with the last point of each of the curves in
-        the calling IdaGeoSeries.
-
-        The resulting point is represented in the spatial reference system of
-        the given curve.
-
-        For None curves the output is None.
-        For empty curves the output is None.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_ENDPOINT() function.
-
-        Examples
-        --------
-        Sample to create in Db2, geometry column with data type ST_LineString
-        Use this sample data for testing:
-
-        >>> sample_lines = IdaGeoDataFrame(idadb, "SAMPLE_LINES", indexer = "ID", geometry  = "LOC")
-        >>> sample_lines['end_point'] = sample_lines.end_point()
-        >>> sample_lines.head()
-        	ID 	    GEOMETRY 	                                        end_point
-        0 	1110 	LINESTRING (850.000000 250.000000, 850.000000 ... 	POINT (850.000000 850.000000)
-        1 	1111 	LINESTRING (90.000000 90.000000, 100.000000 10... 	POINT (100.000000 100.000000)      
-        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_ENDPOINT',
-                valid_types = ['ST_LINESTRING'])
-
-    def mid_point(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_LINESTRING.
-
-        Returns an IdaGeoSeries of points which are equidistant from both ends
-        of each of the curves in the calling IdaGeoSeries, measured along the
-        curve.
-
-        The resulting point is represented in the spatial reference system of
-        the given curve.
-
-        If the curve contains Z coordinates or M coordinates (measures),
-        the midpoint is determined solely by the values of the X and Y
-        coordinates in the curve. The Z coordinate and measure in the returned
-        point are interpolated.
-
-        For None curves the output is None.
-        For empty curves, the output is an empty point.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MIDPOINT() function.
-
-        Examples
-        --------
-        Sample to create in Db2, geometry column with data type ST_LineString
-        Use this sample data for testing:
-        
-        >>> sample_lines = IdaGeoDataFrame(idadb, "SAMPLE_LINES", indexer = "ID", geometry  = "LOC")
-        >>> sample_lines["mid_point"] = sample_lines.mid_point()
-        >>> sample_lines.head()
-        	ID 	    GEOMETRY 	                                    	mid_point
-        0 	1110 	LINESTRING (850.000000 250.000000, 850.000000 ... 	POINT (850.000000 550.000000)
-        1 	1111 	LINESTRING (90.000000 90.000000, 100.000000 10... 	POINT (95.000000 95.000000)        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MIDPOINT',
-                valid_types = ['ST_LINESTRING'])
-
-    def start_point(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_LINESTRING.
-
-        Returns an IdaGeoSeries with the first point of each of the curves in
-        the calling IdaGeoSeries.
-
-        The resulting point is represented in the spatial reference system of
-        the given curve.
-
-        For None curves the output is None.
-        For empty curves the output is None.
-
-        Returns
-        -------
-        IdaGeoSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_STARTPOINT() function.
-
-        Examples
-        --------
-        Sample to create in Db2, geometry column with data type ST_LineString
-        
-        >>> sample_lines = IdaGeoDataFrame(idadb, "SAMPLE_LINES", indexer = "ID", geometry  = "LOC")
-        >>> sample_lines.start_point().head()
-        
-        0    POINT (850.000000 250.000000)
-        1    POINT (90.000000 90.000000)
-        Name: DB2GSE.ST_STARTPOINT(GEOMETRY), dtype: object        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_STARTPOINT',
-                valid_types = ['ST_LINESTRING'])
-
-    def srid(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with integers representing the spatial reference
-        system of each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_SRID() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = 'OBJECTID', geometry = 'SHAPE')        
-        >>> counties.srid().head()
-        0    1005
-        1    1005
-        2    1005
-        3    1005
-        4    1005
-        Name: DB2GSE.ST_SRID(SHAPE), dtype: int64        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_SRID',
-                valid_types = ['ST_GEOMETRY'])
-
-    def srs_name(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with strings representing the name of the spatial
-        reference system of each of the geometries in the calling IdaGeoSeries.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_SRSNAME() function.
-
-        Examples
-        --------
-        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
-        >>> tornadoes.set_geometry('SHAPE')
-        >>> tornadoes['srs_name'] = tornadoes.srs_name()
-        >>> tornadoes[['OBJECTID', 'SHAPE', 'srs_name']].head()
-        OBJECTID   SHAPE 	                                            srs_name
-        1 	       MULTILINESTRING ((-90.2200062071 38.7700071663...    SAMPLE_GCS_WGS_1984
-        2 	       MULTILINESTRING ((-89.3000059755 39.1000072739...    SAMPLE_GCS_WGS_1984
-        3 	       MULTILINESTRING ((-84.5800047496 40.8800078382...    SAMPLE_GCS_WGS_1984
-        4 	       MULTILINESTRING ((-94.3700070010 34.4000061520...    SAMPLE_GCS_WGS_1984
-        5 	       MULTILINESTRING ((-90.6800062393 37.6000069289...    SAMPLE_GCS_WGS_1984
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_SRSNAME',
-                valid_types = ['ST_GEOMETRY'])
-
-    def geometry_type(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry.
-
-        Returns an IdaSeries with strings representing the fully qualified type
-        name of the dynamic type of each of the geometries in the calling
-        IdaGeoSeries.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_GEOMETRYTYPE() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> counties.geometry_type().head(3)
-        0    "DB2GSE  "."ST_MULTIPOLYGON"
-        1    "DB2GSE  "."ST_MULTIPOLYGON"
-        2    "DB2GSE  "."ST_MULTIPOLYGON"
-        Name: DB2GSE.ST_GEOMETRYTYPE(SHAPE), dtype: object
-        
-        See boundary method
-        
-        >>> counties["boundary"].geometry_type().head(3)
-        0    "DB2GSE  "."ST_LINESTRING"
-        1    "DB2GSE  "."ST_LINESTRING"
-        2    "DB2GSE  "."ST_LINESTRING"
-        Name: DB2GSE.ST_GEOMETRYTYPE(DB2GSE.ST_BOUNDARY(SHAPE)), dtype: object
-
-        See centroid method
-        
-        >>> counties["centroid"].geometry_type().head(3) 
-        0    "DB2GSE  "."ST_POINT"
-        1    "DB2GSE  "."ST_POINT"
-        2    "DB2GSE  "."ST_POINT"
-        Name: DB2GSE.ST_GEOMETRYTYPE(DB2GSE.ST_CENTROID(SHAPE)), dtype: object        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_GEOMETRYTYPE',
-                valid_types = ['ST_GEOMETRY'])
-
-    def area(self, unit = None):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with doubles representing the area covered by
-        each of the geometries in the calling IdaGeoSeries, in the given unit
-        or else in the default unit.
-
-        If the geometry is a polygon or multipolygon, then the area covered by
-        the geometry is returned. The area of points, linestrings, multipoints,
-        and multilinestrings is 0 (zero).
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Parameters
-        ----------
-        unit : str, optional
-            Name of the unit, it is case-insensitive.
-            If omitted, the following rules are used:
-
-                * If geometry is in a projected or geocentric coordinate
-                  system, the linear unit associated with this coordinate system
-                  is used.
-                * If geometry is in a geographic coordinate system, the angular
-                  unit associated with this coordinate system is used.
-
-        Returns
-        -------
-        IdaSeries.
-
-        See also
-        --------
-        linear_units
-
-        Notes
-        -----
-        Restrictions on unit conversions: An error (SQLSTATE 38SU4) is returned
-        if any of the following conditions occur:
-
-            * The geometry is in an unspecified coordinate system and the unit
-              parameter is specified.
-            * The geometry is in a projected coordinate system and an angular
-              unit is specified.
-            * The geometry is in a geographic coordinate system, and a linear
-              unit is specified.
-
-        # TODO: handle this SQLSTATE error
-
-        References
-        ----------
-        DB2 Spatial Extender ST_AREA() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> counties['area_in_km'] = counties.area(unit = 'KILOMETER')
-        >>> counties[['NAME','area_in_km']].head()
-        NAME         area_in_km
-        Wood         1606.526429
-        Cass         2485.836511
-        Washington   1459.393496
-        Fulton       1382.620091
-        Clay         2725.095566
-        """
-        additional_args = []
-        if unit is not None:
-            unit = self._check_linear_unit(unit)  # Can raise exceptions
-            additional_args.append(unit)
-        return self._unary_operation_handler(
-            db2gse_function = 'DB2GSE.ST_AREA',
-            valid_types = ['ST_GEOMETRY'],
-            additional_args = additional_args)
-
-    def dimension(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry.
-
-        Returns an IdaSeries with integers representing the dimension of each
-        of the geometries in the calling IdaGeoSeries.
-
-        If the given geometry is empty, then -1 is returned.
-        For points and multipoints, the dimension is 0 (zero).
-        For curves and multicurves, the dimension is 1.
-        For polygons and multipolygons, the dimension is 2.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_DIMENSION() function.
-
-        Examples
-        --------
-        >>> tornadoes = IdaGeoDataFrame(idadb, "SAMPLES.GEO_TORNADO, indexer = 'OBJECTID')
-        >>> tornadoes["buffer_20_km"] =  tornadoes.buffer(distance = 20, unit = 'KILOMETER')
-        >>> tornadoes["buffer_20_km_dim"] = tornadoes["buffer_20_km"].dimension()
-        >>> tornadoes[["buffer_20_km", "buffer_20_km_dim"]].head()
-        	buffer_20_km 	                                   buffer_20_km_dim
-        0 	POLYGON ((-97.6333717493 37.8952302197, -97.64... 	2
-        1 	POLYGON ((-91.1708885166 45.5539303808, -91.18... 	2
-        2 	POLYGON ((-90.3002953079 45.7499538112, -90.31... 	2
-        3 	POLYGON ((-90.5886004074 44.8899496933, -90.59... 	2
-        4 	POLYGON ((-89.6976750543 45.7399220716, -89.71... 	2
-
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>> counties['centroid_dim'] = counties['centroid'].dimension()
-        >>> counties[['centroid', 'centroid_dim']].head()
-        	centroid 	                            centroid_dim
-        0 	POINT (-99.2139812081 34.1463063676) 	0
-        1 	POINT (-96.3135712489 29.8489091869) 	0
-        2 	POINT (-99.4769986945 46.4576651942) 	0
-        3 	POINT (-107.9303239758 37.3196783851) 	0
-        4 	POINT (-91.0781652597 36.3077916744) 	0
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_DIMENSION',
-                valid_types = ['ST_GEOMETRY'])
-
-    def length(self, unit = None):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_LINESTRING, ST_MULTILINESTRING.
-
-        Returns an IdaSeries with doubles representing the length of each of
-        the curves or multicurves in the calling IdaGeoSeries, in the given
-        unit or else in the default unit.
-
-        For None curves or multicurves the output is None.
-        For empty curves or multicurves the output is None.
-
-        Parameters
-        ----------
-        unit : str, optional
-            Name of the unit, it is case-insensitive.
-            If omitted, the following rules are used:
-
-                * If curve is in a projected or geocentric coordinate system,
-                  the linear unit associated with this coordinate system is the
-                  default.
-                * If curve is in a geographic coordinate system, the angular
-                  unit associated with this coordinate system is the default.
-
-        Returns
-        -------
-        IdaSeries.
-
-        See also
-        --------
-        linear_units
-
-        Notes
-        -----
-        Restrictions on unit conversions: An error (SQLSTATE 38SU4) is returned
-        if any of the following conditions occur:
-
-            * The curve is in an unspecified coordinate system and the unit
-              parameter is specified.
-            * The curve is in a projected coordinate system and an angular unit
-              is specified.
-            * The curve is in a geographic coordinate system, and a linear unit
-              is specified.
-
-        # TODO: handle this SQLSTATE error
-
-        References
-        ----------
-        DB2 Spatial Extender ST_LENGTH() function.
-
-        Examples
-        --------
-        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
-        >>> tornadoes.set_geometry('SHAPE')
-        >>> tornadoes['length'] = tornadoes.length(unit = 'KILOMETER')
-        >>> tornadoes[['OBJECTID', 'SHAPE', 'length']].head()
-        OBJECTID    SHAPE                                              length
-        1           MULTILINESTRING ((-90.2200062071 38.7700071663..   17.798545
-        2           MULTILINESTRING ((-89.3000059755 39.1000072739...  6.448745
-        3           MULTILINESTRING ((-84.5800047496 40.8800078382...  0.014213
-        4           MULTILINESTRING ((-94.3700070010 34.4000061520..   0.014173
-        5           MULTILINESTRING ((-90.6800062393 37.6000069289..   4.254681
-        """
-        additional_args = []
-        if unit is not None:
-            unit = self._check_linear_unit(unit)  # Can raise exceptions
-            additional_args.append(unit)
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_LENGTH',
-                valid_types = ['ST_LINESTRING', 'ST_MULTILINESTRING'],
-                additional_args = additional_args)
-
-    def perimeter(self, unit = None):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_POLYGON, ST_MULTIPOLYGON.
-
-        Returns an IdaSeries with doubles representing the perimeter of each of
-        the surfaces or multisurfaces in the calling IdaGeoSeries, in the given
-        unit or else in the default unit.
-
-        For None curves or multicurves the output is None.
-        For empty curves or multicurves the output is None.
-
-        Parameters
-        ----------
-        unit : str, optional
-            Name of the unit, it is case-insensitive.
-            If omitted, the following rules are used:
-
-                * If surface is in a projected or geocentric coordinate system,
-                  the linear unit associated with this coordinate system is the
-                  default.
-                * If surface is in a geographic coordinate system, the angular
-                  unit associated with this coordinate system is the default.
-
-        Returns
-        -------
-        IdaSeries.
-
-        See also
-        --------
-        linear_units
-        
-        Notes
-        -----
-        Restrictions on unit conversions: An error (SQLSTATE 38SU4) is returned
-        if any of the following conditions occur:
-
-            * The geometry is in an unspecified coordinate system and the unit
-              parameter is specified.
-            * The geometry is in a projected coordinate system and an angular
-              unit is specified.
-            * The geometry is in a geographic coordinate system and a linear
-              unit is specified.
-
-        # TODO: handle this SQLSTATE error
-
-        References
-        ----------
-        DB2 Spatial Extender ST_PERIMETER() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = 'OBJECTID')
-        >>> counties["perimeter"] = counties.perimeter()
-        >>> counties[["NAME", "SHAPE", "perimeter"]].head()
-        	NAME 	    SHAPE 	                                           perimeter
-        0 	Claiborne 	MULTIPOLYGON (((-91.1075396745 32.0529371857, ... 	2.033745
-        1 	Otsego 	    MULTIPOLYGON (((-84.3668321129 45.1987705896, ... 	1.656962
-        2 	Madison 	MULTIPOLYGON (((-94.2416445531 41.1571413434, ... 	1.600404
-        3 	Cleveland 	MULTIPOLYGON (((-91.9538053360 34.0641471950, ... 	1.662438
-        4 	McIntosh 	MULTIPOLYGON (((-95.9813144896 35.3768342559, ... 	2.122012       
-        """
-        additional_args = []
-        if unit is not None:
-            unit = self._check_linear_unit(unit)  # Can raise exceptions
-            additional_args.append(unit)
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_PERIMETER',
-                valid_types = ['ST_POLYGON', 'ST_MULTIPOLYGON'],
-                additional_args = additional_args)
-
-    def num_geometries(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_MULTIPOINT, ST_MULTIPOLYGON, ST_MULTILINESTRING.
-
-        Returns an IdaSeries with integers representing the number of
-        geometries in each of the collections in the calling IdaGeoSeries.
-
-        For None collections the output is None.
-        For empty collections the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_NUMGEOMETRIES() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = "OBJECTID", geometry = "SHAPE")
-        >>> print(counties.geometry.dtypes)
-                      TYPENAME
-        SHAPE  ST_MULTIPOLYGON        
-        >>> counties["SHAPE"].num_geometries().head()
-        0    1
-        1    1
-        2    1
-        3    1
-        4    1
-        Name: DB2GSE.ST_NUMGEOMETRIES(SHAPE), dtype: int64
-        
-        Use sample data created in Db2 with SQL script, data type ST_MultiLineString
-        
-        >>> sample_mlines = IdaGeoDataFrame(idadb, "SAMPLE_MLINES", indexer = "ID", geometry = "GEOMETRY")
-        >>> print(sample_mlines.geometry.dtypes)
-                            TYPENAME
-        GEOMETRY  ST_MULTILINESTRING
-        
-        >>> sample_mlines.num_geometries().head()
-        0    3
-        Name: DB2GSE.ST_NUMGEOMETRIES(GEOMETRY), dtype: int64        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_NUMGEOMETRIES',
-                valid_types = ['ST_MULTIPOINT', 'ST_MULTIPOLYGON',
-                               'ST_MULTILINESTRING'])
-
-    def num_interior_ring(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_POLYGON.
-
-        Returns an IdaSeries with integers representing the number of interior
-        rings of each of the polygons in the calling IdaGeoSeries.
-
-        For None collections the output is None.
-        For empty collections the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_NUMINTERIORRING() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_POLYGONS, obtained with SQL script
-        
-        >>> sample_polygons["int_ring"] = sample_polygons.num_interior_ring()
-        >>> sample_polygons[["GEOMETRY", "int_ring"]].head()        
-        	GEOMETRY 	                                       int_ring
-        0 	POLYGON ((110.000000 120.000000, 120.000000 13... 	0
-        1 	POLYGON ((110.000000 120.000000, 130.000000 12... 	1        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_NUMINTERIORRING',
-                valid_types = ['ST_POLYGON'])
-
-    def num_line_strings(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_MULTILINESTRING.
-
-        Returns an IdaSeries with integers representing the number of
-        linestrings of each of the multilinestrings in the calling
-        IdaGeoSeries.
-
-        For None multilinestrings the output is None.
-        For empty multilinestrings the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_NUMLINESTRINGS() function.
-
-        Examples
-        --------
-        Use sample data created in Db2 with SQL script, data type ST_MultiLineString
-        
-        >>> sample_mlines = IdaGeoDataFrame(idadb, "SAMPLE_MLINES", indexer = "ID", geometry = "GEOMETRY")       
-        >>> sample_mlines.num_line_strings().head()
-        0    3
-        Name: DB2GSE.ST_NUMLINESTRINGS(GEOMETRY), dtype: int64
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_NUMLINESTRINGS',
-                valid_types = ['ST_MULTILINESTRING'])
-
-    def num_points(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with integers representing the number of points of
-        each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_NUMPOINTS() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_GEOMETRIES, obtained with SQL script
-        
-        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
-        >>> sample_geometries["num_points"] = sample_geometries.num_points()
-        >>> sample_geometries[["GEOMETRY", "num_points"]].head()
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_NUMPOINTS',
-                valid_types = ['ST_GEOMETRY'])
-
-    def num_polygons(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_MULTIPOLYGON.
-
-        Returns an IdaSeries with integers representing the number of
-        polygons of each of the multipolygons in the calling IdaGeoSeries.
-
-        For None multipolygons the output is None.
-        For empty multipolygons the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_NUMPOLYGONS() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = "OBJECTID", geometry = "SHAPE")
-        >>> counties["NUM_POLY"] = counties.num_polygons()
-        >>> print(counties['NUM_POLY'][counties['NUM_POLY']>1].shape)
-        (57, 1)
-        >>> counties["NUM_POLY"][counties["NUM_POLY"]>1].head()
-        0    4
-        1    2
-        2    2
-        3    2
-        4    2
-        Name: NUM_POLY, dtype: int64
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_NUMPOLYGONS',
-                valid_types = ['ST_MULTIPOLYGON'])
-
-    def coord_dim(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with integers representing the dimensionality of
-        the coordinates of each of the geometries in the calling IdaGeoSeries.
-
-        If the given geometry does not have Z and M coordinates,
-        the dimensionality is 2.
-        If it has Z coordinates and no M coordinates, or if it has M
-        coordinates and no Z coordinates, the dimensionality is 3.
-        If it has Z and M coordinates, the dimensionality is 4.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_COORDDIM() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = "OBJECTID", geometry = "SHAPE")        
-        >>> counties.coord_dim().head()
-        0    2
-        1    2
-        2    2
-        3    2
-        4    2
-        Name: DB2GSE.ST_COORDDIM(DB2GSE.ST_CENTROID(SHAPE)), dtype: int64
-        # use sample table SAMPLE_POINTS, obtained with SQL script
-        >>> sample_points = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID", geometry = "LOC")        
-        >>> sample_points['coord_dim'] = sample_points.coord_dim()
-        >>> sample_points[['ID', 'LOC','coord_dim']].head()
-         	ID 	LOC 	                            coord_dim
-        0 	1 	POINT (14.000000 58.000000) 	      2
-        1 	2 	POINT Z(12.000000 35.000000 12)      3
-        2 	3 	POINT ZM(12.000000 66.000000 43 45)  4
-        3 	4 	POINT M(14.000000 58.000000 4) 	     3
-        4 	5 	POINT Z(12.000000 35.000000 12) 	 3
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_COORDDIM',
-                valid_types = ['ST_GEOMETRY'])
-
-    def is_3d(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with integers (1 if it has Z coordiantes, 0
-        otherwise) for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_IS3D() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_POINTS, obtained with SQL script
-
-        >>> sample_points = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "id", geometry = "LOC")
-        >>> sample_points["is_3d"] = sample_points.is_3d()
-        >>> sample_points[["LOC", "is_3d"]].head()
-         	LOC 	                            is_3d
-        0 	POINT (14.000000 58.000000) 	     0
-        1 	POINT Z(12.000000 35.000000 12) 	 1
-        2 	POINT ZM(12.000000 66.000000 43 45)  1
-        3 	POINT M(14.000000 58.000000 4) 	     0
-        4 	POINT Z(12.000000 35.000000 12) 	 1        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_IS3D',
-                valid_types = ['ST_GEOMETRY'])
-
-    def is_measured(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with integers (1 if it has M coordiantes, 0
-        otherwise) for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_ISMEASURED() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_POINTS, obtained with SQL script
-        
-        >>> sample_points = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "id", geometry = "LOC")
-        >>> sample_points["is_M"]=sample_points.is_measured()
-        >>> sample_points.head()
-        	ID 	LOC 	                              coord_dim is_3d 	is_M
-        0 	1 	POINT (14.000000 58.000000) 	        2 	      0 	0
-        1 	2 	POINT Z(12.000000 35.000000 12) 	    3 	      1 	0
-        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	4 	      1 	1
-        3 	4 	POINT M(14.000000 58.000000 4)      	3 	      0 	1
-        4 	5 	POINT Z(12.000000 35.000000 12) 	    3 	      1 	0
-
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_ISMEASURED',
-                valid_types = ['ST_GEOMETRY'])
-
-    def is_valid(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with integers (1 if it is valid, 0 otherwise) for
-        each of the geometries in the calling IdaGeoSeries.
-
-        A geometry is valid only if all of the attributes in the structured
-        type are consistent with the internal representation of geometry data,
-        and if the internal representation is not corrupted.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_ISVALID() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_POINTS, obtained with SQL script
-        
-        >>> sample_points = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "id", geometry = "LOC")
-        >>> sample_points.is_valid().head()
-        0    1
-        1    1
-        2    1
-        3    1
-        4    1
-        Name: DB2GSE.ST_ISVALID(LOC), dtype: int64        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_ISVALID',
-                valid_types = ['ST_GEOMETRY'])
-
-    def max_m(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with doubles representing the maximum M coordinate
-        for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-        For geometries without M coordinate the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MAXM() function.
-
-        Examples
-        --------
-        Max M, X, Y and Z
-
-        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
-        >>> sample_geometries["max_X"] = sample_geometries.max_x()
-        >>> sample_geometries["max_Y"] = sample_geometries.max_y()
-        >>> sample_geometries["max_Z"] = sample_geometries.max_z()
-        >>> sample_geometries["max_M"] = sample_geometries.max_m()
-         	ID 	GEOMETRY 	              max_X 	max_Y 	max_Z 	max_M
-        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
-        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	5.0 	4.0 	None 	None
-        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
-        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
-        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	35.0 	6.0 	None 	None
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MAXM',
-                valid_types = ['ST_GEOMETRY'])
-
-    def max_x(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with doubles representing the maximum X coordinate
-        for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MAXX() function.
-
-        Examples
-        --------
-        Max M, X, Y and Z        
-        
-        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
-        >>> sample_geometries["max_X"] = sample_geometries.max_x()
-        >>> sample_geometries["max_Y"] = sample_geometries.max_y()
-        >>> sample_geometries["max_Z"] = sample_geometries.max_z()
-        >>> sample_geometries["max_M"] = sample_geometries.max_m()
-         	ID 	GEOMETRY 	              max_X 	max_Y 	max_Z 	max_M
-        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
-        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	5.0 	4.0 	None 	None
-        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
-        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
-        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	35.0 	6.0 	None 	None
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MAXX',
-                valid_types = ['ST_GEOMETRY'])
-
-    def max_y(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with doubles representing the maximum Y coordinate
-        for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MAXY() function.
-
-        Examples
-        --------
-        Max M, X, Y and Z        
-        
-        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
-        >>> sample_geometries["max_X"] = sample_geometries.max_x()
-        >>> sample_geometries["max_Y"] = sample_geometries.max_y()
-        >>> sample_geometries["max_Z"] = sample_geometries.max_z()
-        >>> sample_geometries["max_M"] = sample_geometries.max_m()
-         	ID 	GEOMETRY 	              max_X 	max_Y 	max_Z 	max_M
-        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
-        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	5.0 	4.0 	None 	None
-        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
-        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
-        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	35.0 	6.0 	None 	None
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MAXY',
-                valid_types = ['ST_GEOMETRY'])
-
-    def max_z(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with doubles representing the maximum Z coordinate
-        for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-        For geometries without Z coordinate the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MAXZ() function.
-
-        Examples
-        --------
-        Max M, X, Y and Z        
-        
-        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
-        >>> sample_geometries["max_X"] = sample_geometries.max_x()
-        >>> sample_geometries["max_Y"] = sample_geometries.max_y()
-        >>> sample_geometries["max_Z"] = sample_geometries.max_z()
-        >>> sample_geometries["max_M"] = sample_geometries.max_m()
-         	ID 	GEOMETRY 	              max_X 	max_Y 	max_Z 	max_M
-        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
-        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	5.0 	4.0 	None 	None
-        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
-        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
-        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	35.0 	6.0 	None 	None
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MAXZ',
-                valid_types = ['ST_GEOMETRY'])
-
-    def min_m(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with doubles representing the minimum M coordinate
-        for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-        For geometries without M coordinate the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MINM() function.
-
-        Examples
-        --------
-        Min M, X, Y and Z   
-        Use sample table SAMPLE_GEOMETRIES, obtained with SQL script
-        
-        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
-        >>> sample_geometries["min_X"] = sample_geometries.min_x()
-        >>> sample_geometries["min_Y"] = sample_geometries.min_y()
-        >>> sample_geometries["min_Z"] = sample_geometries.min_z()
-        >>> sample_geometries["min_M"] = sample_geometries.min_m()
-        >>> sample_geometries.head()        
-        	ID 	GEOMETRY 	min_X 	min_Y 	min_Z 	min_M
-        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
-        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	0.0 	0.0 	None 	None
-        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
-        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
-        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	33.0 	2.0 	None 	None
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MINM',
-                valid_types = ['ST_GEOMETRY'])
-
-    def min_x(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with doubles representing the minimum X coordinate
-        for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MINX() function.
-
-        Examples
-        --------
-        >>> counties = IdaDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = 'OBJECTID')
-        >>> counties.set_geometry("SHAPE")
-        >>> counties.min_x().head()
-        0   -100.227146
-        1    -77.749934
-        2    -85.401789
-        3    -83.794279
-        4    -79.856688
-        Name: DB2GSE.ST_MINX(SHAPE), dtype: float64
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MINX',
-                valid_types = ['ST_GEOMETRY'])
-
-    def min_y(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with doubles representing the minimum Y coordinate
-        for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MINY() function.
-
-        Examples
-        --------
-        >>> counties = IdaDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = 'OBJECTID')
-        >>> counties.set_geometry("SHAPE")
-        >>> counties.min_y().head()
-        0    37.912775
-        1    41.998697
-        2    37.630910
-        3    35.562878
-        4    37.005883
-        Name: DB2GSE.ST_MINY(SHAPE), dtype: float64
-
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MINY',
-                valid_types = ['ST_GEOMETRY'])
-
-    def min_z(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with doubles representing the minimum Z coordinate
-        for each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-        For empty geometries the output is None.
-        For geometries without Z coordinate the output is None.
-       
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_MINZ() function.
-
-        Examples
-        --------
-        Min M, X, Y and Z   
-        Use sample table SAMPLE_GEOMETRIES, obtained with SQL script
-        
-        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
-        >>> sample_geometries["min_X"] = sample_geometries.min_x()
-        >>> sample_geometries["min_Y"] = sample_geometries.min_y()
-        >>> sample_geometries["min_Z"] = sample_geometries.min_z()
-        >>> sample_geometries["min_M"] = sample_geometries.min_m()
-        >>> sample_geometries.head()        
-        	ID 	GEOMETRY 	min_X 	min_Y 	min_Z 	min_M
-        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
-        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	0.0 	0.0 	None 	None
-        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
-        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
-        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	33.0 	2.0 	None 	None
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_MINZ',
-                valid_types = ['ST_GEOMETRY'])
-
-    def m(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_POINT.
-
-        Returns an IdaSeries with doubles representing the measure (M)
-        coordinate of each of the points in the calling IdaGeoSeries.
-
-        For None points the output is None.
-        For empty points the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_M() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_POINTS, obtained with SQL script
-        
-        >>> sample_points_extractor = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID")
-        >>> sample_points_extractor.set_geometry("LOC")
-        >>> sample_points_extractor["X"] = sample_points_extractor.x()
-        >>> sample_points_extractor["Y"] = sample_points_extractor.y()
-        >>> sample_points_extractor["Z"] = sample_points_extractor.z()
-        >>> sample_points_extractor["M"] = sample_points_extractor.m()
-        >>> sample_points_extractor.head()
-         	ID 	LOC 	X 	Y 	Z 	M
-        0 	1 	POINT (14.000000 58.000000) 	14.0 	58.0 	NaN 	NaN
-        1 	2 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
-        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	12.0 	66.0 	43.0 	45.0
-        3 	4 	POINT M(14.000000 58.000000 4) 	14.0 	58.0 	NaN 	4.0
-        4 	5 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_M',
-                valid_types = ['ST_POINT'])
-
-    def x(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_POINT.
-
-        Returns an IdaSeries with doubles representing the X coordinate of each
-        of the points in the calling IdaGeoSeries.
-
-        For None points the output is None.
-        For empty points the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_X() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_POINTS, obtained with SQL script
-        
-        >>> sample_points_extractor = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID")
-        >>> sample_points_extractor.set_geometry("LOC")
-        >>> sample_points_extractor["X"] = sample_points_extractor.x()
-        >>> sample_points_extractor["Y"] = sample_points_extractor.y()
-        >>> sample_points_extractor["Z"] = sample_points_extractor.z()
-        >>> sample_points_extractor["M"] = sample_points_extractor.m()
-        >>> sample_points_extractor.head()
-         	ID 	LOC 	X 	Y 	Z 	M
-        0 	1 	POINT (14.000000 58.000000) 	14.0 	58.0 	NaN 	NaN
-        1 	2 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
-        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	12.0 	66.0 	43.0 	45.0
-        3 	4 	POINT M(14.000000 58.000000 4) 	14.0 	58.0 	NaN 	4.0
-        4 	5 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
-
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_X',
-                valid_types = ['ST_POINT'])
-
-    def y(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_POINT.
-
-        Returns an IdaSeries with doubles representing the Y coordinate of each
-        of the points in the calling IdaGeoSeries.
-
-        For None points the output is None.
-        For empty points the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_Y() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_POINTS, obtained with SQL script
-        
-        >>> sample_points_extractor = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID")
-        >>> sample_points_extractor.set_geometry("LOC")
-        >>> sample_points_extractor["X"] = sample_points_extractor.x()
-        >>> sample_points_extractor["Y"] = sample_points_extractor.y()
-        >>> sample_points_extractor["Z"] = sample_points_extractor.z()
-        >>> sample_points_extractor["M"] = sample_points_extractor.m()
-        >>> sample_points_extractor.head()
-         	ID 	LOC 	X 	Y 	Z 	M
-        0 	1 	POINT (14.000000 58.000000) 	14.0 	58.0 	NaN 	NaN
-        1 	2 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
-        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	12.0 	66.0 	43.0 	45.0
-        3 	4 	POINT M(14.000000 58.000000 4) 	14.0 	58.0 	NaN 	4.0
-        4 	5 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
-
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_Y',
-                valid_types = ['ST_POINT'])
-
-    def z(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_POINT.
-
-        Returns an IdaSeries with doubles representing the Z coordinate of each
-        of the points in the calling IdaGeoSeries.
-
-        For None points the output is None.
-        For empty points the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_Z() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_POINTS, obtained with SQL script
-        
-        >>> sample_points_extractor = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID")
-        >>> sample_points_extractor.set_geometry("LOC")
-        >>> sample_points_extractor["X"] = sample_points_extractor.x()
-        >>> sample_points_extractor["Y"] = sample_points_extractor.y()
-        >>> sample_points_extractor["Z"] = sample_points_extractor.z()
-        >>> sample_points_extractor["M"] = sample_points_extractor.m()
-        >>> sample_points_extractor.head()
-         	ID 	LOC 	X 	Y 	Z 	M
-        0 	1 	POINT (14.000000 58.000000) 	14.0 	58.0 	NaN 	NaN
-        1 	2 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
-        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	12.0 	66.0 	43.0 	45.0
-        3 	4 	POINT M(14.000000 58.000000 4) 	14.0 	58.0 	NaN 	4.0
-        4 	5 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
-
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_Z',
-                valid_types = ['ST_POINT'])
-
-    def is_closed(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_LINESTRING, ST_MULTILINESTRING.
-
-        Returns an IdaSeries with integers (1 if it is closed, 0 otherwise) for
-        each of the curves or multicurves in the calling IdaGeoSeries.
-
-        A curve is closed if the start point and end point are equal.
-        If the curve has Z coordinates, the Z coordinates of the start and end
-        point must be equal. Otherwise, the points are not considered equal,
-        and the curve is not closed.
-        A multicurve is closed if each of its curves are closed.
-
-        For None curves or multicurves the output is None.
-        For empty curves or multicurves the output is 0.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_ISCLOSED() function.
-
-        Examples
-        --------
-        Use sample table SAMPLE_LINES, obtained with SQL script
-        
-        >>> samplelines = IdaGeoDataFrame(idadb, "SAMPLE_LINES", indexer = "ID", geometry = "GEOMETRY")
-        >>> sample_lines.is_closed().head()
-        0    0
-        1    0
-        Name: DB2GSE.ST_ISCLOSED(GEOMETRY), dtype: int64
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_ISCLOSED',
-                valid_types = ['ST_LINESTRING', 'ST_MULTILINESTRING'])
-
-    def is_empty(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with integers (1 if it is empty, 0 otherwise) for
-        each of the geometries in the calling IdaGeoSeries.
-
-        For None geometries the output is None.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_ISEMPTY() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>>counties["boundary"] = counties.boundary()
-        >>> counties["boundary"].is_empty().head(3)
-        0    0
-        1    0
-        2    0
-        Name: DB2GSE.ST_ISEMPTY(DB2GSE.ST_BOUNDARY(SHAPE)), dtype: int64        
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_ISEMPTY',
-                valid_types = ['ST_GEOMETRY'])
-
-    def is_simple(self):
-        """
-        Valid types for the column in the calling IdaGeoSeries:
-        ST_Geometry or one of its subtypes.
-
-        Returns an IdaSeries with integers (1 if it is simple, 0 otherwise) for
-        each of the geometries in the calling IdaGeoSeries.
-
-        Points, surfaces, and multisurfaces are always simple.
-        A curve is simple if it does not pass through the same point twice.
-        Amultipoint is simple if it does not contain two equal points.
-        A multicurve is simple if all of its curves are simple and the only
-        intersections occur at points that are on the boundary of the curves in
-        the multicurve.
-
-        For None geometries the output is None.
-        For empty geometries the output is 1.
-
-        Returns
-        -------
-        IdaSeries.
-
-        References
-        ----------
-        DB2 Spatial Extender ST_ISSIMPLE() function.
-
-        Examples
-        --------
-        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
-        >>> counties.set_geometry('SHAPE')
-        >>>counties["boundary"] = counties.boundary()
-        >>> counties["is_simple"] = counties.is_simple()
-        >>> filtered_counties = counties[counties['is_simple'] == 0]
-        >>> filtered_counties.shape
-        (0, 25)
-        
-        >>> counties["is_simple"] = counties['boundary'].is_simple()
-        >>> filtered_counties = counties[counties['is_simple'] == 0]
-        >>> filtered_counties.shape
-        (37, 25)
-        """
-        return self._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_ISSIMPLE',
-                valid_types = ['ST_GEOMETRY'])
-
-
-#==============================================================================
-### Public utilities for geospatial methods
-#==============================================================================
-
-    @lazy
-    def linear_units(self):
-        """
-        Returns
-        -------
-        list of str
-            The list of all allowed linear units that can be passed as option to geospatial methods.
-        """
-            
-        units = self.ida_query(
-            'SELECT UNIT_NAME FROM DB2GSE.ST_UNITS_OF_MEASURE WHERE '
-            'UNIT_TYPE= \'LINEAR\' ORDER BY LENGTH(UNIT_NAME), UNIT_NAME')
-        return units
-
-#==============================================================================
-### Private utilities for geospatial methods
-#==============================================================================
-
-    def _check_linear_unit(self, unit):
-        """
-        Parameters:
-        -----------
-        unit : str
-            Name of a user-entered unit (case-insensitive).
-
-        Returns
-        -------
-        str
-            The name of the unit in uppercase and formatted for DB2GSE syntax.
-
-        Raises
-        ------
-        TypeError
-            * If the unit is not a string
-            * If the unit is a string larger than 128 characters
-        IdaGeoDataFrameError
-            If the given unit is not a valid linear unit of DB2GSE.
-        """
-
-        if not isinstance(unit, six.string_types):
-            raise TypeError("unit must be a string")
-        elif len(unit) > 128:
-            raise TypeError("unit length exceeded")
-        else:
-            unit = unit.upper()
-            if unit not in self.linear_units.tolist():
-                raise IdaGeoDataFrameError(
-                    "Invalid unit\n"
-                    "Hint: use linear_units attribute to see the valid units")
-            else:
-                # Replace single quotation marks with two of them
-                if "\'" in unit:
-                    unit = unit.replace("'", "''")
-
-                # Enclose in single quotation marks
-                unit = '\''+unit+'\''
-                return unit
-
-    def _unary_operation_handler(self, db2gse_function,
-                                 valid_types,
-                                 additional_args = None):
-        """
-        Returns the resulting column of an unary geospatial method as an
-        IdaGeoSeries if it has geometry type, as an IdaSeries otherwise.
-
-        Parameters
-        ----------
-        db2gse_function : str
-            Name of the corresponding DB2GSE function.
-        valid_types : list of str
-            Valid input typenames.
-        additional_args : list of str, optional
-            Additional arguments for the DB2GSE function.
-
-        Returns
-        -------
-        IdaGeoSeries
-            If the resulting column has geometry type.
-        IdaSeries
-            If the resulting column doesn't have geometry type.
-        """
-        if not (self.dtypes.TYPENAME[0] in valid_types or 
-                valid_types[0] == 'ST_GEOMETRY'):
-            raise TypeError("Column " + self.column +
-                            " has incompatible type.")
-
-        # Obtain an IdaSeries object by cloning current one
-        # Then modify its attribute column
-        idaseries = self._clone()
-
-        # Get the first argument for the DB2GSE function, i.e. a column.
-        # Because it might be a non-destructive column that was added by the
-        # user, the column definition is considered, instead of its alias
-        # in the Ida object.
-        column_for_db2gse = self.internal_state.columndict[self.column]
-
-        arguments_for_db2gse_function = [column_for_db2gse]
-
-        if additional_args is not None:
-            for arg in additional_args:
-                arguments_for_db2gse_function.append(arg)
-
-        result_column = (
-            db2gse_function +
-            '(' +
-            ','.join(map(str, arguments_for_db2gse_function)) +
-            ')'
-            )
-
-        new_columndict = OrderedDict()
-        # result_column_key must not include double quotes because it is used as as Python key and as
-        # an SQL alias for the result column expression like in
-        # SELECT DB2GSE.ST_AREA("SHAPE",'KILOMETER') AS "DB2GSE.ST_AREA(SHAPE,'KILOMETER')" FROM SAMPLES.GEO_COUNTY
-        result_column_key = result_column.replace('"', '')
-        new_columndict[result_column_key] = result_column
-
-        idaseries._reset_attributes(["columns", "shape", "dtypes"])
-        idaseries.internal_state.columns = ['\"' + result_column_key + '\"']
-
-        idaseries.internal_state.columndict = new_columndict
-        idaseries.internal_state.update()
-
-        # Set the column attribute of the new idaseries
-        idaseries.column = result_column_key
-        
-        try:
-            del(idaseries.columns)
-        except:
-            pass
-        
-        if idaseries.dtypes.TYPENAME[result_column_key].find('ST_') == 0:
-            return IdaGeoSeries.from_IdaSeries(idaseries)
-        else:
-            return idaseries
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+IdaGeoSeries
+"""
+
+# Ensure Python 2 compatibility
+from __future__ import print_function
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import absolute_import
+from builtins import map
+from builtins import super
+from future import standard_library
+standard_library.install_aliases()
+
+from numbers import Number
+from collections import OrderedDict
+
+from lazy import lazy
+import six
+
+import nzpyida
+from nzpyida.series import IdaSeries
+from nzpyida.exceptions import IdaGeoDataFrameError
+
+class IdaGeoSeries(nzpyida.IdaSeries):
+    """
+    An IdaSeries whose column must have geometry type.
+    It has geospatial methods based on Db2 Warehouse Spatial Extender (DB2GSE).
+    
+    Note on sample data used for the examples:
+
+        * Sample tables available out of the box in Db2 Warehouse:
+
+          GEO_TORNADO, GEO_COUNTY
+
+        * Sample tables which you can create by executing the SQL statements in
+          https://github.com/ibmdbanalytics/ibmdbpy/blob/master/ibmdbpy/sampledata/sql_script:
+
+          SAMPLE_POLYGONS, SAMPLE_LINES, SAMPLE_GEOMETRIES, SAMPLE_MLINES, SAMPLE_POINTS
+
+    Notes
+    -----
+    IdaGeoDataSeries objects are not supported on Netezza.
+
+    An IdaGeoSeries doesn't have an indexer attribute because geometries are
+    unorderable in DB2 Spatial Extender.
+
+    Examples
+    --------
+    >>> idageodf = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer='OBJECTID', geometry = "SHAPE")
+    >>> idageoseries = idageodf["SHAPE"]
+    >>> idageoseries.dtypes
+                 -------------------
+                | TYPE_NAME         |
+         ----------------------------
+        | SHAPE | ST_MULTIPOLYGON   |
+         ----------------------------
+
+    """
+    def __init__(self, idadb, tablename, indexer, column):
+        """
+        Ensures that the specified column has geometry type.
+        See __init__ of IdaSeries.
+
+        Parameters
+        ----------
+        column : str
+            Column name. It must have geometry type.
+
+        Notes
+        -----
+        Even though geometry types are unorderable in DB2GSE, the IdaGeoSeries
+        might have as indexer another column of the table whose column the
+        IdaGeoSeries refers to.
+        """
+
+        if (idadb.__class__.__name__ == "IdaDataBase") & idadb._is_netezza_system():
+                    raise IdaGeoDataFrameError("IdaGeoDataSeries objects are not supported on Netezza.")
+
+        super(IdaGeoSeries, self).__init__(idadb, tablename, indexer, column)
+        if self.dtypes.TYPENAME[self.column].find('ST_') != 0:
+            raise TypeError("Specified column doesn't have geometry type. "
+                            "Cannot create IdaGeoSeries object")
+
+    @classmethod
+    def from_IdaSeries(cls, idaseries):
+        """
+        Creates an IdaGeoSeries from an IdaSeries, ensuring that the column
+        of the given IdaSeries has geometry type.
+        """
+        if not isinstance(idaseries, IdaSeries):
+            raise TypeError("Expected IdaSeries")
+        else:
+            # Mind that the given IdaSeries might have non-destructive
+            # columns that were added by the user. That's why __init__ is not
+            # used for this purpose.
+            if idaseries.dtypes.TYPENAME[idaseries.column].find('ST_') != 0:
+                raise TypeError(
+                    "The column of the IdaSeries doesn't have geometry type. "
+                    "Cannot create IdaGeoSeries object")
+            else:
+                idageoseries = idaseries
+                idageoseries.__class__ = IdaGeoSeries
+                return idageoseries
+
+#==============================================================================
+### Methods whose behavior is not defined for geometry types in DB2GSE.
+#==============================================================================
+
+    # TODO: Override all the methods of IdaSeries (and those of its parent,
+    # i.e. IdaDataFrame, which are not defined in DB2GSE for geometry columnns,
+    # like min(), max(), etc.)
+
+    def min(self):
+        raise TypeError("Unorderable geometries")
+        pass
+
+    def max(self):
+        raise TypeError("Unorderable geometries")
+        pass
+
+#==============================================================================
+### Unary geospatial methods
+#==============================================================================
+
+    def generalize(self, threshold):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoSeries of geometries which represent each of the
+        geometries in the calling IdaGeoSeries, but with a reduced number of
+        points, while preserving the general characteristics of the geometry.
+
+        The Douglas-Peucker line-simplification algorithm is used, by which the
+        sequence of points that define the geometry is recursively subdivided
+        until a run of the points can be replaced by a straight line segment.
+        In this line segment, none of the defining points deviates from the
+        straight line segment by more than the given threshold. Z and M
+        coordinates are not considered for the simplification. The resulting
+        geometry is in the spatial reference system of the given geometry.
+
+        For empty geometries, the output is an empty geometry of type ST_Point.
+        For None geometries the output is None.
+
+        Parameters
+        ----------
+        threshold : float
+            Threshold to be used for the line-simplification algorithm.
+            The threshold must be greater than or equal to 0.
+            The larger the threshold, the smaller the number of points that
+            will be used to represent the generalized geometry.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_GENERALIZE() function.
+
+        Examples
+        --------
+        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
+        >>> tornadoes.set_geometry('SHAPE')
+        >>> tornadoes['generalize'] = tornadoes.generalize(threshold = 4)
+        >>> tornadoes[['OBJECTID','generalize']].head()
+        OBJECTID  generalize
+        1         MULTILINESTRING ((-90.2200062071 38.7700071663...
+        2         MULTILINESTRING ((-89.3000059755 39.1000072739...
+        3         MULTILINESTRING ((-84.5800047496 40.8800078382...
+        4         MULTILINESTRING ((-94.3700070010 34.4000061520...
+        5         MULTILINESTRING ((-90.6800062393 37.6000069289...
+        """
+        try:
+            threshold = float(threshold)
+        except:
+            raise TypeError("threshold must be float")        
+        if threshold < 0:
+            raise ValueError("threshold must be greater than or equal to 0")
+        additional_args = [threshold]
+        return self._unary_operation_handler(
+            db2gse_function = 'DB2GSE.ST_GENERALIZE',
+            valid_types = ['ST_GEOMETRY'],
+            additional_args = additional_args)
+
+    def buffer(self, distance, unit = None):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoSeries of geometries in which each point is the
+        specified distance away from the geometries in the calling
+        IdaGeoSeries, measured in the given unit.
+
+        Parameters
+        ----------
+        distance : float
+            Distance, can be positive or negative.
+        unit : str, optional
+            Name of the unit, it is case-insensitive.
+            If omitted, the following rules are used:
+
+                * If geometry is in a projected or geocentric coordinate
+                  system, the linear unit associated with this coordinate system
+                  is the default.
+                * If geometry is in a geographic coordinate system, the angular
+                  unit associated with this coordinate system is the default.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        See also
+        ---------
+        linear_units
+
+        Notes
+        -----
+        Restrictions on unit conversions: An error (SQLSTATE 38SU4) is returned
+        if any of the following conditions occur:
+
+            * The geometry is in an unspecified coordinate system and the unit
+              parameter is specified.
+            * The geometry is in a projected coordinate system and an angular
+              unit is specified.
+            * The geometry is in a geographic coordinate system, but is not an
+              ST_Point value , and a linear unit is specified.
+
+        # TODO: handle this SQLSTATE error
+
+        References
+        ----------
+        DB2 Spatial Extender ST_BUFFER() function.
+
+        Examples
+        --------
+        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
+        >>> tornadoes.set_geometry('SHAPE')
+        >>> tornadoes['buffer_20_km'] = tornadoes.buffer(distance = 20, unit = 'KILOMETER')
+        >>> tornadoes[['OBJECTID','SHAPE','buffer_20_km']].head()
+        OBJECTID  SHAPE                                        buffer_20_km
+        1         MULTILINESTRING ((-90.2200062071 38.770....  POLYGON ((-90.3065519651 38.9369737029, -90.32..
+        2         MULTILINESTRING ((-89.3000059755 39.100....  POLYGON ((-89.3798853739 39.2690904737, -89.39.
+        3         MULTILINESTRING ((-84.5800047496 40.880....  POLYGON ((-84.7257488606 41.0222185578, -84.73...
+        4         MULTILINESTRING ((-94.3700070010 34.400....  POLYGON ((-94.5212609425 34.5296645617, -94.53...
+        5         MULTILINESTRING ((-90.6800062393 37.600....  POLYGON ((-90.8575378881 37.7120296620, -90.86...
+        """
+        if not isinstance(distance, Number):
+            # distance can be positive or negative
+            raise TypeError("Distance must be numerical")
+        additional_args = []
+        additional_args.append(distance)
+        if unit is not None:
+            unit = self._check_linear_unit(unit)  # Can raise exceptions
+            additional_args.append(unit)
+        return self._unary_operation_handler(
+            db2gse_function = 'DB2GSE.ST_BUFFER',
+            valid_types = ['ST_GEOMETRY'],
+            additional_args = additional_args)
+
+    def centroid(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoSeries of points which represent the geometric center
+        of each of the geometries in the calling IdaGeoSeries.
+
+        The geometric center is the center of the minimum bounding rectangle of
+        the given geometry, as a point.
+
+        The resulting point is represented in the spatial reference system of
+        the given geometry.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_CENTROID() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> counties['centroid'] = counties.centroid()
+        >>> counties[['NAME','centroid']].head()
+        NAME         centroid
+        Wood         POINT (-83.6490410160 41.3923524865)
+        Cass         POINT (-94.3483719161 33.0944709011)
+        Washington   POINT (-89.4241634562 38.3657576429)
+        Fulton       POINT (-74.4337987380 43.1359187016)
+        Clay         POINT (-96.5066339619 46.8908550036)
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_CENTROID',
+                valid_types = ['ST_GEOMETRY'])
+
+    def convex_hull(self):
+        """
+        The convex hull of a shape, also called convex envelope or convex closure, is the smallest convex set that contains it.
+        For example, if you have a bounded subset of points in the Euclidean space, the convex hull may be visualized as 
+        the shape enclosed by an elastic band stretched around the outside points of the subset. 
+        If vertices of the geometry do not form a convex, convexhull returns a null.
+        
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        If possible, the specific type of the returned geometry will be ST_Point, ST_LineString, or ST_Polygon.
+        The convex hull of a convex polygon with no holes is a single linestring, represented as ST_LineString. 
+        The convex hull of a non convex polygon does not exit. 
+        
+        Returns
+        -------
+        IdaGeoSeries
+
+            Returns an IdaGeoSeries containing geometries which are the convex hull of each
+            of the geometries in the calling IdaGeoSeries.
+            The resulting geometry is represented in the spatial reference system
+            of the given geometry.
+            For None geometries, for empty geometries and for non convex geometries the output is None.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_CONVEXHULL() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> counties['convex_envelope'] = counties["SHAPE"].convex_hull()
+        >>> counties[['OBJECTID','SHAPE','convex_envelope']].head()
+                OBJECTID 	SHAPE 	convex_envelope
+        0 	1 	MULTIPOLYGON (((-99.4756582604 33.8340108094, ... 	POLYGON ((-99.4756582604 33.8340108094, -99.47...
+        1 	2 	MULTIPOLYGON (((-96.6219873342 30.0442882117, ... 	POLYGON ((-96.6219873342 30.0442882117, -96.55...
+        2 	3 	MULTIPOLYGON (((-99.4497297204 46.6316377481, ... 	POLYGON ((-99.9174847900 46.3122496703, -99.91...
+        3 	4 	MULTIPOLYGON (((-107.4817473750 37.0000108736,... 	POLYGON ((-108.3792135685 36.9995188176, -108....
+        4 	5 	MULTIPOLYGON (((-91.2589262966 36.2578866492, ... 	POLYGON ((-91.4074433538 36.4871686853, -91.24...
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_CONVEXHULL',
+                valid_types = ['ST_GEOMETRY'])
+
+    def boundary(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoSeries of geometries which are the boundary of each
+        of the geometries in the calling IdaGeoSeries.
+
+        The resulting geometry is represented in the spatial reference system
+        of the given geometry.
+
+        If the given geometry is a point, multipoint, closed curve, or closed
+        multicurve, or if it is empty, then the result is an empty geometry of
+        type ST_Point. For curves or multicurves that are not closed, the start
+        points and end points of the curves are returned as an ST_MultiPoint
+        value, unless such a point is the start or end point of an even number
+        of curves. For surfaces and multisurfaces, the curve defining the
+        boundary of the given geometry is returned, either as an ST_Curve or an
+        ST_MultiCurve value.
+
+        If possible, the specific type of the returned geometry will be
+        ST_Point, ST_LineString, or ST_Polygon. For example, the boundary of a
+        polygon with no holes is a single linestring, represented as
+        ST_LineString. The boundary of a polygon with one or more holes
+        consists of multiple linestrings, represented as ST_MultiLineString.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_BOUNDARY() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> counties['boundary'] = counties.boundary()
+        >>> counties[['NAME','boundary']].head()
+        NAME         boundary
+        Madison      LINESTRING (-90.4500428418 32.5737889565, -90....
+        Lake         LINESTRING (-114.6043395348 47.7897504535, -11...
+        Broward      LINESTRING (-80.8798118938 26.2594597939, -80....
+        Buena Vista  LINESTRING (-95.3880180283 42.5617494883, -95....
+        Jones        LINESTRING (-77.0903250894 34.8027619185, -77..
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_BOUNDARY',
+                valid_types = ['ST_GEOMETRY'])
+
+    def envelope(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry.
+
+        Returns an IdaGeoSeries of polygons which are an envelope around each
+        of the geometries in the calling IdaGeoSeries. The envelope is a
+        rectangle that is represented as a polygon.
+
+        If the given geometry is a point, a horizontal linestring, or a
+        vertical linestring, then a rectangle, which is slightly larger than
+        the given geometry, is returned. Otherwise, the minimum bounding
+        rectangle of the geometry is returned as the envelope.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        See also
+        --------
+        mbr
+
+        References
+        ----------
+        DB2 Spatial Extender ST_ENVELOPE() function.
+
+        Examples
+        --------
+        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
+        >>> tornadoes.set_geometry('SHAPE')
+        >>> tornadoes['envelope'] = tornadoes.envelope()
+        >>> tornadoes[['OBJECTID', 'SHAPE', 'envelope']].head()
+        OBJECTID   SHAPE                                      envelope
+        1          MULTILINESTRING ((-90.2200062071 38.77..   POLYGON ((-90.2200062071 38.77..
+        2          MULTILINESTRING ((-89.3000059755 39.10..   POLYGON ((-89.3000059755 39.10..
+        3          MULTILINESTRING ((-84.5800047496 40.88..   POLYGON ((-84.5800047496 40.88..
+        4          MULTILINESTRING ((-94.3700070010 34.40..   POLYGON ((-94.3700070010 34.40..
+        5          MULTILINESTRING ((-90.6800062393 37.60..   POLYGON ((-90.6800062393 37.60..
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_ENVELOPE',
+                valid_types = ['ST_GEOMETRY'])
+
+    def exterior_ring(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Polygon.
+
+        Returns an IdaGeoSeries of curves which are the exterior ring of each
+        of the geometries in the calling IdaGeoSeries.
+
+        The resulting curve is represented in the spatial reference system of
+        the given polygon.
+
+        If the polygon does not have any interior rings, the returned exterior
+        ring is identical to the boundary of the polygon.
+
+        For None polygons the output is None.
+        For empty polygons the output is None.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_EXTERIORRING() function.
+
+        Examples
+        --------
+        >>> sample_polygons["ext_ring"] = sample_polygons.exterior_ring()
+        >>> sample_polygons.head()
+        ID 	GEOMETRY 	ext_ring
+        0 	1101 	POLYGON ((110.000000 120.000000, 120.000000 13... 	LINESTRING (110.000000 120.000000, 120.000000 ...
+        1 	1102 	POLYGON ((110.000000 120.000000, 130.000000 12... 	LINESTRING (110.000000 120.000000, 130.000000 ...
+
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_EXTERIORRING',
+                valid_types = ['ST_POLYGON'])
+
+    def mbr(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaGeoSeries of geometries which are the minimum bounding
+        rectangle of each of the geometries in the calling IdaGeoSeries.
+
+        If the given geometry is a point, then the point itself is returned.
+        If the geometry is a horizontal linestring or a vertical linestring,
+        the horizontal or vertical linestring itself is returned.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MBR() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> counties["MBR"] = counties.mbr()
+        >>> counties[["NAME", "SHAPE", "MBR"]].head()
+                NAME 	    SHAPE 	                                          MBR
+        0 	  Lafayette MULTIPOLYGON (((-90.4263836312 42.5071807967, ... 	POLYGON ((-90.4269086653 42.5056648248, -89.83...
+        1 	  Sanilac 	MULTIPOLYGON (((-82.1455052616 43.6955954588, ... 	POLYGON ((-83.1204005291 43.1541073218, -82.12...
+        2 	  Taylor 	MULTIPOLYGON (((-84.0691810519 32.5918031946, ... 	POLYGON ((-84.4532361602 32.3720591397, -84.00...
+        3 	  Ohio 	    MULTIPOLYGON (((-80.5191234475 40.0164178652, ... 	POLYGON ((-80.7338065145 40.0164178652, -80.51...
+        4 	  Houston 	MULTIPOLYGON (((-83.7877562454 32.5016909466, ... 	POLYGON ((-83.8568549803 32.2825891390, -83.48...
+
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MBR',
+                valid_types = ['ST_GEOMETRY'])
+
+    def end_point(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_LINESTRING.
+
+        Returns an IdaGeoSeries with the last point of each of the curves in
+        the calling IdaGeoSeries.
+
+        The resulting point is represented in the spatial reference system of
+        the given curve.
+
+        For None curves the output is None.
+        For empty curves the output is None.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_ENDPOINT() function.
+
+        Examples
+        --------
+        Sample to create in Db2, geometry column with data type ST_LineString
+        Use this sample data for testing:
+
+        >>> sample_lines = IdaGeoDataFrame(idadb, "SAMPLE_LINES", indexer = "ID", geometry  = "LOC")
+        >>> sample_lines['end_point'] = sample_lines.end_point()
+        >>> sample_lines.head()
+        	ID 	    GEOMETRY 	                                        end_point
+        0 	1110 	LINESTRING (850.000000 250.000000, 850.000000 ... 	POINT (850.000000 850.000000)
+        1 	1111 	LINESTRING (90.000000 90.000000, 100.000000 10... 	POINT (100.000000 100.000000)      
+        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_ENDPOINT',
+                valid_types = ['ST_LINESTRING'])
+
+    def mid_point(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_LINESTRING.
+
+        Returns an IdaGeoSeries of points which are equidistant from both ends
+        of each of the curves in the calling IdaGeoSeries, measured along the
+        curve.
+
+        The resulting point is represented in the spatial reference system of
+        the given curve.
+
+        If the curve contains Z coordinates or M coordinates (measures),
+        the midpoint is determined solely by the values of the X and Y
+        coordinates in the curve. The Z coordinate and measure in the returned
+        point are interpolated.
+
+        For None curves the output is None.
+        For empty curves, the output is an empty point.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MIDPOINT() function.
+
+        Examples
+        --------
+        Sample to create in Db2, geometry column with data type ST_LineString
+        Use this sample data for testing:
+        
+        >>> sample_lines = IdaGeoDataFrame(idadb, "SAMPLE_LINES", indexer = "ID", geometry  = "LOC")
+        >>> sample_lines["mid_point"] = sample_lines.mid_point()
+        >>> sample_lines.head()
+        	ID 	    GEOMETRY 	                                    	mid_point
+        0 	1110 	LINESTRING (850.000000 250.000000, 850.000000 ... 	POINT (850.000000 550.000000)
+        1 	1111 	LINESTRING (90.000000 90.000000, 100.000000 10... 	POINT (95.000000 95.000000)        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MIDPOINT',
+                valid_types = ['ST_LINESTRING'])
+
+    def start_point(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_LINESTRING.
+
+        Returns an IdaGeoSeries with the first point of each of the curves in
+        the calling IdaGeoSeries.
+
+        The resulting point is represented in the spatial reference system of
+        the given curve.
+
+        For None curves the output is None.
+        For empty curves the output is None.
+
+        Returns
+        -------
+        IdaGeoSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_STARTPOINT() function.
+
+        Examples
+        --------
+        Sample to create in Db2, geometry column with data type ST_LineString
+        
+        >>> sample_lines = IdaGeoDataFrame(idadb, "SAMPLE_LINES", indexer = "ID", geometry  = "LOC")
+        >>> sample_lines.start_point().head()
+        
+        0    POINT (850.000000 250.000000)
+        1    POINT (90.000000 90.000000)
+        Name: DB2GSE.ST_STARTPOINT(GEOMETRY), dtype: object        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_STARTPOINT',
+                valid_types = ['ST_LINESTRING'])
+
+    def srid(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with integers representing the spatial reference
+        system of each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_SRID() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = 'OBJECTID', geometry = 'SHAPE')        
+        >>> counties.srid().head()
+        0    1005
+        1    1005
+        2    1005
+        3    1005
+        4    1005
+        Name: DB2GSE.ST_SRID(SHAPE), dtype: int64        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_SRID',
+                valid_types = ['ST_GEOMETRY'])
+
+    def srs_name(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with strings representing the name of the spatial
+        reference system of each of the geometries in the calling IdaGeoSeries.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_SRSNAME() function.
+
+        Examples
+        --------
+        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
+        >>> tornadoes.set_geometry('SHAPE')
+        >>> tornadoes['srs_name'] = tornadoes.srs_name()
+        >>> tornadoes[['OBJECTID', 'SHAPE', 'srs_name']].head()
+        OBJECTID   SHAPE 	                                            srs_name
+        1 	       MULTILINESTRING ((-90.2200062071 38.7700071663...    SAMPLE_GCS_WGS_1984
+        2 	       MULTILINESTRING ((-89.3000059755 39.1000072739...    SAMPLE_GCS_WGS_1984
+        3 	       MULTILINESTRING ((-84.5800047496 40.8800078382...    SAMPLE_GCS_WGS_1984
+        4 	       MULTILINESTRING ((-94.3700070010 34.4000061520...    SAMPLE_GCS_WGS_1984
+        5 	       MULTILINESTRING ((-90.6800062393 37.6000069289...    SAMPLE_GCS_WGS_1984
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_SRSNAME',
+                valid_types = ['ST_GEOMETRY'])
+
+    def geometry_type(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry.
+
+        Returns an IdaSeries with strings representing the fully qualified type
+        name of the dynamic type of each of the geometries in the calling
+        IdaGeoSeries.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_GEOMETRYTYPE() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> counties.geometry_type().head(3)
+        0    "DB2GSE  "."ST_MULTIPOLYGON"
+        1    "DB2GSE  "."ST_MULTIPOLYGON"
+        2    "DB2GSE  "."ST_MULTIPOLYGON"
+        Name: DB2GSE.ST_GEOMETRYTYPE(SHAPE), dtype: object
+        
+        See boundary method
+        
+        >>> counties["boundary"].geometry_type().head(3)
+        0    "DB2GSE  "."ST_LINESTRING"
+        1    "DB2GSE  "."ST_LINESTRING"
+        2    "DB2GSE  "."ST_LINESTRING"
+        Name: DB2GSE.ST_GEOMETRYTYPE(DB2GSE.ST_BOUNDARY(SHAPE)), dtype: object
+
+        See centroid method
+        
+        >>> counties["centroid"].geometry_type().head(3) 
+        0    "DB2GSE  "."ST_POINT"
+        1    "DB2GSE  "."ST_POINT"
+        2    "DB2GSE  "."ST_POINT"
+        Name: DB2GSE.ST_GEOMETRYTYPE(DB2GSE.ST_CENTROID(SHAPE)), dtype: object        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_GEOMETRYTYPE',
+                valid_types = ['ST_GEOMETRY'])
+
+    def area(self, unit = None):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with doubles representing the area covered by
+        each of the geometries in the calling IdaGeoSeries, in the given unit
+        or else in the default unit.
+
+        If the geometry is a polygon or multipolygon, then the area covered by
+        the geometry is returned. The area of points, linestrings, multipoints,
+        and multilinestrings is 0 (zero).
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Parameters
+        ----------
+        unit : str, optional
+            Name of the unit, it is case-insensitive.
+            If omitted, the following rules are used:
+
+                * If geometry is in a projected or geocentric coordinate
+                  system, the linear unit associated with this coordinate system
+                  is used.
+                * If geometry is in a geographic coordinate system, the angular
+                  unit associated with this coordinate system is used.
+
+        Returns
+        -------
+        IdaSeries.
+
+        See also
+        --------
+        linear_units
+
+        Notes
+        -----
+        Restrictions on unit conversions: An error (SQLSTATE 38SU4) is returned
+        if any of the following conditions occur:
+
+            * The geometry is in an unspecified coordinate system and the unit
+              parameter is specified.
+            * The geometry is in a projected coordinate system and an angular
+              unit is specified.
+            * The geometry is in a geographic coordinate system, and a linear
+              unit is specified.
+
+        # TODO: handle this SQLSTATE error
+
+        References
+        ----------
+        DB2 Spatial Extender ST_AREA() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> counties['area_in_km'] = counties.area(unit = 'KILOMETER')
+        >>> counties[['NAME','area_in_km']].head()
+        NAME         area_in_km
+        Wood         1606.526429
+        Cass         2485.836511
+        Washington   1459.393496
+        Fulton       1382.620091
+        Clay         2725.095566
+        """
+        additional_args = []
+        if unit is not None:
+            unit = self._check_linear_unit(unit)  # Can raise exceptions
+            additional_args.append(unit)
+        return self._unary_operation_handler(
+            db2gse_function = 'DB2GSE.ST_AREA',
+            valid_types = ['ST_GEOMETRY'],
+            additional_args = additional_args)
+
+    def dimension(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry.
+
+        Returns an IdaSeries with integers representing the dimension of each
+        of the geometries in the calling IdaGeoSeries.
+
+        If the given geometry is empty, then -1 is returned.
+        For points and multipoints, the dimension is 0 (zero).
+        For curves and multicurves, the dimension is 1.
+        For polygons and multipolygons, the dimension is 2.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_DIMENSION() function.
+
+        Examples
+        --------
+        >>> tornadoes = IdaGeoDataFrame(idadb, "SAMPLES.GEO_TORNADO, indexer = 'OBJECTID')
+        >>> tornadoes["buffer_20_km"] =  tornadoes.buffer(distance = 20, unit = 'KILOMETER')
+        >>> tornadoes["buffer_20_km_dim"] = tornadoes["buffer_20_km"].dimension()
+        >>> tornadoes[["buffer_20_km", "buffer_20_km_dim"]].head()
+        	buffer_20_km 	                                   buffer_20_km_dim
+        0 	POLYGON ((-97.6333717493 37.8952302197, -97.64... 	2
+        1 	POLYGON ((-91.1708885166 45.5539303808, -91.18... 	2
+        2 	POLYGON ((-90.3002953079 45.7499538112, -90.31... 	2
+        3 	POLYGON ((-90.5886004074 44.8899496933, -90.59... 	2
+        4 	POLYGON ((-89.6976750543 45.7399220716, -89.71... 	2
+
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>> counties['centroid_dim'] = counties['centroid'].dimension()
+        >>> counties[['centroid', 'centroid_dim']].head()
+        	centroid 	                            centroid_dim
+        0 	POINT (-99.2139812081 34.1463063676) 	0
+        1 	POINT (-96.3135712489 29.8489091869) 	0
+        2 	POINT (-99.4769986945 46.4576651942) 	0
+        3 	POINT (-107.9303239758 37.3196783851) 	0
+        4 	POINT (-91.0781652597 36.3077916744) 	0
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_DIMENSION',
+                valid_types = ['ST_GEOMETRY'])
+
+    def length(self, unit = None):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_LINESTRING, ST_MULTILINESTRING.
+
+        Returns an IdaSeries with doubles representing the length of each of
+        the curves or multicurves in the calling IdaGeoSeries, in the given
+        unit or else in the default unit.
+
+        For None curves or multicurves the output is None.
+        For empty curves or multicurves the output is None.
+
+        Parameters
+        ----------
+        unit : str, optional
+            Name of the unit, it is case-insensitive.
+            If omitted, the following rules are used:
+
+                * If curve is in a projected or geocentric coordinate system,
+                  the linear unit associated with this coordinate system is the
+                  default.
+                * If curve is in a geographic coordinate system, the angular
+                  unit associated with this coordinate system is the default.
+
+        Returns
+        -------
+        IdaSeries.
+
+        See also
+        --------
+        linear_units
+
+        Notes
+        -----
+        Restrictions on unit conversions: An error (SQLSTATE 38SU4) is returned
+        if any of the following conditions occur:
+
+            * The curve is in an unspecified coordinate system and the unit
+              parameter is specified.
+            * The curve is in a projected coordinate system and an angular unit
+              is specified.
+            * The curve is in a geographic coordinate system, and a linear unit
+              is specified.
+
+        # TODO: handle this SQLSTATE error
+
+        References
+        ----------
+        DB2 Spatial Extender ST_LENGTH() function.
+
+        Examples
+        --------
+        >>> tornadoes = IdaGeoDataFrame(idadb,'SAMPLES.GEO_TORNADO',indexer='OBJECTID')
+        >>> tornadoes.set_geometry('SHAPE')
+        >>> tornadoes['length'] = tornadoes.length(unit = 'KILOMETER')
+        >>> tornadoes[['OBJECTID', 'SHAPE', 'length']].head()
+        OBJECTID    SHAPE                                              length
+        1           MULTILINESTRING ((-90.2200062071 38.7700071663..   17.798545
+        2           MULTILINESTRING ((-89.3000059755 39.1000072739...  6.448745
+        3           MULTILINESTRING ((-84.5800047496 40.8800078382...  0.014213
+        4           MULTILINESTRING ((-94.3700070010 34.4000061520..   0.014173
+        5           MULTILINESTRING ((-90.6800062393 37.6000069289..   4.254681
+        """
+        additional_args = []
+        if unit is not None:
+            unit = self._check_linear_unit(unit)  # Can raise exceptions
+            additional_args.append(unit)
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_LENGTH',
+                valid_types = ['ST_LINESTRING', 'ST_MULTILINESTRING'],
+                additional_args = additional_args)
+
+    def perimeter(self, unit = None):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_POLYGON, ST_MULTIPOLYGON.
+
+        Returns an IdaSeries with doubles representing the perimeter of each of
+        the surfaces or multisurfaces in the calling IdaGeoSeries, in the given
+        unit or else in the default unit.
+
+        For None curves or multicurves the output is None.
+        For empty curves or multicurves the output is None.
+
+        Parameters
+        ----------
+        unit : str, optional
+            Name of the unit, it is case-insensitive.
+            If omitted, the following rules are used:
+
+                * If surface is in a projected or geocentric coordinate system,
+                  the linear unit associated with this coordinate system is the
+                  default.
+                * If surface is in a geographic coordinate system, the angular
+                  unit associated with this coordinate system is the default.
+
+        Returns
+        -------
+        IdaSeries.
+
+        See also
+        --------
+        linear_units
+        
+        Notes
+        -----
+        Restrictions on unit conversions: An error (SQLSTATE 38SU4) is returned
+        if any of the following conditions occur:
+
+            * The geometry is in an unspecified coordinate system and the unit
+              parameter is specified.
+            * The geometry is in a projected coordinate system and an angular
+              unit is specified.
+            * The geometry is in a geographic coordinate system and a linear
+              unit is specified.
+
+        # TODO: handle this SQLSTATE error
+
+        References
+        ----------
+        DB2 Spatial Extender ST_PERIMETER() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = 'OBJECTID')
+        >>> counties["perimeter"] = counties.perimeter()
+        >>> counties[["NAME", "SHAPE", "perimeter"]].head()
+        	NAME 	    SHAPE 	                                           perimeter
+        0 	Claiborne 	MULTIPOLYGON (((-91.1075396745 32.0529371857, ... 	2.033745
+        1 	Otsego 	    MULTIPOLYGON (((-84.3668321129 45.1987705896, ... 	1.656962
+        2 	Madison 	MULTIPOLYGON (((-94.2416445531 41.1571413434, ... 	1.600404
+        3 	Cleveland 	MULTIPOLYGON (((-91.9538053360 34.0641471950, ... 	1.662438
+        4 	McIntosh 	MULTIPOLYGON (((-95.9813144896 35.3768342559, ... 	2.122012       
+        """
+        additional_args = []
+        if unit is not None:
+            unit = self._check_linear_unit(unit)  # Can raise exceptions
+            additional_args.append(unit)
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_PERIMETER',
+                valid_types = ['ST_POLYGON', 'ST_MULTIPOLYGON'],
+                additional_args = additional_args)
+
+    def num_geometries(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_MULTIPOINT, ST_MULTIPOLYGON, ST_MULTILINESTRING.
+
+        Returns an IdaSeries with integers representing the number of
+        geometries in each of the collections in the calling IdaGeoSeries.
+
+        For None collections the output is None.
+        For empty collections the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_NUMGEOMETRIES() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = "OBJECTID", geometry = "SHAPE")
+        >>> print(counties.geometry.dtypes)
+                      TYPENAME
+        SHAPE  ST_MULTIPOLYGON        
+        >>> counties["SHAPE"].num_geometries().head()
+        0    1
+        1    1
+        2    1
+        3    1
+        4    1
+        Name: DB2GSE.ST_NUMGEOMETRIES(SHAPE), dtype: int64
+        
+        Use sample data created in Db2 with SQL script, data type ST_MultiLineString
+        
+        >>> sample_mlines = IdaGeoDataFrame(idadb, "SAMPLE_MLINES", indexer = "ID", geometry = "GEOMETRY")
+        >>> print(sample_mlines.geometry.dtypes)
+                            TYPENAME
+        GEOMETRY  ST_MULTILINESTRING
+        
+        >>> sample_mlines.num_geometries().head()
+        0    3
+        Name: DB2GSE.ST_NUMGEOMETRIES(GEOMETRY), dtype: int64        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_NUMGEOMETRIES',
+                valid_types = ['ST_MULTIPOINT', 'ST_MULTIPOLYGON',
+                               'ST_MULTILINESTRING'])
+
+    def num_interior_ring(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_POLYGON.
+
+        Returns an IdaSeries with integers representing the number of interior
+        rings of each of the polygons in the calling IdaGeoSeries.
+
+        For None collections the output is None.
+        For empty collections the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_NUMINTERIORRING() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_POLYGONS, obtained with SQL script
+        
+        >>> sample_polygons["int_ring"] = sample_polygons.num_interior_ring()
+        >>> sample_polygons[["GEOMETRY", "int_ring"]].head()        
+        	GEOMETRY 	                                       int_ring
+        0 	POLYGON ((110.000000 120.000000, 120.000000 13... 	0
+        1 	POLYGON ((110.000000 120.000000, 130.000000 12... 	1        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_NUMINTERIORRING',
+                valid_types = ['ST_POLYGON'])
+
+    def num_line_strings(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_MULTILINESTRING.
+
+        Returns an IdaSeries with integers representing the number of
+        linestrings of each of the multilinestrings in the calling
+        IdaGeoSeries.
+
+        For None multilinestrings the output is None.
+        For empty multilinestrings the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_NUMLINESTRINGS() function.
+
+        Examples
+        --------
+        Use sample data created in Db2 with SQL script, data type ST_MultiLineString
+        
+        >>> sample_mlines = IdaGeoDataFrame(idadb, "SAMPLE_MLINES", indexer = "ID", geometry = "GEOMETRY")       
+        >>> sample_mlines.num_line_strings().head()
+        0    3
+        Name: DB2GSE.ST_NUMLINESTRINGS(GEOMETRY), dtype: int64
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_NUMLINESTRINGS',
+                valid_types = ['ST_MULTILINESTRING'])
+
+    def num_points(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with integers representing the number of points of
+        each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_NUMPOINTS() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_GEOMETRIES, obtained with SQL script
+        
+        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
+        >>> sample_geometries["num_points"] = sample_geometries.num_points()
+        >>> sample_geometries[["GEOMETRY", "num_points"]].head()
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_NUMPOINTS',
+                valid_types = ['ST_GEOMETRY'])
+
+    def num_polygons(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_MULTIPOLYGON.
+
+        Returns an IdaSeries with integers representing the number of
+        polygons of each of the multipolygons in the calling IdaGeoSeries.
+
+        For None multipolygons the output is None.
+        For empty multipolygons the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_NUMPOLYGONS() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = "OBJECTID", geometry = "SHAPE")
+        >>> counties["NUM_POLY"] = counties.num_polygons()
+        >>> print(counties['NUM_POLY'][counties['NUM_POLY']>1].shape)
+        (57, 1)
+        >>> counties["NUM_POLY"][counties["NUM_POLY"]>1].head()
+        0    4
+        1    2
+        2    2
+        3    2
+        4    2
+        Name: NUM_POLY, dtype: int64
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_NUMPOLYGONS',
+                valid_types = ['ST_MULTIPOLYGON'])
+
+    def coord_dim(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with integers representing the dimensionality of
+        the coordinates of each of the geometries in the calling IdaGeoSeries.
+
+        If the given geometry does not have Z and M coordinates,
+        the dimensionality is 2.
+        If it has Z coordinates and no M coordinates, or if it has M
+        coordinates and no Z coordinates, the dimensionality is 3.
+        If it has Z and M coordinates, the dimensionality is 4.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_COORDDIM() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = "OBJECTID", geometry = "SHAPE")        
+        >>> counties.coord_dim().head()
+        0    2
+        1    2
+        2    2
+        3    2
+        4    2
+        Name: DB2GSE.ST_COORDDIM(DB2GSE.ST_CENTROID(SHAPE)), dtype: int64
+        # use sample table SAMPLE_POINTS, obtained with SQL script
+        >>> sample_points = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID", geometry = "LOC")        
+        >>> sample_points['coord_dim'] = sample_points.coord_dim()
+        >>> sample_points[['ID', 'LOC','coord_dim']].head()
+         	ID 	LOC 	                            coord_dim
+        0 	1 	POINT (14.000000 58.000000) 	      2
+        1 	2 	POINT Z(12.000000 35.000000 12)      3
+        2 	3 	POINT ZM(12.000000 66.000000 43 45)  4
+        3 	4 	POINT M(14.000000 58.000000 4) 	     3
+        4 	5 	POINT Z(12.000000 35.000000 12) 	 3
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_COORDDIM',
+                valid_types = ['ST_GEOMETRY'])
+
+    def is_3d(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with integers (1 if it has Z coordiantes, 0
+        otherwise) for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_IS3D() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_POINTS, obtained with SQL script
+
+        >>> sample_points = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "id", geometry = "LOC")
+        >>> sample_points["is_3d"] = sample_points.is_3d()
+        >>> sample_points[["LOC", "is_3d"]].head()
+         	LOC 	                            is_3d
+        0 	POINT (14.000000 58.000000) 	     0
+        1 	POINT Z(12.000000 35.000000 12) 	 1
+        2 	POINT ZM(12.000000 66.000000 43 45)  1
+        3 	POINT M(14.000000 58.000000 4) 	     0
+        4 	POINT Z(12.000000 35.000000 12) 	 1        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_IS3D',
+                valid_types = ['ST_GEOMETRY'])
+
+    def is_measured(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with integers (1 if it has M coordiantes, 0
+        otherwise) for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_ISMEASURED() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_POINTS, obtained with SQL script
+        
+        >>> sample_points = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "id", geometry = "LOC")
+        >>> sample_points["is_M"]=sample_points.is_measured()
+        >>> sample_points.head()
+        	ID 	LOC 	                              coord_dim is_3d 	is_M
+        0 	1 	POINT (14.000000 58.000000) 	        2 	      0 	0
+        1 	2 	POINT Z(12.000000 35.000000 12) 	    3 	      1 	0
+        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	4 	      1 	1
+        3 	4 	POINT M(14.000000 58.000000 4)      	3 	      0 	1
+        4 	5 	POINT Z(12.000000 35.000000 12) 	    3 	      1 	0
+
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_ISMEASURED',
+                valid_types = ['ST_GEOMETRY'])
+
+    def is_valid(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with integers (1 if it is valid, 0 otherwise) for
+        each of the geometries in the calling IdaGeoSeries.
+
+        A geometry is valid only if all of the attributes in the structured
+        type are consistent with the internal representation of geometry data,
+        and if the internal representation is not corrupted.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_ISVALID() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_POINTS, obtained with SQL script
+        
+        >>> sample_points = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "id", geometry = "LOC")
+        >>> sample_points.is_valid().head()
+        0    1
+        1    1
+        2    1
+        3    1
+        4    1
+        Name: DB2GSE.ST_ISVALID(LOC), dtype: int64        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_ISVALID',
+                valid_types = ['ST_GEOMETRY'])
+
+    def max_m(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with doubles representing the maximum M coordinate
+        for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+        For geometries without M coordinate the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MAXM() function.
+
+        Examples
+        --------
+        Max M, X, Y and Z
+
+        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
+        >>> sample_geometries["max_X"] = sample_geometries.max_x()
+        >>> sample_geometries["max_Y"] = sample_geometries.max_y()
+        >>> sample_geometries["max_Z"] = sample_geometries.max_z()
+        >>> sample_geometries["max_M"] = sample_geometries.max_m()
+         	ID 	GEOMETRY 	              max_X 	max_Y 	max_Z 	max_M
+        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
+        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	5.0 	4.0 	None 	None
+        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
+        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
+        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	35.0 	6.0 	None 	None
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MAXM',
+                valid_types = ['ST_GEOMETRY'])
+
+    def max_x(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with doubles representing the maximum X coordinate
+        for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MAXX() function.
+
+        Examples
+        --------
+        Max M, X, Y and Z        
+        
+        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
+        >>> sample_geometries["max_X"] = sample_geometries.max_x()
+        >>> sample_geometries["max_Y"] = sample_geometries.max_y()
+        >>> sample_geometries["max_Z"] = sample_geometries.max_z()
+        >>> sample_geometries["max_M"] = sample_geometries.max_m()
+         	ID 	GEOMETRY 	              max_X 	max_Y 	max_Z 	max_M
+        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
+        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	5.0 	4.0 	None 	None
+        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
+        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
+        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	35.0 	6.0 	None 	None
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MAXX',
+                valid_types = ['ST_GEOMETRY'])
+
+    def max_y(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with doubles representing the maximum Y coordinate
+        for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MAXY() function.
+
+        Examples
+        --------
+        Max M, X, Y and Z        
+        
+        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
+        >>> sample_geometries["max_X"] = sample_geometries.max_x()
+        >>> sample_geometries["max_Y"] = sample_geometries.max_y()
+        >>> sample_geometries["max_Z"] = sample_geometries.max_z()
+        >>> sample_geometries["max_M"] = sample_geometries.max_m()
+         	ID 	GEOMETRY 	              max_X 	max_Y 	max_Z 	max_M
+        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
+        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	5.0 	4.0 	None 	None
+        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
+        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
+        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	35.0 	6.0 	None 	None
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MAXY',
+                valid_types = ['ST_GEOMETRY'])
+
+    def max_z(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with doubles representing the maximum Z coordinate
+        for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+        For geometries without Z coordinate the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MAXZ() function.
+
+        Examples
+        --------
+        Max M, X, Y and Z        
+        
+        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
+        >>> sample_geometries["max_X"] = sample_geometries.max_x()
+        >>> sample_geometries["max_Y"] = sample_geometries.max_y()
+        >>> sample_geometries["max_Z"] = sample_geometries.max_z()
+        >>> sample_geometries["max_M"] = sample_geometries.max_m()
+         	ID 	GEOMETRY 	              max_X 	max_Y 	max_Z 	max_M
+        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
+        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	5.0 	4.0 	None 	None
+        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
+        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
+        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	35.0 	6.0 	None 	None
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MAXZ',
+                valid_types = ['ST_GEOMETRY'])
+
+    def min_m(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with doubles representing the minimum M coordinate
+        for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+        For geometries without M coordinate the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MINM() function.
+
+        Examples
+        --------
+        Min M, X, Y and Z   
+        Use sample table SAMPLE_GEOMETRIES, obtained with SQL script
+        
+        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
+        >>> sample_geometries["min_X"] = sample_geometries.min_x()
+        >>> sample_geometries["min_Y"] = sample_geometries.min_y()
+        >>> sample_geometries["min_Z"] = sample_geometries.min_z()
+        >>> sample_geometries["min_M"] = sample_geometries.min_m()
+        >>> sample_geometries.head()        
+        	ID 	GEOMETRY 	min_X 	min_Y 	min_Z 	min_M
+        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
+        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	0.0 	0.0 	None 	None
+        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
+        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
+        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	33.0 	2.0 	None 	None
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MINM',
+                valid_types = ['ST_GEOMETRY'])
+
+    def min_x(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with doubles representing the minimum X coordinate
+        for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MINX() function.
+
+        Examples
+        --------
+        >>> counties = IdaDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = 'OBJECTID')
+        >>> counties.set_geometry("SHAPE")
+        >>> counties.min_x().head()
+        0   -100.227146
+        1    -77.749934
+        2    -85.401789
+        3    -83.794279
+        4    -79.856688
+        Name: DB2GSE.ST_MINX(SHAPE), dtype: float64
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MINX',
+                valid_types = ['ST_GEOMETRY'])
+
+    def min_y(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with doubles representing the minimum Y coordinate
+        for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MINY() function.
+
+        Examples
+        --------
+        >>> counties = IdaDataFrame(idadb, 'SAMPLES.GEO_COUNTY', indexer = 'OBJECTID')
+        >>> counties.set_geometry("SHAPE")
+        >>> counties.min_y().head()
+        0    37.912775
+        1    41.998697
+        2    37.630910
+        3    35.562878
+        4    37.005883
+        Name: DB2GSE.ST_MINY(SHAPE), dtype: float64
+
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MINY',
+                valid_types = ['ST_GEOMETRY'])
+
+    def min_z(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with doubles representing the minimum Z coordinate
+        for each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+        For empty geometries the output is None.
+        For geometries without Z coordinate the output is None.
+       
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_MINZ() function.
+
+        Examples
+        --------
+        Min M, X, Y and Z   
+        Use sample table SAMPLE_GEOMETRIES, obtained with SQL script
+        
+        >>> sample_geometries = IdaGeoDataFrame(idadb, "SAMPLE_GEOMETRIES", indexer = "ID", geometry = "GEOMETRY")
+        >>> sample_geometries["min_X"] = sample_geometries.min_x()
+        >>> sample_geometries["min_Y"] = sample_geometries.min_y()
+        >>> sample_geometries["min_Z"] = sample_geometries.min_z()
+        >>> sample_geometries["min_M"] = sample_geometries.min_m()
+        >>> sample_geometries.head()        
+        	ID 	GEOMETRY 	min_X 	min_Y 	min_Z 	min_M
+        0 	1 	POINT (1.000000 2.000000) 	1.0 	2.0 	None 	None
+        1 	2 	POLYGON ((0.000000 0.000000, 5.000000 0.000000... 	0.0 	0.0 	None 	None
+        2 	3 	POINT EMPTY 	NaN 	NaN 	None 	None
+        3 	4 	MULTIPOLYGON EMPTY 	NaN 	NaN 	None 	None
+        4 	5 	LINESTRING (33.000000 2.000000, 34.000000 3.00... 	33.0 	2.0 	None 	None
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_MINZ',
+                valid_types = ['ST_GEOMETRY'])
+
+    def m(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_POINT.
+
+        Returns an IdaSeries with doubles representing the measure (M)
+        coordinate of each of the points in the calling IdaGeoSeries.
+
+        For None points the output is None.
+        For empty points the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_M() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_POINTS, obtained with SQL script
+        
+        >>> sample_points_extractor = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID")
+        >>> sample_points_extractor.set_geometry("LOC")
+        >>> sample_points_extractor["X"] = sample_points_extractor.x()
+        >>> sample_points_extractor["Y"] = sample_points_extractor.y()
+        >>> sample_points_extractor["Z"] = sample_points_extractor.z()
+        >>> sample_points_extractor["M"] = sample_points_extractor.m()
+        >>> sample_points_extractor.head()
+         	ID 	LOC 	X 	Y 	Z 	M
+        0 	1 	POINT (14.000000 58.000000) 	14.0 	58.0 	NaN 	NaN
+        1 	2 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
+        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	12.0 	66.0 	43.0 	45.0
+        3 	4 	POINT M(14.000000 58.000000 4) 	14.0 	58.0 	NaN 	4.0
+        4 	5 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_M',
+                valid_types = ['ST_POINT'])
+
+    def x(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_POINT.
+
+        Returns an IdaSeries with doubles representing the X coordinate of each
+        of the points in the calling IdaGeoSeries.
+
+        For None points the output is None.
+        For empty points the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_X() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_POINTS, obtained with SQL script
+        
+        >>> sample_points_extractor = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID")
+        >>> sample_points_extractor.set_geometry("LOC")
+        >>> sample_points_extractor["X"] = sample_points_extractor.x()
+        >>> sample_points_extractor["Y"] = sample_points_extractor.y()
+        >>> sample_points_extractor["Z"] = sample_points_extractor.z()
+        >>> sample_points_extractor["M"] = sample_points_extractor.m()
+        >>> sample_points_extractor.head()
+         	ID 	LOC 	X 	Y 	Z 	M
+        0 	1 	POINT (14.000000 58.000000) 	14.0 	58.0 	NaN 	NaN
+        1 	2 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
+        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	12.0 	66.0 	43.0 	45.0
+        3 	4 	POINT M(14.000000 58.000000 4) 	14.0 	58.0 	NaN 	4.0
+        4 	5 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
+
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_X',
+                valid_types = ['ST_POINT'])
+
+    def y(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_POINT.
+
+        Returns an IdaSeries with doubles representing the Y coordinate of each
+        of the points in the calling IdaGeoSeries.
+
+        For None points the output is None.
+        For empty points the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_Y() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_POINTS, obtained with SQL script
+        
+        >>> sample_points_extractor = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID")
+        >>> sample_points_extractor.set_geometry("LOC")
+        >>> sample_points_extractor["X"] = sample_points_extractor.x()
+        >>> sample_points_extractor["Y"] = sample_points_extractor.y()
+        >>> sample_points_extractor["Z"] = sample_points_extractor.z()
+        >>> sample_points_extractor["M"] = sample_points_extractor.m()
+        >>> sample_points_extractor.head()
+         	ID 	LOC 	X 	Y 	Z 	M
+        0 	1 	POINT (14.000000 58.000000) 	14.0 	58.0 	NaN 	NaN
+        1 	2 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
+        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	12.0 	66.0 	43.0 	45.0
+        3 	4 	POINT M(14.000000 58.000000 4) 	14.0 	58.0 	NaN 	4.0
+        4 	5 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
+
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_Y',
+                valid_types = ['ST_POINT'])
+
+    def z(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_POINT.
+
+        Returns an IdaSeries with doubles representing the Z coordinate of each
+        of the points in the calling IdaGeoSeries.
+
+        For None points the output is None.
+        For empty points the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_Z() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_POINTS, obtained with SQL script
+        
+        >>> sample_points_extractor = IdaGeoDataFrame(idadb, "SAMPLE_POINTS", indexer = "ID")
+        >>> sample_points_extractor.set_geometry("LOC")
+        >>> sample_points_extractor["X"] = sample_points_extractor.x()
+        >>> sample_points_extractor["Y"] = sample_points_extractor.y()
+        >>> sample_points_extractor["Z"] = sample_points_extractor.z()
+        >>> sample_points_extractor["M"] = sample_points_extractor.m()
+        >>> sample_points_extractor.head()
+         	ID 	LOC 	X 	Y 	Z 	M
+        0 	1 	POINT (14.000000 58.000000) 	14.0 	58.0 	NaN 	NaN
+        1 	2 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
+        2 	3 	POINT ZM(12.000000 66.000000 43 45) 	12.0 	66.0 	43.0 	45.0
+        3 	4 	POINT M(14.000000 58.000000 4) 	14.0 	58.0 	NaN 	4.0
+        4 	5 	POINT Z(12.000000 35.000000 12) 	12.0 	35.0 	12.0 	NaN
+
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_Z',
+                valid_types = ['ST_POINT'])
+
+    def is_closed(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_LINESTRING, ST_MULTILINESTRING.
+
+        Returns an IdaSeries with integers (1 if it is closed, 0 otherwise) for
+        each of the curves or multicurves in the calling IdaGeoSeries.
+
+        A curve is closed if the start point and end point are equal.
+        If the curve has Z coordinates, the Z coordinates of the start and end
+        point must be equal. Otherwise, the points are not considered equal,
+        and the curve is not closed.
+        A multicurve is closed if each of its curves are closed.
+
+        For None curves or multicurves the output is None.
+        For empty curves or multicurves the output is 0.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_ISCLOSED() function.
+
+        Examples
+        --------
+        Use sample table SAMPLE_LINES, obtained with SQL script
+        
+        >>> samplelines = IdaGeoDataFrame(idadb, "SAMPLE_LINES", indexer = "ID", geometry = "GEOMETRY")
+        >>> sample_lines.is_closed().head()
+        0    0
+        1    0
+        Name: DB2GSE.ST_ISCLOSED(GEOMETRY), dtype: int64
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_ISCLOSED',
+                valid_types = ['ST_LINESTRING', 'ST_MULTILINESTRING'])
+
+    def is_empty(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with integers (1 if it is empty, 0 otherwise) for
+        each of the geometries in the calling IdaGeoSeries.
+
+        For None geometries the output is None.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_ISEMPTY() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>>counties["boundary"] = counties.boundary()
+        >>> counties["boundary"].is_empty().head(3)
+        0    0
+        1    0
+        2    0
+        Name: DB2GSE.ST_ISEMPTY(DB2GSE.ST_BOUNDARY(SHAPE)), dtype: int64        
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_ISEMPTY',
+                valid_types = ['ST_GEOMETRY'])
+
+    def is_simple(self):
+        """
+        Valid types for the column in the calling IdaGeoSeries:
+        ST_Geometry or one of its subtypes.
+
+        Returns an IdaSeries with integers (1 if it is simple, 0 otherwise) for
+        each of the geometries in the calling IdaGeoSeries.
+
+        Points, surfaces, and multisurfaces are always simple.
+        A curve is simple if it does not pass through the same point twice.
+        Amultipoint is simple if it does not contain two equal points.
+        A multicurve is simple if all of its curves are simple and the only
+        intersections occur at points that are on the boundary of the curves in
+        the multicurve.
+
+        For None geometries the output is None.
+        For empty geometries the output is 1.
+
+        Returns
+        -------
+        IdaSeries.
+
+        References
+        ----------
+        DB2 Spatial Extender ST_ISSIMPLE() function.
+
+        Examples
+        --------
+        >>> counties = IdaGeoDataFrame(idadb,'SAMPLES.GEO_COUNTY',indexer='OBJECTID')
+        >>> counties.set_geometry('SHAPE')
+        >>>counties["boundary"] = counties.boundary()
+        >>> counties["is_simple"] = counties.is_simple()
+        >>> filtered_counties = counties[counties['is_simple'] == 0]
+        >>> filtered_counties.shape
+        (0, 25)
+        
+        >>> counties["is_simple"] = counties['boundary'].is_simple()
+        >>> filtered_counties = counties[counties['is_simple'] == 0]
+        >>> filtered_counties.shape
+        (37, 25)
+        """
+        return self._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_ISSIMPLE',
+                valid_types = ['ST_GEOMETRY'])
+
+
+#==============================================================================
+### Public utilities for geospatial methods
+#==============================================================================
+
+    @lazy
+    def linear_units(self):
+        """
+        Returns
+        -------
+        list of str
+            The list of all allowed linear units that can be passed as option to geospatial methods.
+        """
+            
+        units = self.ida_query(
+            'SELECT UNIT_NAME FROM DB2GSE.ST_UNITS_OF_MEASURE WHERE '
+            'UNIT_TYPE= \'LINEAR\' ORDER BY LENGTH(UNIT_NAME), UNIT_NAME')
+        return units
+
+#==============================================================================
+### Private utilities for geospatial methods
+#==============================================================================
+
+    def _check_linear_unit(self, unit):
+        """
+        Parameters:
+        -----------
+        unit : str
+            Name of a user-entered unit (case-insensitive).
+
+        Returns
+        -------
+        str
+            The name of the unit in uppercase and formatted for DB2GSE syntax.
+
+        Raises
+        ------
+        TypeError
+            * If the unit is not a string
+            * If the unit is a string larger than 128 characters
+        IdaGeoDataFrameError
+            If the given unit is not a valid linear unit of DB2GSE.
+        """
+
+        if not isinstance(unit, six.string_types):
+            raise TypeError("unit must be a string")
+        elif len(unit) > 128:
+            raise TypeError("unit length exceeded")
+        else:
+            unit = unit.upper()
+            if unit not in self.linear_units.tolist():
+                raise IdaGeoDataFrameError(
+                    "Invalid unit\n"
+                    "Hint: use linear_units attribute to see the valid units")
+            else:
+                # Replace single quotation marks with two of them
+                if "\'" in unit:
+                    unit = unit.replace("'", "''")
+
+                # Enclose in single quotation marks
+                unit = '\''+unit+'\''
+                return unit
+
+    def _unary_operation_handler(self, db2gse_function,
+                                 valid_types,
+                                 additional_args = None):
+        """
+        Returns the resulting column of an unary geospatial method as an
+        IdaGeoSeries if it has geometry type, as an IdaSeries otherwise.
+
+        Parameters
+        ----------
+        db2gse_function : str
+            Name of the corresponding DB2GSE function.
+        valid_types : list of str
+            Valid input typenames.
+        additional_args : list of str, optional
+            Additional arguments for the DB2GSE function.
+
+        Returns
+        -------
+        IdaGeoSeries
+            If the resulting column has geometry type.
+        IdaSeries
+            If the resulting column doesn't have geometry type.
+        """
+        if not (self.dtypes.TYPENAME[0] in valid_types or 
+                valid_types[0] == 'ST_GEOMETRY'):
+            raise TypeError("Column " + self.column +
+                            " has incompatible type.")
+
+        # Obtain an IdaSeries object by cloning current one
+        # Then modify its attribute column
+        idaseries = self._clone()
+
+        # Get the first argument for the DB2GSE function, i.e. a column.
+        # Because it might be a non-destructive column that was added by the
+        # user, the column definition is considered, instead of its alias
+        # in the Ida object.
+        column_for_db2gse = self.internal_state.columndict[self.column]
+
+        arguments_for_db2gse_function = [column_for_db2gse]
+
+        if additional_args is not None:
+            for arg in additional_args:
+                arguments_for_db2gse_function.append(arg)
+
+        result_column = (
+            db2gse_function +
+            '(' +
+            ','.join(map(str, arguments_for_db2gse_function)) +
+            ')'
+            )
+
+        new_columndict = OrderedDict()
+        # result_column_key must not include double quotes because it is used as as Python key and as
+        # an SQL alias for the result column expression like in
+        # SELECT DB2GSE.ST_AREA("SHAPE",'KILOMETER') AS "DB2GSE.ST_AREA(SHAPE,'KILOMETER')" FROM SAMPLES.GEO_COUNTY
+        result_column_key = result_column.replace('"', '')
+        new_columndict[result_column_key] = result_column
+
+        idaseries._reset_attributes(["columns", "shape", "dtypes"])
+        idaseries.internal_state.columns = ['\"' + result_column_key + '\"']
+
+        idaseries.internal_state.columndict = new_columndict
+        idaseries.internal_state.update()
+
+        # Set the column attribute of the new idaseries
+        idaseries.column = result_column_key
+        
+        try:
+            del(idaseries.columns)
+        except:
+            pass
+        
+        if idaseries.dtypes.TYPENAME[result_column_key].find('ST_') == 0:
+            return IdaGeoSeries.from_IdaSeries(idaseries)
+        else:
+            return idaseries
```

### Comparing `nzpyida-0.2.2.6/nzpyida/indexing.py` & `nzpyida-0.3.3/nzpyida/indexing.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,119 +1,119 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Classes used for subsetting IdaDataFrames.
-Indexer currently available : loc
-"""
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-import warnings
-
-import six
-from nzpyida.exceptions import IdaDataBaseError
-
-#----------------------------------------------------------------------------
-# Class for the loc object for IdaDataFrames
-
-class Loc(object):
-        """
-        The Loc class is used to select and project IdaDataFrames.
-        It implements a Pandas-like interface.
-        """
-        def __init__(self, idadf):
-            self.idadf = idadf
-
-        def __getitem__(self, key):
-            """
-            Use the loc object of an IdaDataFrame or IdaSeries instance to
-            do projection or selection in a table.
-            
-            Notes
-            -----
-            The determinism of the result is guaranteed only if the IdaDataFrame
-            has a valid indexer. 
-            
-            Examples
-            --------
-            >>> idairis.loc[0:49] # Select the first 50 rows
-            >>> idairis.loc[2, "SepalLength"] # Select the 3rd rows and column "SepalLength"
-            >>> idairis.loc[0:len(idairis), ["SepalLength", "SepalWidth"]] # Select all rows and columns "SepalLength", "SepalWidth"
-            """
-            if type(key) is tuple:
-                if len(key) > 2:
-                    raise ValueError("Too many indexer (expected 2)")
-                else:
-                    index = key[0]
-                    cols = key[1]
-            else:
-                index = key
-                cols = None
-
-
-            if isinstance(index, six.integer_types):
-                if self.idadf.indexer is None:
-                    if (index < 0)|(index > self.idadf.shape[0]):
-                        raise KeyError("The label [%s] is not in the [index]" %(index))
-            if isinstance(index, list):
-                if self.idadf.indexer is None:
-                    if False in [isinstance(x, six.integer_types) for x in index]:
-                        raise IdaDataBaseError("The IdaDataFrame has no indexer, so 'index' should be an integer or a list of integers")
-                    for x in index:
-                        if (x < 0)|(x > self.idadf.shape[0]):
-                            raise ValueError("The index [%s] is out of range" %(index))
-
-            if cols is not None:
-                if isinstance(cols, six.string_types):
-                    if cols not in self.idadf.columns:
-                            raise KeyError("The label %s is not in the [columns]" %(cols))
-                    newidadf = self.idadf._clone_as_serie(cols)
-                else:
-                    not_existing = [col for col in cols if col not in self.idadf.columns]
-                    if not_existing:
-                        raise KeyError("The label [%s] is not in the [columns]" %(not_existing))
-                    newidadf = self.idadf._clone()
-
-                for col in newidadf.internal_state.columndict.keys():
-                    if col not in cols:
-                        del newidadf.internal_state.columndict[col]
-
-                if self.idadf.indexer is not None:
-                    if self.idadf.indexer not in cols:
-                        newidadf.internal_state.columndict[self.idadf.indexer] = "\""+self.idadf.indexer+"\""
-
-                newidadf.internal_state.index = index
-                newidadf.internal_state.update()
-                #newidadf._reset_attributes(["shape", "dtypes", "index", "columns"]) # this was causing troubles 
-
-                if self.idadf.indexer is not None:
-                    if self.idadf.indexer not in cols:
-                        
-                        del newidadf.internal_state.columndict[self.idadf.indexer]
-                        newidadf.internal_state.update()
-                        newidadf._reset_attributes(["shape", "dtypes", "index", "columns"])
-
-
-            else:
-                newidadf = self.idadf._clone()
-                newidadf.internal_state.index = index
-                newidadf.internal_state.update()
-                newidadf._reset_attributes(["shape", "index"])
-
-            if self.idadf.indexer is None:
-                if not " ORDER BY " in self.idadf.internal_state.get_state():
-                    warnings.warn("Row order is not guaranteed if no indexer" +
-                                  " was given and the dataset was not sorted")
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Classes used for subsetting IdaDataFrames.
+Indexer currently available : loc
+"""
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+import warnings
+
+import six
+from nzpyida.exceptions import IdaDataBaseError
+
+#----------------------------------------------------------------------------
+# Class for the loc object for IdaDataFrames
+
+class Loc(object):
+        """
+        The Loc class is used to select and project IdaDataFrames.
+        It implements a Pandas-like interface.
+        """
+        def __init__(self, idadf):
+            self.idadf = idadf
+
+        def __getitem__(self, key):
+            """
+            Use the loc object of an IdaDataFrame or IdaSeries instance to
+            do projection or selection in a table.
+            
+            Notes
+            -----
+            The determinism of the result is guaranteed only if the IdaDataFrame
+            has a valid indexer. 
+            
+            Examples
+            --------
+            >>> idairis.loc[0:49] # Select the first 50 rows
+            >>> idairis.loc[2, "SepalLength"] # Select the 3rd rows and column "SepalLength"
+            >>> idairis.loc[0:len(idairis), ["SepalLength", "SepalWidth"]] # Select all rows and columns "SepalLength", "SepalWidth"
+            """
+            if type(key) is tuple:
+                if len(key) > 2:
+                    raise ValueError("Too many indexer (expected 2)")
+                else:
+                    index = key[0]
+                    cols = key[1]
+            else:
+                index = key
+                cols = None
+
+
+            if isinstance(index, six.integer_types):
+                if self.idadf.indexer is None:
+                    if (index < 0)|(index > self.idadf.shape[0]):
+                        raise KeyError("The label [%s] is not in the [index]" %(index))
+            if isinstance(index, list):
+                if self.idadf.indexer is None:
+                    if False in [isinstance(x, six.integer_types) for x in index]:
+                        raise IdaDataBaseError("The IdaDataFrame has no indexer, so 'index' should be an integer or a list of integers")
+                    for x in index:
+                        if (x < 0)|(x > self.idadf.shape[0]):
+                            raise ValueError("The index [%s] is out of range" %(index))
+
+            if cols is not None:
+                if isinstance(cols, six.string_types):
+                    if cols not in self.idadf.columns:
+                            raise KeyError("The label %s is not in the [columns]" %(cols))
+                    newidadf = self.idadf._clone_as_serie(cols)
+                else:
+                    not_existing = [col for col in cols if col not in self.idadf.columns]
+                    if not_existing:
+                        raise KeyError("The label [%s] is not in the [columns]" %(not_existing))
+                    newidadf = self.idadf._clone()
+
+                for col in newidadf.internal_state.columndict.keys():
+                    if col not in cols:
+                        del newidadf.internal_state.columndict[col]
+
+                if self.idadf.indexer is not None:
+                    if self.idadf.indexer not in cols:
+                        newidadf.internal_state.columndict[self.idadf.indexer] = "\""+self.idadf.indexer+"\""
+
+                newidadf.internal_state.index = index
+                newidadf.internal_state.update()
+                #newidadf._reset_attributes(["shape", "dtypes", "index", "columns"]) # this was causing troubles 
+
+                if self.idadf.indexer is not None:
+                    if self.idadf.indexer not in cols:
+                        
+                        del newidadf.internal_state.columndict[self.idadf.indexer]
+                        newidadf.internal_state.update()
+                        newidadf._reset_attributes(["shape", "dtypes", "index", "columns"])
+
+
+            else:
+                newidadf = self.idadf._clone()
+                newidadf.internal_state.index = index
+                newidadf.internal_state.update()
+                newidadf._reset_attributes(["shape", "index"])
+
+            if self.idadf.indexer is None:
+                if not " ORDER BY " in self.idadf.internal_state.get_state():
+                    warnings.warn("Row order is not guaranteed if no indexer" +
+                                  " was given and the dataset was not sorted")
             return newidadf
```

### Comparing `nzpyida-0.2.2.6/nzpyida/internals.py` & `nzpyida-0.3.3/nzpyida/internals.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,447 +1,453 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Definition of internal tools for non-destructive modification of IdaDataFrames.
-"""
-
-# Python 2 compatibility
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from builtins import str
-from builtins import range
-from future import standard_library
-standard_library.install_aliases()
-
-from functools import wraps, partial
-from numbers import Number
-from collections import OrderedDict
-
-from lazy import lazy
-import six
-
-import nzpyida
-
-from copy import deepcopy
-
-def idadf_state(function=None, force=False):
-    """
-    View management for IdaDataFrame.
-    This decorator creates the view reflecting the current state of an
-    IdaDataFrame, so that the operation it decorates can work on this "virtual"
-    version.
-    """
-    if function is None:
-        return partial(idadf_state, force=force)
-    @wraps(function)
-    def wrapper(self, *args, **kwds):
-        # Note: 5 is arbitrary. Maybe not the most relevant threshold
-        # Means that not view should be created if the idadataframe was 
-        # not modified at least 5 times. 
-        if (len(self.internal_state.views) < 5)&(force is False):
-            # Here, avoid creating a view but putting in the stack the string
-            # that corresponds to the state of the idadataframe
-            if self._idadb._is_netezza_system():
-                asTemp = " AS TEMP"
-            else:
-                asTemp = ""
-            if self.internal_state.views:
-                self.internal_state.viewstack.append("(" + self.internal_state.get_state() + ")" + asTemp)
-            try:
-                result = function(self, *args, **kwds)
-            except:
-                raise
-            finally:
-                if self.internal_state.views:
-                    if self.internal_state.viewstack:
-                        self.internal_state.viewstack.pop()
-        else:
-            self.internal_state._create_view()
-            try:
-                result = function(self, *args, **kwds)
-            except:
-                raise
-            finally:
-                self.internal_state._delete_view()
-        return result
-    return wrapper
-
-class InternalState(object):
-    """
-    Records and maintains an internal state of an IdaDataFrame.
-    This class is used internally to manage modified version of IdaDataFrames
-    without modifying the real version In-DataBase.
-    """
-    def __init__(self, idadf):
-        """
-        Constructor for InternalSate objects. InternalState objects shall be
-        created only by IdaDataFrame/IdaSeries objects.
-
-        Parameters
-        ----------
-        idadf : str
-            IdaDataFrame to which self relates.
-
-        Attributes
-        ----------
-        _idadf : IdaDataFrame
-            IdaDataFrame to which self relates.
-        name : str
-            In database Name of the idaobject its belongs to.
-        ascending : bool, default: True
-            Indicate if the dataframe should be sorted in ascending or descending order.
-        columns: str, default: '*'
-            Columns that are selected in the IdaDataFrame, the order determine
-            the order they are displayed too. Follow this model : ""Col1", "Col2", "Col3"".
-        order: str, default: ''
-            "Order by" part of a corresponding SQL query. Follow this model :
-            " ORDER BY "Col1", "Col2", "Col3" ASC".
-        _views : list of str
-            List of nested SQL queries that describe the state of the
-            IdaDataFrame. String that are stored  in this list may not be
-            modified.
-        _cumulative : str
-            Cumulative represent the most current nested SQL queries. The main
-            different with those in _views is that cumulative may still be
-            modified. When the state of the IdaDataFrame is modified in the way
-            that it is not possible to modify cumulative without violating the
-            order the operation where may, the cumulative function is stored
-            in _views, and a new cumulative gets created.
-        viewstack : list of str
-            Stack of view names that are created dynamically. Used to memorize
-            the name of the temporary views so that they can get  deleted when
-            the operation is finished.
-        """
-        self._idadf = idadf
-        self.name = idadf._name
-        self.tablename = idadf.tablename
-        self.ascending = True # per default
-        self.index = None
-        self.order = None
-
-        self._views = []
-        self._cumulative = []
-        self.viewstack = []
-
-    @lazy
-    def columndict(self):
-        """
-        Returns an ordered dictionnary that contains available columns as keys
-        and their "real" expression as values, exactly in the way they should
-        appear in modified SQL queries for creating views.
-        """
-        #return OrderedDict((cols,"\""+cols+"\"") for cols in self._idadf.columns)
-        columns = deepcopy(OrderedDict((cols,"\""+cols+"\"") for cols in self._idadf.columns))
-        
-        #Remove the quotation marks enclosing DB2GSE functions
-        #ibmdbpy stores and keeps track of columns internally enclosing them
-        #with double quotation marks, but they must be removed for functions, 
-        #so that the SQL query is interpreted correctly
-        
-        #TODO: Surround the geometries with ST_AsText()
-        #so that the database retrieves geometries as CLOB with WKT instead 
-        #of a geometry (which internally is a STRUCT type).
-        #Although this STRUCT type has the method getSubString() of CLOB types, 
-        #it makes jaydebeapi raise a warning of no mapping of STRUCT type
-        #Pending to do this because knowing the type of a column by its name 
-        #requires accessing the dtypes attribute, which is usually erased after
-        #manipulating the Ida(Geo)DataFrame. In order to do this, refreshing
-        #instead of erasing dtypes attribute would be a better approach
-
-        for column in columns.keys():
-            #if the column is an ST_ function
-            if columns[column].find('ST_') == 0:
-                columns[column] = columns[column][1:-1]
-                
-        return columns
-
-    @property
-    def views(self):
-        """
-        Returns the set of all views, including the cumulating view.
-        """
-        return self._views + self._cumulative
-
-    @property
-    def current_state(self):
-        """
-        If some temporary view have been created, i.e. the InternalState was
-        modified and viewstack in not empty, returns the name of the last
-        created view, otherwise returns the name of the table.
-        """
-        if self.viewstack:
-            return self.viewstack[-1]
-        else:
-            return self.name
-
-    def _create_view(self, viewname = None):
-        """
-        Creates a view that represent the current state of the idea dataframe
-
-        Arguments
-        ---------
-        viewname : str, optional
-            Name of the view to be created, if not given, then a unique name
-            view will be generated.
-        """
-        if not self.views : return
-        if viewname:
-            view = viewname
-        else:
-            prefix = "TEMP_VIEW_%s_"%(self.tablename)
-            view = self._idadf._idadb._get_valid_viewname(prefix)
-
-        query = "CREATE VIEW \"%s\" AS (%s)"%(view, self.get_state())
-        self._idadf._prepare_and_execute(query, autocommit = True)
-        self.viewstack.append(view)
-        return view
-
-    def _delete_view(self, viewname = None):
-        """
-        Deletes a view related to the internal state
-
-        Arguments
-        ---------
-        viewname : str, optional
-            Deletes the view that has this name. If no viewname is given,
-            deletes the most recently created view, i.e. the one on the stack.
-        """
-        if viewname:
-            view = viewname
-        else:
-            if self.viewstack:
-                view = self.viewstack.pop()
-            else:
-                return
-        query = "DROP VIEW \"" + view + "\""
-        self._idadf._prepare_and_execute(query, autocommit = True)
-        return
-
-    def __del__(self):
-        """
-        Overriding the destructor for InternalState object.
-        Making sure all the views in the stack are deleted before deleting self.
-        """
-        if self.viewstack:
-            while self.viewstack:
-                self._delete_view(self.viewstack.pop())
-
-    def update(self, filter_query=None):
-        """
-        Update _views and/or _cumulative with an SQL select expression that
-        corresponds to the lastest modification of the internal state.
-        Determines automatically if it is enough to replace _cumulative
-        or to store it in _views and create a new _cumulative view.
-        The situation, in which we should stop cumulating are :
-        Filter, order, selection (of rows)
-        """
-        if isinstance(filter_query, nzpyida.filtering.FilterQuery):
-            query = filter_query.query
-            #self.stop_cumulating_columns() # not sure ?
-            self._views = self._views + self._cumulative
-            self._cumulative = []
-            self._views.append(query)
-        elif self.order is not None:
-            # Assumption : order is the only modification
-            query = "SELECT * FROM (SELECT * FROM %s"+self.get_order()+") AS TEMP"
-            # self.order = None
-            #self.stop_cumulating_columns()
-            self._views = self._views + self._cumulative
-            self._cumulative = []
-            self._views.append(query)
-        else:
-            if self.index is None:
-                query = ("SELECT " + self.get_columns() + " FROM %s")
-                self._cumulative = [query]
-            else:
-                if isinstance(self.index, Number):
-                    indexstring = " = " + str(self.index)
-                elif isinstance(self.index, six.string_types):
-                    if "'" in self.index:
-                        self.index = self.index.replace("'", "\'")
-                    indexstring = " = '" + str(self.index) + "'"
-                elif isinstance(self.index, list):
-                    indexstring = " in (" + str(self.index)[1:-1] + ")" #[1:-1] to strip the brackets
-                elif isinstance(self.index, slice):
-                    start = self.index.start
-                    if not start:
-                        start = 0
-                    stop = self.index.stop
-                    if not self.index.stop:
-                        stop = self._idadf.shape[0]
-                    if self.index.step:
-                        # TODO: Improve this by using a modulo operation instead of listing
-                        # This would prevent SQL overflow for big datasets
-                        indexlist = list(range(start, stop)[start:stop:self.index.step])
-                        indexstring = " in (" + str(indexlist)[1:-1] + ")"
-                    else:
-                        indexstring = " BETWEEN " + str(start) + " AND " + str(stop)
-
-                columns = self.get_columns()
-                #columns = self._idadf._get_columns()
-                
-                #columns = "\"" + "\",\"".join(set(self._idadf._get_columns())|set(self._idadf.columns)) + "\"" # TODO: Check if it is good
-                #if columns == "*":
-                    #columns = "\"" + "\",\"".join(self._idadf.columns) + "\""
-
-                if self._idadf.indexer:
-                    query = ("SELECT " + columns + " FROM %s AS TEMP1 WHERE " +
-                             self._idadf.indexer + indexstring)
-                else:
-                    if self._idadf._idadb._is_netezza_system():
-                        order_by = "ORDER BY NULL"
-                    else:
-                        order_by = ""
-                    query = ("SELECT " + columns + " FROM (SELECT TEMP1.*, "+
-                            "(ROW_NUMBER() OVER(" + order_by + ")-1) AS RN FROM %s AS TEMP1) AS TEMP2 "+
-                            "WHERE RN " + indexstring)
-
-                self.index = None
-
-                #self.stop_cumulating_columns()
-                self._views = self._views + self._cumulative
-                self._cumulative = []
-                self._views.append(query)
-
-
-    def get_state(self):
-        """
-        Returns the sql query corresponding to the current state of the
-        IdaDataFrame. The query is created by nesting of all strings in views
-        (which is the concatenation of _views and _cumulative).
-        """
-        if not self.views:
-            return ("SELECT " + self.get_columns() + " FROM " + self.name)
-
-        query = "%s"
-        for index, view in enumerate(self.views[::-1]):
-            if index == (len(self.views)-1):
-                view = view % self.name
-            if index != 0:
-                view = "(" + view + ")"
-            query = query %(view)
-        return query
-
-    def get_columns(self):
-        """
-        Returns a string containing selected columns in the current state.
-        For each column in in columndict, column expression are either :
-            * The value of the column if it was unchanged ("<CONAME>")
-            * The value of the column following by " AS \"<COLNAME>\""
-        """
-        columns = []
-        for key,value in self.columndict.items():
-            if value[1:-1] == key:
-                columns.append(value)
-            else:
-                columns.append("%s AS \"%s\""%(value, key))
-
-        return ",".join(columns)
-
-
-    def get_order(self):
-        """
-        Returns the "Order by" clause of a corresponding SQL query, according
-        to the value of the attribute order. If order is None, returns an
-        empty string.
-
-        Examples
-        --------
-        >>> self.get_order()
-        " ORDER BY "Col1" ASC, "Col2" ASC, "Col3" ASC"
-        """
-        if not self.order:
-            return ''
-        else:
-            if self.ascending:
-                return (" ORDER BY \"" + "\" ASC, \"".join(self.order) + "\" ASC")
-            else:
-                return (" ORDER BY \"" + "\" DESC, \"".join(self.order) + "\" DESC")
-
-    def is_default(self):
-        """
-        Checks if the InternalState object corresponds to the default values.
-
-        Returns
-        -------
-        bool
-            1 : The InternalState has default values
-            0 : The InternalState is modifed
-        """
-        return self.get_state() == InternalState(self.name).get_state()
-
-    def set_order(self, order, ascending):
-        """
-        Changes order in the internal state of the IdaDataframe.
-        """
-        print("set_order " + order[0])
-        self.order = order
-        self.ascending = ascending
-
-    def reset_order(self):
-        """
-        Resets the state to original order. Delete all views that have an ORDER
-        clause. Based on the assumption that ORDER is the only modification
-        of those queries.
-        """
-        # This is based on the assumption than no other modifications than
-        # sorting was done when sorting the idadf
-        self._views = [view for view in self._views if "ORDER BY" not in view]
-
-    def stop_cumulating_columns(self):
-        # TODO: do not use anymore ??
-        # You actually don't want to use column that you have not selected anymore ? 
-        # except if you want to sum maybe ...
-        """
-        Management of views to ensure that some specific operations such as
-        Filter, order, selection are registred in the list of views so that
-        the real order of operation is respected.
-        Here is what the function does:
-        If _cumulative is not None,
-        then the _cumulative is augmented with all available columns in the
-        database, if any and then appended in _views.
-        _cumulative is then reset to new query that only select the columns
-        that were selected on that point. This is made so, because it allows
-        users to still use all available columns in the table, even if they are
-        not selected in the current state.
-        """
-        # TODO: Improve documentation for this method (experimental)
-
-        # Add all columns available originally in the table in the history
-        if self._cumulative:
-            all_columns = self._idadf._get_all_columns_in_table()
-            deleted_columns = []
-            for column in all_columns:
-                if column not in list(self.columndict.keys()):
-                    deleted_columns.append(column)
-
-            if deleted_columns:
-                for column in deleted_columns:
-                     self.columndict[column] = ("\""+column+"\"")
-                self._cumulative = ["SELECT " + self.get_columns() + " FROM %s"]
-
-            self._views = self._views + self._cumulative
-            self._cumulative = []
-
-            # Select only the columns that were available in the representation
-            # Those 2 steps are made so to enable users to still use columns
-            # that were not selected anymore in the current table representation
-            # but still existing in the idadataframe
-
-            if deleted_columns:
-                for column in deleted_columns:
-                    del self.columndict[column]
-                self.columndict = OrderedDict((cols,"\""+cols+"\"") for cols in self.columndict)
-                self._cumulative = ["SELECT " + self.get_columns() + " FROM %s"]
-            else:
-                self.columndict = OrderedDict((cols,"\""+cols+"\"") for cols in self.columndict)
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Definition of internal tools for non-destructive modification of IdaDataFrames.
+"""
+
+# Python 2 compatibility
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from builtins import str
+from builtins import range
+from future import standard_library
+standard_library.install_aliases()
+
+from functools import wraps, partial
+from numbers import Number
+from collections import OrderedDict
+
+from lazy import lazy
+import six
+
+import nzpyida
+
+from copy import deepcopy
+
+def idadf_state(function=None, force=False):
+    """
+    View management for IdaDataFrame.
+    This decorator creates the view reflecting the current state of an
+    IdaDataFrame, so that the operation it decorates can work on this "virtual"
+    version.
+    """
+    if function is None:
+        return partial(idadf_state, force=force)
+    @wraps(function)
+    def wrapper(self, *args, **kwds):
+        # Note: 5 is arbitrary. Maybe not the most relevant threshold
+        # Means that not view should be created if the idadataframe was 
+        # not modified at least 5 times. 
+        if (len(self.internal_state.views) < 5)&(force is False):
+            # Here, avoid creating a view but putting in the stack the string
+            # that corresponds to the state of the idadataframe
+            if self._idadb._is_netezza_system():
+                asTemp = " AS TEMP"
+            else:
+                asTemp = ""
+            if self.internal_state.views:
+                self.internal_state.viewstack.append("(" + self.internal_state.get_state() + ")" + asTemp)
+            try:
+                result = function(self, *args, **kwds)
+            except:
+                raise
+            finally:
+                if self.internal_state.views:
+                    if self.internal_state.viewstack:
+                        self.internal_state.viewstack.pop()
+        else:
+            self.internal_state._create_view()
+            try:
+                result = function(self, *args, **kwds)
+            except:
+                raise
+            finally:
+                self.internal_state._delete_view()
+        return result
+    return wrapper
+
+class InternalState(object):
+    """
+    Records and maintains an internal state of an IdaDataFrame.
+    This class is used internally to manage modified version of IdaDataFrames
+    without modifying the real version In-DataBase.
+    """
+    def __init__(self, idadf):
+        """
+        Constructor for InternalSate objects. InternalState objects shall be
+        created only by IdaDataFrame/IdaSeries objects.
+
+        Parameters
+        ----------
+        idadf : str
+            IdaDataFrame to which self relates.
+
+        Attributes
+        ----------
+        _idadf : IdaDataFrame
+            IdaDataFrame to which self relates.
+        name : str
+            In database Name of the idaobject its belongs to.
+        ascending : bool, default: True
+            Indicate if the dataframe should be sorted in ascending or descending order.
+        columns: str, default: '*'
+            Columns that are selected in the IdaDataFrame, the order determine
+            the order they are displayed too. Follow this model : ""Col1", "Col2", "Col3"".
+        order: str, default: ''
+            "Order by" part of a corresponding SQL query. Follow this model :
+            " ORDER BY "Col1", "Col2", "Col3" ASC".
+        _views : list of str
+            List of nested SQL queries that describe the state of the
+            IdaDataFrame. String that are stored  in this list may not be
+            modified.
+        _cumulative : str
+            Cumulative represent the most current nested SQL queries. The main
+            different with those in _views is that cumulative may still be
+            modified. When the state of the IdaDataFrame is modified in the way
+            that it is not possible to modify cumulative without violating the
+            order the operation where may, the cumulative function is stored
+            in _views, and a new cumulative gets created.
+        viewstack : list of str
+            Stack of view names that are created dynamically. Used to memorize
+            the name of the temporary views so that they can get  deleted when
+            the operation is finished.
+        """
+        self._idadf = idadf
+        self.name = idadf._name
+        self.tablename = idadf.tablename
+        self.ascending = True # per default
+        self.index = None
+        self.order = None
+
+        self._views = []
+        self._cumulative = []
+        self.viewstack = []
+
+    @lazy
+    def columndict(self):
+        """
+        Returns an ordered dictionnary that contains available columns as keys
+        and their "real" expression as values, exactly in the way they should
+        appear in modified SQL queries for creating views.
+        """
+        #return OrderedDict((cols,"\""+cols+"\"") for cols in self._idadf.columns)
+        columns = deepcopy(OrderedDict((cols,"\""+cols+"\"") for cols in self._idadf.columns))
+        
+        #Remove the quotation marks enclosing DB2GSE functions
+        #ibmdbpy stores and keeps track of columns internally enclosing them
+        #with double quotation marks, but they must be removed for functions, 
+        #so that the SQL query is interpreted correctly
+        
+        #TODO: Surround the geometries with ST_AsText()
+        #so that the database retrieves geometries as CLOB with WKT instead 
+        #of a geometry (which internally is a STRUCT type).
+        #Although this STRUCT type has the method getSubString() of CLOB types, 
+        #it makes jaydebeapi raise a warning of no mapping of STRUCT type
+        #Pending to do this because knowing the type of a column by its name 
+        #requires accessing the dtypes attribute, which is usually erased after
+        #manipulating the Ida(Geo)DataFrame. In order to do this, refreshing
+        #instead of erasing dtypes attribute would be a better approach
+
+        for column in columns.keys():
+            #if the column is an ST_ function
+            if columns[column].find('ST_') == 0:
+                columns[column] = columns[column][1:-1]
+                
+        return columns
+
+    @property
+    def views(self):
+        """
+        Returns the set of all views, including the cumulating view.
+        """
+        return self._views + self._cumulative
+
+    @property
+    def current_state(self):
+        """
+        If some temporary view have been created, i.e. the InternalState was
+        modified and viewstack in not empty, returns the name of the last
+        created view, otherwise returns the name of the table.
+        """
+        if self.viewstack:
+            return self.viewstack[-1]
+        else:
+            return self.name
+
+    def _create_view(self, viewname = None):
+        """
+        Creates a view that represent the current state of the idea dataframe
+
+        Arguments
+        ---------
+        viewname : str, optional
+            Name of the view to be created, if not given, then a unique name
+            view will be generated.
+        """
+        if not self.views : return
+        if viewname:
+            view = viewname
+        else:
+            prefix = "TEMP_VIEW_%s_"%(self.tablename)
+            view = self._idadf._idadb._get_valid_viewname(prefix)
+
+        query = "CREATE VIEW \"%s\" AS (%s)"%(view, self.get_state())
+        self._idadf._prepare_and_execute(query, autocommit = True)
+        self.viewstack.append(view)
+        return view
+
+    def _delete_view(self, viewname = None):
+        """
+        Deletes a view related to the internal state
+
+        Arguments
+        ---------
+        viewname : str, optional
+            Deletes the view that has this name. If no viewname is given,
+            deletes the most recently created view, i.e. the one on the stack.
+        """
+        if viewname:
+            view = viewname
+        else:
+            if self.viewstack:
+                view = self.viewstack.pop()
+            else:
+                return
+        query = "DROP VIEW \"" + view + "\""
+        self._idadf._prepare_and_execute(query, autocommit = True)
+        return
+
+    def __del__(self):
+        """
+        Overriding the destructor for InternalState object.
+        Making sure all the views in the stack are deleted before deleting self.
+        """
+        if self.viewstack:
+            while self.viewstack:
+                self._delete_view(self.viewstack.pop())
+
+    def update(self, filter_query=None):
+        """
+        Update _views and/or _cumulative with an SQL select expression that
+        corresponds to the lastest modification of the internal state.
+        Determines automatically if it is enough to replace _cumulative
+        or to store it in _views and create a new _cumulative view.
+        The situation, in which we should stop cumulating are :
+        Filter, order, selection (of rows)
+        """
+        if isinstance(filter_query, nzpyida.filtering.FilterQuery):
+            query = filter_query.query
+            #self.stop_cumulating_columns() # not sure ?
+            self._views = self._views + self._cumulative
+            self._cumulative = []
+            self._views.append(query)
+        elif self.order is not None:
+            # Assumption : order is the only modification
+            query = "SELECT * FROM (SELECT * FROM %s"+self.get_order()+") AS TEMP"
+            # self.order = None
+            #self.stop_cumulating_columns()
+            self._views = self._views + self._cumulative
+            self._cumulative = []
+            self._views.append(query)
+        else:
+            if self.index is None:
+                query = ("SELECT " + self.get_columns() + " FROM %s")
+                self._cumulative = [query]
+            else:
+                if isinstance(self.index, Number):
+                    indexstring = " = " + str(self.index)
+                elif isinstance(self.index, six.string_types):
+                    if "'" in self.index:
+                        self.index = self.index.replace("'", "\'")
+                    indexstring = " = '" + str(self.index) + "'"
+                elif isinstance(self.index, list):
+                    indexstring = " in (" + str(self.index)[1:-1] + ")" #[1:-1] to strip the brackets
+                elif isinstance(self.index, slice):
+                    start = self.index.start
+                    if not start:
+                        start = 0
+                    stop = self.index.stop
+                    if not self.index.stop:
+                        stop = self._idadf.shape[0]
+                    if self.index.step:
+                        # TODO: Improve this by using a modulo operation instead of listing
+                        # This would prevent SQL overflow for big datasets
+                        indexlist = list(range(start, stop)[start:stop:self.index.step])
+                        indexstring = " in (" + str(indexlist)[1:-1] + ")"
+                    else:
+                        indexstring = " BETWEEN " + str(start) + " AND " + str(stop)
+
+                columns = self.get_columns()
+                #columns = self._idadf._get_columns()
+                
+                #columns = "\"" + "\",\"".join(set(self._idadf._get_columns())|set(self._idadf.columns)) + "\"" # TODO: Check if it is good
+                #if columns == "*":
+                    #columns = "\"" + "\",\"".join(self._idadf.columns) + "\""
+
+                if self._idadf._idadb._is_netezza_system():
+                    order_by = "ORDER BY NULL"
+                    as_temp1 = ""
+                    temp1 = ""
+                else:
+                    order_by = ""
+                    as_temp1 = "AS TEMP1"
+                    temp1 = "TEMP1."
+
+                if self._idadf.indexer:
+                    query = ("SELECT " + columns + " FROM %s " + as_temp1 + " WHERE " +
+                             self._idadf.indexer + indexstring)
+                else:
+                    query = ("SELECT " + columns + " FROM (SELECT " + temp1 + "*, "+
+                            "(ROW_NUMBER() OVER(" + order_by + ")-1) AS RN FROM %s " + as_temp1 + ") AS TEMP2 "+
+                            "WHERE RN " + indexstring)
+
+                self.index = None
+
+                #self.stop_cumulating_columns()
+                self._views = self._views + self._cumulative
+                self._cumulative = []
+                self._views.append(query)
+
+
+    def get_state(self):
+        """
+        Returns the sql query corresponding to the current state of the
+        IdaDataFrame. The query is created by nesting of all strings in views
+        (which is the concatenation of _views and _cumulative).
+        """
+        if not self.views:
+            return ("SELECT " + self.get_columns() + " FROM " + self.name)
+
+        query = "%s"
+        for index, view in enumerate(self.views[::-1]):
+            alias = " as t{} ".format(index) if self._idadf._idadb._is_netezza_system() else ""
+            if index == (len(self.views)-1):
+                view = view % self.name
+            if index != 0:
+                view = "(" + view + ")%s" %(alias)
+            query = query %(view)
+        return query
+
+    def get_columns(self):
+        """
+        Returns a string containing selected columns in the current state.
+        For each column in in columndict, column expression are either :
+            * The value of the column if it was unchanged ("<CONAME>")
+            * The value of the column following by " AS \"<COLNAME>\""
+        """
+        columns = []
+        for key,value in self.columndict.items():
+            if value[1:-1] == key:
+                columns.append(value)
+            else:
+                columns.append("%s AS \"%s\""%(value, key))
+
+        return ",".join(columns)
+
+
+    def get_order(self):
+        """
+        Returns the "Order by" clause of a corresponding SQL query, according
+        to the value of the attribute order. If order is None, returns an
+        empty string.
+
+        Examples
+        --------
+        >>> self.get_order()
+        " ORDER BY "Col1" ASC, "Col2" ASC, "Col3" ASC"
+        """
+        if not self.order:
+            return ''
+        else:
+            if self.ascending:
+                return (" ORDER BY \"" + "\" ASC, \"".join(self.order) + "\" ASC")
+            else:
+                return (" ORDER BY \"" + "\" DESC, \"".join(self.order) + "\" DESC")
+
+    def is_default(self):
+        """
+        Checks if the InternalState object corresponds to the default values.
+
+        Returns
+        -------
+        bool
+            1 : The InternalState has default values
+            0 : The InternalState is modifed
+        """
+        return self.get_state() == InternalState(self.name).get_state()
+
+    def set_order(self, order, ascending):
+        """
+        Changes order in the internal state of the IdaDataframe.
+        """
+        print("set_order " + order[0])
+        self.order = order
+        self.ascending = ascending
+
+    def reset_order(self):
+        """
+        Resets the state to original order. Delete all views that have an ORDER
+        clause. Based on the assumption that ORDER is the only modification
+        of those queries.
+        """
+        # This is based on the assumption than no other modifications than
+        # sorting was done when sorting the idadf
+        self._views = [view for view in self._views if "ORDER BY" not in view]
+
+    def stop_cumulating_columns(self):
+        # TODO: do not use anymore ??
+        # You actually don't want to use column that you have not selected anymore ? 
+        # except if you want to sum maybe ...
+        """
+        Management of views to ensure that some specific operations such as
+        Filter, order, selection are registred in the list of views so that
+        the real order of operation is respected.
+        Here is what the function does:
+        If _cumulative is not None,
+        then the _cumulative is augmented with all available columns in the
+        database, if any and then appended in _views.
+        _cumulative is then reset to new query that only select the columns
+        that were selected on that point. This is made so, because it allows
+        users to still use all available columns in the table, even if they are
+        not selected in the current state.
+        """
+        # TODO: Improve documentation for this method (experimental)
+
+        # Add all columns available originally in the table in the history
+        if self._cumulative:
+            all_columns = self._idadf._get_all_columns_in_table()
+            deleted_columns = []
+            for column in all_columns:
+                if column not in list(self.columndict.keys()):
+                    deleted_columns.append(column)
+
+            if deleted_columns:
+                for column in deleted_columns:
+                     self.columndict[column] = ("\""+column+"\"")
+                self._cumulative = ["SELECT " + self.get_columns() + " FROM %s"]
+
+            self._views = self._views + self._cumulative
+            self._cumulative = []
+
+            # Select only the columns that were available in the representation
+            # Those 2 steps are made so to enable users to still use columns
+            # that were not selected anymore in the current table representation
+            # but still existing in the idadataframe
+
+            if deleted_columns:
+                for column in deleted_columns:
+                    del self.columndict[column]
+                self.columndict = OrderedDict((cols,"\""+cols+"\"") for cols in self.columndict)
+                self._cumulative = ["SELECT " + self.get_columns() + " FROM %s"]
+            else:
+                self.columndict = OrderedDict((cols,"\""+cols+"\"") for cols in self.columndict)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/learn/__init__.py` & `nzpyida-0.3.3/nzpyida/learn/__init__.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-from .kmeans import KMeans
-from .naive_bayes import NaiveBayes
-from .association_rules import AssociationRules
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+from .kmeans import KMeans
+from .naive_bayes import NaiveBayes
+from .association_rules import AssociationRules
+
 __all__ = ['KMeans', 'NaiveBayes', 'AssociationRules']
```

### Comparing `nzpyida-0.2.2.6/nzpyida/learn/association_rules.py` & `nzpyida-0.3.3/nzpyida/learn/association_rules.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,607 +1,607 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-In-Database Association Rules Mining
-"""
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-from builtins import dict
-from future import standard_library
-standard_library.install_aliases()
-
-import nzpyida
-from nzpyida.exceptions import IdaAssociationRulesError
-import six
-
-#----------------------------------------------------------------------
-# AssociationRules class
-class AssociationRules(object):
-    """
-    Association rules mining can be used to discover interesting and useful 
-    relations between items in a large-scale transaction table. You can 
-    identify strong rules between related items by using different measures of 
-    relevance. Apriori or FP-Growth are well-known algorithms for association 
-    rules mining. For analytic stored procedures, the PrefixSpan algorithm is 
-    preferred due to its scalability.
-
-    The AssociationRules class provides an interface for using the
-    ASSOCRULES amd PREDICT_ASSOCRULES IDAX/INZA methods.
-    """
-
-    def __init__(self, modelname = None, minsupport = None, maxlen = 5, maxheadlen = 1, minconf = 0.5):
-        """
-        Constructor for association rules model
-
-        Parameters
-        ----------
-        modelname : str
-            The name of the Association Rules model that will be built.
-            It should contain only alphanumeric characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If it is not given, it will be generated automatically.
-            If the parameter corresponds to an existing model in the database,
-            it will be replaced during the fitting step.
-        
-        minsupport : float or integer, optional
-            The minimum fraction (0.0 - 1.0) or the minimum number (above 1) of
-            transactions that must contain a pattern to be considered as frequent.
-
-            Default:
-                * system-determined
-
-            Range:
-                * >0.0 and <1.0 for a minimum fraction
-                * >1 for a minimum number of transactions.
-        
-        maxlen : int, optional, >=2, default: 5
-            The maximum length of a pattern or a rule, that is,
-            the maximum number of items per pattern or rule.
-        
-        maxheadlen : int, optional, >= 1 and <maxlen, default: 1
-            The maximum length of a rule head, that is, the maximum number of
-            items that might belong to the item set on the right side of a rule.
-            Increasing this value might significantly increase the number of detected rules.
-        
-        minconf : float, optional, >=0.0 and <= 1, default: 0.5
-            The minimum confidence that a rule must achieve to be kept in the model of the pattern.
-            
-        Attributes
-        ----------
-               
-        nametable
-            Gets set at fit step. The name of the optional table which contains a mapping
-            between the items from the input table and the item names.
-            This attribute is set through the nametable option of the fit method.
-            This table must contain at least two columns, where
-
-            * The first column has the same name as the column that is
-              contained in the item parameter of the input table
-            * The second column has the same name as the name that is
-              defined in the namecol parameter
-        
-        namecol
-            Gets set at fit step. The name of the optional column which
-            contains item names as defined in the nametable attribute.
-            This attribute is set through the fit method and cannot be specified
-            of the nametable parameter of fit is not specified.
-        
-        outtable
-            Gets set at predict step, name of the optional output table
-            in which the mapping between the input
-            sequences and the associated rules or patterns is written.
-            If the parameter corresponds to an existing table in the database,
-            it is replaced.
-        
-        type : str, optional, default : "rules".
-            Gets set at predict step.
-            The type of information that is written in the output table.
-            The following values are possible: ‘rules’ and ‘patterns’;
-        
-        limit: Gets set at predict step int, optional, >=1, default: 1
-            The maximum number of rules or patterns that is written in the 
-            output table for each input sequence.;
-        
-        sort: Gets set at predict step, str or list, optional
-            A list of keywords that indicates the order in which the rules or 
-            patterns are written in the output table. The order of the list is 
-            descending. The items are separated by semicolons. The following 
-            values are possible: ‘support’, ‘confidence’, ‘lift’, and ‘length’. 
-            The ‘confidence’ value can only be specified if the type parameter 
-            is ‘rules’. If the type parameter is ‘rules’, the default is: 
-            support;confidence;length.  If the type parameter is ‘patterns’, 
-            the default is: support;lift;length. ;
-
-
-        Returns
-        -------
-        The AssociationRules object, ready to be used for fitting and prediction
-
-       	Examples
-       	--------
-       	>>> idadb = IdaDataBase("BLUDB-TEST")
-       	>>> idadf = IdaDataFrame(idadb, "GROCERIES")
-       	>>> arules = AssociationRules("ASSOCRULES_TEST")
-       	>>> arules.fit(idadf, transaction_id = "TID", item_id = "SID")
-
-        Notes
-        -----
-        Inner parameters of the model can be printed and modified by using 
-        get_params and set_params. But we recommend creating a new 
-        AssociationRules model instead of modifying it.
-
-        """
-        # Get set at fit step
-        self._idadf = None
-        self._idadb = None
-        self._transaction_id = None
-        self._item_id = None
-        self.nametable = None
-        self.namecol = None
-
-        # Get set at predict step
-        self.outtable = None
-        self.type = None
-        self.limit = None
-        self.sort = None
-
-        self.modelname = modelname
-        self.minsupport = minsupport
-        self.maxlen = maxlen
-        self.maxheadlen = maxheadlen
-        self.minconf = minconf
-
-
-
-    def get_params(self):
-        """
-        Return the parameters of the Association Rules model.
-        """
-        params = dict()
-        params['modelname'] = self.modelname
-        params['minsupport'] = self.minsupport
-        params['maxlen'] = self.maxlen
-        params['maxheadlen'] = self.maxheadlen
-        params['minconf'] = self.minconf
-
-        params['nametable'] = self.nametable
-        params['namecol'] = self.namecol
-
-        params['outtable'] = self.outtable
-        params['type'] = self.type
-        params['limit'] = self.limit
-        params['sort'] = self.sort
-        return params
-
-    def set_params(self, **params):
-        """
-        Modify the parameters of the Association Rules model.
-        """
-        if not params:
-            # Simple optimisation to gain speed (inspect is slow)
-            return self
-        valid_params = self.get_params()
-        for key, value in six.iteritems(params):
-            if key not in valid_params:
-                raise ValueError('Invalid parameter %s for estimator %s' %
-                                     (key, self.__class__.__name__))
-            setattr(self, key, value)
-        return self
-
-    def fit(self, idadf, transaction_id, item_id,  nametable=None, namecol=None, verbose=False):
-        """
-        Create an Association Rules model from an IdaDataFrame.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-            The IdaDataFrame to be used as input.
-        transaction_id : str
-            The column of the input table that identifies the transaction ID.
-        item_id : str
-            The column of the input table that identifies an item of the transaction.
-        nametable : str, optional
-            The IdaDataFrame or the name of the table that contains a mapping of the items in the input table
-            and their names. The table name should contain only alphanumeric characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            The name table must contain at least two columns, where
-
-            * The first column has the same name as the column that is
-              contained in the item parameter of the input table
-            * The second column has the same name as the name that is
-              defined in the namecol parameter
-        namecol : str, optional
-            The column that contains the item name that is defined in the
-            nametable parameter. You cannot specify this parameter if the
-            nametable parameter is not specified.
-        """
-        if not type(idadf).__name__ == 'IdaDataFrame':
-            raise TypeError("Argument should be an IdaDataFrame")
-        if transaction_id not in idadf.columns:
-            raise ValueError("transaction_id is not a column in " + idadf.name)
-        if item_id is None:
-            raise ValueError("item_id cannot be None (should be a column in " + idadf.name + ")")
-        if item_id not in idadf.columns:
-            raise ValueError("item_id is not a column in " + idadf.name)
-
-        idadf._idadb._check_procedure("ASSOCRULES", "Association Rules")
-
-        # Check the ID
-        if transaction_id not in idadf.columns:
-            raise ValueError("Transaction id column"+ transaction_id +" is not available in IdaDataFrame:" )
-
-        self._idadb = idadf._idadb
-        self._idadf = idadf
-        self._transaction_id = transaction_id
-        self._item_id = item_id
-
-        if not (nametable is None or isinstance(nametable, nzpyida.frame.IdaDataFrame)):
-            nametable = nzpyida.utils.check_tablename(nametable)
-        self.nametable = nametable
-        self.namecol = namecol
-
-        # Check or create a model name
-        if self.modelname is None:
-            self.modelname = idadf._idadb._get_valid_modelname('ASSOCRULES_')
-        else:
-            self.modelname = nzpyida.utils.check_tablename(self.modelname)
-            if idadf._idadb.exists_model(self.modelname):
-                idadf._idadb.drop_model(self.modelname)
-
-        # Create a temporay view
-        tmp_view_name = idadf.internal_state._create_view()
-        # tmp_view_name = idadf.internal_state.current_state
-        print(tmp_view_name)
-        
-        #if "." in tmp_view_name:
-            #tmp_view_name = tmp_view_name.split('.')[-1]
-
-        try:
-            idadf._idadb._call_stored_procedure("ASSOCRULES ",
-                                                 model = self.modelname,
-                                                 intable = tmp_view_name,
-                                                 tid = transaction_id,
-                                                 item = item_id,
-                                                 minsupport = self.minsupport,
-                                                 maxlen = self.maxlen,
-                                                 maxheadlen = self.maxheadlen,
-                                                 minconf = self.minconf,
-                                                 nametable = self.nametable,
-                                                 namecol = self.namecol)
-        except:
-            raise
-        finally:
-            idadf.internal_state._delete_view()
-            idadf.commit()
-
-        self._retrieve_AssociationRules_Model(self.modelname, verbose)
-        return
-
-    def prune(self, itemsin = None, itemsout = None, minlen = 1, maxlen = None,
-              minsupport = 0, maxsupport = 1, minlift = None, maxlift = None,
-              minconf = None, maxconf = None, reset = False):
-        """
-        Prune the rules and patterns of an association rules model. To remove 
-        rules and pattern which you are not interested in, you can use filters 
-        to exclude these rules and patterns. These rules and patterns are then 
-        marked as not valid in the model and are no longer shown.
-
-
-        Parameters
-        ----------
-        itemsin : str or list, optional
-            A list of item names that must be contained in the rules or
-            patterns to be kept. The items are separated by semicolons. At
-            least one of the listed items must be contained in a rule or
-            pattern to be kept.
-            For rules, the following conditions apply:
-
-                * To indicate that the item must be contained in the head of then rule,
-                  the item names can be succeeded by :h or :head.
-                * To indicate that the item must be contained in the body of the rule,
-                  the item names can be succeeded by :b or :body
-
-            If this parameter is not specified, no constraint is applied.
-
-        itemsout : str or list, optional
-            A list of item names that must not be contained in the rules or
-            patterns to be kept. The items are separated by semicolons.
-            If this parameter is not specified, no constraint is applied.
-
-        minlen : int, optional, >=1, default: 1
-            The minimum number of items that are to be kept in the rules or
-            patterns.
-
-        maxlen : int, optional, >=1, default: the longest pattern of the model
-            The maximum number of items that are to be kept in the rules or
-            patterns.
-
-        minsupport : float, optional, >=0.0 and <=maxsupport, default : 0
-            The minimum support for the rules or patterns that are to be kept.
-
-        maxsupport : float, optional, >=minsupport and <=1.0, default : 1
-            The maximum support for the rules or patterns that are to be kept.
-
-        minlift : float, optional, >=0.0 and <=maxlift, defaukt : 0
-            The minimum lift of the rules or patterns that are to be kept.
-
-        maxlift : float, optional, >=minlift, default: the maximum lift of the patterns of the model
-            The maximum lift of the rules or patterns that are to be kept.
-
-        minconf : float, optional, >=0.0 and <= maxconf, default : 0
-            The minimum confidence of the rules that are to be kept.
-
-        maxconf : float, optional, >=minconf and <= 1.0, default : 1
-            The maximum confidence of the rules that are to be kept.
-
-        reset : bool, optional, default: false
-            If you specify reset=true, all rules and patterns are first reset
-            to not pruned.
-            If you specify reset=true or reset=false, the rules and patterns
-            that are not to be kept are marked as pruned.
-        """
-
-        self._idadf._idadb._check_procedure("PRUNE_ASSOCRULES", "Pruning for Association Rules")
-
-        if isinstance(itemsin, list):
-            itemsin = ";".join(itemsin)
-
-        try:
-            self._idadf._idadb._call_stored_procedure("PRUNE_ASSOCRULES ",
-                                                 model = self.modelname,
-                                                 itemsin = itemsin,
-                                                 itemsout = itemsout,
-                                                 minlen = minlen,
-                                                 maxlen = maxlen,
-                                                 minsupport = minsupport,
-                                                 maxsupport = maxsupport,
-                                                 minlift = minlift,
-                                                 maxlift = maxlift,
-                                                 minconf = minconf,
-                                                 maxconf = maxconf,
-                                                 reset = reset)
-        except:
-            raise
-        else:
-        	return
-
-    def predict(self, idadf, outtable=None, transaction_id=None, item_id=None,
-                type="rules", limit=1, sort=None):
-        """
-        Apply the rules and patterns of an association rules model to other
-        transactions. You can apply all rules or only specific rules according
-        to specified criteria.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-            IdaDataFrame to be used as input.
-
-        outtable : str, optional
-            The name of the output table in which the mapping between the input 
-            sequences and the associated rules or patterns is written.
-            It should contain only alphanumeric characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If the parameter corresponds to an existing table in the database, it is
-            replaced.
-
-        transaction_id : str, optional
-            The column of the input table that identifies the transaction ID. 
-            By default, this is the same tid column that is specified in the 
-            stored procedure to build the model.
-
-
-        item_id : str, optional
-            The column of the input table that identifies an item of the 
-            transaction. By default, this is the same item column that is 
-            specified in the stored procedure to build the model.
-
-
-        type : str, optional, default : "rules"
-            The type of information that is written in the output table. The 
-            following values are possible: ‘rules’ and ‘patterns’.
-
-        limit : int, optional, >=1, default: 1
-            The maximum number of rules or patterns that is written in the 
-            output table for each input sequence.
-
-        sort : str or list, optional
-            A list of keywords that indicates the order in which the rules or 
-            patterns are written in the output table. The order of the list is 
-            descending. The items are separated by semicolons. The following 
-            values are possible: ‘support’, ‘confidence’, ‘lift’, and ‘length’. 
-            The ‘confidence’ value can only be specified if the type parameter 
-            is ‘rules’. If the type parameter is ‘rules’, the default is: 
-            support;confidence;length.  If the type parameter is ‘patterns’, 
-            the default is: support;lift;length. 
-
-        Notes
-        -----
-        When "type" is set to "rules", it looks like nothing is returned.
-        """
-        if not isinstance(idadf, nzpyida.IdaDataFrame):
-            raise TypeError("Argument should be an IdaDataFrame")
-
-        if sort is not None:
-            sort = ';'.join(sort)
-
-        if transaction_id is None:
-            transaction_id = self.transaction_id
-        if item_id is None:
-            item_id = self.item_id
-
-        # Check the ID
-        if transaction_id not in idadf.columns:
-            raise ValueError("Transaction id column"+ transaction_id +" is not available in IdaDataFrame." )
-
-        if self._idadb is None:
-            raise IdaAssociationRulesError("No Association rules model was trained before.")
-
-        # The version where we don't replace the outtable if it exists but raise an exception
-        #if outtable is not None:
-        #    if idadf._idadb.exists_table(outtable):
-        #        raise ValueError("Table "+ outtable +" already exists.")
-        #else:
-        #    outtable = idadf._idadb._get_valid_modelname('PREDICT_ASSOCRULES_')
-
-        if self.outtable is None:
-            self.outtable = idadf._idadb._get_valid_tablename('ASSOCRULES_')
-        else:
-            self.outtable = nzpyida.utils.check_tablename(self.outtable)
-            if idadf._idadb.exists_table(self.outtable):
-                idadf._idadb.drop_table(self.outtable)
-
-        self.outtable = outtable
-        self.type = type
-        self.limit = limit
-        self.sort = sort
-
-        # Create a temporay view
-        tmp_view_name = idadf.internal_state._create_view()
-        # tmp_view_name = idadf.internal_state.current_state
-        
-        #if "." in tmp_view_name:
-            #tmp_view_name = tmp_view_name.split('.')[-1]
-
-        try:
-            idadf._idadb._call_stored_procedure("PREDICT_ASSOCRULES ",
-                                                 model = self.modelname,
-                                                 intable = tmp_view_name,
-                                                 outtable = outtable,
-                                                 tid = transaction_id,
-                                                 item = item_id,
-                                                 type = type,
-                                                 limit = limit,
-                                                 sort = sort
-                                                 )
-        except:
-            raise
-        finally:
-            idadf.internal_state._delete_view()
-            idadf.commit()
-
-        self.labels_ = nzpyida.IdaDataFrame(idadf._idadb, outtable)
-        return self.labels_
-
-    def fit_predict(self, idadf, transaction_id, item_id,  nametable=None,
-    				namecol=None, outtable=None, type="rules", limit=1,
-                    sort=None, verbose=False):
-        """
-        Convenience function for fitting the model and using it to make 
-        predictions about the same dataset. See the fit and predict 
-        documentation for an explanation about their attributes.
-
-
-        Notes
-        -----
-        If you use this function, you are not able to use the prune step 
-        between the fit and the predict step. However, you can still prune 
-        afterwards and reuse the predict function.
-        """
-        #TODO: Is it relevant ? Result of predict on the same dataset looks empty
-        #for type "rules"
-        self.fit(idadf, transaction_id, item_id,  nametable, namecol, verbose)
-        return self.predict(idadf, outtable, transaction_id, item_id, type, limit, sort)
-
-    def describe(self, detail=False):
-        """
-        Parameters
-        ----------
-        detail: bool, optional. False by default.
-        
-        Returns
-        -------
-        Returns a description of Association Rules Model. If detail is set to False, then
-        only a table with the summary of the rules is displayed, if detail is set to True,
-        then thenintermediary tables are shown too.
-        """
-        if self._idadb is None:
-            return self.get_params
-        else:
-            try:
-                # res = self._idadb.ida_query("CALL IDAX.PRINT_MODEL('model = " + self.modelname +"')")
-                res = self._idadb._call_stored_procedure("PRINT_MODEL ", model = self.modelname)
-                if detail:
-                    self._retrieve_AssociationRules_Model(self.modelname, verbose=True)
-            except:
-                raise
-            else:
-                print('Summary of the rules')
-                print(res)
-            return
-
-    def _retrieve_AssociationRules_Model(self, modelname, verbose=False):
-        """
-        Retrieve information about the model to print the results. The 
-        Association Rules IDAX/INZA function stores its result in 4 tables:
-
-            * <MODELNAME>_ASSOCPATTERNS
-            * <MODELNAME>_ASSOCPATTERNS_STATISTICS
-            * <MODELNAME>_ASSOCRULES
-            * <MODELNAME>_ITEMS
-
-        Parameters
-        ----------
-        modelname : str
-            The name of the model that is retrieved.
-        verbose : bool, default: False
-            Verbosity mode.
-
-        Notes
-        -----
-        Needs better formatting instead of printing the tables
-        """
-        modelname = nzpyida.utils.check_modelname(modelname)
-
-        if self._idadb is None:
-            raise IdaAssociationRulesError("No Association rules model was trained before.")
-
-        # Note: The name of the columns in hardcoded, this is done so as a 
-        # workaround for some bug in a specific ODBC linux driver. 
-        # In case the implementation of the IDA method changes, this may break
-        # But still would not be difficult to fix 
-
-        assocpatterns = self._idadb.ida_query('SELECT * FROM ' + modelname + '_ASSOCPATTERNS')
-        assocpatterns.columns = ["ITEMSETID","ITEMID"]
-        assocpatterns.columns = [x.upper() for x in assocpatterns.columns]
-
-        assocpatterns_stats = self._idadb.ida_query('SELECT * FROM ' + modelname + '_ASSOCPATTERNS_STATISTICS')
-        assocpatterns_stats.columns = ["ITEMSETID" , "LENGTH" , "COUNT"  , "SUPPORT" , "LIFT"  ,"PRUNED"]
-        assocpatterns_stats.columns = [x.upper() for x in assocpatterns_stats.columns]
-
-        assocrules = self._idadb.ida_query('SELECT * FROM ' + modelname + '_ASSOCRULES')
-        assocrules.columns = ["RULEID", "ITEMSETID", "BODYID", "HEADID", "CONFIDENCE", "PRUNED"]
-        assocrules.columns = [x.upper() for x in assocrules.columns]
-
-        items = self._idadb.ida_query('SELECT * FROM ' + modelname + '_ITEMS')
-        items.columns = ["ITEMID","ITEM","ITEMNAME","COUNT","SUPPORT"]
-        items.columns = [x.upper() for x in items.columns]
-
-        if verbose is True:
-            print("assocpatterns")
-            print(assocpatterns)
-            print(" ")
-
-            print("assocpatterns_stats")
-            print(assocpatterns_stats)
-            print(" ")
-
-            print("assocrules")
-            print(assocrules)
-            print(" ")
-
-            print("items")
-            print(items)
-            print(" ")
-
-        return
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+In-Database Association Rules Mining
+"""
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+from builtins import dict
+from future import standard_library
+standard_library.install_aliases()
+
+import nzpyida
+from nzpyida.exceptions import IdaAssociationRulesError
+import six
+
+#----------------------------------------------------------------------
+# AssociationRules class
+class AssociationRules(object):
+    """
+    Association rules mining can be used to discover interesting and useful 
+    relations between items in a large-scale transaction table. You can 
+    identify strong rules between related items by using different measures of 
+    relevance. Apriori or FP-Growth are well-known algorithms for association 
+    rules mining. For analytic stored procedures, the PrefixSpan algorithm is 
+    preferred due to its scalability.
+
+    The AssociationRules class provides an interface for using the
+    ASSOCRULES amd PREDICT_ASSOCRULES IDAX/INZA methods.
+    """
+
+    def __init__(self, modelname = None, minsupport = None, maxlen = 5, maxheadlen = 1, minconf = 0.5):
+        """
+        Constructor for association rules model
+
+        Parameters
+        ----------
+        modelname : str
+            The name of the Association Rules model that will be built.
+            It should contain only alphanumeric characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If it is not given, it will be generated automatically.
+            If the parameter corresponds to an existing model in the database,
+            it will be replaced during the fitting step.
+        
+        minsupport : float or integer, optional
+            The minimum fraction (0.0 - 1.0) or the minimum number (above 1) of
+            transactions that must contain a pattern to be considered as frequent.
+
+            Default:
+                * system-determined
+
+            Range:
+                * >0.0 and <1.0 for a minimum fraction
+                * >1 for a minimum number of transactions.
+        
+        maxlen : int, optional, >=2, default: 5
+            The maximum length of a pattern or a rule, that is,
+            the maximum number of items per pattern or rule.
+        
+        maxheadlen : int, optional, >= 1 and <maxlen, default: 1
+            The maximum length of a rule head, that is, the maximum number of
+            items that might belong to the item set on the right side of a rule.
+            Increasing this value might significantly increase the number of detected rules.
+        
+        minconf : float, optional, >=0.0 and <= 1, default: 0.5
+            The minimum confidence that a rule must achieve to be kept in the model of the pattern.
+            
+        Attributes
+        ----------
+               
+        nametable
+            Gets set at fit step. The name of the optional table which contains a mapping
+            between the items from the input table and the item names.
+            This attribute is set through the nametable option of the fit method.
+            This table must contain at least two columns, where
+
+            * The first column has the same name as the column that is
+              contained in the item parameter of the input table
+            * The second column has the same name as the name that is
+              defined in the namecol parameter
+        
+        namecol
+            Gets set at fit step. The name of the optional column which
+            contains item names as defined in the nametable attribute.
+            This attribute is set through the fit method and cannot be specified
+            of the nametable parameter of fit is not specified.
+        
+        outtable
+            Gets set at predict step, name of the optional output table
+            in which the mapping between the input
+            sequences and the associated rules or patterns is written.
+            If the parameter corresponds to an existing table in the database,
+            it is replaced.
+        
+        type : str, optional, default : "rules".
+            Gets set at predict step.
+            The type of information that is written in the output table.
+            The following values are possible: ‘rules’ and ‘patterns’;
+        
+        limit: Gets set at predict step int, optional, >=1, default: 1
+            The maximum number of rules or patterns that is written in the 
+            output table for each input sequence.;
+        
+        sort: Gets set at predict step, str or list, optional
+            A list of keywords that indicates the order in which the rules or 
+            patterns are written in the output table. The order of the list is 
+            descending. The items are separated by semicolons. The following 
+            values are possible: ‘support’, ‘confidence’, ‘lift’, and ‘length’. 
+            The ‘confidence’ value can only be specified if the type parameter 
+            is ‘rules’. If the type parameter is ‘rules’, the default is: 
+            support;confidence;length.  If the type parameter is ‘patterns’, 
+            the default is: support;lift;length. ;
+
+
+        Returns
+        -------
+        The AssociationRules object, ready to be used for fitting and prediction
+
+       	Examples
+       	--------
+       	>>> idadb = IdaDataBase("BLUDB-TEST")
+       	>>> idadf = IdaDataFrame(idadb, "GROCERIES")
+       	>>> arules = AssociationRules("ASSOCRULES_TEST")
+       	>>> arules.fit(idadf, transaction_id = "TID", item_id = "SID")
+
+        Notes
+        -----
+        Inner parameters of the model can be printed and modified by using 
+        get_params and set_params. But we recommend creating a new 
+        AssociationRules model instead of modifying it.
+
+        """
+        # Get set at fit step
+        self._idadf = None
+        self._idadb = None
+        self._transaction_id = None
+        self._item_id = None
+        self.nametable = None
+        self.namecol = None
+
+        # Get set at predict step
+        self.outtable = None
+        self.type = None
+        self.limit = None
+        self.sort = None
+
+        self.modelname = modelname
+        self.minsupport = minsupport
+        self.maxlen = maxlen
+        self.maxheadlen = maxheadlen
+        self.minconf = minconf
+
+
+
+    def get_params(self):
+        """
+        Return the parameters of the Association Rules model.
+        """
+        params = dict()
+        params['modelname'] = self.modelname
+        params['minsupport'] = self.minsupport
+        params['maxlen'] = self.maxlen
+        params['maxheadlen'] = self.maxheadlen
+        params['minconf'] = self.minconf
+
+        params['nametable'] = self.nametable
+        params['namecol'] = self.namecol
+
+        params['outtable'] = self.outtable
+        params['type'] = self.type
+        params['limit'] = self.limit
+        params['sort'] = self.sort
+        return params
+
+    def set_params(self, **params):
+        """
+        Modify the parameters of the Association Rules model.
+        """
+        if not params:
+            # Simple optimisation to gain speed (inspect is slow)
+            return self
+        valid_params = self.get_params()
+        for key, value in six.iteritems(params):
+            if key not in valid_params:
+                raise ValueError('Invalid parameter %s for estimator %s' %
+                                     (key, self.__class__.__name__))
+            setattr(self, key, value)
+        return self
+
+    def fit(self, idadf, transaction_id, item_id,  nametable=None, namecol=None, verbose=False):
+        """
+        Create an Association Rules model from an IdaDataFrame.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+            The IdaDataFrame to be used as input.
+        transaction_id : str
+            The column of the input table that identifies the transaction ID.
+        item_id : str
+            The column of the input table that identifies an item of the transaction.
+        nametable : str, optional
+            The IdaDataFrame or the name of the table that contains a mapping of the items in the input table
+            and their names. The table name should contain only alphanumeric characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            The name table must contain at least two columns, where
+
+            * The first column has the same name as the column that is
+              contained in the item parameter of the input table
+            * The second column has the same name as the name that is
+              defined in the namecol parameter
+        namecol : str, optional
+            The column that contains the item name that is defined in the
+            nametable parameter. You cannot specify this parameter if the
+            nametable parameter is not specified.
+        """
+        if not type(idadf).__name__ == 'IdaDataFrame':
+            raise TypeError("Argument should be an IdaDataFrame")
+        if transaction_id not in idadf.columns:
+            raise ValueError("transaction_id is not a column in " + idadf.name)
+        if item_id is None:
+            raise ValueError("item_id cannot be None (should be a column in " + idadf.name + ")")
+        if item_id not in idadf.columns:
+            raise ValueError("item_id is not a column in " + idadf.name)
+
+        idadf._idadb._check_procedure("ASSOCRULES", "Association Rules")
+
+        # Check the ID
+        if transaction_id not in idadf.columns:
+            raise ValueError("Transaction id column"+ transaction_id +" is not available in IdaDataFrame:" )
+
+        self._idadb = idadf._idadb
+        self._idadf = idadf
+        self._transaction_id = transaction_id
+        self._item_id = item_id
+
+        if not (nametable is None or isinstance(nametable, nzpyida.frame.IdaDataFrame)):
+            nametable = nzpyida.utils.check_tablename(nametable)
+        self.nametable = nametable
+        self.namecol = namecol
+
+        # Check or create a model name
+        if self.modelname is None:
+            self.modelname = idadf._idadb._get_valid_modelname('ASSOCRULES_')
+        else:
+            self.modelname = nzpyida.utils.check_tablename(self.modelname)
+            if idadf._idadb.exists_model(self.modelname):
+                idadf._idadb.drop_model(self.modelname)
+
+        # Create a temporay view
+        tmp_view_name = idadf.internal_state._create_view()
+        # tmp_view_name = idadf.internal_state.current_state
+        print(tmp_view_name)
+        
+        #if "." in tmp_view_name:
+            #tmp_view_name = tmp_view_name.split('.')[-1]
+
+        try:
+            idadf._idadb._call_stored_procedure("ASSOCRULES ",
+                                                 model = self.modelname,
+                                                 intable = tmp_view_name,
+                                                 tid = transaction_id,
+                                                 item = item_id,
+                                                 minsupport = self.minsupport,
+                                                 maxlen = self.maxlen,
+                                                 maxheadlen = self.maxheadlen,
+                                                 minconf = self.minconf,
+                                                 nametable = self.nametable,
+                                                 namecol = self.namecol)
+        except:
+            raise
+        finally:
+            idadf.internal_state._delete_view()
+            idadf.commit()
+
+        self._retrieve_AssociationRules_Model(self.modelname, verbose)
+        return
+
+    def prune(self, itemsin = None, itemsout = None, minlen = 1, maxlen = None,
+              minsupport = 0, maxsupport = 1, minlift = None, maxlift = None,
+              minconf = None, maxconf = None, reset = False):
+        """
+        Prune the rules and patterns of an association rules model. To remove 
+        rules and pattern which you are not interested in, you can use filters 
+        to exclude these rules and patterns. These rules and patterns are then 
+        marked as not valid in the model and are no longer shown.
+
+
+        Parameters
+        ----------
+        itemsin : str or list, optional
+            A list of item names that must be contained in the rules or
+            patterns to be kept. The items are separated by semicolons. At
+            least one of the listed items must be contained in a rule or
+            pattern to be kept.
+            For rules, the following conditions apply:
+
+                * To indicate that the item must be contained in the head of then rule,
+                  the item names can be succeeded by :h or :head.
+                * To indicate that the item must be contained in the body of the rule,
+                  the item names can be succeeded by :b or :body
+
+            If this parameter is not specified, no constraint is applied.
+
+        itemsout : str or list, optional
+            A list of item names that must not be contained in the rules or
+            patterns to be kept. The items are separated by semicolons.
+            If this parameter is not specified, no constraint is applied.
+
+        minlen : int, optional, >=1, default: 1
+            The minimum number of items that are to be kept in the rules or
+            patterns.
+
+        maxlen : int, optional, >=1, default: the longest pattern of the model
+            The maximum number of items that are to be kept in the rules or
+            patterns.
+
+        minsupport : float, optional, >=0.0 and <=maxsupport, default : 0
+            The minimum support for the rules or patterns that are to be kept.
+
+        maxsupport : float, optional, >=minsupport and <=1.0, default : 1
+            The maximum support for the rules or patterns that are to be kept.
+
+        minlift : float, optional, >=0.0 and <=maxlift, defaukt : 0
+            The minimum lift of the rules or patterns that are to be kept.
+
+        maxlift : float, optional, >=minlift, default: the maximum lift of the patterns of the model
+            The maximum lift of the rules or patterns that are to be kept.
+
+        minconf : float, optional, >=0.0 and <= maxconf, default : 0
+            The minimum confidence of the rules that are to be kept.
+
+        maxconf : float, optional, >=minconf and <= 1.0, default : 1
+            The maximum confidence of the rules that are to be kept.
+
+        reset : bool, optional, default: false
+            If you specify reset=true, all rules and patterns are first reset
+            to not pruned.
+            If you specify reset=true or reset=false, the rules and patterns
+            that are not to be kept are marked as pruned.
+        """
+
+        self._idadf._idadb._check_procedure("PRUNE_ASSOCRULES", "Pruning for Association Rules")
+
+        if isinstance(itemsin, list):
+            itemsin = ";".join(itemsin)
+
+        try:
+            self._idadf._idadb._call_stored_procedure("PRUNE_ASSOCRULES ",
+                                                 model = self.modelname,
+                                                 itemsin = itemsin,
+                                                 itemsout = itemsout,
+                                                 minlen = minlen,
+                                                 maxlen = maxlen,
+                                                 minsupport = minsupport,
+                                                 maxsupport = maxsupport,
+                                                 minlift = minlift,
+                                                 maxlift = maxlift,
+                                                 minconf = minconf,
+                                                 maxconf = maxconf,
+                                                 reset = reset)
+        except:
+            raise
+        else:
+        	return
+
+    def predict(self, idadf, outtable=None, transaction_id=None, item_id=None,
+                type="rules", limit=1, sort=None):
+        """
+        Apply the rules and patterns of an association rules model to other
+        transactions. You can apply all rules or only specific rules according
+        to specified criteria.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+            IdaDataFrame to be used as input.
+
+        outtable : str, optional
+            The name of the output table in which the mapping between the input 
+            sequences and the associated rules or patterns is written.
+            It should contain only alphanumeric characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If the parameter corresponds to an existing table in the database, it is
+            replaced.
+
+        transaction_id : str, optional
+            The column of the input table that identifies the transaction ID. 
+            By default, this is the same tid column that is specified in the 
+            stored procedure to build the model.
+
+
+        item_id : str, optional
+            The column of the input table that identifies an item of the 
+            transaction. By default, this is the same item column that is 
+            specified in the stored procedure to build the model.
+
+
+        type : str, optional, default : "rules"
+            The type of information that is written in the output table. The 
+            following values are possible: ‘rules’ and ‘patterns’.
+
+        limit : int, optional, >=1, default: 1
+            The maximum number of rules or patterns that is written in the 
+            output table for each input sequence.
+
+        sort : str or list, optional
+            A list of keywords that indicates the order in which the rules or 
+            patterns are written in the output table. The order of the list is 
+            descending. The items are separated by semicolons. The following 
+            values are possible: ‘support’, ‘confidence’, ‘lift’, and ‘length’. 
+            The ‘confidence’ value can only be specified if the type parameter 
+            is ‘rules’. If the type parameter is ‘rules’, the default is: 
+            support;confidence;length.  If the type parameter is ‘patterns’, 
+            the default is: support;lift;length. 
+
+        Notes
+        -----
+        When "type" is set to "rules", it looks like nothing is returned.
+        """
+        if not isinstance(idadf, nzpyida.IdaDataFrame):
+            raise TypeError("Argument should be an IdaDataFrame")
+
+        if sort is not None:
+            sort = ';'.join(sort)
+
+        if transaction_id is None:
+            transaction_id = self.transaction_id
+        if item_id is None:
+            item_id = self.item_id
+
+        # Check the ID
+        if transaction_id not in idadf.columns:
+            raise ValueError("Transaction id column"+ transaction_id +" is not available in IdaDataFrame." )
+
+        if self._idadb is None:
+            raise IdaAssociationRulesError("No Association rules model was trained before.")
+
+        # The version where we don't replace the outtable if it exists but raise an exception
+        #if outtable is not None:
+        #    if idadf._idadb.exists_table(outtable):
+        #        raise ValueError("Table "+ outtable +" already exists.")
+        #else:
+        #    outtable = idadf._idadb._get_valid_modelname('PREDICT_ASSOCRULES_')
+
+        if self.outtable is None:
+            self.outtable = idadf._idadb._get_valid_tablename('ASSOCRULES_')
+        else:
+            self.outtable = nzpyida.utils.check_tablename(self.outtable)
+            if idadf._idadb.exists_table(self.outtable):
+                idadf._idadb.drop_table(self.outtable)
+
+        self.outtable = outtable
+        self.type = type
+        self.limit = limit
+        self.sort = sort
+
+        # Create a temporay view
+        tmp_view_name = idadf.internal_state._create_view()
+        # tmp_view_name = idadf.internal_state.current_state
+        
+        #if "." in tmp_view_name:
+            #tmp_view_name = tmp_view_name.split('.')[-1]
+
+        try:
+            idadf._idadb._call_stored_procedure("PREDICT_ASSOCRULES ",
+                                                 model = self.modelname,
+                                                 intable = tmp_view_name,
+                                                 outtable = outtable,
+                                                 tid = transaction_id,
+                                                 item = item_id,
+                                                 type = type,
+                                                 limit = limit,
+                                                 sort = sort
+                                                 )
+        except:
+            raise
+        finally:
+            idadf.internal_state._delete_view()
+            idadf.commit()
+
+        self.labels_ = nzpyida.IdaDataFrame(idadf._idadb, outtable)
+        return self.labels_
+
+    def fit_predict(self, idadf, transaction_id, item_id,  nametable=None,
+    				namecol=None, outtable=None, type="rules", limit=1,
+                    sort=None, verbose=False):
+        """
+        Convenience function for fitting the model and using it to make 
+        predictions about the same dataset. See the fit and predict 
+        documentation for an explanation about their attributes.
+
+
+        Notes
+        -----
+        If you use this function, you are not able to use the prune step 
+        between the fit and the predict step. However, you can still prune 
+        afterwards and reuse the predict function.
+        """
+        #TODO: Is it relevant ? Result of predict on the same dataset looks empty
+        #for type "rules"
+        self.fit(idadf, transaction_id, item_id,  nametable, namecol, verbose)
+        return self.predict(idadf, outtable, transaction_id, item_id, type, limit, sort)
+
+    def describe(self, detail=False):
+        """
+        Parameters
+        ----------
+        detail: bool, optional. False by default.
+        
+        Returns
+        -------
+        Returns a description of Association Rules Model. If detail is set to False, then
+        only a table with the summary of the rules is displayed, if detail is set to True,
+        then thenintermediary tables are shown too.
+        """
+        if self._idadb is None:
+            return self.get_params
+        else:
+            try:
+                # res = self._idadb.ida_query("CALL IDAX.PRINT_MODEL('model = " + self.modelname +"')")
+                res = self._idadb._call_stored_procedure("PRINT_MODEL ", model = self.modelname)
+                if detail:
+                    self._retrieve_AssociationRules_Model(self.modelname, verbose=True)
+            except:
+                raise
+            else:
+                print('Summary of the rules')
+                print(res)
+            return
+
+    def _retrieve_AssociationRules_Model(self, modelname, verbose=False):
+        """
+        Retrieve information about the model to print the results. The 
+        Association Rules IDAX/INZA function stores its result in 4 tables:
+
+            * <MODELNAME>_ASSOCPATTERNS
+            * <MODELNAME>_ASSOCPATTERNS_STATISTICS
+            * <MODELNAME>_ASSOCRULES
+            * <MODELNAME>_ITEMS
+
+        Parameters
+        ----------
+        modelname : str
+            The name of the model that is retrieved.
+        verbose : bool, default: False
+            Verbosity mode.
+
+        Notes
+        -----
+        Needs better formatting instead of printing the tables
+        """
+        modelname = nzpyida.utils.check_modelname(modelname)
+
+        if self._idadb is None:
+            raise IdaAssociationRulesError("No Association rules model was trained before.")
+
+        # Note: The name of the columns in hardcoded, this is done so as a 
+        # workaround for some bug in a specific ODBC linux driver. 
+        # In case the implementation of the IDA method changes, this may break
+        # But still would not be difficult to fix 
+
+        assocpatterns = self._idadb.ida_query('SELECT * FROM ' + modelname + '_ASSOCPATTERNS')
+        assocpatterns.columns = ["ITEMSETID","ITEMID"]
+        assocpatterns.columns = [x.upper() for x in assocpatterns.columns]
+
+        assocpatterns_stats = self._idadb.ida_query('SELECT * FROM ' + modelname + '_ASSOCPATTERNS_STATISTICS')
+        assocpatterns_stats.columns = ["ITEMSETID" , "LENGTH" , "COUNT"  , "SUPPORT" , "LIFT"  ,"PRUNED"]
+        assocpatterns_stats.columns = [x.upper() for x in assocpatterns_stats.columns]
+
+        assocrules = self._idadb.ida_query('SELECT * FROM ' + modelname + '_ASSOCRULES')
+        assocrules.columns = ["RULEID", "ITEMSETID", "BODYID", "HEADID", "CONFIDENCE", "PRUNED"]
+        assocrules.columns = [x.upper() for x in assocrules.columns]
+
+        items = self._idadb.ida_query('SELECT * FROM ' + modelname + '_ITEMS')
+        items.columns = ["ITEMID","ITEM","ITEMNAME","COUNT","SUPPORT"]
+        items.columns = [x.upper() for x in items.columns]
+
+        if verbose is True:
+            print("assocpatterns")
+            print(assocpatterns)
+            print(" ")
+
+            print("assocpatterns_stats")
+            print(assocpatterns_stats)
+            print(" ")
+
+            print("assocrules")
+            print(assocrules)
+            print(" ")
+
+            print("items")
+            print(items)
+            print(" ")
+
+        return
```

### Comparing `nzpyida-0.2.2.6/nzpyida/learn/kmeans.py` & `nzpyida-0.3.3/nzpyida/learn/kmeans.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,527 +1,527 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-In-Database K-means. Copies the interface of sklearn.cluster.KMeans
-"""
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-from builtins import int
-from builtins import dict
-from builtins import str
-from future import standard_library
-standard_library.install_aliases()
-
-from lazy import lazy
-import pandas as pd
-
-import nzpyida
-from nzpyida import IdaDataFrame
-from nzpyida.exceptions import IdaKMeansError
-import six
-
-class KMeans(object):
-    """
-    The K-means algorithm is the most widely used clustering algorithm that 
-    uses an explicit distance measure to partition the data set into clusters.
-
-    The K-means algorithm represents each cluster by the vector of the mean 
-    attribute values of all training instances - for numeric attributes - and 
-    by the vector of modal (most frequent) values - for nominal attributes - 
-    that are assigned to that cluster. This cluster representation is called 
-    cluster center.
-
-
-    The KMeans class provides an interface for using the KMEANS
-    and PREDICT_KMEANS IDAX/INZA methods.
-    """
-    def __init__(self, n_clusters=3, modelname=None, max_iter=5, distance="euclidean",
-                 random_state=12345, idbased=False, statistics=None):
-        """ 
-        Constructor for K-means clustering.
-
-        Parameters
-        ----------
-
-        n_cluster : int, optional, default: 3
-            The number of cluster centers.
-            Range : > 2
-
-        modelname : str, optional
-            The name of the clustering model that will be built.
-            It should contain only alphanumeric characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If it is not given, it will be generated automatically.
-            If the parameter corresponds to an existing model in the database,
-            it is replaced during the fitting step.
-
-
-        max_iter : int, > 1 and <= 1000, default = 5
-            The maximum number of iterations.
-
-        distance : str, default: "euclidean"
-             The distance function. The following values are allowed: “euclidean” and “norm_euclidean”.
-
-
-        random_state : int, default: 12345
-            The random seed of the generator.
-
-        idbased : bool, optional, default: False
-            Specifies that the random seed of the generator is based on the value of the ID column.
-
-        statistics : str, optional
-            Indicates the statistics that are collected.
-
-            The following values are allowed: ‘none’, ‘columns’, ‘values:n’, and ‘all’:
-                * If statistics='none' is specified, no statistics are collected.
-                * If statistics='columns' is specified, statistics on the columns
-                  of the input table are collected, for example, mean values.
-                * If statistics='values:n' is specified, and if n is a positive number, statistics
-                  on the columns and the column values are collected.
-
-                    Up to <n> column value statistics are collected.
-                        * If a nominal column contains more than <n> values,
-                          only the <n> most frequent column statistics are kept.
-                        * If a numeric column contains more than <n> values, the values
-                          are discretized, and the statistics are collected on the discretized values.
-                * statistics=all is identical to statistics=values:100.
-
-        Attributes
-        ----------
-        centers: table containing the coordinates of each cluster center;
-
-        cluster_centers_: coordinates of each cluster center, as list;
-
-        withinss: Within cluster sum of squares, by cluster, as a list;
-
-        size_clusters: number of items in each cluster, as list;
-
-        inertia_: float, total inertia of the system, defined as the sum of each cluster's within cluster sum of squares.
-
-        Returns
-        -------
-            The KMeans object, ready to be used for fitting and prediction
-
-        Examples
-        --------
-        >>> idadb = IdaDataBase("BLUDB")
-        >>> idadf = IdaDataFrame(idadb, "IRIS", indexer = "ID")
-        >>> kmeans = KMeans(3) # clustering with 3 clusters
-        >>> kmeans.fit(idadf)
-        >>> kmeans.predict(idadf)
-
-        Notes
-        -----
-        Inner parameters of the model can be printed and modified by using 
-        get_params and set_params. But we recommend creating a new KMeans model 
-        instead of modifying it.
-
-        """
-        self.modelname = modelname
-        self.n_clusters = n_clusters
-        self.max_iter = max_iter
-        self.distance = distance
-        self.random_state = random_state
-        self.idbased = idbased
-        self.statistics = statistics
-
-        # Get set at fit step
-        self._idadb = None
-        self._idadf = None
-        self._column_id = None
-        self.incolumn = None
-        self.coldeftype = None
-        self.coldefrole = None
-        self.colPropertiesTable = None
-
-        # Get set at predict step
-        self.outtable = None
-
-    @lazy
-    def labels_(self):
-        """
-        Return the corresponding labels for each ID.
-        """
-        try:
-            return self.predict(self._idadf, self._column_id)
-        except:
-            raise AttributeError(str(self.__class__) + " object has no attribute 'labels_'")
-
-    def get_params(self):
-        """
-        Return the parameters of the K-means clustering.
-        """
-        params = dict()
-        params['modelname'] = self.modelname
-        params['n_clusters'] = self.n_clusters
-        params['max_iter'] = self.max_iter
-        params['distance'] = self.distance
-        params['random_state'] = self.random_state
-        params['idbased'] = self.idbased
-        params['statistics'] = self.statistics
-
-        params['incolumn'] = self.incolumn
-        params['coldeftype'] = self.coldeftype
-        params['coldefrole'] = self.coldefrole
-        params['colPropertiesTable'] = self.colPropertiesTable
-
-        params['outtable'] = self.outtable
-        return params
-
-    def set_params(self, **params):
-        """
-        Change the parameters of the K-means clustering.
-        """
-        if not params:
-            # Simple optimisation to gain speed (inspect is slow)
-            return self
-        valid_params = self.get_params()
-        for key, value in six.iteritems(params):
-            if key not in valid_params:
-                raise ValueError('Invalid parameter %s for estimator %s' %
-                                     (key, self.__class__.__name__))
-            setattr(self, key, value)
-        return self
-
-    def fit(self, idadf, column_id="ID", incolumn=None, coldeftype=None,
-            coldefrole=None, colPropertiesTable=None, verbose=False):
-        """
-        Use the KMEANS stored procedure to build a K-means clustering model 
-        that clusters the input data into k centers.
-
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-            The name of the input IdaDataFrame.
-
-        column_id : str, default: "ID"
-            The column of the input IdaDataFrame that identifies a unique
-            instance ID.
-
-        incolumn : dict, optional
-            The columns of the input table that have specific properties, which
-            are separated by a semi-colon (;).
-            Each column is succeeded by one or more of the following properties:
-
-                * By type nominal (':nom') or by type continuous (':cont'). By default,
-                  numerical types are continuous, and all other types nominal.
-                * By role ':id', ':target', ':input', or ':ignore'.
-
-            If this parameter is not specified, all columns of the input table
-            have default properties.
-
-        coldeftype : dict, optional
-            The default type of the input table columns. The following values 
-            are allowed: ‘nom’ and ‘cont’. If the parameter is not specified, 
-            numeric columns are continuous and all other columns are nominal.
-
-
-        coldefrole : dict, optional
-            The default role of the input table columns. The following values 
-            are allowed: ‘input’ and ‘ignore’. If the parameter is not 
-            specified, all columns are input columns.
-
-        colPropertiesTable : idaDataFrame, optional
-            The input IdaDataFrame or name of the table where the properties of the
-            columns of the input IdaDataFrame (idadf) are stored.
-            The table name should contain only alphanumeric characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If this parameter is not specified, the column properties of the input table column
-            properties are detected automatically.
-
-        verbose : bool, default: False
-            Verbosity mode.
-
-        """
-        if not type(idadf).__name__ == 'IdaDataFrame':
-            raise TypeError("Argument should be an IdaDataFrame")
-
-        idadf._idadb._check_procedure("KMEANS", "KMeans")
-
-        # Check the ID
-        if column_id not in idadf.columns:
-            raise ValueError("No id columns is available in IdaDataFrame:" + column_id +
-                             ". Either create a new ID column using add_column_id function" +
-                             " or give the name of a column that can be used as ID")
-
-        self._idadb = idadf._idadb
-        self._idadf = idadf
-        self._column_id = column_id
-
-        # Check or create a model name
-        if self.modelname is None:
-            self.modelname = idadf._idadb._get_valid_modelname('KMEANS_')
-        else:
-            self.modelname = nzpyida.utils.check_modelname(self.modelname)
-            if idadf._idadb.exists_model(self.modelname):
-                idadf._idadb.drop_model(self.modelname)
-
-        self.incolumn = incolumn
-        self.coldeftype = coldeftype
-        self.coldefrole = coldefrole
-        if not (colPropertiesTable is None or isinstance(colPropertiesTable, nzpyida.frame.IdaDataFrame)):
-            colPropertiesTable = nzpyida.utils.check_tablename(colPropertiesTable)
-        self.colPropertiesTable = colPropertiesTable
-
-        # Create a temporay view
-        tmp_view_name = idadf.internal_state._create_view()
-        # tmp_view_name = idadf.internal_state.current_state # deprecated, hange to idadf.name
-        
-        #if "." in tmp_view_name:
-            #tmp_view_name = tmp_view_name.split('.')[-1]
-        if (self.outtable == None) & self._idadb._is_netezza_system():
-            self.outtable = self._idadb._get_valid_tablename(prefix="OUTTABLE_KMEANS_")
-        try:
-            # TODO: outtable is optional but this does not match with the doc
-            # Defect to declare
-            idadf._idadb._call_stored_procedure("KMEANS ",
-                                                model = self.modelname,
-                                                intable = tmp_view_name,
-                                                k = self.n_clusters,
-                                                maxiter = self.max_iter,
-                                                outtable = self.outtable,
-                                                distance = self.distance,
-                                                id = self._column_id,
-                                                randseed = self.random_state,
-                                                statistics = self.statistics,
-                                                idbased = self.idbased,
-                                                incolumn = self.incolumn,
-                                                coldeftype = self.coldeftype,
-                                                coldefrole = self.coldefrole,
-                                                colPropertiesTable = self.colPropertiesTable)
-
-        except:
-            raise
-        finally:
-            idadf.internal_state._delete_view()
-            idadf.commit()
-
-        result = self._retrieve_KMeans_Model(self.modelname, verbose)
-        self.centers = result['centers']
-        self.cluster_centers_ = result['centers'].values
-        self.withinss = result['withinss']
-        self.size_clusters = [int(str(x)) for x in result['size']]
-        self.inertia_ = sum(self.withinss)
-
-        if verbose is True:
-            self.describe()
-
-        return
-
-    def predict(self, idadf, column_id=None, outtable=None):
-        """
-        Apply the K-means clustering model to new data.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-            IdaDataFrame to be used as input.
-
-        column_id : str
-            The column of the input table that identifies a unique instance ID.
-            Default: the same id column that is specified in the stored procedure to build the model.
-
-        outtable : str
-            The name of the output table where the assigned clusters are stored.
-            It should contain only alphanumeric characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If this parameter is not specified, it is generated automatically.
-            If the parameter corresponds to an existing table in the database,
-            it is replaced.
-
-        Returns
-        -------
-        IdaDataFrame
-            IdaDataFrame containing the closest cluster for each data point referenced by its ID.
-        """
-        if not type(idadf).__name__ == 'IdaDataFrame':
-            raise TypeError("Argument should be an IdaDataFrame")
-
-        # Check the ID
-        if column_id is None:
-            column_id = self._column_id
-        if column_id not in idadf.columns:
-            raise ValueError("No id columns is available in IdaDataFrame:" + column_id +
-                             ". Either create a new ID column using add_column_id function" +
-                             " or give the name of a column that can be used as ID")
-
-        if self._idadb is None:
-            raise IdaKMeansError("No KMeans model was trained before")
-
-
-        if outtable is None:
-            outtable = idadf._idadb._get_valid_modelname('PREDICT_KMEANS_')
-        else:
-            if self.outtable:
-                outtable = self.outtable
-            outtable = nzpyida.utils.check_tablename(outtable)
-            if idadf._idadb.exists_table(outtable):
-                idadf._idadb.drop_table(outtable)
-
-        self.outtable = outtable
-        # Create a temporay view
-        tmp_view_name = idadf.internal_state._create_view()
-        # tmp_view_name = idadf.internal_state.current_state
-        
-        #if "." in tmp_view_name:
-            #tmp_view_name = tmp_view_name.split('.')[-1]
-            
-        try:
-            idadf._idadb._call_stored_procedure("PREDICT_KMEANS ",
-                                                 model = self.modelname,
-                                                 intable = tmp_view_name,
-                                                 id = column_id,
-                                                 outtable = self.outtable
-                                                 )
-        except:
-            raise
-        finally:
-            idadf.internal_state._delete_view()
-            idadf._idadb.commit()
-
-        self.labels_ = nzpyida.IdaDataFrame(idadf._idadb, outtable, indexer=column_id)
-        return self.labels_
-
-    def fit_predict(self, idadf, column_id="ID", incolumn=None, coldeftype=None,
-                    coldefrole=None, colPropertiesTable=None, outtable=None,
-                    verbose=False):
-        """
-        Convenience function for fitting the model and using it to make 
-        predictions on the same dataset. See the fit and predict documentation 
-        for an explanation about their attributes.
-        """
-        self.fit(idadf, column_id, incolumn, coldeftype, coldefrole,
-                 colPropertiesTable, verbose)
-        return self.predict(idadf, column_id, outtable)
-
-    def describe(self):
-        """
-        Return a description of the K-means clustering, if a prediction was 
-        made. Otherwise,  this function returns the parameters of the model.
-        """
-        if self._idadb is None:
-            return self.get_params
-        else:
-            print("KMeans clustering with " + str(self.n_clusters) +
-            " clusters of sizes " + ', '.join([str(x) for x in self.size_clusters]))
-            print()
-            print("Cluster means: ")
-            print(self.centers)
-            print()
-            print("Within cluster sum of squares by cluster:")
-            print(self.withinss)
-            try:
-                self._idadb._call_stored_procedure("PRINT_MODEL ", model = self.modelname)
-            except:
-                raise
-            return
-
-    def _retrieve_KMeans_Model(self, modelname, verbose=False):
-        """
-        Retrieve information about the model to print the results. The KMEANS 
-        IDAX/INZA function stores its result in 4 tables:
-
-            * <MODELNAME>_MODEL
-            * <MODELNAME>_COLUMNS
-            * <MODELNAME>_COLUMN_STATISTICS
-            * <MODELNAME>_CLUSTERS
-
-        Parameters
-        ----------
-        modelname : str
-            The name of the model that is retrieved.
-
-        verbose : bol, default: False
-            Verbosity mode.
-        """
-        modelname = nzpyida.utils.check_modelname(modelname)
-
-        if self._idadb is None:
-            raise IdaKMeansError("No KMeans model was trained before")
-
-        if self._idadb._is_netezza_system():
-            modeltable_prefix = "INZA.NZA_META_"
-        else:
-            modeltable_prefix = ""
-        model_main = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_MODEL')
-        # Woraround for specific version of ODBC
-        model_main.columns = ['MODELCLASS', 'COMPARISONTYPE', 'COMPARISONMEASURE', 'NUMCLUSTERS']
-        model_main.columns = [x.upper() for x in model_main.columns]
-
-
-        col_info = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_COLUMNS',)
-        col_info.columns = ['COLUMNNAME', 'DATATYPE', 'OPTYPE', 'USAGETYPE', 'COLUMNWEIGHT',
-       'AUTOTRANSFORM', 'TRANSFORMEDCOLUMN', 'COMPAREFUNCTION', 'IMPORTANCE',
-       'OUTLIERTREATMENT', 'LOWERLIMIT', 'UPPERLIMIT', 'CLOSURE',
-       'STATISTICSTYPE']
-        col_info.columns = [x.upper() for x in col_info.columns]
-
-        col_stats = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_COLUMN_STATISTICS')
-        col_stats.columns = ['CLUSTERID', 'COLUMNNAME', 'CARDINALITY', 'MODE', 'MINIMUM', 'MAXIMUM',
-       'MEAN', 'VARIANCE', 'VALIDFREQ', 'MISSINGFREQ', 'INVALIDFREQ',
-       'IMPORTANCE']
-        col_stats.columns = [x.upper() for x in col_stats.columns]
-
-        km_out_stat = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_CLUSTERS')
-        km_out_stat.columns = ['CLUSTERID', 'NAME', 'DESCRIPTION', 'SIZE', 'RELSIZE', 'WITHINSS']
-        km_out_stat.columns = [x.upper() for x in km_out_stat.columns]
-
-        k = model_main.iloc[0][3]
-        distance = model_main.iloc[0][2]
-        cont_cols = col_info.loc[(col_info['USAGETYPE'] == 'active') & (col_info['OPTYPE'] == 'continuous'), ['COLUMNNAME']]
-        cat_cols = col_info.loc[(col_info['USAGETYPE'] == 'active') & (col_info['OPTYPE'] == 'categorical'), ['COLUMNNAME']]
-
-        columns = []
-        for x in col_stats['COLUMNNAME'].values:
-            if x not in columns:
-                columns.append(x)
-
-        clusters = km_out_stat['CLUSTERID'].values
-        clusters.sort()
-
-        cluster_centers = []
-        for cluster in clusters:
-            tmp = [cluster]
-            for column in columns:
-                if column in cont_cols.values:
-                    tmp.append(col_stats.loc[(col_stats['CLUSTERID'] == cluster) & (col_stats['COLUMNNAME'] == column)]['MEAN'].values[0])
-                elif column in cat_cols.values:
-                    tmp.append(col_stats.loc[(col_stats['CLUSTERID'] == cluster) & (col_stats['COLUMNNAME'] == column)]['MODE'].values[0])
-                else:
-                    raise TypeError("Unexpected column category")
-            cluster_centers.append(tmp)
-
-        centers = pd.DataFrame([tuple(x) for x in cluster_centers])
-        centers.columns = ['CLUSTERID'] + columns
-
-        if verbose is True:
-            print("MODEL")
-            print(model_main)
-            print("COLUMNS")
-            print(col_info)
-            print("COLUMNS_STATISTICS")
-            print(col_stats)
-            print("CLUSTERS")
-            print(km_out_stat)
-
-        result = dict()
-        result['withinss'] = km_out_stat['WITHINSS'].values
-        result['size'] = km_out_stat['SIZE'].values
-        result['relsize'] = km_out_stat['RELSIZE'].values
-        result['distance'] = distance
-
-        result['k'] = k
-        result['centers'] = centers
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+In-Database K-means. Copies the interface of sklearn.cluster.KMeans
+"""
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+from builtins import int
+from builtins import dict
+from builtins import str
+from future import standard_library
+standard_library.install_aliases()
+
+from lazy import lazy
+import pandas as pd
+
+import nzpyida
+from nzpyida import IdaDataFrame
+from nzpyida.exceptions import IdaKMeansError
+import six
+
+class KMeans(object):
+    """
+    The K-means algorithm is the most widely used clustering algorithm that 
+    uses an explicit distance measure to partition the data set into clusters.
+
+    The K-means algorithm represents each cluster by the vector of the mean 
+    attribute values of all training instances - for numeric attributes - and 
+    by the vector of modal (most frequent) values - for nominal attributes - 
+    that are assigned to that cluster. This cluster representation is called 
+    cluster center.
+
+
+    The KMeans class provides an interface for using the KMEANS
+    and PREDICT_KMEANS IDAX/INZA methods.
+    """
+    def __init__(self, n_clusters=3, modelname=None, max_iter=5, distance="euclidean",
+                 random_state=12345, idbased=False, statistics=None):
+        """ 
+        Constructor for K-means clustering.
+
+        Parameters
+        ----------
+
+        n_cluster : int, optional, default: 3
+            The number of cluster centers.
+            Range : > 2
+
+        modelname : str, optional
+            The name of the clustering model that will be built.
+            It should contain only alphanumeric characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If it is not given, it will be generated automatically.
+            If the parameter corresponds to an existing model in the database,
+            it is replaced during the fitting step.
+
+
+        max_iter : int, > 1 and <= 1000, default = 5
+            The maximum number of iterations.
+
+        distance : str, default: "euclidean"
+             The distance function. The following values are allowed: “euclidean” and “norm_euclidean”.
+
+
+        random_state : int, default: 12345
+            The random seed of the generator.
+
+        idbased : bool, optional, default: False
+            Specifies that the random seed of the generator is based on the value of the ID column.
+
+        statistics : str, optional
+            Indicates the statistics that are collected.
+
+            The following values are allowed: ‘none’, ‘columns’, ‘values:n’, and ‘all’:
+                * If statistics='none' is specified, no statistics are collected.
+                * If statistics='columns' is specified, statistics on the columns
+                  of the input table are collected, for example, mean values.
+                * If statistics='values:n' is specified, and if n is a positive number, statistics
+                  on the columns and the column values are collected.
+
+                    Up to <n> column value statistics are collected.
+                        * If a nominal column contains more than <n> values,
+                          only the <n> most frequent column statistics are kept.
+                        * If a numeric column contains more than <n> values, the values
+                          are discretized, and the statistics are collected on the discretized values.
+                * statistics=all is identical to statistics=values:100.
+
+        Attributes
+        ----------
+        centers: table containing the coordinates of each cluster center;
+
+        cluster_centers_: coordinates of each cluster center, as list;
+
+        withinss: Within cluster sum of squares, by cluster, as a list;
+
+        size_clusters: number of items in each cluster, as list;
+
+        inertia_: float, total inertia of the system, defined as the sum of each cluster's within cluster sum of squares.
+
+        Returns
+        -------
+            The KMeans object, ready to be used for fitting and prediction
+
+        Examples
+        --------
+        >>> idadb = IdaDataBase("BLUDB")
+        >>> idadf = IdaDataFrame(idadb, "IRIS", indexer = "ID")
+        >>> kmeans = KMeans(3) # clustering with 3 clusters
+        >>> kmeans.fit(idadf)
+        >>> kmeans.predict(idadf)
+
+        Notes
+        -----
+        Inner parameters of the model can be printed and modified by using 
+        get_params and set_params. But we recommend creating a new KMeans model 
+        instead of modifying it.
+
+        """
+        self.modelname = modelname
+        self.n_clusters = n_clusters
+        self.max_iter = max_iter
+        self.distance = distance
+        self.random_state = random_state
+        self.idbased = idbased
+        self.statistics = statistics
+
+        # Get set at fit step
+        self._idadb = None
+        self._idadf = None
+        self._column_id = None
+        self.incolumn = None
+        self.coldeftype = None
+        self.coldefrole = None
+        self.colPropertiesTable = None
+
+        # Get set at predict step
+        self.outtable = None
+
+    @lazy
+    def labels_(self):
+        """
+        Return the corresponding labels for each ID.
+        """
+        try:
+            return self.predict(self._idadf, self._column_id)
+        except:
+            raise AttributeError(str(self.__class__) + " object has no attribute 'labels_'")
+
+    def get_params(self):
+        """
+        Return the parameters of the K-means clustering.
+        """
+        params = dict()
+        params['modelname'] = self.modelname
+        params['n_clusters'] = self.n_clusters
+        params['max_iter'] = self.max_iter
+        params['distance'] = self.distance
+        params['random_state'] = self.random_state
+        params['idbased'] = self.idbased
+        params['statistics'] = self.statistics
+
+        params['incolumn'] = self.incolumn
+        params['coldeftype'] = self.coldeftype
+        params['coldefrole'] = self.coldefrole
+        params['colPropertiesTable'] = self.colPropertiesTable
+
+        params['outtable'] = self.outtable
+        return params
+
+    def set_params(self, **params):
+        """
+        Change the parameters of the K-means clustering.
+        """
+        if not params:
+            # Simple optimisation to gain speed (inspect is slow)
+            return self
+        valid_params = self.get_params()
+        for key, value in six.iteritems(params):
+            if key not in valid_params:
+                raise ValueError('Invalid parameter %s for estimator %s' %
+                                     (key, self.__class__.__name__))
+            setattr(self, key, value)
+        return self
+
+    def fit(self, idadf, column_id="ID", incolumn=None, coldeftype=None,
+            coldefrole=None, colPropertiesTable=None, verbose=False):
+        """
+        Use the KMEANS stored procedure to build a K-means clustering model 
+        that clusters the input data into k centers.
+
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+            The name of the input IdaDataFrame.
+
+        column_id : str, default: "ID"
+            The column of the input IdaDataFrame that identifies a unique
+            instance ID.
+
+        incolumn : dict, optional
+            The columns of the input table that have specific properties, which
+            are separated by a semi-colon (;).
+            Each column is succeeded by one or more of the following properties:
+
+                * By type nominal (':nom') or by type continuous (':cont'). By default,
+                  numerical types are continuous, and all other types nominal.
+                * By role ':id', ':target', ':input', or ':ignore'.
+
+            If this parameter is not specified, all columns of the input table
+            have default properties.
+
+        coldeftype : dict, optional
+            The default type of the input table columns. The following values 
+            are allowed: ‘nom’ and ‘cont’. If the parameter is not specified, 
+            numeric columns are continuous and all other columns are nominal.
+
+
+        coldefrole : dict, optional
+            The default role of the input table columns. The following values 
+            are allowed: ‘input’ and ‘ignore’. If the parameter is not 
+            specified, all columns are input columns.
+
+        colPropertiesTable : idaDataFrame, optional
+            The input IdaDataFrame or name of the table where the properties of the
+            columns of the input IdaDataFrame (idadf) are stored.
+            The table name should contain only alphanumeric characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If this parameter is not specified, the column properties of the input table column
+            properties are detected automatically.
+
+        verbose : bool, default: False
+            Verbosity mode.
+
+        """
+        if not type(idadf).__name__ == 'IdaDataFrame':
+            raise TypeError("Argument should be an IdaDataFrame")
+
+        idadf._idadb._check_procedure("KMEANS", "KMeans")
+
+        # Check the ID
+        if column_id not in idadf.columns:
+            raise ValueError("No id columns is available in IdaDataFrame:" + column_id +
+                             ". Either create a new ID column using add_column_id function" +
+                             " or give the name of a column that can be used as ID")
+
+        self._idadb = idadf._idadb
+        self._idadf = idadf
+        self._column_id = column_id
+
+        # Check or create a model name
+        if self.modelname is None:
+            self.modelname = idadf._idadb._get_valid_modelname('KMEANS_')
+        else:
+            self.modelname = nzpyida.utils.check_modelname(self.modelname)
+            if idadf._idadb.exists_model(self.modelname):
+                idadf._idadb.drop_model(self.modelname)
+
+        self.incolumn = incolumn
+        self.coldeftype = coldeftype
+        self.coldefrole = coldefrole
+        if not (colPropertiesTable is None or isinstance(colPropertiesTable, nzpyida.frame.IdaDataFrame)):
+            colPropertiesTable = nzpyida.utils.check_tablename(colPropertiesTable)
+        self.colPropertiesTable = colPropertiesTable
+
+        # Create a temporay view
+        tmp_view_name = idadf.internal_state._create_view()
+        # tmp_view_name = idadf.internal_state.current_state # deprecated, hange to idadf.name
+        
+        #if "." in tmp_view_name:
+            #tmp_view_name = tmp_view_name.split('.')[-1]
+        if (self.outtable == None) & self._idadb._is_netezza_system():
+            self.outtable = self._idadb._get_valid_tablename(prefix="OUTTABLE_KMEANS_")
+        try:
+            # TODO: outtable is optional but this does not match with the doc
+            # Defect to declare
+            idadf._idadb._call_stored_procedure("KMEANS ",
+                                                model = self.modelname,
+                                                intable = tmp_view_name,
+                                                k = self.n_clusters,
+                                                maxiter = self.max_iter,
+                                                outtable = self.outtable,
+                                                distance = self.distance,
+                                                id = self._column_id,
+                                                randseed = self.random_state,
+                                                statistics = self.statistics,
+                                                idbased = self.idbased,
+                                                incolumn = self.incolumn,
+                                                coldeftype = self.coldeftype,
+                                                coldefrole = self.coldefrole,
+                                                colPropertiesTable = self.colPropertiesTable)
+
+        except:
+            raise
+        finally:
+            idadf.internal_state._delete_view()
+            idadf.commit()
+
+        result = self._retrieve_KMeans_Model(self.modelname, verbose)
+        self.centers = result['centers']
+        self.cluster_centers_ = result['centers'].values
+        self.withinss = result['withinss']
+        self.size_clusters = [int(str(x)) for x in result['size']]
+        self.inertia_ = sum(self.withinss)
+
+        if verbose is True:
+            self.describe()
+
+        return
+
+    def predict(self, idadf, column_id=None, outtable=None):
+        """
+        Apply the K-means clustering model to new data.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+            IdaDataFrame to be used as input.
+
+        column_id : str
+            The column of the input table that identifies a unique instance ID.
+            Default: the same id column that is specified in the stored procedure to build the model.
+
+        outtable : str
+            The name of the output table where the assigned clusters are stored.
+            It should contain only alphanumeric characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If this parameter is not specified, it is generated automatically.
+            If the parameter corresponds to an existing table in the database,
+            it is replaced.
+
+        Returns
+        -------
+        IdaDataFrame
+            IdaDataFrame containing the closest cluster for each data point referenced by its ID.
+        """
+        if not type(idadf).__name__ == 'IdaDataFrame':
+            raise TypeError("Argument should be an IdaDataFrame")
+
+        # Check the ID
+        if column_id is None:
+            column_id = self._column_id
+        if column_id not in idadf.columns:
+            raise ValueError("No id columns is available in IdaDataFrame:" + column_id +
+                             ". Either create a new ID column using add_column_id function" +
+                             " or give the name of a column that can be used as ID")
+
+        if self._idadb is None:
+            raise IdaKMeansError("No KMeans model was trained before")
+
+
+        if outtable is None:
+            outtable = idadf._idadb._get_valid_modelname('PREDICT_KMEANS_')
+        else:
+            if self.outtable:
+                outtable = self.outtable
+            outtable = nzpyida.utils.check_tablename(outtable)
+            if idadf._idadb.exists_table(outtable):
+                idadf._idadb.drop_table(outtable)
+
+        self.outtable = outtable
+        # Create a temporay view
+        tmp_view_name = idadf.internal_state._create_view()
+        # tmp_view_name = idadf.internal_state.current_state
+        
+        #if "." in tmp_view_name:
+            #tmp_view_name = tmp_view_name.split('.')[-1]
+            
+        try:
+            idadf._idadb._call_stored_procedure("PREDICT_KMEANS ",
+                                                 model = self.modelname,
+                                                 intable = tmp_view_name,
+                                                 id = column_id,
+                                                 outtable = self.outtable
+                                                 )
+        except:
+            raise
+        finally:
+            idadf.internal_state._delete_view()
+            idadf._idadb.commit()
+
+        self.labels_ = nzpyida.IdaDataFrame(idadf._idadb, outtable, indexer=column_id)
+        return self.labels_
+
+    def fit_predict(self, idadf, column_id="ID", incolumn=None, coldeftype=None,
+                    coldefrole=None, colPropertiesTable=None, outtable=None,
+                    verbose=False):
+        """
+        Convenience function for fitting the model and using it to make 
+        predictions on the same dataset. See the fit and predict documentation 
+        for an explanation about their attributes.
+        """
+        self.fit(idadf, column_id, incolumn, coldeftype, coldefrole,
+                 colPropertiesTable, verbose)
+        return self.predict(idadf, column_id, outtable)
+
+    def describe(self):
+        """
+        Return a description of the K-means clustering, if a prediction was 
+        made. Otherwise,  this function returns the parameters of the model.
+        """
+        if self._idadb is None:
+            return self.get_params
+        else:
+            print("KMeans clustering with " + str(self.n_clusters) +
+            " clusters of sizes " + ', '.join([str(x) for x in self.size_clusters]))
+            print()
+            print("Cluster means: ")
+            print(self.centers)
+            print()
+            print("Within cluster sum of squares by cluster:")
+            print(self.withinss)
+            try:
+                self._idadb._call_stored_procedure("PRINT_MODEL ", model = self.modelname)
+            except:
+                raise
+            return
+
+    def _retrieve_KMeans_Model(self, modelname, verbose=False):
+        """
+        Retrieve information about the model to print the results. The KMEANS 
+        IDAX/INZA function stores its result in 4 tables:
+
+            * <MODELNAME>_MODEL
+            * <MODELNAME>_COLUMNS
+            * <MODELNAME>_COLUMN_STATISTICS
+            * <MODELNAME>_CLUSTERS
+
+        Parameters
+        ----------
+        modelname : str
+            The name of the model that is retrieved.
+
+        verbose : bol, default: False
+            Verbosity mode.
+        """
+        modelname = nzpyida.utils.check_modelname(modelname)
+
+        if self._idadb is None:
+            raise IdaKMeansError("No KMeans model was trained before")
+
+        if self._idadb._is_netezza_system():
+            modeltable_prefix = "INZA.NZA_META_"
+        else:
+            modeltable_prefix = ""
+        model_main = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_MODEL')
+        # Woraround for specific version of ODBC
+        model_main.columns = ['MODELCLASS', 'COMPARISONTYPE', 'COMPARISONMEASURE', 'NUMCLUSTERS']
+        model_main.columns = [x.upper() for x in model_main.columns]
+
+
+        col_info = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_COLUMNS',)
+        col_info.columns = ['COLUMNNAME', 'DATATYPE', 'OPTYPE', 'USAGETYPE', 'COLUMNWEIGHT',
+       'AUTOTRANSFORM', 'TRANSFORMEDCOLUMN', 'COMPAREFUNCTION', 'IMPORTANCE',
+       'OUTLIERTREATMENT', 'LOWERLIMIT', 'UPPERLIMIT', 'CLOSURE',
+       'STATISTICSTYPE']
+        col_info.columns = [x.upper() for x in col_info.columns]
+
+        col_stats = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_COLUMN_STATISTICS')
+        col_stats.columns = ['CLUSTERID', 'COLUMNNAME', 'CARDINALITY', 'MODE', 'MINIMUM', 'MAXIMUM',
+       'MEAN', 'VARIANCE', 'VALIDFREQ', 'MISSINGFREQ', 'INVALIDFREQ',
+       'IMPORTANCE']
+        col_stats.columns = [x.upper() for x in col_stats.columns]
+
+        km_out_stat = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_CLUSTERS')
+        km_out_stat.columns = ['CLUSTERID', 'NAME', 'DESCRIPTION', 'SIZE', 'RELSIZE', 'WITHINSS']
+        km_out_stat.columns = [x.upper() for x in km_out_stat.columns]
+
+        k = model_main.iloc[0][3]
+        distance = model_main.iloc[0][2]
+        cont_cols = col_info.loc[(col_info['USAGETYPE'] == 'active') & (col_info['OPTYPE'] == 'continuous'), ['COLUMNNAME']]
+        cat_cols = col_info.loc[(col_info['USAGETYPE'] == 'active') & (col_info['OPTYPE'] == 'categorical'), ['COLUMNNAME']]
+
+        columns = []
+        for x in col_stats['COLUMNNAME'].values:
+            if x not in columns:
+                columns.append(x)
+
+        clusters = km_out_stat['CLUSTERID'].values
+        clusters.sort()
+
+        cluster_centers = []
+        for cluster in clusters:
+            tmp = [cluster]
+            for column in columns:
+                if column in cont_cols.values:
+                    tmp.append(col_stats.loc[(col_stats['CLUSTERID'] == cluster) & (col_stats['COLUMNNAME'] == column)]['MEAN'].values[0])
+                elif column in cat_cols.values:
+                    tmp.append(col_stats.loc[(col_stats['CLUSTERID'] == cluster) & (col_stats['COLUMNNAME'] == column)]['MODE'].values[0])
+                else:
+                    raise TypeError("Unexpected column category")
+            cluster_centers.append(tmp)
+
+        centers = pd.DataFrame([tuple(x) for x in cluster_centers])
+        centers.columns = ['CLUSTERID'] + columns
+
+        if verbose is True:
+            print("MODEL")
+            print(model_main)
+            print("COLUMNS")
+            print(col_info)
+            print("COLUMNS_STATISTICS")
+            print(col_stats)
+            print("CLUSTERS")
+            print(km_out_stat)
+
+        result = dict()
+        result['withinss'] = km_out_stat['WITHINSS'].values
+        result['size'] = km_out_stat['SIZE'].values
+        result['relsize'] = km_out_stat['RELSIZE'].values
+        result['distance'] = distance
+
+        result['k'] = k
+        result['centers'] = centers
+
         return result
```

### Comparing `nzpyida-0.2.2.6/nzpyida/learn/naive_bayes.py` & `nzpyida-0.3.3/nzpyida/learn/naive_bayes.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,528 +1,528 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-In-Database Naive Bayes modelization and prediction.
-Copies the interface of sklearn.naive_bayes
-"""
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-from builtins import dict
-from builtins import str
-from future import standard_library
-standard_library.install_aliases()
-
-from lazy import lazy
-import nzpyida
-from nzpyida.exceptions import IdaNaiveBayesError
-import six
-
-class NaiveBayes(object):
-    """
-    The Naive Bayes classification algorithm is a probabilistic classifier.
-    It is based on probability models that incorporate strong independence
-    assumptions. Often, the independence assumptions do not have an impact
-    on reality. Therefore, they are considered naive.
-
-    The NaiveBayes class provides an interface for using the NAIVEBAYES
-    and PREDICT_NAIVEBAYES IDAX/INZA methods.
-    """
-
-    def __init__(self, modelname = None, disc = None, bins = None):
-        """
-        Constructor for NaiveBayes model objects
-
-        Parameters
-        ----------
-        modelname : str, optional
-            The name of the Naive Bayes model that will be built.
-            Should contain only alphanumerical characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If no name is specified, it will be generated automatically.
-            If the parameter corresponds to an existing model in the database,
-            it is replaced during the fitting step.
-
-        disc : str, optional, default: ew
-            Determine the automatic discretization of all continuous attributes. 
-            The following values are allowed: ef, em, ew, and ewn.
-
-                * disc=ef
-                    Equal-frequency discretization.
-                    An unsupervised discretization algorithm that uses the equal
-                    frequency criterion for interval bound setting.
-                * disc=em
-                    Minimal entropy discretization.
-                    An unsupervised discretization algorithm that uses the minimal
-                    entropy criterion for interval bound setting.
-                * disc=ew (default)
-                    Equal-width discretization.
-                    An unsupervised discretization algorithm that uses the equal
-                    width criterion for interval bound setting.
-                * disc=ewn
-                    Equal-width discretization with nice bucket limits.
-                    An unsupervised discretization algorithm that uses the equal
-                    width criterion for interval bound setting.
-
-        bins : int, optional, default : 10
-            Number of bins for numeric columns.
-            
-        Attributes
-        ----------
-        
-        _idadf: Get set at fit step, IdaDataFrame used as input to fit the model;
-        
-        _idadb: Get set at fit step, IdaDataBase object holding the connection to Db2;
-        
-        _column_id: Get set at fit step, name of the column of the input table which identifies the transaction or class ID.;
-        
-        target: Get set at fit step; str
-            The column of the input table that represents the class
-        
-        incolumn: Get set at fit step; str, optional
-            The columns of the input table that have specific properties,
-            which are separated by a semi-colon (;). Each column is succeeded
-            by one or more of the following properties:
-
-                * By type nominal (':nom') or by type continuous (':cont'). By default,
-                  numerical types are continuous, and all other types are nominal.
-                * By role ':id', ':target', ':input', or ':ignore'.
-
-        coldeftype: Get set at fit step; str, optional
-            The default type of the input table columns.
-            The following values are allowed: 'nom' and 'cont'.
-            If the parameter is not specified, numeric columns are continuous,
-            and all other columns are nominal.
-        
-        coldefrole: Get set at fit step; str, optional
-            The default role of the input table columns.
-            The following values are allowed: 'input' and 'ignore'.
-            If the parameter is not specified, all columns are input columns.
-        
-        colpropertiestable: Get set at fit step; str, optional
-            The name of the input table where the properties of the columns of the input table
-            are stored. If this parameter is not specified, the column properties of the input table
-            column properties are detected automatically.
-        
-        outable: Get set at predict step; str, optional
-            The name of the output table where the predictions are stored. If
-            this parameter is not specified, it is generated automatically. If
-            the parameter corresponds to an existing table in the database, it
-            will be replaced.
-        
-        outtableProb: Get set at predict step; str, optional
-            The output table where the probabilities for each of the classes are stored.
-            If this parameter is not specified, the table is not created. If
-            the parameter corresponds to an existing table in the database, it
-            will be replaced.
-        
-        mestimation: Get set at predict step; flag, default: False
-            A flag that indicates the use of m-estimation for probabilities.
-            This kind of estimation might be slower than other ones, but it
-            might produce better results for small or unbalanced data sets.
-        
-        modelname: see parameters;
-        
-        disc: see parameters;
-        
-        bins: see parameters;
-        
-
-        Returns
-        -------
-            The NaiveBayes object, ready to be used for fitting and prediction.
-
-        Examples
-        --------
-        >>> idadb = IdaDataBase("BLUDB-TEST")
-        >>> idadf = IdaDataFrame(idadb, "IRIS")
-        >>> bayes = NaiveBayes("NAIVEBAYES_TEST")
-        >>> bayes.fit(idadf, column_id="ID", target="species")
-        >>> bayes.predict(idadf, outtable="IRIS_PREDICTION", outtableProb="IRIS_PREDICTIONPROB")
-
-        Notes
-        -----
-            Inner parameters of the model can be printed and modified by using 
-            get_params and set_params. But we recommend creating a new 
-            NaiveBayes model instead of modifying an existing model.
-        """
-        # Get set at fit step
-        self._idadf = None
-        self._idadb = None
-        self._column_id = None
-        self.target = None
-        self.incolumn = None
-        self.coldeftype = None
-        self.coldefrole = None
-        self.colpropertiestable = None
-
-        # Get set at predict step
-        self.outtable = None
-        self.outtableProb = None
-        self.mestimation = None
-
-        self.modelname = nzpyida.utils.check_modelname(modelname)
-        self.disc = disc
-        self.bins = bins
-
-
-    @lazy
-    def labels_(self):
-        """
-        Return the labels of the classification if available
-        """
-        try:
-            return self.predict(self._idadf, self._column_id)
-        except:
-            raise AttributeError(str(self.__class__) + " object has no attribute 'labels_'")
-
-    def get_params(self):
-        """
-        Return the parameters of the Naive Byes model.
-        """
-        print(dir(self))
-        params = dict()
-        params['modelname'] = self.modelname
-        params['disc'] = self.disc
-        params['bins'] = self.bins
-
-        params['target'] = self.target
-        params['incolumn'] = self.incolumn
-        params['coldeftype'] = self.coldeftype
-        params['coldefrole'] = self.coldefrole
-        params['colpropertiestable'] = self.colpropertiestable
-
-        params['outtable'] = self.outtable
-        params['outtableProb'] = self.outtableProb
-        params['mestimation'] = self.mestimation
-
-        return params
-
-    def set_params(self, **params):
-        """
-        Modify the parameters of the Naive Bayes model.
-        """
-        if not params:
-            # Simple optimisation to gain speed (inspect is slow)
-            return self
-        valid_params = self.get_params()
-        for key, value in six.iteritems(params):
-            if key not in valid_params:
-                raise ValueError('Invalid parameter %s for estimator %s' %
-                                     (key, self.__class__.__name__))
-            setattr(self, key, value)
-        return self
-
-    def fit(self, idadf, target, column_id="ID", incolumn=None, coldeftype=None,
-            coldefrole=None, colpropertiestable=None, verbose=False):
-        """
-        Create a Naive Bayes model from an IdaDataFrame.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-            The IdaDataFrame to be used as input.
-
-        target : str
-            The column of the input table that represents the class
-
-        column_id : str, default: "ID
-            The column of the input table that identifies the transaction ID.
-
-        incolumn : str, optional
-            The columns of the input table that have specific properties,
-            which are separated by a semi-colon (;). Each column is succeeded
-            by one or more of the following properties:
-
-                * By type nominal (':nom') or by type continuous (':cont'). By default,
-                  numerical types are continuous, and all other types are nominal.
-                * By role ':id', ':target', ':input', or ':ignore'.
-
-            If this parameter is not specified, all columns of the input table have default properties.
-
-        coldeftype : str, optional
-            The default type of the input table columns.
-            The following values are allowed: 'nom' and 'cont'.
-            If the parameter is not specified, numeric columns are continuous,
-            and all other columns are nominal.
-
-        coldefrole : str, optional
-            The default role of the input table columns.
-            The following values are allowed: 'input' and 'ignore'.
-            If the parameter is not specified, all columns are input columns.
-
-        colpropertiestable : str, optional
-            The input IdaDataFrame or the name of the table where the properties of the
-            columns of the input IdaDataFrame (idadf) are stored.
-            The table name should contain only alphanumerical characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If this parameter is not specified, the column properties of the input table
-            column properties are detected automatically.
-
-        verbose : bool, default: False
-            Verbosity mode.
-        """
-
-        # Some basic checks
-        if not isinstance(idadf, nzpyida.IdaDataFrame):
-            raise TypeError("Argument should be an IdaDataFrame")
-        if target not in idadf.columns:
-            raise ValueError("Target is not a column in " + idadf.name)
-
-        idadf._idadb._check_procedure("NAIVEBAYES", "Naive Bayes")
-
-        # Check the ID
-        if column_id not in idadf.columns:
-            raise ValueError("No id columns is available in IdaDataFrame:" + column_id +
-                             ". Either create a new ID column using add_column_id function" +
-                             " or give the name of a column that can be used as ID")
-
-        self._idadb = idadf._idadb
-        self._idadf = idadf
-        self._column_id = column_id
-
-        self.target = target
-        self.incolumn = incolumn
-        self.coldeftype = coldeftype
-        self.coldefrole = coldefrole
-        if not (colpropertiestable is None or isinstance(colpropertiestable, nzpyida.frame.IdaDataFrame)):
-            colpropertiestable = nzpyida.utils.check_tablename(colpropertiestable)
-        self.colpropertiestable = colpropertiestable
-
-        # Check or create a model name, drop it if it already exists.
-        if self.modelname is None:
-            self.modelname = idadf._idadb._get_valid_modelname('NAIVEBAYES_')
-        else:
-            self.modelname = nzpyida.utils.check_modelname(self.modelname)
-            if idadf._idadb.exists_model(self.modelname):
-                idadf._idadb.drop_model(self.modelname)
-
-        # Create a temporay view
-        # TODO: Why do we need actually to create a view ?
-        tmp_view_name = idadf.internal_state._create_view()
-        # tmp_view_name = idadf.internal_state.current_state
-        
-        #if "." in tmp_view_name:
-            #tmp_view_name = tmp_view_name.split('.')[-1]
-
-        try:
-            idadf._idadb._call_stored_procedure("NAIVEBAYES ",
-                                                 model = self.modelname,
-                                                 intable = tmp_view_name,
-                                                 id = self._column_id,
-                                                 target = self.target,
-                                                 incolumn = self.incolumn,
-                                                 coldeftype = self.coldeftype,
-                                                 coldefrole = self.coldefrole,
-                                                 colPropertiesTable = self.colpropertiestable,
-                                                 disc = self.disc,
-                                                 bins = self.bins)
-        except:
-            raise
-        finally:
-            idadf.internal_state._delete_view()
-            idadf.commit()
-
-        self._retrieve_NaiveBayes_Model(self.modelname, verbose)
-
-        if verbose is True:
-            self.describe()
-
-        return
-
-    def predict(self, idadf, column_id=None, outtable=None, outtableProb=None,
-                mestimation=False):
-        """
-        Use the Naive Bayes predict stored procedure to apply a Naive Bayes model
-        to generate classification predictions for a data set.
-
-        Parameters
-        ----------
-        idadf : IdaDataFrame
-             IdaDataFrame to be used as input.
-
-        column_id : str, optional
-            The column of the input table that identifies a unique instance ID.
-            By default, the same id column that is specified in the stored
-            procedure to build the model.
-
-        outtable : str, optional
-            The name of the output table where the predictions are stored.
-            It should contain only alphanumerical characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If this parameter is not specified, it is generated automatically. If
-            the parameter corresponds to an existing table in the database, it
-            will be replaced.
-
-        outtableProb : str, optional
-            The name of the output table where the probabilities for each of the classes are stored.
-            It should contain only alphanumerical characters and underscores.
-            All lower case characters will be converted to upper case characters.
-            If this parameter is not specified, the table is not created.
-            If the parameter corresponds to an existing table in the database, it
-            will be replaced.
-
-        mestimation : flag, default: False
-            A flag that indicates the use of m-estimation for probabilities.
-            This kind of estimation might be slower than other ones, but it
-            might produce better results for small or unbalanced data sets.
-
-        Returns
-        -------
-        IdaDataFrame
-            IdaDataFrame containing the classification decision for each
-            datapoints referenced by their ID.
-        """
-        if not isinstance(idadf, nzpyida.IdaDataFrame):
-            raise TypeError("Argument should be an IdaDataFrame")
-
-        idadf._idadb._check_procedure("PREDICT_NAIVEBAYES", "Prediction for Naive Bayes")
-
-        # Check the ID
-        if column_id is None :
-            column_id = self._column_id
-        if column_id not in idadf.columns:
-            raise ValueError("No id columns is available in IdaDataFrame:" + column_id +
-                             ". Either create a new ID column using add_column_id function" +
-                             " or give the name of a column that can be used as ID")
-
-        if self._idadb is None:
-            raise IdaNaiveBayesError("The Naive Bayes model was not trained before.")
-
-        # Check or create an outtable name, drop it if it already exists.
-        if outtable is None:
-            outtable = idadf._idadb._get_valid_tablename('PREDICT_NAIVEBAYES_')
-        else:
-            outtable = nzpyida.utils.check_tablename(outtable)
-            if idadf._idadb.exists_table(outtable):
-                idadf._idadb.drop_table(outtable)
-
-        if outtableProb is not None:
-            outtableProb = nzpyida.utils.check_tablename(outtableProb)
-            if idadf._idadb.exists_table(outtableProb):
-                idadf._idadb.drop_table(outtableProb)
-
-        self.outtable = outtable
-        self.outtableProb = outtableProb
-        self.mestimation = mestimation
-
-        # Create a temporay view
-        tmp_view_name = idadf.internal_state._create_view()
-        # tmp_view_name = idadf.internal_state.current_state
-        
-        #if "." in tmp_view_name:
-            #tmp_view_name = tmp_view_name.split('.')[-1]
-
-        try:
-            idadf._idadb._call_stored_procedure("PREDICT_NAIVEBAYES ",
-                                                 model = self.modelname,
-                                                 intable = tmp_view_name,
-                                                 id = column_id,
-                                                 outtable = self.outtable,
-                                                 outtableProb = self.outtableProb,
-                                                 mestimation = self.mestimation
-                                                 )
-        except:
-            raise
-        finally:
-            idadf.internal_state._delete_view()
-            idadf._idadb._autocommit()
-
-        self.labels_ = nzpyida.IdaDataFrame(idadf._idadb, self.outtable)
-        return self.labels_
-
-    def fit_predict(self, idadf, column_id="ID", incolumn=None, coldeftype=None,
-                    coldefrole=None, colprepertiesTable=None, outtable=None,
-                    outtableProb=None, mestimation=False, verbose=False):
-        """
-        Convenience function for fitting the model and using it to make 
-        predictions about the same dataset. See to fit and predict 
-        documentation for an explanation about their attributes.
-        """
-        self.fit(idadf, column_id, incolumn, coldeftype, coldefrole, colprepertiesTable, verbose)
-        return self.predict(idadf, column_id, outtable, outtableProb, mestimation)
-
-    def describe(self, detail=True):
-        """
-        Parameters
-        ----------
-        detail: bool, optional. True by default.
-        
-        Returns
-        -------
-        Return a description of Naives Bayes.
-        If False, only the a priori probabilities of each class are returned, 
-        displayed in table format. If True, then intermediary tables are shown.
-        """
-        if self._idadb is None:
-            return self.get_params
-        else:
-            try:
-                if detail:
-                    self._retrieve_NaiveBayes_Model(self.modelname, verbose=True)
-                # PRINT_MODEL not supported for Naive Bayes on Netezza‚
-                if not self._idadb._is_netezza_system():
-                     # res = self._idadb.ida_query("CALL IDAX.PRINT_MODEL('model = " + self.modelname +"')")
-                    self._idadb._call_stored_procedure("PRINT_MODEL ", model = self.modelname)
-            except:
-                raise
-            return
-
-
-    def _retrieve_NaiveBayes_Model(self, modelname, verbose=False):
-        """
-        Retrieve information about the model to print the results. The Naive 
-        Bayes IDAX/INZA function stores its result in 2 tables:
-
-            * <MODELNAME>_MODEL
-            * <MODELNAME>_DISCRANGES
-
-        Parameters
-        ----------
-        modelname : str
-            The name of the model that is retrieved.
-
-        verbose : bol, default: False
-            Verbosity mode.
-
-        Notes
-        -----
-        Needs better formatting instead of printing the tables.
-        """
-        modelname = nzpyida.utils.check_modelname(modelname)
-
-        if self._idadb is None:
-            raise IdaNaiveBayesError("The Naive Bayes model was not trained before.")
-
-        if self._idadb._is_netezza_system():
-            modeltable_prefix = "INZA.NZA_META_"
-            disctable_postfix =  "_DISC"
-        else:
-            modeltable_prefix = ""
-            disctable_postfix =  "_DISCRANGES"
-
-        model_main = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_MODEL')
-        model_main.columns = ['ATTRIBUTE', 'VAL', 'CLASS', 'CLASSVALCOUNT', 'ATTRCLASSCOUNT',
-       'CLASSCOUNT', 'TOTALCOUNT']
-        model_main.columns = [x.upper() for x in model_main.columns]
-
-        disc = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + disctable_postfix)
-        disc.columns = ['COLNAME', 'BREAK']
-        disc.columns = [x.upper() for x in disc.columns]
-
-        if verbose is True:
-            print("MODEL")
-            print(model_main)
-            print("DISCRANGES")
-            print(disc)
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+In-Database Naive Bayes modelization and prediction.
+Copies the interface of sklearn.naive_bayes
+"""
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+from builtins import dict
+from builtins import str
+from future import standard_library
+standard_library.install_aliases()
+
+from lazy import lazy
+import nzpyida
+from nzpyida.exceptions import IdaNaiveBayesError
+import six
+
+class NaiveBayes(object):
+    """
+    The Naive Bayes classification algorithm is a probabilistic classifier.
+    It is based on probability models that incorporate strong independence
+    assumptions. Often, the independence assumptions do not have an impact
+    on reality. Therefore, they are considered naive.
+
+    The NaiveBayes class provides an interface for using the NAIVEBAYES
+    and PREDICT_NAIVEBAYES IDAX/INZA methods.
+    """
+
+    def __init__(self, modelname = None, disc = None, bins = None):
+        """
+        Constructor for NaiveBayes model objects
+
+        Parameters
+        ----------
+        modelname : str, optional
+            The name of the Naive Bayes model that will be built.
+            Should contain only alphanumerical characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If no name is specified, it will be generated automatically.
+            If the parameter corresponds to an existing model in the database,
+            it is replaced during the fitting step.
+
+        disc : str, optional, default: ew
+            Determine the automatic discretization of all continuous attributes. 
+            The following values are allowed: ef, em, ew, and ewn.
+
+                * disc=ef
+                    Equal-frequency discretization.
+                    An unsupervised discretization algorithm that uses the equal
+                    frequency criterion for interval bound setting.
+                * disc=em
+                    Minimal entropy discretization.
+                    An unsupervised discretization algorithm that uses the minimal
+                    entropy criterion for interval bound setting.
+                * disc=ew (default)
+                    Equal-width discretization.
+                    An unsupervised discretization algorithm that uses the equal
+                    width criterion for interval bound setting.
+                * disc=ewn
+                    Equal-width discretization with nice bucket limits.
+                    An unsupervised discretization algorithm that uses the equal
+                    width criterion for interval bound setting.
+
+        bins : int, optional, default : 10
+            Number of bins for numeric columns.
+            
+        Attributes
+        ----------
+        
+        _idadf: Get set at fit step, IdaDataFrame used as input to fit the model;
+        
+        _idadb: Get set at fit step, IdaDataBase object holding the connection to Db2;
+        
+        _column_id: Get set at fit step, name of the column of the input table which identifies the transaction or class ID.;
+        
+        target: Get set at fit step; str
+            The column of the input table that represents the class
+        
+        incolumn: Get set at fit step; str, optional
+            The columns of the input table that have specific properties,
+            which are separated by a semi-colon (;). Each column is succeeded
+            by one or more of the following properties:
+
+                * By type nominal (':nom') or by type continuous (':cont'). By default,
+                  numerical types are continuous, and all other types are nominal.
+                * By role ':id', ':target', ':input', or ':ignore'.
+
+        coldeftype: Get set at fit step; str, optional
+            The default type of the input table columns.
+            The following values are allowed: 'nom' and 'cont'.
+            If the parameter is not specified, numeric columns are continuous,
+            and all other columns are nominal.
+        
+        coldefrole: Get set at fit step; str, optional
+            The default role of the input table columns.
+            The following values are allowed: 'input' and 'ignore'.
+            If the parameter is not specified, all columns are input columns.
+        
+        colpropertiestable: Get set at fit step; str, optional
+            The name of the input table where the properties of the columns of the input table
+            are stored. If this parameter is not specified, the column properties of the input table
+            column properties are detected automatically.
+        
+        outable: Get set at predict step; str, optional
+            The name of the output table where the predictions are stored. If
+            this parameter is not specified, it is generated automatically. If
+            the parameter corresponds to an existing table in the database, it
+            will be replaced.
+        
+        outtableProb: Get set at predict step; str, optional
+            The output table where the probabilities for each of the classes are stored.
+            If this parameter is not specified, the table is not created. If
+            the parameter corresponds to an existing table in the database, it
+            will be replaced.
+        
+        mestimation: Get set at predict step; flag, default: False
+            A flag that indicates the use of m-estimation for probabilities.
+            This kind of estimation might be slower than other ones, but it
+            might produce better results for small or unbalanced data sets.
+        
+        modelname: see parameters;
+        
+        disc: see parameters;
+        
+        bins: see parameters;
+        
+
+        Returns
+        -------
+            The NaiveBayes object, ready to be used for fitting and prediction.
+
+        Examples
+        --------
+        >>> idadb = IdaDataBase("BLUDB-TEST")
+        >>> idadf = IdaDataFrame(idadb, "IRIS")
+        >>> bayes = NaiveBayes("NAIVEBAYES_TEST")
+        >>> bayes.fit(idadf, column_id="ID", target="species")
+        >>> bayes.predict(idadf, outtable="IRIS_PREDICTION", outtableProb="IRIS_PREDICTIONPROB")
+
+        Notes
+        -----
+            Inner parameters of the model can be printed and modified by using 
+            get_params and set_params. But we recommend creating a new 
+            NaiveBayes model instead of modifying an existing model.
+        """
+        # Get set at fit step
+        self._idadf = None
+        self._idadb = None
+        self._column_id = None
+        self.target = None
+        self.incolumn = None
+        self.coldeftype = None
+        self.coldefrole = None
+        self.colpropertiestable = None
+
+        # Get set at predict step
+        self.outtable = None
+        self.outtableProb = None
+        self.mestimation = None
+
+        self.modelname = nzpyida.utils.check_modelname(modelname)
+        self.disc = disc
+        self.bins = bins
+
+
+    @lazy
+    def labels_(self):
+        """
+        Return the labels of the classification if available
+        """
+        try:
+            return self.predict(self._idadf, self._column_id)
+        except:
+            raise AttributeError(str(self.__class__) + " object has no attribute 'labels_'")
+
+    def get_params(self):
+        """
+        Return the parameters of the Naive Byes model.
+        """
+        print(dir(self))
+        params = dict()
+        params['modelname'] = self.modelname
+        params['disc'] = self.disc
+        params['bins'] = self.bins
+
+        params['target'] = self.target
+        params['incolumn'] = self.incolumn
+        params['coldeftype'] = self.coldeftype
+        params['coldefrole'] = self.coldefrole
+        params['colpropertiestable'] = self.colpropertiestable
+
+        params['outtable'] = self.outtable
+        params['outtableProb'] = self.outtableProb
+        params['mestimation'] = self.mestimation
+
+        return params
+
+    def set_params(self, **params):
+        """
+        Modify the parameters of the Naive Bayes model.
+        """
+        if not params:
+            # Simple optimisation to gain speed (inspect is slow)
+            return self
+        valid_params = self.get_params()
+        for key, value in six.iteritems(params):
+            if key not in valid_params:
+                raise ValueError('Invalid parameter %s for estimator %s' %
+                                     (key, self.__class__.__name__))
+            setattr(self, key, value)
+        return self
+
+    def fit(self, idadf, target, column_id="ID", incolumn=None, coldeftype=None,
+            coldefrole=None, colpropertiestable=None, verbose=False):
+        """
+        Create a Naive Bayes model from an IdaDataFrame.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+            The IdaDataFrame to be used as input.
+
+        target : str
+            The column of the input table that represents the class
+
+        column_id : str, default: "ID
+            The column of the input table that identifies the transaction ID.
+
+        incolumn : str, optional
+            The columns of the input table that have specific properties,
+            which are separated by a semi-colon (;). Each column is succeeded
+            by one or more of the following properties:
+
+                * By type nominal (':nom') or by type continuous (':cont'). By default,
+                  numerical types are continuous, and all other types are nominal.
+                * By role ':id', ':target', ':input', or ':ignore'.
+
+            If this parameter is not specified, all columns of the input table have default properties.
+
+        coldeftype : str, optional
+            The default type of the input table columns.
+            The following values are allowed: 'nom' and 'cont'.
+            If the parameter is not specified, numeric columns are continuous,
+            and all other columns are nominal.
+
+        coldefrole : str, optional
+            The default role of the input table columns.
+            The following values are allowed: 'input' and 'ignore'.
+            If the parameter is not specified, all columns are input columns.
+
+        colpropertiestable : str, optional
+            The input IdaDataFrame or the name of the table where the properties of the
+            columns of the input IdaDataFrame (idadf) are stored.
+            The table name should contain only alphanumerical characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If this parameter is not specified, the column properties of the input table
+            column properties are detected automatically.
+
+        verbose : bool, default: False
+            Verbosity mode.
+        """
+
+        # Some basic checks
+        if not isinstance(idadf, nzpyida.IdaDataFrame):
+            raise TypeError("Argument should be an IdaDataFrame")
+        if target not in idadf.columns:
+            raise ValueError("Target is not a column in " + idadf.name)
+
+        idadf._idadb._check_procedure("NAIVEBAYES", "Naive Bayes")
+
+        # Check the ID
+        if column_id not in idadf.columns:
+            raise ValueError("No id columns is available in IdaDataFrame:" + column_id +
+                             ". Either create a new ID column using add_column_id function" +
+                             " or give the name of a column that can be used as ID")
+
+        self._idadb = idadf._idadb
+        self._idadf = idadf
+        self._column_id = column_id
+
+        self.target = target
+        self.incolumn = incolumn
+        self.coldeftype = coldeftype
+        self.coldefrole = coldefrole
+        if not (colpropertiestable is None or isinstance(colpropertiestable, nzpyida.frame.IdaDataFrame)):
+            colpropertiestable = nzpyida.utils.check_tablename(colpropertiestable)
+        self.colpropertiestable = colpropertiestable
+
+        # Check or create a model name, drop it if it already exists.
+        if self.modelname is None:
+            self.modelname = idadf._idadb._get_valid_modelname('NAIVEBAYES_')
+        else:
+            self.modelname = nzpyida.utils.check_modelname(self.modelname)
+            if idadf._idadb.exists_model(self.modelname):
+                idadf._idadb.drop_model(self.modelname)
+
+        # Create a temporay view
+        # TODO: Why do we need actually to create a view ?
+        tmp_view_name = idadf.internal_state._create_view()
+        # tmp_view_name = idadf.internal_state.current_state
+        
+        #if "." in tmp_view_name:
+            #tmp_view_name = tmp_view_name.split('.')[-1]
+
+        try:
+            idadf._idadb._call_stored_procedure("NAIVEBAYES ",
+                                                 model = self.modelname,
+                                                 intable = tmp_view_name,
+                                                 id = self._column_id,
+                                                 target = self.target,
+                                                 incolumn = self.incolumn,
+                                                 coldeftype = self.coldeftype,
+                                                 coldefrole = self.coldefrole,
+                                                 colPropertiesTable = self.colpropertiestable,
+                                                 disc = self.disc,
+                                                 bins = self.bins)
+        except:
+            raise
+        finally:
+            idadf.internal_state._delete_view()
+            idadf.commit()
+
+        self._retrieve_NaiveBayes_Model(self.modelname, verbose)
+
+        if verbose is True:
+            self.describe()
+
+        return
+
+    def predict(self, idadf, column_id=None, outtable=None, outtableProb=None,
+                mestimation=False):
+        """
+        Use the Naive Bayes predict stored procedure to apply a Naive Bayes model
+        to generate classification predictions for a data set.
+
+        Parameters
+        ----------
+        idadf : IdaDataFrame
+             IdaDataFrame to be used as input.
+
+        column_id : str, optional
+            The column of the input table that identifies a unique instance ID.
+            By default, the same id column that is specified in the stored
+            procedure to build the model.
+
+        outtable : str, optional
+            The name of the output table where the predictions are stored.
+            It should contain only alphanumerical characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If this parameter is not specified, it is generated automatically. If
+            the parameter corresponds to an existing table in the database, it
+            will be replaced.
+
+        outtableProb : str, optional
+            The name of the output table where the probabilities for each of the classes are stored.
+            It should contain only alphanumerical characters and underscores.
+            All lower case characters will be converted to upper case characters.
+            If this parameter is not specified, the table is not created.
+            If the parameter corresponds to an existing table in the database, it
+            will be replaced.
+
+        mestimation : flag, default: False
+            A flag that indicates the use of m-estimation for probabilities.
+            This kind of estimation might be slower than other ones, but it
+            might produce better results for small or unbalanced data sets.
+
+        Returns
+        -------
+        IdaDataFrame
+            IdaDataFrame containing the classification decision for each
+            datapoints referenced by their ID.
+        """
+        if not isinstance(idadf, nzpyida.IdaDataFrame):
+            raise TypeError("Argument should be an IdaDataFrame")
+
+        idadf._idadb._check_procedure("PREDICT_NAIVEBAYES", "Prediction for Naive Bayes")
+
+        # Check the ID
+        if column_id is None :
+            column_id = self._column_id
+        if column_id not in idadf.columns:
+            raise ValueError("No id columns is available in IdaDataFrame:" + column_id +
+                             ". Either create a new ID column using add_column_id function" +
+                             " or give the name of a column that can be used as ID")
+
+        if self._idadb is None:
+            raise IdaNaiveBayesError("The Naive Bayes model was not trained before.")
+
+        # Check or create an outtable name, drop it if it already exists.
+        if outtable is None:
+            outtable = idadf._idadb._get_valid_tablename('PREDICT_NAIVEBAYES_')
+        else:
+            outtable = nzpyida.utils.check_tablename(outtable)
+            if idadf._idadb.exists_table(outtable):
+                idadf._idadb.drop_table(outtable)
+
+        if outtableProb is not None:
+            outtableProb = nzpyida.utils.check_tablename(outtableProb)
+            if idadf._idadb.exists_table(outtableProb):
+                idadf._idadb.drop_table(outtableProb)
+
+        self.outtable = outtable
+        self.outtableProb = outtableProb
+        self.mestimation = mestimation
+
+        # Create a temporay view
+        tmp_view_name = idadf.internal_state._create_view()
+        # tmp_view_name = idadf.internal_state.current_state
+        
+        #if "." in tmp_view_name:
+            #tmp_view_name = tmp_view_name.split('.')[-1]
+
+        try:
+            idadf._idadb._call_stored_procedure("PREDICT_NAIVEBAYES ",
+                                                 model = self.modelname,
+                                                 intable = tmp_view_name,
+                                                 id = column_id,
+                                                 outtable = self.outtable,
+                                                 outtableProb = self.outtableProb,
+                                                 mestimation = self.mestimation
+                                                 )
+        except:
+            raise
+        finally:
+            idadf.internal_state._delete_view()
+            idadf._idadb._autocommit()
+
+        self.labels_ = nzpyida.IdaDataFrame(idadf._idadb, self.outtable)
+        return self.labels_
+
+    def fit_predict(self, idadf, column_id="ID", incolumn=None, coldeftype=None,
+                    coldefrole=None, colprepertiesTable=None, outtable=None,
+                    outtableProb=None, mestimation=False, verbose=False):
+        """
+        Convenience function for fitting the model and using it to make 
+        predictions about the same dataset. See to fit and predict 
+        documentation for an explanation about their attributes.
+        """
+        self.fit(idadf, column_id, incolumn, coldeftype, coldefrole, colprepertiesTable, verbose)
+        return self.predict(idadf, column_id, outtable, outtableProb, mestimation)
+
+    def describe(self, detail=True):
+        """
+        Parameters
+        ----------
+        detail: bool, optional. True by default.
+        
+        Returns
+        -------
+        Return a description of Naives Bayes.
+        If False, only the a priori probabilities of each class are returned, 
+        displayed in table format. If True, then intermediary tables are shown.
+        """
+        if self._idadb is None:
+            return self.get_params
+        else:
+            try:
+                if detail:
+                    self._retrieve_NaiveBayes_Model(self.modelname, verbose=True)
+                # PRINT_MODEL not supported for Naive Bayes on Netezza‚
+                if not self._idadb._is_netezza_system():
+                     # res = self._idadb.ida_query("CALL IDAX.PRINT_MODEL('model = " + self.modelname +"')")
+                    self._idadb._call_stored_procedure("PRINT_MODEL ", model = self.modelname)
+            except:
+                raise
+            return
+
+
+    def _retrieve_NaiveBayes_Model(self, modelname, verbose=False):
+        """
+        Retrieve information about the model to print the results. The Naive 
+        Bayes IDAX/INZA function stores its result in 2 tables:
+
+            * <MODELNAME>_MODEL
+            * <MODELNAME>_DISCRANGES
+
+        Parameters
+        ----------
+        modelname : str
+            The name of the model that is retrieved.
+
+        verbose : bol, default: False
+            Verbosity mode.
+
+        Notes
+        -----
+        Needs better formatting instead of printing the tables.
+        """
+        modelname = nzpyida.utils.check_modelname(modelname)
+
+        if self._idadb is None:
+            raise IdaNaiveBayesError("The Naive Bayes model was not trained before.")
+
+        if self._idadb._is_netezza_system():
+            modeltable_prefix = "INZA.NZA_META_"
+            disctable_postfix =  "_DISC"
+        else:
+            modeltable_prefix = ""
+            disctable_postfix =  "_DISCRANGES"
+
+        model_main = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + '_MODEL')
+        model_main.columns = ['ATTRIBUTE', 'VAL', 'CLASS', 'CLASSVALCOUNT', 'ATTRCLASSCOUNT',
+       'CLASSCOUNT', 'TOTALCOUNT']
+        model_main.columns = [x.upper() for x in model_main.columns]
+
+        disc = self._idadb.ida_query('SELECT * FROM ' + modeltable_prefix + modelname + disctable_postfix)
+        disc.columns = ['COLNAME', 'BREAK']
+        disc.columns = [x.upper() for x in disc.columns]
+
+        if verbose is True:
+            print("MODEL")
+            print(model_main)
+            print("DISCRANGES")
+            print(disc)
+
         return
```

### Comparing `nzpyida-0.2.2.6/nzpyida/sampledata/__init__.py` & `nzpyida-0.3.3/nzpyida/sampledata/__init__.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-from .iris import iris
-from .swiss import swiss
-from .titanic import titanic 
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+from .iris import iris
+from .swiss import swiss
+from .titanic import titanic 
 __all__ = ['iris', 'swiss', 'titanic']
```

### Comparing `nzpyida-0.2.2.6/nzpyida/sampledata/iris.txt` & `nzpyida-0.3.3/nzpyida/sampledata/iris.txt`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,151 +1,151 @@
-sepal_length,sepal_width,petal_length,petal_width,species
-5.1,3.5,1.4,0.2,setosa
-4.9,3.0,1.4,0.2,setosa
-4.7,3.2,1.3,0.2,setosa
-4.6,3.1,1.5,0.2,setosa
-5.0,3.6,1.4,0.2,setosa
-5.4,3.9,1.7,0.4,setosa
-4.6,3.4,1.4,0.3,setosa
-5.0,3.4,1.5,0.2,setosa
-4.4,2.9,1.4,0.2,setosa
-4.9,3.1,1.5,0.1,setosa
-5.4,3.7,1.5,0.2,setosa
-4.8,3.4,1.6,0.2,setosa
-4.8,3.0,1.4,0.1,setosa
-4.3,3.0,1.1,0.1,setosa
-5.8,4.0,1.2,0.2,setosa
-5.7,4.4,1.5,0.4,setosa
-5.4,3.9,1.3,0.4,setosa
-5.1,3.5,1.4,0.3,setosa
-5.7,3.8,1.7,0.3,setosa
-5.1,3.8,1.5,0.3,setosa
-5.4,3.4,1.7,0.2,setosa
-5.1,3.7,1.5,0.4,setosa
-4.6,3.6,1.0,0.2,setosa
-5.1,3.3,1.7,0.5,setosa
-4.8,3.4,1.9,0.2,setosa
-5.0,3.0,1.6,0.2,setosa
-5.0,3.4,1.6,0.4,setosa
-5.2,3.5,1.5,0.2,setosa
-5.2,3.4,1.4,0.2,setosa
-4.7,3.2,1.6,0.2,setosa
-4.8,3.1,1.6,0.2,setosa
-5.4,3.4,1.5,0.4,setosa
-5.2,4.1,1.5,0.1,setosa
-5.5,4.2,1.4,0.2,setosa
-4.9,3.1,1.5,0.2,setosa
-5.0,3.2,1.2,0.2,setosa
-5.5,3.5,1.3,0.2,setosa
-4.9,3.6,1.4,0.1,setosa
-4.4,3.0,1.3,0.2,setosa
-5.1,3.4,1.5,0.2,setosa
-5.0,3.5,1.3,0.3,setosa
-4.5,2.3,1.3,0.3,setosa
-4.4,3.2,1.3,0.2,setosa
-5.0,3.5,1.6,0.6,setosa
-5.1,3.8,1.9,0.4,setosa
-4.8,3.0,1.4,0.3,setosa
-5.1,3.8,1.6,0.2,setosa
-4.6,3.2,1.4,0.2,setosa
-5.3,3.7,1.5,0.2,setosa
-5.0,3.3,1.4,0.2,setosa
-7.0,3.2,4.7,1.4,versicolor
-6.4,3.2,4.5,1.5,versicolor
-6.9,3.1,4.9,1.5,versicolor
-5.5,2.3,4.0,1.3,versicolor
-6.5,2.8,4.6,1.5,versicolor
-5.7,2.8,4.5,1.3,versicolor
-6.3,3.3,4.7,1.6,versicolor
-4.9,2.4,3.3,1.0,versicolor
-6.6,2.9,4.6,1.3,versicolor
-5.2,2.7,3.9,1.4,versicolor
-5.0,2.0,3.5,1.0,versicolor
-5.9,3.0,4.2,1.5,versicolor
-6.0,2.2,4.0,1.0,versicolor
-6.1,2.9,4.7,1.4,versicolor
-5.6,2.9,3.6,1.3,versicolor
-6.7,3.1,4.4,1.4,versicolor
-5.6,3.0,4.5,1.5,versicolor
-5.8,2.7,4.1,1.0,versicolor
-6.2,2.2,4.5,1.5,versicolor
-5.6,2.5,3.9,1.1,versicolor
-5.9,3.2,4.8,1.8,versicolor
-6.1,2.8,4.0,1.3,versicolor
-6.3,2.5,4.9,1.5,versicolor
-6.1,2.8,4.7,1.2,versicolor
-6.4,2.9,4.3,1.3,versicolor
-6.6,3.0,4.4,1.4,versicolor
-6.8,2.8,4.8,1.4,versicolor
-6.7,3.0,5.0,1.7,versicolor
-6.0,2.9,4.5,1.5,versicolor
-5.7,2.6,3.5,1.0,versicolor
-5.5,2.4,3.8,1.1,versicolor
-5.5,2.4,3.7,1.0,versicolor
-5.8,2.7,3.9,1.2,versicolor
-6.0,2.7,5.1,1.6,versicolor
-5.4,3.0,4.5,1.5,versicolor
-6.0,3.4,4.5,1.6,versicolor
-6.7,3.1,4.7,1.5,versicolor
-6.3,2.3,4.4,1.3,versicolor
-5.6,3.0,4.1,1.3,versicolor
-5.5,2.5,4.0,1.3,versicolor
-5.5,2.6,4.4,1.2,versicolor
-6.1,3.0,4.6,1.4,versicolor
-5.8,2.6,4.0,1.2,versicolor
-5.0,2.3,3.3,1.0,versicolor
-5.6,2.7,4.2,1.3,versicolor
-5.7,3.0,4.2,1.2,versicolor
-5.7,2.9,4.2,1.3,versicolor
-6.2,2.9,4.3,1.3,versicolor
-5.1,2.5,3.0,1.1,versicolor
-5.7,2.8,4.1,1.3,versicolor
-6.3,3.3,6.0,2.5,virginica
-5.8,2.7,5.1,1.9,virginica
-7.1,3.0,5.9,2.1,virginica
-6.3,2.9,5.6,1.8,virginica
-6.5,3.0,5.8,2.2,virginica
-7.6,3.0,6.6,2.1,virginica
-4.9,2.5,4.5,1.7,virginica
-7.3,2.9,6.3,1.8,virginica
-6.7,2.5,5.8,1.8,virginica
-7.2,3.6,6.1,2.5,virginica
-6.5,3.2,5.1,2.0,virginica
-6.4,2.7,5.3,1.9,virginica
-6.8,3.0,5.5,2.1,virginica
-5.7,2.5,5.0,2.0,virginica
-5.8,2.8,5.1,2.4,virginica
-6.4,3.2,5.3,2.3,virginica
-6.5,3.0,5.5,1.8,virginica
-7.7,3.8,6.7,2.2,virginica
-7.7,2.6,6.9,2.3,virginica
-6.0,2.2,5.0,1.5,virginica
-6.9,3.2,5.7,2.3,virginica
-5.6,2.8,4.9,2.0,virginica
-7.7,2.8,6.7,2.0,virginica
-6.3,2.7,4.9,1.8,virginica
-6.7,3.3,5.7,2.1,virginica
-7.2,3.2,6.0,1.8,virginica
-6.2,2.8,4.8,1.8,virginica
-6.1,3.0,4.9,1.8,virginica
-6.4,2.8,5.6,2.1,virginica
-7.2,3.0,5.8,1.6,virginica
-7.4,2.8,6.1,1.9,virginica
-7.9,3.8,6.4,2.0,virginica
-6.4,2.8,5.6,2.2,virginica
-6.3,2.8,5.1,1.5,virginica
-6.1,2.6,5.6,1.4,virginica
-7.7,3.0,6.1,2.3,virginica
-6.3,3.4,5.6,2.4,virginica
-6.4,3.1,5.5,1.8,virginica
-6.0,3.0,4.8,1.8,virginica
-6.9,3.1,5.4,2.1,virginica
-6.7,3.1,5.6,2.4,virginica
-6.9,3.1,5.1,2.3,virginica
-5.8,2.7,5.1,1.9,virginica
-6.8,3.2,5.9,2.3,virginica
-6.7,3.3,5.7,2.5,virginica
-6.7,3.0,5.2,2.3,virginica
-6.3,2.5,5.0,1.9,virginica
-6.5,3.0,5.2,2.0,virginica
-6.2,3.4,5.4,2.3,virginica
-5.9,3.0,5.1,1.8,virginica
+sepal_length,sepal_width,petal_length,petal_width,species
+5.1,3.5,1.4,0.2,setosa
+4.9,3.0,1.4,0.2,setosa
+4.7,3.2,1.3,0.2,setosa
+4.6,3.1,1.5,0.2,setosa
+5.0,3.6,1.4,0.2,setosa
+5.4,3.9,1.7,0.4,setosa
+4.6,3.4,1.4,0.3,setosa
+5.0,3.4,1.5,0.2,setosa
+4.4,2.9,1.4,0.2,setosa
+4.9,3.1,1.5,0.1,setosa
+5.4,3.7,1.5,0.2,setosa
+4.8,3.4,1.6,0.2,setosa
+4.8,3.0,1.4,0.1,setosa
+4.3,3.0,1.1,0.1,setosa
+5.8,4.0,1.2,0.2,setosa
+5.7,4.4,1.5,0.4,setosa
+5.4,3.9,1.3,0.4,setosa
+5.1,3.5,1.4,0.3,setosa
+5.7,3.8,1.7,0.3,setosa
+5.1,3.8,1.5,0.3,setosa
+5.4,3.4,1.7,0.2,setosa
+5.1,3.7,1.5,0.4,setosa
+4.6,3.6,1.0,0.2,setosa
+5.1,3.3,1.7,0.5,setosa
+4.8,3.4,1.9,0.2,setosa
+5.0,3.0,1.6,0.2,setosa
+5.0,3.4,1.6,0.4,setosa
+5.2,3.5,1.5,0.2,setosa
+5.2,3.4,1.4,0.2,setosa
+4.7,3.2,1.6,0.2,setosa
+4.8,3.1,1.6,0.2,setosa
+5.4,3.4,1.5,0.4,setosa
+5.2,4.1,1.5,0.1,setosa
+5.5,4.2,1.4,0.2,setosa
+4.9,3.1,1.5,0.2,setosa
+5.0,3.2,1.2,0.2,setosa
+5.5,3.5,1.3,0.2,setosa
+4.9,3.6,1.4,0.1,setosa
+4.4,3.0,1.3,0.2,setosa
+5.1,3.4,1.5,0.2,setosa
+5.0,3.5,1.3,0.3,setosa
+4.5,2.3,1.3,0.3,setosa
+4.4,3.2,1.3,0.2,setosa
+5.0,3.5,1.6,0.6,setosa
+5.1,3.8,1.9,0.4,setosa
+4.8,3.0,1.4,0.3,setosa
+5.1,3.8,1.6,0.2,setosa
+4.6,3.2,1.4,0.2,setosa
+5.3,3.7,1.5,0.2,setosa
+5.0,3.3,1.4,0.2,setosa
+7.0,3.2,4.7,1.4,versicolor
+6.4,3.2,4.5,1.5,versicolor
+6.9,3.1,4.9,1.5,versicolor
+5.5,2.3,4.0,1.3,versicolor
+6.5,2.8,4.6,1.5,versicolor
+5.7,2.8,4.5,1.3,versicolor
+6.3,3.3,4.7,1.6,versicolor
+4.9,2.4,3.3,1.0,versicolor
+6.6,2.9,4.6,1.3,versicolor
+5.2,2.7,3.9,1.4,versicolor
+5.0,2.0,3.5,1.0,versicolor
+5.9,3.0,4.2,1.5,versicolor
+6.0,2.2,4.0,1.0,versicolor
+6.1,2.9,4.7,1.4,versicolor
+5.6,2.9,3.6,1.3,versicolor
+6.7,3.1,4.4,1.4,versicolor
+5.6,3.0,4.5,1.5,versicolor
+5.8,2.7,4.1,1.0,versicolor
+6.2,2.2,4.5,1.5,versicolor
+5.6,2.5,3.9,1.1,versicolor
+5.9,3.2,4.8,1.8,versicolor
+6.1,2.8,4.0,1.3,versicolor
+6.3,2.5,4.9,1.5,versicolor
+6.1,2.8,4.7,1.2,versicolor
+6.4,2.9,4.3,1.3,versicolor
+6.6,3.0,4.4,1.4,versicolor
+6.8,2.8,4.8,1.4,versicolor
+6.7,3.0,5.0,1.7,versicolor
+6.0,2.9,4.5,1.5,versicolor
+5.7,2.6,3.5,1.0,versicolor
+5.5,2.4,3.8,1.1,versicolor
+5.5,2.4,3.7,1.0,versicolor
+5.8,2.7,3.9,1.2,versicolor
+6.0,2.7,5.1,1.6,versicolor
+5.4,3.0,4.5,1.5,versicolor
+6.0,3.4,4.5,1.6,versicolor
+6.7,3.1,4.7,1.5,versicolor
+6.3,2.3,4.4,1.3,versicolor
+5.6,3.0,4.1,1.3,versicolor
+5.5,2.5,4.0,1.3,versicolor
+5.5,2.6,4.4,1.2,versicolor
+6.1,3.0,4.6,1.4,versicolor
+5.8,2.6,4.0,1.2,versicolor
+5.0,2.3,3.3,1.0,versicolor
+5.6,2.7,4.2,1.3,versicolor
+5.7,3.0,4.2,1.2,versicolor
+5.7,2.9,4.2,1.3,versicolor
+6.2,2.9,4.3,1.3,versicolor
+5.1,2.5,3.0,1.1,versicolor
+5.7,2.8,4.1,1.3,versicolor
+6.3,3.3,6.0,2.5,virginica
+5.8,2.7,5.1,1.9,virginica
+7.1,3.0,5.9,2.1,virginica
+6.3,2.9,5.6,1.8,virginica
+6.5,3.0,5.8,2.2,virginica
+7.6,3.0,6.6,2.1,virginica
+4.9,2.5,4.5,1.7,virginica
+7.3,2.9,6.3,1.8,virginica
+6.7,2.5,5.8,1.8,virginica
+7.2,3.6,6.1,2.5,virginica
+6.5,3.2,5.1,2.0,virginica
+6.4,2.7,5.3,1.9,virginica
+6.8,3.0,5.5,2.1,virginica
+5.7,2.5,5.0,2.0,virginica
+5.8,2.8,5.1,2.4,virginica
+6.4,3.2,5.3,2.3,virginica
+6.5,3.0,5.5,1.8,virginica
+7.7,3.8,6.7,2.2,virginica
+7.7,2.6,6.9,2.3,virginica
+6.0,2.2,5.0,1.5,virginica
+6.9,3.2,5.7,2.3,virginica
+5.6,2.8,4.9,2.0,virginica
+7.7,2.8,6.7,2.0,virginica
+6.3,2.7,4.9,1.8,virginica
+6.7,3.3,5.7,2.1,virginica
+7.2,3.2,6.0,1.8,virginica
+6.2,2.8,4.8,1.8,virginica
+6.1,3.0,4.9,1.8,virginica
+6.4,2.8,5.6,2.1,virginica
+7.2,3.0,5.8,1.6,virginica
+7.4,2.8,6.1,1.9,virginica
+7.9,3.8,6.4,2.0,virginica
+6.4,2.8,5.6,2.2,virginica
+6.3,2.8,5.1,1.5,virginica
+6.1,2.6,5.6,1.4,virginica
+7.7,3.0,6.1,2.3,virginica
+6.3,3.4,5.6,2.4,virginica
+6.4,3.1,5.5,1.8,virginica
+6.0,3.0,4.8,1.8,virginica
+6.9,3.1,5.4,2.1,virginica
+6.7,3.1,5.6,2.4,virginica
+6.9,3.1,5.1,2.3,virginica
+5.8,2.7,5.1,1.9,virginica
+6.8,3.2,5.9,2.3,virginica
+6.7,3.3,5.7,2.5,virginica
+6.7,3.0,5.2,2.3,virginica
+6.3,2.5,5.0,1.9,virginica
+6.5,3.0,5.2,2.0,virginica
+6.2,3.4,5.4,2.3,virginica
+5.9,3.0,5.1,1.8,virginica
```

### Comparing `nzpyida-0.2.2.6/nzpyida/sampledata/swiss.py` & `nzpyida-0.3.3/nzpyida/sampledata/swiss.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-'''
-This module provides a data set containing standardized fertility measure and
-socio-economic indicators for each of 47 French-speaking provinces of
-Switzerland at about 1888. As user, you can import the object 'swiss' which
-is a pandas dataframe with 47 rows and the following fields:
-    swiss['Fertility']
-    swiss['Agriculture']
-    swiss['Examination']
-    swiss['Education']
-    swiss['Catholic']
-    swiss['Infant.Mortality']
-
-Examples
---------
->>> from nzpyida.sampledata.swiss import swiss
-'''
-
-from __future__ import absolute_import
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from future import standard_library
-standard_library.install_aliases()
-from os.path import dirname, join
-import pandas as pd
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+'''
+This module provides a data set containing standardized fertility measure and
+socio-economic indicators for each of 47 French-speaking provinces of
+Switzerland at about 1888. As user, you can import the object 'swiss' which
+is a pandas dataframe with 47 rows and the following fields:
+    swiss['Fertility']
+    swiss['Agriculture']
+    swiss['Examination']
+    swiss['Education']
+    swiss['Catholic']
+    swiss['Infant.Mortality']
+
+Examples
+--------
+>>> from nzpyida.sampledata.swiss import swiss
+'''
+
+from __future__ import absolute_import
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from future import standard_library
+standard_library.install_aliases()
+from os.path import dirname, join
+import pandas as pd
+
 swiss = pd.read_csv(join(dirname(__file__), 'swiss.txt'), index_col = 0)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/sampledata/swiss.txt` & `nzpyida-0.3.3/nzpyida/sampledata/swiss.txt`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-"","Fertility","Agriculture","Examination","Education","Catholic","Infant.Mortality"
-"Courtelary",80.2,17,15,12,9.96,22.2
-"Delemont",83.1,45.1,6,9,84.84,22.2
-"Franches-Mnt",92.5,39.7,5,5,93.4,20.2
-"Moutier",85.8,36.5,12,7,33.77,20.3
-"Neuveville",76.9,43.5,17,15,5.16,20.6
-"Porrentruy",76.1,35.3,9,7,90.57,26.6
-"Broye",83.8,70.2,16,7,92.85,23.6
-"Glane",92.4,67.8,14,8,97.16,24.9
-"Gruyere",82.4,53.3,12,7,97.67,21
-"Sarine",82.9,45.2,16,13,91.38,24.4
-"Veveyse",87.1,64.5,14,6,98.61,24.5
-"Aigle",64.1,62,21,12,8.52,16.5
-"Aubonne",66.9,67.5,14,7,2.27,19.1
-"Avenches",68.9,60.7,19,12,4.43,22.7
-"Cossonay",61.7,69.3,22,5,2.82,18.7
-"Echallens",68.3,72.6,18,2,24.2,21.2
-"Grandson",71.7,34,17,8,3.3,20
-"Lausanne",55.7,19.4,26,28,12.11,20.2
-"La Vallee",54.3,15.2,31,20,2.15,10.8
-"Lavaux",65.1,73,19,9,2.84,20
-"Morges",65.5,59.8,22,10,5.23,18
-"Moudon",65,55.1,14,3,4.52,22.4
-"Nyone",56.6,50.9,22,12,15.14,16.7
-"Orbe",57.4,54.1,20,6,4.2,15.3
-"Oron",72.5,71.2,12,1,2.4,21
-"Payerne",74.2,58.1,14,8,5.23,23.8
-"Paysd'enhaut",72,63.5,6,3,2.56,18
-"Rolle",60.5,60.8,16,10,7.72,16.3
-"Vevey",58.3,26.8,25,19,18.46,20.9
-"Yverdon",65.4,49.5,15,8,6.1,22.5
-"Conthey",75.5,85.9,3,2,99.71,15.1
-"Entremont",69.3,84.9,7,6,99.68,19.8
-"Herens",77.3,89.7,5,2,100,18.3
-"Martigwy",70.5,78.2,12,6,98.96,19.4
-"Monthey",79.4,64.9,7,3,98.22,20.2
-"St Maurice",65,75.9,9,9,99.06,17.8
-"Sierre",92.2,84.6,3,3,99.46,16.3
-"Sion",79.3,63.1,13,13,96.83,18.1
-"Boudry",70.4,38.4,26,12,5.62,20.3
-"La Chauxdfnd",65.7,7.7,29,11,13.79,20.5
-"Le Locle",72.7,16.7,22,13,11.22,18.9
-"Neuchatel",64.4,17.6,35,32,16.92,23
-"Val de Ruz",77.6,37.6,15,7,4.97,20
-"ValdeTravers",67.6,18.7,25,7,8.65,19.5
-"V. De Geneve",35,1.2,37,53,42.34,18
-"Rive Droite",44.7,46.6,16,29,50.43,18.2
-"Rive Gauche",42.8,27.7,22,29,58.33,19.3
+"","Fertility","Agriculture","Examination","Education","Catholic","Infant.Mortality"
+"Courtelary",80.2,17,15,12,9.96,22.2
+"Delemont",83.1,45.1,6,9,84.84,22.2
+"Franches-Mnt",92.5,39.7,5,5,93.4,20.2
+"Moutier",85.8,36.5,12,7,33.77,20.3
+"Neuveville",76.9,43.5,17,15,5.16,20.6
+"Porrentruy",76.1,35.3,9,7,90.57,26.6
+"Broye",83.8,70.2,16,7,92.85,23.6
+"Glane",92.4,67.8,14,8,97.16,24.9
+"Gruyere",82.4,53.3,12,7,97.67,21
+"Sarine",82.9,45.2,16,13,91.38,24.4
+"Veveyse",87.1,64.5,14,6,98.61,24.5
+"Aigle",64.1,62,21,12,8.52,16.5
+"Aubonne",66.9,67.5,14,7,2.27,19.1
+"Avenches",68.9,60.7,19,12,4.43,22.7
+"Cossonay",61.7,69.3,22,5,2.82,18.7
+"Echallens",68.3,72.6,18,2,24.2,21.2
+"Grandson",71.7,34,17,8,3.3,20
+"Lausanne",55.7,19.4,26,28,12.11,20.2
+"La Vallee",54.3,15.2,31,20,2.15,10.8
+"Lavaux",65.1,73,19,9,2.84,20
+"Morges",65.5,59.8,22,10,5.23,18
+"Moudon",65,55.1,14,3,4.52,22.4
+"Nyone",56.6,50.9,22,12,15.14,16.7
+"Orbe",57.4,54.1,20,6,4.2,15.3
+"Oron",72.5,71.2,12,1,2.4,21
+"Payerne",74.2,58.1,14,8,5.23,23.8
+"Paysd'enhaut",72,63.5,6,3,2.56,18
+"Rolle",60.5,60.8,16,10,7.72,16.3
+"Vevey",58.3,26.8,25,19,18.46,20.9
+"Yverdon",65.4,49.5,15,8,6.1,22.5
+"Conthey",75.5,85.9,3,2,99.71,15.1
+"Entremont",69.3,84.9,7,6,99.68,19.8
+"Herens",77.3,89.7,5,2,100,18.3
+"Martigwy",70.5,78.2,12,6,98.96,19.4
+"Monthey",79.4,64.9,7,3,98.22,20.2
+"St Maurice",65,75.9,9,9,99.06,17.8
+"Sierre",92.2,84.6,3,3,99.46,16.3
+"Sion",79.3,63.1,13,13,96.83,18.1
+"Boudry",70.4,38.4,26,12,5.62,20.3
+"La Chauxdfnd",65.7,7.7,29,11,13.79,20.5
+"Le Locle",72.7,16.7,22,13,11.22,18.9
+"Neuchatel",64.4,17.6,35,32,16.92,23
+"Val de Ruz",77.6,37.6,15,7,4.97,20
+"ValdeTravers",67.6,18.7,25,7,8.65,19.5
+"V. De Geneve",35,1.2,37,53,42.34,18
+"Rive Droite",44.7,46.6,16,29,50.43,18.2
+"Rive Gauche",42.8,27.7,22,29,58.33,19.3
```

### Comparing `nzpyida-0.2.2.6/nzpyida/sampledata/titanic.py` & `nzpyida-0.3.3/nzpyida/sampledata/titanic.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-'''
-This module provides a data set containing (fake) information about passengers
-of the titanic. This dataset was used in a famous Kaggle data science competition.
-As user, you can import the object 'titanic' which is a pandas dataframe with
-891 rows and the following fields:
-    titanic['PASSENGERID']
-    titanic['SURVIVED']
-    titanic['PCLASS']
-    titanic['NAME']
-    titanic['SEX']
-    titanic['AGE']
-    titanic['SIBSP']
-    titanic['PARCH']
-    titanic['TICKET']
-    titanic['FARE']
-    titanic['CABIN']
-    titanic['EMBARKED']
-
-Examples
---------
->>> from nzpyida.sampledata.titanic import titanic
-'''
-
-from __future__ import absolute_import
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from future import standard_library
-standard_library.install_aliases()
-from os.path import dirname, join
-import pandas as pd
-
-titanic = pd.read_csv(join(dirname(__file__), 'titanic.txt'), sep = '|')
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+'''
+This module provides a data set containing (fake) information about passengers
+of the titanic. This dataset was used in a famous Kaggle data science competition.
+As user, you can import the object 'titanic' which is a pandas dataframe with
+891 rows and the following fields:
+    titanic['PASSENGERID']
+    titanic['SURVIVED']
+    titanic['PCLASS']
+    titanic['NAME']
+    titanic['SEX']
+    titanic['AGE']
+    titanic['SIBSP']
+    titanic['PARCH']
+    titanic['TICKET']
+    titanic['FARE']
+    titanic['CABIN']
+    titanic['EMBARKED']
+
+Examples
+--------
+>>> from nzpyida.sampledata.titanic import titanic
+'''
+
+from __future__ import absolute_import
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from future import standard_library
+standard_library.install_aliases()
+from os.path import dirname, join
+import pandas as pd
+
+titanic = pd.read_csv(join(dirname(__file__), 'titanic.txt'), sep = '|')
 titanic.name = "titanic"
```

### Comparing `nzpyida-0.2.2.6/nzpyida/sampledata/titanic.txt` & `nzpyida-0.3.3/nzpyida/sampledata/titanic.txt`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,892 +1,892 @@
-"PASSENGERID"|"SURVIVED"|"PCLASS"|"NAME"|"SEX"|"AGE"|"SIBSP"|"PARCH"|"TICKET"|"FARE"|"CABIN"|"EMBARKED"
-1|0|3|"Braund, Mr. Owen Harris"|"male"|22|1|0|"A/5 21171"|7.25|""|"S"
-2|1|1|"Cumings, Mrs. John Bradley (Florence Briggs Thayer)"|"female"|38|1|0|"PC 17599"|71.2833|"C85"|"C"
-3|1|3|"Heikkinen, Miss. Laina"|"female"|26|0|0|"STON/O2. 3101282"|7.925|""|"S"
-4|1|1|"Futrelle, Mrs. Jacques Heath (Lily May Peel)"|"female"|35|1|0|"113803"|53.1|"C123"|"S"
-5|0|3|"Allen, Mr. William Henry"|"male"|35|0|0|"373450"|8.05|""|"S"
-6|0|3|"Moran, Mr. James"|"male"|NA|0|0|"330877"|8.4583|""|"Q"
-7|0|1|"McCarthy, Mr. Timothy J"|"male"|54|0|0|"17463"|51.8625|"E46"|"S"
-8|0|3|"Palsson, Master. Gosta Leonard"|"male"|2|3|1|"349909"|21.075|""|"S"
-9|1|3|"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)"|"female"|27|0|2|"347742"|11.1333|""|"S"
-10|1|2|"Nasser, Mrs. Nicholas (Adele Achem)"|"female"|14|1|0|"237736"|30.0708|""|"C"
-11|1|3|"Sandstrom, Miss. Marguerite Rut"|"female"|4|1|1|"PP 9549"|16.7|"G6"|"S"
-12|1|1|"Bonnell, Miss. Elizabeth"|"female"|58|0|0|"113783"|26.55|"C103"|"S"
-13|0|3|"Saundercock, Mr. William Henry"|"male"|20|0|0|"A/5. 2151"|8.05|""|"S"
-14|0|3|"Andersson, Mr. Anders Johan"|"male"|39|1|5|"347082"|31.275|""|"S"
-15|0|3|"Vestrom, Miss. Hulda Amanda Adolfina"|"female"|14|0|0|"350406"|7.8542|""|"S"
-16|1|2|"Hewlett, Mrs. (Mary D Kingcome) "|"female"|55|0|0|"248706"|16|""|"S"
-17|0|3|"Rice, Master. Eugene"|"male"|2|4|1|"382652"|29.125|""|"Q"
-18|1|2|"Williams, Mr. Charles Eugene"|"male"|NA|0|0|"244373"|13|""|"S"
-19|0|3|"Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)"|"female"|31|1|0|"345763"|18|""|"S"
-20|1|3|"Masselmani, Mrs. Fatima"|"female"|NA|0|0|"2649"|7.225|""|"C"
-21|0|2|"Fynney, Mr. Joseph J"|"male"|35|0|0|"239865"|26|""|"S"
-22|1|2|"Beesley, Mr. Lawrence"|"male"|34|0|0|"248698"|13|"D56"|"S"
-23|1|3|"McGowan, Miss. Anna \Annie\"|"female"|15|0|0|"330923"|8.0292|""|"Q"
-24|1|1|"Sloper, Mr. William Thompson"|"male"|28|0|0|"113788"|35.5|"A6"|"S"
-25|0|3|"Palsson, Miss. Torborg Danira"|"female"|8|3|1|"349909"|21.075|""|"S"
-26|1|3|"Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)"|"female"|38|1|5|"347077"|31.3875|""|"S"
-27|0|3|"Emir, Mr. Farred Chehab"|"male"|NA|0|0|"2631"|7.225|""|"C"
-28|0|1|"Fortune, Mr. Charles Alexander"|"male"|19|3|2|"19950"|263|"C23 C25 C27"|"S"
-29|1|3|"O'Dwyer, Miss. Ellen \Nellie\"|"female"|NA|0|0|"330959"|7.8792|""|"Q"
-30|0|3|"Todoroff, Mr. Lalio"|"male"|NA|0|0|"349216"|7.8958|""|"S"
-31|0|1|"Uruchurtu, Don. Manuel E"|"male"|40|0|0|"PC 17601"|27.7208|""|"C"
-32|1|1|"Spencer, Mrs. William Augustus (Marie Eugenie)"|"female"|NA|1|0|"PC 17569"|146.5208|"B78"|"C"
-33|1|3|"Glynn, Miss. Mary Agatha"|"female"|NA|0|0|"335677"|7.75|""|"Q"
-34|0|2|"Wheadon, Mr. Edward H"|"male"|66|0|0|"C.A. 24579"|10.5|""|"S"
-35|0|1|"Meyer, Mr. Edgar Joseph"|"male"|28|1|0|"PC 17604"|82.1708|""|"C"
-36|0|1|"Holverson, Mr. Alexander Oskar"|"male"|42|1|0|"113789"|52|""|"S"
-37|1|3|"Mamee, Mr. Hanna"|"male"|NA|0|0|"2677"|7.2292|""|"C"
-38|0|3|"Cann, Mr. Ernest Charles"|"male"|21|0|0|"A./5. 2152"|8.05|""|"S"
-39|0|3|"Vander Planke, Miss. Augusta Maria"|"female"|18|2|0|"345764"|18|""|"S"
-40|1|3|"Nicola-Yarred, Miss. Jamila"|"female"|14|1|0|"2651"|11.2417|""|"C"
-41|0|3|"Ahlin, Mrs. Johan (Johanna Persdotter Larsson)"|"female"|40|1|0|"7546"|9.475|""|"S"
-42|0|2|"Turpin, Mrs. William John Robert (Dorothy Ann Wonnacott)"|"female"|27|1|0|"11668"|21|""|"S"
-43|0|3|"Kraeff, Mr. Theodor"|"male"|NA|0|0|"349253"|7.8958|""|"C"
-44|1|2|"Laroche, Miss. Simonne Marie Anne Andree"|"female"|3|1|2|"SC/Paris 2123"|41.5792|""|"C"
-45|1|3|"Devaney, Miss. Margaret Delia"|"female"|19|0|0|"330958"|7.8792|""|"Q"
-46|0|3|"Rogers, Mr. William John"|"male"|NA|0|0|"S.C./A.4. 23567"|8.05|""|"S"
-47|0|3|"Lennon, Mr. Denis"|"male"|NA|1|0|"370371"|15.5|""|"Q"
-48|1|3|"O'Driscoll, Miss. Bridget"|"female"|NA|0|0|"14311"|7.75|""|"Q"
-49|0|3|"Samaan, Mr. Youssef"|"male"|NA|2|0|"2662"|21.6792|""|"C"
-50|0|3|"Arnold-Franchi, Mrs. Josef (Josefine Franchi)"|"female"|18|1|0|"349237"|17.8|""|"S"
-51|0|3|"Panula, Master. Juha Niilo"|"male"|7|4|1|"3101295"|39.6875|""|"S"
-52|0|3|"Nosworthy, Mr. Richard Cater"|"male"|21|0|0|"A/4. 39886"|7.8|""|"S"
-53|1|1|"Harper, Mrs. Henry Sleeper (Myna Haxtun)"|"female"|49|1|0|"PC 17572"|76.7292|"D33"|"C"
-54|1|2|"Faunthorpe, Mrs. Lizzie (Elizabeth Anne Wilkinson)"|"female"|29|1|0|"2926"|26|""|"S"
-55|0|1|"Ostby, Mr. Engelhart Cornelius"|"male"|65|0|1|"113509"|61.9792|"B30"|"C"
-56|1|1|"Woolner, Mr. Hugh"|"male"|NA|0|0|"19947"|35.5|"C52"|"S"
-57|1|2|"Rugg, Miss. Emily"|"female"|21|0|0|"C.A. 31026"|10.5|""|"S"
-58|0|3|"Novel, Mr. Mansouer"|"male"|28.5|0|0|"2697"|7.2292|""|"C"
-59|1|2|"West, Miss. Constance Mirium"|"female"|5|1|2|"C.A. 34651"|27.75|""|"S"
-60|0|3|"Goodwin, Master. William Frederick"|"male"|11|5|2|"CA 2144"|46.9|""|"S"
-61|0|3|"Sirayanian, Mr. Orsen"|"male"|22|0|0|"2669"|7.2292|""|"C"
-62|1|1|"Icard, Miss. Amelie"|"female"|38|0|0|"113572"|80|"B28"|""
-63|0|1|"Harris, Mr. Henry Birkhardt"|"male"|45|1|0|"36973"|83.475|"C83"|"S"
-64|0|3|"Skoog, Master. Harald"|"male"|4|3|2|"347088"|27.9|""|"S"
-65|0|1|"Stewart, Mr. Albert A"|"male"|NA|0|0|"PC 17605"|27.7208|""|"C"
-66|1|3|"Moubarek, Master. Gerios"|"male"|NA|1|1|"2661"|15.2458|""|"C"
-67|1|2|"Nye, Mrs. (Elizabeth Ramell)"|"female"|29|0|0|"C.A. 29395"|10.5|"F33"|"S"
-68|0|3|"Crease, Mr. Ernest James"|"male"|19|0|0|"S.P. 3464"|8.1583|""|"S"
-69|1|3|"Andersson, Miss. Erna Alexandra"|"female"|17|4|2|"3101281"|7.925|""|"S"
-70|0|3|"Kink, Mr. Vincenz"|"male"|26|2|0|"315151"|8.6625|""|"S"
-71|0|2|"Jenkin, Mr. Stephen Curnow"|"male"|32|0|0|"C.A. 33111"|10.5|""|"S"
-72|0|3|"Goodwin, Miss. Lillian Amy"|"female"|16|5|2|"CA 2144"|46.9|""|"S"
-73|0|2|"Hood, Mr. Ambrose Jr"|"male"|21|0|0|"S.O.C. 14879"|73.5|""|"S"
-74|0|3|"Chronopoulos, Mr. Apostolos"|"male"|26|1|0|"2680"|14.4542|""|"C"
-75|1|3|"Bing, Mr. Lee"|"male"|32|0|0|"1601"|56.4958|""|"S"
-76|0|3|"Moen, Mr. Sigurd Hansen"|"male"|25|0|0|"348123"|7.65|"F G73"|"S"
-77|0|3|"Staneff, Mr. Ivan"|"male"|NA|0|0|"349208"|7.8958|""|"S"
-78|0|3|"Moutal, Mr. Rahamin Haim"|"male"|NA|0|0|"374746"|8.05|""|"S"
-79|1|2|"Caldwell, Master. Alden Gates"|"male"|0.83|0|2|"248738"|29|""|"S"
-80|1|3|"Dowdell, Miss. Elizabeth"|"female"|30|0|0|"364516"|12.475|""|"S"
-81|0|3|"Waelens, Mr. Achille"|"male"|22|0|0|"345767"|9|""|"S"
-82|1|3|"Sheerlinck, Mr. Jan Baptist"|"male"|29|0|0|"345779"|9.5|""|"S"
-83|1|3|"McDermott, Miss. Brigdet Delia"|"female"|NA|0|0|"330932"|7.7875|""|"Q"
-84|0|1|"Carrau, Mr. Francisco M"|"male"|28|0|0|"113059"|47.1|""|"S"
-85|1|2|"Ilett, Miss. Bertha"|"female"|17|0|0|"SO/C 14885"|10.5|""|"S"
-86|1|3|"Backstrom, Mrs. Karl Alfred (Maria Mathilda Gustafsson)"|"female"|33|3|0|"3101278"|15.85|""|"S"
-87|0|3|"Ford, Mr. William Neal"|"male"|16|1|3|"W./C. 6608"|34.375|""|"S"
-88|0|3|"Slocovski, Mr. Selman Francis"|"male"|NA|0|0|"SOTON/OQ 392086"|8.05|""|"S"
-89|1|1|"Fortune, Miss. Mabel Helen"|"female"|23|3|2|"19950"|263|"C23 C25 C27"|"S"
-90|0|3|"Celotti, Mr. Francesco"|"male"|24|0|0|"343275"|8.05|""|"S"
-91|0|3|"Christmann, Mr. Emil"|"male"|29|0|0|"343276"|8.05|""|"S"
-92|0|3|"Andreasson, Mr. Paul Edvin"|"male"|20|0|0|"347466"|7.8542|""|"S"
-93|0|1|"Chaffee, Mr. Herbert Fuller"|"male"|46|1|0|"W.E.P. 5734"|61.175|"E31"|"S"
-94|0|3|"Dean, Mr. Bertram Frank"|"male"|26|1|2|"C.A. 2315"|20.575|""|"S"
-95|0|3|"Coxon, Mr. Daniel"|"male"|59|0|0|"364500"|7.25|""|"S"
-96|0|3|"Shorney, Mr. Charles Joseph"|"male"|NA|0|0|"374910"|8.05|""|"S"
-97|0|1|"Goldschmidt, Mr. George B"|"male"|71|0|0|"PC 17754"|34.6542|"A5"|"C"
-98|1|1|"Greenfield, Mr. William Bertram"|"male"|23|0|1|"PC 17759"|63.3583|"D10 D12"|"C"
-99|1|2|"Doling, Mrs. John T (Ada Julia Bone)"|"female"|34|0|1|"231919"|23|""|"S"
-100|0|2|"Kantor, Mr. Sinai"|"male"|34|1|0|"244367"|26|""|"S"
-101|0|3|"Petranec, Miss. Matilda"|"female"|28|0|0|"349245"|7.8958|""|"S"
-102|0|3|"Petroff, Mr. Pastcho (\Pentcho\)"|"male"|NA|0|0|"349215"|7.8958|""|"S"
-103|0|1|"White, Mr. Richard Frasar"|"male"|21|0|1|"35281"|77.2875|"D26"|"S"
-104|0|3|"Johansson, Mr. Gustaf Joel"|"male"|33|0|0|"7540"|8.6542|""|"S"
-105|0|3|"Gustafsson, Mr. Anders Vilhelm"|"male"|37|2|0|"3101276"|7.925|""|"S"
-106|0|3|"Mionoff, Mr. Stoytcho"|"male"|28|0|0|"349207"|7.8958|""|"S"
-107|1|3|"Salkjelsvik, Miss. Anna Kristine"|"female"|21|0|0|"343120"|7.65|""|"S"
-108|1|3|"Moss, Mr. Albert Johan"|"male"|NA|0|0|"312991"|7.775|""|"S"
-109|0|3|"Rekic, Mr. Tido"|"male"|38|0|0|"349249"|7.8958|""|"S"
-110|1|3|"Moran, Miss. Bertha"|"female"|NA|1|0|"371110"|24.15|""|"Q"
-111|0|1|"Porter, Mr. Walter Chamberlain"|"male"|47|0|0|"110465"|52|"C110"|"S"
-112|0|3|"Zabour, Miss. Hileni"|"female"|14.5|1|0|"2665"|14.4542|""|"C"
-113|0|3|"Barton, Mr. David John"|"male"|22|0|0|"324669"|8.05|""|"S"
-114|0|3|"Jussila, Miss. Katriina"|"female"|20|1|0|"4136"|9.825|""|"S"
-115|0|3|"Attalah, Miss. Malake"|"female"|17|0|0|"2627"|14.4583|""|"C"
-116|0|3|"Pekoniemi, Mr. Edvard"|"male"|21|0|0|"STON/O 2. 3101294"|7.925|""|"S"
-117|0|3|"Connors, Mr. Patrick"|"male"|70.5|0|0|"370369"|7.75|""|"Q"
-118|0|2|"Turpin, Mr. William John Robert"|"male"|29|1|0|"11668"|21|""|"S"
-119|0|1|"Baxter, Mr. Quigg Edmond"|"male"|24|0|1|"PC 17558"|247.5208|"B58 B60"|"C"
-120|0|3|"Andersson, Miss. Ellis Anna Maria"|"female"|2|4|2|"347082"|31.275|""|"S"
-121|0|2|"Hickman, Mr. Stanley George"|"male"|21|2|0|"S.O.C. 14879"|73.5|""|"S"
-122|0|3|"Moore, Mr. Leonard Charles"|"male"|NA|0|0|"A4. 54510"|8.05|""|"S"
-123|0|2|"Nasser, Mr. Nicholas"|"male"|32.5|1|0|"237736"|30.0708|""|"C"
-124|1|2|"Webber, Miss. Susan"|"female"|32.5|0|0|"27267"|13|"E101"|"S"
-125|0|1|"White, Mr. Percival Wayland"|"male"|54|0|1|"35281"|77.2875|"D26"|"S"
-126|1|3|"Nicola-Yarred, Master. Elias"|"male"|12|1|0|"2651"|11.2417|""|"C"
-127|0|3|"McMahon, Mr. Martin"|"male"|NA|0|0|"370372"|7.75|""|"Q"
-128|1|3|"Madsen, Mr. Fridtjof Arne"|"male"|24|0|0|"C 17369"|7.1417|""|"S"
-129|1|3|"Peter, Miss. Anna"|"female"|NA|1|1|"2668"|22.3583|"F E69"|"C"
-130|0|3|"Ekstrom, Mr. Johan"|"male"|45|0|0|"347061"|6.975|""|"S"
-131|0|3|"Drazenoic, Mr. Jozef"|"male"|33|0|0|"349241"|7.8958|""|"C"
-132|0|3|"Coelho, Mr. Domingos Fernandeo"|"male"|20|0|0|"SOTON/O.Q. 3101307"|7.05|""|"S"
-133|0|3|"Robins, Mrs. Alexander A (Grace Charity Laury)"|"female"|47|1|0|"A/5. 3337"|14.5|""|"S"
-134|1|2|"Weisz, Mrs. Leopold (Mathilde Francoise Pede)"|"female"|29|1|0|"228414"|26|""|"S"
-135|0|2|"Sobey, Mr. Samuel James Hayden"|"male"|25|0|0|"C.A. 29178"|13|""|"S"
-136|0|2|"Richard, Mr. Emile"|"male"|23|0|0|"SC/PARIS 2133"|15.0458|""|"C"
-137|1|1|"Newsom, Miss. Helen Monypeny"|"female"|19|0|2|"11752"|26.2833|"D47"|"S"
-138|0|1|"Futrelle, Mr. Jacques Heath"|"male"|37|1|0|"113803"|53.1|"C123"|"S"
-139|0|3|"Osen, Mr. Olaf Elon"|"male"|16|0|0|"7534"|9.2167|""|"S"
-140|0|1|"Giglio, Mr. Victor"|"male"|24|0|0|"PC 17593"|79.2|"B86"|"C"
-141|0|3|"Boulos, Mrs. Joseph (Sultana)"|"female"|NA|0|2|"2678"|15.2458|""|"C"
-142|1|3|"Nysten, Miss. Anna Sofia"|"female"|22|0|0|"347081"|7.75|""|"S"
-143|1|3|"Hakkarainen, Mrs. Pekka Pietari (Elin Matilda Dolck)"|"female"|24|1|0|"STON/O2. 3101279"|15.85|""|"S"
-144|0|3|"Burke, Mr. Jeremiah"|"male"|19|0|0|"365222"|6.75|""|"Q"
-145|0|2|"Andrew, Mr. Edgardo Samuel"|"male"|18|0|0|"231945"|11.5|""|"S"
-146|0|2|"Nicholls, Mr. Joseph Charles"|"male"|19|1|1|"C.A. 33112"|36.75|""|"S"
-147|1|3|"Andersson, Mr. August Edvard (\Wennerstrom\)"|"male"|27|0|0|"350043"|7.7958|""|"S"
-148|0|3|"Ford, Miss. Robina Maggie \Ruby\"|"female"|9|2|2|"W./C. 6608"|34.375|""|"S"
-149|0|2|"Navratil, Mr. Michel (\Louis M Hoffman\)"|"male"|36.5|0|2|"230080"|26|"F2"|"S"
-150|0|2|"Byles, Rev. Thomas Roussel Davids"|"male"|42|0|0|"244310"|13|""|"S"
-151|0|2|"Bateman, Rev. Robert James"|"male"|51|0|0|"S.O.P. 1166"|12.525|""|"S"
-152|1|1|"Pears, Mrs. Thomas (Edith Wearne)"|"female"|22|1|0|"113776"|66.6|"C2"|"S"
-153|0|3|"Meo, Mr. Alfonzo"|"male"|55.5|0|0|"A.5. 11206"|8.05|""|"S"
-154|0|3|"van Billiard, Mr. Austin Blyler"|"male"|40.5|0|2|"A/5. 851"|14.5|""|"S"
-155|0|3|"Olsen, Mr. Ole Martin"|"male"|NA|0|0|"Fa 265302"|7.3125|""|"S"
-156|0|1|"Williams, Mr. Charles Duane"|"male"|51|0|1|"PC 17597"|61.3792|""|"C"
-157|1|3|"Gilnagh, Miss. Katherine \Katie\"|"female"|16|0|0|"35851"|7.7333|""|"Q"
-158|0|3|"Corn, Mr. Harry"|"male"|30|0|0|"SOTON/OQ 392090"|8.05|""|"S"
-159|0|3|"Smiljanic, Mr. Mile"|"male"|NA|0|0|"315037"|8.6625|""|"S"
-160|0|3|"Sage, Master. Thomas Henry"|"male"|NA|8|2|"CA. 2343"|69.55|""|"S"
-161|0|3|"Cribb, Mr. John Hatfield"|"male"|44|0|1|"371362"|16.1|""|"S"
-162|1|2|"Watt, Mrs. James (Elizabeth \Bessie\ Inglis Milne)"|"female"|40|0|0|"C.A. 33595"|15.75|""|"S"
-163|0|3|"Bengtsson, Mr. John Viktor"|"male"|26|0|0|"347068"|7.775|""|"S"
-164|0|3|"Calic, Mr. Jovo"|"male"|17|0|0|"315093"|8.6625|""|"S"
-165|0|3|"Panula, Master. Eino Viljami"|"male"|1|4|1|"3101295"|39.6875|""|"S"
-166|1|3|"Goldsmith, Master. Frank John William \Frankie\"|"male"|9|0|2|"363291"|20.525|""|"S"
-167|1|1|"Chibnall, Mrs. (Edith Martha Bowerman)"|"female"|NA|0|1|"113505"|55|"E33"|"S"
-168|0|3|"Skoog, Mrs. William (Anna Bernhardina Karlsson)"|"female"|45|1|4|"347088"|27.9|""|"S"
-169|0|1|"Baumann, Mr. John D"|"male"|NA|0|0|"PC 17318"|25.925|""|"S"
-170|0|3|"Ling, Mr. Lee"|"male"|28|0|0|"1601"|56.4958|""|"S"
-171|0|1|"Van der hoef, Mr. Wyckoff"|"male"|61|0|0|"111240"|33.5|"B19"|"S"
-172|0|3|"Rice, Master. Arthur"|"male"|4|4|1|"382652"|29.125|""|"Q"
-173|1|3|"Johnson, Miss. Eleanor Ileen"|"female"|1|1|1|"347742"|11.1333|""|"S"
-174|0|3|"Sivola, Mr. Antti Wilhelm"|"male"|21|0|0|"STON/O 2. 3101280"|7.925|""|"S"
-175|0|1|"Smith, Mr. James Clinch"|"male"|56|0|0|"17764"|30.6958|"A7"|"C"
-176|0|3|"Klasen, Mr. Klas Albin"|"male"|18|1|1|"350404"|7.8542|""|"S"
-177|0|3|"Lefebre, Master. Henry Forbes"|"male"|NA|3|1|"4133"|25.4667|""|"S"
-178|0|1|"Isham, Miss. Ann Elizabeth"|"female"|50|0|0|"PC 17595"|28.7125|"C49"|"C"
-179|0|2|"Hale, Mr. Reginald"|"male"|30|0|0|"250653"|13|""|"S"
-180|0|3|"Leonard, Mr. Lionel"|"male"|36|0|0|"LINE"|0|""|"S"
-181|0|3|"Sage, Miss. Constance Gladys"|"female"|NA|8|2|"CA. 2343"|69.55|""|"S"
-182|0|2|"Pernot, Mr. Rene"|"male"|NA|0|0|"SC/PARIS 2131"|15.05|""|"C"
-183|0|3|"Asplund, Master. Clarence Gustaf Hugo"|"male"|9|4|2|"347077"|31.3875|""|"S"
-184|1|2|"Becker, Master. Richard F"|"male"|1|2|1|"230136"|39|"F4"|"S"
-185|1|3|"Kink-Heilmann, Miss. Luise Gretchen"|"female"|4|0|2|"315153"|22.025|""|"S"
-186|0|1|"Rood, Mr. Hugh Roscoe"|"male"|NA|0|0|"113767"|50|"A32"|"S"
-187|1|3|"O'Brien, Mrs. Thomas (Johanna \Hannah\ Godfrey)"|"female"|NA|1|0|"370365"|15.5|""|"Q"
-188|1|1|"Romaine, Mr. Charles Hallace (\Mr C Rolmane\)"|"male"|45|0|0|"111428"|26.55|""|"S"
-189|0|3|"Bourke, Mr. John"|"male"|40|1|1|"364849"|15.5|""|"Q"
-190|0|3|"Turcin, Mr. Stjepan"|"male"|36|0|0|"349247"|7.8958|""|"S"
-191|1|2|"Pinsky, Mrs. (Rosa)"|"female"|32|0|0|"234604"|13|""|"S"
-192|0|2|"Carbines, Mr. William"|"male"|19|0|0|"28424"|13|""|"S"
-193|1|3|"Andersen-Jensen, Miss. Carla Christine Nielsine"|"female"|19|1|0|"350046"|7.8542|""|"S"
-194|1|2|"Navratil, Master. Michel M"|"male"|3|1|1|"230080"|26|"F2"|"S"
-195|1|1|"Brown, Mrs. James Joseph (Margaret Tobin)"|"female"|44|0|0|"PC 17610"|27.7208|"B4"|"C"
-196|1|1|"Lurette, Miss. Elise"|"female"|58|0|0|"PC 17569"|146.5208|"B80"|"C"
-197|0|3|"Mernagh, Mr. Robert"|"male"|NA|0|0|"368703"|7.75|""|"Q"
-198|0|3|"Olsen, Mr. Karl Siegwart Andreas"|"male"|42|0|1|"4579"|8.4042|""|"S"
-199|1|3|"Madigan, Miss. Margaret \Maggie\"|"female"|NA|0|0|"370370"|7.75|""|"Q"
-200|0|2|"Yrois, Miss. Henriette (\Mrs Harbeck\)"|"female"|24|0|0|"248747"|13|""|"S"
-201|0|3|"Vande Walle, Mr. Nestor Cyriel"|"male"|28|0|0|"345770"|9.5|""|"S"
-202|0|3|"Sage, Mr. Frederick"|"male"|NA|8|2|"CA. 2343"|69.55|""|"S"
-203|0|3|"Johanson, Mr. Jakob Alfred"|"male"|34|0|0|"3101264"|6.4958|""|"S"
-204|0|3|"Youseff, Mr. Gerious"|"male"|45.5|0|0|"2628"|7.225|""|"C"
-205|1|3|"Cohen, Mr. Gurshon \Gus\"|"male"|18|0|0|"A/5 3540"|8.05|""|"S"
-206|0|3|"Strom, Miss. Telma Matilda"|"female"|2|0|1|"347054"|10.4625|"G6"|"S"
-207|0|3|"Backstrom, Mr. Karl Alfred"|"male"|32|1|0|"3101278"|15.85|""|"S"
-208|1|3|"Albimona, Mr. Nassef Cassem"|"male"|26|0|0|"2699"|18.7875|""|"C"
-209|1|3|"Carr, Miss. Helen \Ellen\"|"female"|16|0|0|"367231"|7.75|""|"Q"
-210|1|1|"Blank, Mr. Henry"|"male"|40|0|0|"112277"|31|"A31"|"C"
-211|0|3|"Ali, Mr. Ahmed"|"male"|24|0|0|"SOTON/O.Q. 3101311"|7.05|""|"S"
-212|1|2|"Cameron, Miss. Clear Annie"|"female"|35|0|0|"F.C.C. 13528"|21|""|"S"
-213|0|3|"Perkin, Mr. John Henry"|"male"|22|0|0|"A/5 21174"|7.25|""|"S"
-214|0|2|"Givard, Mr. Hans Kristensen"|"male"|30|0|0|"250646"|13|""|"S"
-215|0|3|"Kiernan, Mr. Philip"|"male"|NA|1|0|"367229"|7.75|""|"Q"
-216|1|1|"Newell, Miss. Madeleine"|"female"|31|1|0|"35273"|113.275|"D36"|"C"
-217|1|3|"Honkanen, Miss. Eliina"|"female"|27|0|0|"STON/O2. 3101283"|7.925|""|"S"
-218|0|2|"Jacobsohn, Mr. Sidney Samuel"|"male"|42|1|0|"243847"|27|""|"S"
-219|1|1|"Bazzani, Miss. Albina"|"female"|32|0|0|"11813"|76.2917|"D15"|"C"
-220|0|2|"Harris, Mr. Walter"|"male"|30|0|0|"W/C 14208"|10.5|""|"S"
-221|1|3|"Sunderland, Mr. Victor Francis"|"male"|16|0|0|"SOTON/OQ 392089"|8.05|""|"S"
-222|0|2|"Bracken, Mr. James H"|"male"|27|0|0|"220367"|13|""|"S"
-223|0|3|"Green, Mr. George Henry"|"male"|51|0|0|"21440"|8.05|""|"S"
-224|0|3|"Nenkoff, Mr. Christo"|"male"|NA|0|0|"349234"|7.8958|""|"S"
-225|1|1|"Hoyt, Mr. Frederick Maxfield"|"male"|38|1|0|"19943"|90|"C93"|"S"
-226|0|3|"Berglund, Mr. Karl Ivar Sven"|"male"|22|0|0|"PP 4348"|9.35|""|"S"
-227|1|2|"Mellors, Mr. William John"|"male"|19|0|0|"SW/PP 751"|10.5|""|"S"
-228|0|3|"Lovell, Mr. John Hall (\Henry\)"|"male"|20.5|0|0|"A/5 21173"|7.25|""|"S"
-229|0|2|"Fahlstrom, Mr. Arne Jonas"|"male"|18|0|0|"236171"|13|""|"S"
-230|0|3|"Lefebre, Miss. Mathilde"|"female"|NA|3|1|"4133"|25.4667|""|"S"
-231|1|1|"Harris, Mrs. Henry Birkhardt (Irene Wallach)"|"female"|35|1|0|"36973"|83.475|"C83"|"S"
-232|0|3|"Larsson, Mr. Bengt Edvin"|"male"|29|0|0|"347067"|7.775|""|"S"
-233|0|2|"Sjostedt, Mr. Ernst Adolf"|"male"|59|0|0|"237442"|13.5|""|"S"
-234|1|3|"Asplund, Miss. Lillian Gertrud"|"female"|5|4|2|"347077"|31.3875|""|"S"
-235|0|2|"Leyson, Mr. Robert William Norman"|"male"|24|0|0|"C.A. 29566"|10.5|""|"S"
-236|0|3|"Harknett, Miss. Alice Phoebe"|"female"|NA|0|0|"W./C. 6609"|7.55|""|"S"
-237|0|2|"Hold, Mr. Stephen"|"male"|44|1|0|"26707"|26|""|"S"
-238|1|2|"Collyer, Miss. Marjorie \Lottie\"|"female"|8|0|2|"C.A. 31921"|26.25|""|"S"
-239|0|2|"Pengelly, Mr. Frederick William"|"male"|19|0|0|"28665"|10.5|""|"S"
-240|0|2|"Hunt, Mr. George Henry"|"male"|33|0|0|"SCO/W 1585"|12.275|""|"S"
-241|0|3|"Zabour, Miss. Thamine"|"female"|NA|1|0|"2665"|14.4542|""|"C"
-242|1|3|"Murphy, Miss. Katherine \Kate\"|"female"|NA|1|0|"367230"|15.5|""|"Q"
-243|0|2|"Coleridge, Mr. Reginald Charles"|"male"|29|0|0|"W./C. 14263"|10.5|""|"S"
-244|0|3|"Maenpaa, Mr. Matti Alexanteri"|"male"|22|0|0|"STON/O 2. 3101275"|7.125|""|"S"
-245|0|3|"Attalah, Mr. Sleiman"|"male"|30|0|0|"2694"|7.225|""|"C"
-246|0|1|"Minahan, Dr. William Edward"|"male"|44|2|0|"19928"|90|"C78"|"Q"
-247|0|3|"Lindahl, Miss. Agda Thorilda Viktoria"|"female"|25|0|0|"347071"|7.775|""|"S"
-248|1|2|"Hamalainen, Mrs. William (Anna)"|"female"|24|0|2|"250649"|14.5|""|"S"
-249|1|1|"Beckwith, Mr. Richard Leonard"|"male"|37|1|1|"11751"|52.5542|"D35"|"S"
-250|0|2|"Carter, Rev. Ernest Courtenay"|"male"|54|1|0|"244252"|26|""|"S"
-251|0|3|"Reed, Mr. James George"|"male"|NA|0|0|"362316"|7.25|""|"S"
-252|0|3|"Strom, Mrs. Wilhelm (Elna Matilda Persson)"|"female"|29|1|1|"347054"|10.4625|"G6"|"S"
-253|0|1|"Stead, Mr. William Thomas"|"male"|62|0|0|"113514"|26.55|"C87"|"S"
-254|0|3|"Lobb, Mr. William Arthur"|"male"|30|1|0|"A/5. 3336"|16.1|""|"S"
-255|0|3|"Rosblom, Mrs. Viktor (Helena Wilhelmina)"|"female"|41|0|2|"370129"|20.2125|""|"S"
-256|1|3|"Touma, Mrs. Darwis (Hanne Youssef Razi)"|"female"|29|0|2|"2650"|15.2458|""|"C"
-257|1|1|"Thorne, Mrs. Gertrude Maybelle"|"female"|NA|0|0|"PC 17585"|79.2|""|"C"
-258|1|1|"Cherry, Miss. Gladys"|"female"|30|0|0|"110152"|86.5|"B77"|"S"
-259|1|1|"Ward, Miss. Anna"|"female"|35|0|0|"PC 17755"|512.3292|""|"C"
-260|1|2|"Parrish, Mrs. (Lutie Davis)"|"female"|50|0|1|"230433"|26|""|"S"
-261|0|3|"Smith, Mr. Thomas"|"male"|NA|0|0|"384461"|7.75|""|"Q"
-262|1|3|"Asplund, Master. Edvin Rojj Felix"|"male"|3|4|2|"347077"|31.3875|""|"S"
-263|0|1|"Taussig, Mr. Emil"|"male"|52|1|1|"110413"|79.65|"E67"|"S"
-264|0|1|"Harrison, Mr. William"|"male"|40|0|0|"112059"|0|"B94"|"S"
-265|0|3|"Henry, Miss. Delia"|"female"|NA|0|0|"382649"|7.75|""|"Q"
-266|0|2|"Reeves, Mr. David"|"male"|36|0|0|"C.A. 17248"|10.5|""|"S"
-267|0|3|"Panula, Mr. Ernesti Arvid"|"male"|16|4|1|"3101295"|39.6875|""|"S"
-268|1|3|"Persson, Mr. Ernst Ulrik"|"male"|25|1|0|"347083"|7.775|""|"S"
-269|1|1|"Graham, Mrs. William Thompson (Edith Junkins)"|"female"|58|0|1|"PC 17582"|153.4625|"C125"|"S"
-270|1|1|"Bissette, Miss. Amelia"|"female"|35|0|0|"PC 17760"|135.6333|"C99"|"S"
-271|0|1|"Cairns, Mr. Alexander"|"male"|NA|0|0|"113798"|31|""|"S"
-272|1|3|"Tornquist, Mr. William Henry"|"male"|25|0|0|"LINE"|0|""|"S"
-273|1|2|"Mellinger, Mrs. (Elizabeth Anne Maidment)"|"female"|41|0|1|"250644"|19.5|""|"S"
-274|0|1|"Natsch, Mr. Charles H"|"male"|37|0|1|"PC 17596"|29.7|"C118"|"C"
-275|1|3|"Healy, Miss. Hanora \Nora\"|"female"|NA|0|0|"370375"|7.75|""|"Q"
-276|1|1|"Andrews, Miss. Kornelia Theodosia"|"female"|63|1|0|"13502"|77.9583|"D7"|"S"
-277|0|3|"Lindblom, Miss. Augusta Charlotta"|"female"|45|0|0|"347073"|7.75|""|"S"
-278|0|2|"Parkes, Mr. Francis \Frank\"|"male"|NA|0|0|"239853"|0|""|"S"
-279|0|3|"Rice, Master. Eric"|"male"|7|4|1|"382652"|29.125|""|"Q"
-280|1|3|"Abbott, Mrs. Stanton (Rosa Hunt)"|"female"|35|1|1|"C.A. 2673"|20.25|""|"S"
-281|0|3|"Duane, Mr. Frank"|"male"|65|0|0|"336439"|7.75|""|"Q"
-282|0|3|"Olsson, Mr. Nils Johan Goransson"|"male"|28|0|0|"347464"|7.8542|""|"S"
-283|0|3|"de Pelsmaeker, Mr. Alfons"|"male"|16|0|0|"345778"|9.5|""|"S"
-284|1|3|"Dorking, Mr. Edward Arthur"|"male"|19|0|0|"A/5. 10482"|8.05|""|"S"
-285|0|1|"Smith, Mr. Richard William"|"male"|NA|0|0|"113056"|26|"A19"|"S"
-286|0|3|"Stankovic, Mr. Ivan"|"male"|33|0|0|"349239"|8.6625|""|"C"
-287|1|3|"de Mulder, Mr. Theodore"|"male"|30|0|0|"345774"|9.5|""|"S"
-288|0|3|"Naidenoff, Mr. Penko"|"male"|22|0|0|"349206"|7.8958|""|"S"
-289|1|2|"Hosono, Mr. Masabumi"|"male"|42|0|0|"237798"|13|""|"S"
-290|1|3|"Connolly, Miss. Kate"|"female"|22|0|0|"370373"|7.75|""|"Q"
-291|1|1|"Barber, Miss. Ellen \Nellie\"|"female"|26|0|0|"19877"|78.85|""|"S"
-292|1|1|"Bishop, Mrs. Dickinson H (Helen Walton)"|"female"|19|1|0|"11967"|91.0792|"B49"|"C"
-293|0|2|"Levy, Mr. Rene Jacques"|"male"|36|0|0|"SC/Paris 2163"|12.875|"D"|"C"
-294|0|3|"Haas, Miss. Aloisia"|"female"|24|0|0|"349236"|8.85|""|"S"
-295|0|3|"Mineff, Mr. Ivan"|"male"|24|0|0|"349233"|7.8958|""|"S"
-296|0|1|"Lewy, Mr. Ervin G"|"male"|NA|0|0|"PC 17612"|27.7208|""|"C"
-297|0|3|"Hanna, Mr. Mansour"|"male"|23.5|0|0|"2693"|7.2292|""|"C"
-298|0|1|"Allison, Miss. Helen Loraine"|"female"|2|1|2|"113781"|151.55|"C22 C26"|"S"
-299|1|1|"Saalfeld, Mr. Adolphe"|"male"|NA|0|0|"19988"|30.5|"C106"|"S"
-300|1|1|"Baxter, Mrs. James (Helene DeLaudeniere Chaput)"|"female"|50|0|1|"PC 17558"|247.5208|"B58 B60"|"C"
-301|1|3|"Kelly, Miss. Anna Katherine \Annie Kate\"|"female"|NA|0|0|"9234"|7.75|""|"Q"
-302|1|3|"McCoy, Mr. Bernard"|"male"|NA|2|0|"367226"|23.25|""|"Q"
-303|0|3|"Johnson, Mr. William Cahoone Jr"|"male"|19|0|0|"LINE"|0|""|"S"
-304|1|2|"Keane, Miss. Nora A"|"female"|NA|0|0|"226593"|12.35|"E101"|"Q"
-305|0|3|"Williams, Mr. Howard Hugh \Harry\"|"male"|NA|0|0|"A/5 2466"|8.05|""|"S"
-306|1|1|"Allison, Master. Hudson Trevor"|"male"|0.92|1|2|"113781"|151.55|"C22 C26"|"S"
-307|1|1|"Fleming, Miss. Margaret"|"female"|NA|0|0|"17421"|110.8833|""|"C"
-308|1|1|"Penasco y Castellana, Mrs. Victor de Satode (Maria Josefa Perez de Soto y Vallejo)"|"female"|17|1|0|"PC 17758"|108.9|"C65"|"C"
-309|0|2|"Abelson, Mr. Samuel"|"male"|30|1|0|"P/PP 3381"|24|""|"C"
-310|1|1|"Francatelli, Miss. Laura Mabel"|"female"|30|0|0|"PC 17485"|56.9292|"E36"|"C"
-311|1|1|"Hays, Miss. Margaret Bechstein"|"female"|24|0|0|"11767"|83.1583|"C54"|"C"
-312|1|1|"Ryerson, Miss. Emily Borie"|"female"|18|2|2|"PC 17608"|262.375|"B57 B59 B63 B66"|"C"
-313|0|2|"Lahtinen, Mrs. William (Anna Sylfven)"|"female"|26|1|1|"250651"|26|""|"S"
-314|0|3|"Hendekovic, Mr. Ignjac"|"male"|28|0|0|"349243"|7.8958|""|"S"
-315|0|2|"Hart, Mr. Benjamin"|"male"|43|1|1|"F.C.C. 13529"|26.25|""|"S"
-316|1|3|"Nilsson, Miss. Helmina Josefina"|"female"|26|0|0|"347470"|7.8542|""|"S"
-317|1|2|"Kantor, Mrs. Sinai (Miriam Sternin)"|"female"|24|1|0|"244367"|26|""|"S"
-318|0|2|"Moraweck, Dr. Ernest"|"male"|54|0|0|"29011"|14|""|"S"
-319|1|1|"Wick, Miss. Mary Natalie"|"female"|31|0|2|"36928"|164.8667|"C7"|"S"
-320|1|1|"Spedden, Mrs. Frederic Oakley (Margaretta Corning Stone)"|"female"|40|1|1|"16966"|134.5|"E34"|"C"
-321|0|3|"Dennis, Mr. Samuel"|"male"|22|0|0|"A/5 21172"|7.25|""|"S"
-322|0|3|"Danoff, Mr. Yoto"|"male"|27|0|0|"349219"|7.8958|""|"S"
-323|1|2|"Slayter, Miss. Hilda Mary"|"female"|30|0|0|"234818"|12.35|""|"Q"
-324|1|2|"Caldwell, Mrs. Albert Francis (Sylvia Mae Harbaugh)"|"female"|22|1|1|"248738"|29|""|"S"
-325|0|3|"Sage, Mr. George John Jr"|"male"|NA|8|2|"CA. 2343"|69.55|""|"S"
-326|1|1|"Young, Miss. Marie Grice"|"female"|36|0|0|"PC 17760"|135.6333|"C32"|"C"
-327|0|3|"Nysveen, Mr. Johan Hansen"|"male"|61|0|0|"345364"|6.2375|""|"S"
-328|1|2|"Ball, Mrs. (Ada E Hall)"|"female"|36|0|0|"28551"|13|"D"|"S"
-329|1|3|"Goldsmith, Mrs. Frank John (Emily Alice Brown)"|"female"|31|1|1|"363291"|20.525|""|"S"
-330|1|1|"Hippach, Miss. Jean Gertrude"|"female"|16|0|1|"111361"|57.9792|"B18"|"C"
-331|1|3|"McCoy, Miss. Agnes"|"female"|NA|2|0|"367226"|23.25|""|"Q"
-332|0|1|"Partner, Mr. Austen"|"male"|45.5|0|0|"113043"|28.5|"C124"|"S"
-333|0|1|"Graham, Mr. George Edward"|"male"|38|0|1|"PC 17582"|153.4625|"C91"|"S"
-334|0|3|"Vander Planke, Mr. Leo Edmondus"|"male"|16|2|0|"345764"|18|""|"S"
-335|1|1|"Frauenthal, Mrs. Henry William (Clara Heinsheimer)"|"female"|NA|1|0|"PC 17611"|133.65|""|"S"
-336|0|3|"Denkoff, Mr. Mitto"|"male"|NA|0|0|"349225"|7.8958|""|"S"
-337|0|1|"Pears, Mr. Thomas Clinton"|"male"|29|1|0|"113776"|66.6|"C2"|"S"
-338|1|1|"Burns, Miss. Elizabeth Margaret"|"female"|41|0|0|"16966"|134.5|"E40"|"C"
-339|1|3|"Dahl, Mr. Karl Edwart"|"male"|45|0|0|"7598"|8.05|""|"S"
-340|0|1|"Blackwell, Mr. Stephen Weart"|"male"|45|0|0|"113784"|35.5|"T"|"S"
-341|1|2|"Navratil, Master. Edmond Roger"|"male"|2|1|1|"230080"|26|"F2"|"S"
-342|1|1|"Fortune, Miss. Alice Elizabeth"|"female"|24|3|2|"19950"|263|"C23 C25 C27"|"S"
-343|0|2|"Collander, Mr. Erik Gustaf"|"male"|28|0|0|"248740"|13|""|"S"
-344|0|2|"Sedgwick, Mr. Charles Frederick Waddington"|"male"|25|0|0|"244361"|13|""|"S"
-345|0|2|"Fox, Mr. Stanley Hubert"|"male"|36|0|0|"229236"|13|""|"S"
-346|1|2|"Brown, Miss. Amelia \Mildred\"|"female"|24|0|0|"248733"|13|"F33"|"S"
-347|1|2|"Smith, Miss. Marion Elsie"|"female"|40|0|0|"31418"|13|""|"S"
-348|1|3|"Davison, Mrs. Thomas Henry (Mary E Finck)"|"female"|NA|1|0|"386525"|16.1|""|"S"
-349|1|3|"Coutts, Master. William Loch \William\"|"male"|3|1|1|"C.A. 37671"|15.9|""|"S"
-350|0|3|"Dimic, Mr. Jovan"|"male"|42|0|0|"315088"|8.6625|""|"S"
-351|0|3|"Odahl, Mr. Nils Martin"|"male"|23|0|0|"7267"|9.225|""|"S"
-352|0|1|"Williams-Lambert, Mr. Fletcher Fellows"|"male"|NA|0|0|"113510"|35|"C128"|"S"
-353|0|3|"Elias, Mr. Tannous"|"male"|15|1|1|"2695"|7.2292|""|"C"
-354|0|3|"Arnold-Franchi, Mr. Josef"|"male"|25|1|0|"349237"|17.8|""|"S"
-355|0|3|"Yousif, Mr. Wazli"|"male"|NA|0|0|"2647"|7.225|""|"C"
-356|0|3|"Vanden Steen, Mr. Leo Peter"|"male"|28|0|0|"345783"|9.5|""|"S"
-357|1|1|"Bowerman, Miss. Elsie Edith"|"female"|22|0|1|"113505"|55|"E33"|"S"
-358|0|2|"Funk, Miss. Annie Clemmer"|"female"|38|0|0|"237671"|13|""|"S"
-359|1|3|"McGovern, Miss. Mary"|"female"|NA|0|0|"330931"|7.8792|""|"Q"
-360|1|3|"Mockler, Miss. Helen Mary \Ellie\"|"female"|NA|0|0|"330980"|7.8792|""|"Q"
-361|0|3|"Skoog, Mr. Wilhelm"|"male"|40|1|4|"347088"|27.9|""|"S"
-362|0|2|"del Carlo, Mr. Sebastiano"|"male"|29|1|0|"SC/PARIS 2167"|27.7208|""|"C"
-363|0|3|"Barbara, Mrs. (Catherine David)"|"female"|45|0|1|"2691"|14.4542|""|"C"
-364|0|3|"Asim, Mr. Adola"|"male"|35|0|0|"SOTON/O.Q. 3101310"|7.05|""|"S"
-365|0|3|"O'Brien, Mr. Thomas"|"male"|NA|1|0|"370365"|15.5|""|"Q"
-366|0|3|"Adahl, Mr. Mauritz Nils Martin"|"male"|30|0|0|"C 7076"|7.25|""|"S"
-367|1|1|"Warren, Mrs. Frank Manley (Anna Sophia Atkinson)"|"female"|60|1|0|"110813"|75.25|"D37"|"C"
-368|1|3|"Moussa, Mrs. (Mantoura Boulos)"|"female"|NA|0|0|"2626"|7.2292|""|"C"
-369|1|3|"Jermyn, Miss. Annie"|"female"|NA|0|0|"14313"|7.75|""|"Q"
-370|1|1|"Aubart, Mme. Leontine Pauline"|"female"|24|0|0|"PC 17477"|69.3|"B35"|"C"
-371|1|1|"Harder, Mr. George Achilles"|"male"|25|1|0|"11765"|55.4417|"E50"|"C"
-372|0|3|"Wiklund, Mr. Jakob Alfred"|"male"|18|1|0|"3101267"|6.4958|""|"S"
-373|0|3|"Beavan, Mr. William Thomas"|"male"|19|0|0|"323951"|8.05|""|"S"
-374|0|1|"Ringhini, Mr. Sante"|"male"|22|0|0|"PC 17760"|135.6333|""|"C"
-375|0|3|"Palsson, Miss. Stina Viola"|"female"|3|3|1|"349909"|21.075|""|"S"
-376|1|1|"Meyer, Mrs. Edgar Joseph (Leila Saks)"|"female"|NA|1|0|"PC 17604"|82.1708|""|"C"
-377|1|3|"Landergren, Miss. Aurora Adelia"|"female"|22|0|0|"C 7077"|7.25|""|"S"
-378|0|1|"Widener, Mr. Harry Elkins"|"male"|27|0|2|"113503"|211.5|"C82"|"C"
-379|0|3|"Betros, Mr. Tannous"|"male"|20|0|0|"2648"|4.0125|""|"C"
-380|0|3|"Gustafsson, Mr. Karl Gideon"|"male"|19|0|0|"347069"|7.775|""|"S"
-381|1|1|"Bidois, Miss. Rosalie"|"female"|42|0|0|"PC 17757"|227.525|""|"C"
-382|1|3|"Nakid, Miss. Maria (\Mary\)"|"female"|1|0|2|"2653"|15.7417|""|"C"
-383|0|3|"Tikkanen, Mr. Juho"|"male"|32|0|0|"STON/O 2. 3101293"|7.925|""|"S"
-384|1|1|"Holverson, Mrs. Alexander Oskar (Mary Aline Towner)"|"female"|35|1|0|"113789"|52|""|"S"
-385|0|3|"Plotcharsky, Mr. Vasil"|"male"|NA|0|0|"349227"|7.8958|""|"S"
-386|0|2|"Davies, Mr. Charles Henry"|"male"|18|0|0|"S.O.C. 14879"|73.5|""|"S"
-387|0|3|"Goodwin, Master. Sidney Leonard"|"male"|1|5|2|"CA 2144"|46.9|""|"S"
-388|1|2|"Buss, Miss. Kate"|"female"|36|0|0|"27849"|13|""|"S"
-389|0|3|"Sadlier, Mr. Matthew"|"male"|NA|0|0|"367655"|7.7292|""|"Q"
-390|1|2|"Lehmann, Miss. Bertha"|"female"|17|0|0|"SC 1748"|12|""|"C"
-391|1|1|"Carter, Mr. William Ernest"|"male"|36|1|2|"113760"|120|"B96 B98"|"S"
-392|1|3|"Jansson, Mr. Carl Olof"|"male"|21|0|0|"350034"|7.7958|""|"S"
-393|0|3|"Gustafsson, Mr. Johan Birger"|"male"|28|2|0|"3101277"|7.925|""|"S"
-394|1|1|"Newell, Miss. Marjorie"|"female"|23|1|0|"35273"|113.275|"D36"|"C"
-395|1|3|"Sandstrom, Mrs. Hjalmar (Agnes Charlotta Bengtsson)"|"female"|24|0|2|"PP 9549"|16.7|"G6"|"S"
-396|0|3|"Johansson, Mr. Erik"|"male"|22|0|0|"350052"|7.7958|""|"S"
-397|0|3|"Olsson, Miss. Elina"|"female"|31|0|0|"350407"|7.8542|""|"S"
-398|0|2|"McKane, Mr. Peter David"|"male"|46|0|0|"28403"|26|""|"S"
-399|0|2|"Pain, Dr. Alfred"|"male"|23|0|0|"244278"|10.5|""|"S"
-400|1|2|"Trout, Mrs. William H (Jessie L)"|"female"|28|0|0|"240929"|12.65|""|"S"
-401|1|3|"Niskanen, Mr. Juha"|"male"|39|0|0|"STON/O 2. 3101289"|7.925|""|"S"
-402|0|3|"Adams, Mr. John"|"male"|26|0|0|"341826"|8.05|""|"S"
-403|0|3|"Jussila, Miss. Mari Aina"|"female"|21|1|0|"4137"|9.825|""|"S"
-404|0|3|"Hakkarainen, Mr. Pekka Pietari"|"male"|28|1|0|"STON/O2. 3101279"|15.85|""|"S"
-405|0|3|"Oreskovic, Miss. Marija"|"female"|20|0|0|"315096"|8.6625|""|"S"
-406|0|2|"Gale, Mr. Shadrach"|"male"|34|1|0|"28664"|21|""|"S"
-407|0|3|"Widegren, Mr. Carl/Charles Peter"|"male"|51|0|0|"347064"|7.75|""|"S"
-408|1|2|"Richards, Master. William Rowe"|"male"|3|1|1|"29106"|18.75|""|"S"
-409|0|3|"Birkeland, Mr. Hans Martin Monsen"|"male"|21|0|0|"312992"|7.775|""|"S"
-410|0|3|"Lefebre, Miss. Ida"|"female"|NA|3|1|"4133"|25.4667|""|"S"
-411|0|3|"Sdycoff, Mr. Todor"|"male"|NA|0|0|"349222"|7.8958|""|"S"
-412|0|3|"Hart, Mr. Henry"|"male"|NA|0|0|"394140"|6.8583|""|"Q"
-413|1|1|"Minahan, Miss. Daisy E"|"female"|33|1|0|"19928"|90|"C78"|"Q"
-414|0|2|"Cunningham, Mr. Alfred Fleming"|"male"|NA|0|0|"239853"|0|""|"S"
-415|1|3|"Sundman, Mr. Johan Julian"|"male"|44|0|0|"STON/O 2. 3101269"|7.925|""|"S"
-416|0|3|"Meek, Mrs. Thomas (Annie Louise Rowley)"|"female"|NA|0|0|"343095"|8.05|""|"S"
-417|1|2|"Drew, Mrs. James Vivian (Lulu Thorne Christian)"|"female"|34|1|1|"28220"|32.5|""|"S"
-418|1|2|"Silven, Miss. Lyyli Karoliina"|"female"|18|0|2|"250652"|13|""|"S"
-419|0|2|"Matthews, Mr. William John"|"male"|30|0|0|"28228"|13|""|"S"
-420|0|3|"Van Impe, Miss. Catharina"|"female"|10|0|2|"345773"|24.15|""|"S"
-421|0|3|"Gheorgheff, Mr. Stanio"|"male"|NA|0|0|"349254"|7.8958|""|"C"
-422|0|3|"Charters, Mr. David"|"male"|21|0|0|"A/5. 13032"|7.7333|""|"Q"
-423|0|3|"Zimmerman, Mr. Leo"|"male"|29|0|0|"315082"|7.875|""|"S"
-424|0|3|"Danbom, Mrs. Ernst Gilbert (Anna Sigrid Maria Brogren)"|"female"|28|1|1|"347080"|14.4|""|"S"
-425|0|3|"Rosblom, Mr. Viktor Richard"|"male"|18|1|1|"370129"|20.2125|""|"S"
-426|0|3|"Wiseman, Mr. Phillippe"|"male"|NA|0|0|"A/4. 34244"|7.25|""|"S"
-427|1|2|"Clarke, Mrs. Charles V (Ada Maria Winfield)"|"female"|28|1|0|"2003"|26|""|"S"
-428|1|2|"Phillips, Miss. Kate Florence (\Mrs Kate Louise Phillips Marshall\)"|"female"|19|0|0|"250655"|26|""|"S"
-429|0|3|"Flynn, Mr. James"|"male"|NA|0|0|"364851"|7.75|""|"Q"
-430|1|3|"Pickard, Mr. Berk (Berk Trembisky)"|"male"|32|0|0|"SOTON/O.Q. 392078"|8.05|"E10"|"S"
-431|1|1|"Bjornstrom-Steffansson, Mr. Mauritz Hakan"|"male"|28|0|0|"110564"|26.55|"C52"|"S"
-432|1|3|"Thorneycroft, Mrs. Percival (Florence Kate White)"|"female"|NA|1|0|"376564"|16.1|""|"S"
-433|1|2|"Louch, Mrs. Charles Alexander (Alice Adelaide Slow)"|"female"|42|1|0|"SC/AH 3085"|26|""|"S"
-434|0|3|"Kallio, Mr. Nikolai Erland"|"male"|17|0|0|"STON/O 2. 3101274"|7.125|""|"S"
-435|0|1|"Silvey, Mr. William Baird"|"male"|50|1|0|"13507"|55.9|"E44"|"S"
-436|1|1|"Carter, Miss. Lucile Polk"|"female"|14|1|2|"113760"|120|"B96 B98"|"S"
-437|0|3|"Ford, Miss. Doolina Margaret \Daisy\"|"female"|21|2|2|"W./C. 6608"|34.375|""|"S"
-438|1|2|"Richards, Mrs. Sidney (Emily Hocking)"|"female"|24|2|3|"29106"|18.75|""|"S"
-439|0|1|"Fortune, Mr. Mark"|"male"|64|1|4|"19950"|263|"C23 C25 C27"|"S"
-440|0|2|"Kvillner, Mr. Johan Henrik Johannesson"|"male"|31|0|0|"C.A. 18723"|10.5|""|"S"
-441|1|2|"Hart, Mrs. Benjamin (Esther Ada Bloomfield)"|"female"|45|1|1|"F.C.C. 13529"|26.25|""|"S"
-442|0|3|"Hampe, Mr. Leon"|"male"|20|0|0|"345769"|9.5|""|"S"
-443|0|3|"Petterson, Mr. Johan Emil"|"male"|25|1|0|"347076"|7.775|""|"S"
-444|1|2|"Reynaldo, Ms. Encarnacion"|"female"|28|0|0|"230434"|13|""|"S"
-445|1|3|"Johannesen-Bratthammer, Mr. Bernt"|"male"|NA|0|0|"65306"|8.1125|""|"S"
-446|1|1|"Dodge, Master. Washington"|"male"|4|0|2|"33638"|81.8583|"A34"|"S"
-447|1|2|"Mellinger, Miss. Madeleine Violet"|"female"|13|0|1|"250644"|19.5|""|"S"
-448|1|1|"Seward, Mr. Frederic Kimber"|"male"|34|0|0|"113794"|26.55|""|"S"
-449|1|3|"Baclini, Miss. Marie Catherine"|"female"|5|2|1|"2666"|19.2583|""|"C"
-450|1|1|"Peuchen, Major. Arthur Godfrey"|"male"|52|0|0|"113786"|30.5|"C104"|"S"
-451|0|2|"West, Mr. Edwy Arthur"|"male"|36|1|2|"C.A. 34651"|27.75|""|"S"
-452|0|3|"Hagland, Mr. Ingvald Olai Olsen"|"male"|NA|1|0|"65303"|19.9667|""|"S"
-453|0|1|"Foreman, Mr. Benjamin Laventall"|"male"|30|0|0|"113051"|27.75|"C111"|"C"
-454|1|1|"Goldenberg, Mr. Samuel L"|"male"|49|1|0|"17453"|89.1042|"C92"|"C"
-455|0|3|"Peduzzi, Mr. Joseph"|"male"|NA|0|0|"A/5 2817"|8.05|""|"S"
-456|1|3|"Jalsevac, Mr. Ivan"|"male"|29|0|0|"349240"|7.8958|""|"C"
-457|0|1|"Millet, Mr. Francis Davis"|"male"|65|0|0|"13509"|26.55|"E38"|"S"
-458|1|1|"Kenyon, Mrs. Frederick R (Marion)"|"female"|NA|1|0|"17464"|51.8625|"D21"|"S"
-459|1|2|"Toomey, Miss. Ellen"|"female"|50|0|0|"F.C.C. 13531"|10.5|""|"S"
-460|0|3|"O'Connor, Mr. Maurice"|"male"|NA|0|0|"371060"|7.75|""|"Q"
-461|1|1|"Anderson, Mr. Harry"|"male"|48|0|0|"19952"|26.55|"E12"|"S"
-462|0|3|"Morley, Mr. William"|"male"|34|0|0|"364506"|8.05|""|"S"
-463|0|1|"Gee, Mr. Arthur H"|"male"|47|0|0|"111320"|38.5|"E63"|"S"
-464|0|2|"Milling, Mr. Jacob Christian"|"male"|48|0|0|"234360"|13|""|"S"
-465|0|3|"Maisner, Mr. Simon"|"male"|NA|0|0|"A/S 2816"|8.05|""|"S"
-466|0|3|"Goncalves, Mr. Manuel Estanslas"|"male"|38|0|0|"SOTON/O.Q. 3101306"|7.05|""|"S"
-467|0|2|"Campbell, Mr. William"|"male"|NA|0|0|"239853"|0|""|"S"
-468|0|1|"Smart, Mr. John Montgomery"|"male"|56|0|0|"113792"|26.55|""|"S"
-469|0|3|"Scanlan, Mr. James"|"male"|NA|0|0|"36209"|7.725|""|"Q"
-470|1|3|"Baclini, Miss. Helene Barbara"|"female"|0.75|2|1|"2666"|19.2583|""|"C"
-471|0|3|"Keefe, Mr. Arthur"|"male"|NA|0|0|"323592"|7.25|""|"S"
-472|0|3|"Cacic, Mr. Luka"|"male"|38|0|0|"315089"|8.6625|""|"S"
-473|1|2|"West, Mrs. Edwy Arthur (Ada Mary Worth)"|"female"|33|1|2|"C.A. 34651"|27.75|""|"S"
-474|1|2|"Jerwan, Mrs. Amin S (Marie Marthe Thuillard)"|"female"|23|0|0|"SC/AH Basle 541"|13.7917|"D"|"C"
-475|0|3|"Strandberg, Miss. Ida Sofia"|"female"|22|0|0|"7553"|9.8375|""|"S"
-476|0|1|"Clifford, Mr. George Quincy"|"male"|NA|0|0|"110465"|52|"A14"|"S"
-477|0|2|"Renouf, Mr. Peter Henry"|"male"|34|1|0|"31027"|21|""|"S"
-478|0|3|"Braund, Mr. Lewis Richard"|"male"|29|1|0|"3460"|7.0458|""|"S"
-479|0|3|"Karlsson, Mr. Nils August"|"male"|22|0|0|"350060"|7.5208|""|"S"
-480|1|3|"Hirvonen, Miss. Hildur E"|"female"|2|0|1|"3101298"|12.2875|""|"S"
-481|0|3|"Goodwin, Master. Harold Victor"|"male"|9|5|2|"CA 2144"|46.9|""|"S"
-482|0|2|"Frost, Mr. Anthony Wood \Archie\"|"male"|NA|0|0|"239854"|0|""|"S"
-483|0|3|"Rouse, Mr. Richard Henry"|"male"|50|0|0|"A/5 3594"|8.05|""|"S"
-484|1|3|"Turkula, Mrs. (Hedwig)"|"female"|63|0|0|"4134"|9.5875|""|"S"
-485|1|1|"Bishop, Mr. Dickinson H"|"male"|25|1|0|"11967"|91.0792|"B49"|"C"
-486|0|3|"Lefebre, Miss. Jeannie"|"female"|NA|3|1|"4133"|25.4667|""|"S"
-487|1|1|"Hoyt, Mrs. Frederick Maxfield (Jane Anne Forby)"|"female"|35|1|0|"19943"|90|"C93"|"S"
-488|0|1|"Kent, Mr. Edward Austin"|"male"|58|0|0|"11771"|29.7|"B37"|"C"
-489|0|3|"Somerton, Mr. Francis William"|"male"|30|0|0|"A.5. 18509"|8.05|""|"S"
-490|1|3|"Coutts, Master. Eden Leslie \Neville\"|"male"|9|1|1|"C.A. 37671"|15.9|""|"S"
-491|0|3|"Hagland, Mr. Konrad Mathias Reiersen"|"male"|NA|1|0|"65304"|19.9667|""|"S"
-492|0|3|"Windelov, Mr. Einar"|"male"|21|0|0|"SOTON/OQ 3101317"|7.25|""|"S"
-493|0|1|"Molson, Mr. Harry Markland"|"male"|55|0|0|"113787"|30.5|"C30"|"S"
-494|0|1|"Artagaveytia, Mr. Ramon"|"male"|71|0|0|"PC 17609"|49.5042|""|"C"
-495|0|3|"Stanley, Mr. Edward Roland"|"male"|21|0|0|"A/4 45380"|8.05|""|"S"
-496|0|3|"Yousseff, Mr. Gerious"|"male"|NA|0|0|"2627"|14.4583|""|"C"
-497|1|1|"Eustis, Miss. Elizabeth Mussey"|"female"|54|1|0|"36947"|78.2667|"D20"|"C"
-498|0|3|"Shellard, Mr. Frederick William"|"male"|NA|0|0|"C.A. 6212"|15.1|""|"S"
-499|0|1|"Allison, Mrs. Hudson J C (Bessie Waldo Daniels)"|"female"|25|1|2|"113781"|151.55|"C22 C26"|"S"
-500|0|3|"Svensson, Mr. Olof"|"male"|24|0|0|"350035"|7.7958|""|"S"
-501|0|3|"Calic, Mr. Petar"|"male"|17|0|0|"315086"|8.6625|""|"S"
-502|0|3|"Canavan, Miss. Mary"|"female"|21|0|0|"364846"|7.75|""|"Q"
-503|0|3|"O'Sullivan, Miss. Bridget Mary"|"female"|NA|0|0|"330909"|7.6292|""|"Q"
-504|0|3|"Laitinen, Miss. Kristina Sofia"|"female"|37|0|0|"4135"|9.5875|""|"S"
-505|1|1|"Maioni, Miss. Roberta"|"female"|16|0|0|"110152"|86.5|"B79"|"S"
-506|0|1|"Penasco y Castellana, Mr. Victor de Satode"|"male"|18|1|0|"PC 17758"|108.9|"C65"|"C"
-507|1|2|"Quick, Mrs. Frederick Charles (Jane Richards)"|"female"|33|0|2|"26360"|26|""|"S"
-508|1|1|"Bradley, Mr. George (\George Arthur Brayton\)"|"male"|NA|0|0|"111427"|26.55|""|"S"
-509|0|3|"Olsen, Mr. Henry Margido"|"male"|28|0|0|"C 4001"|22.525|""|"S"
-510|1|3|"Lang, Mr. Fang"|"male"|26|0|0|"1601"|56.4958|""|"S"
-511|1|3|"Daly, Mr. Eugene Patrick"|"male"|29|0|0|"382651"|7.75|""|"Q"
-512|0|3|"Webber, Mr. James"|"male"|NA|0|0|"SOTON/OQ 3101316"|8.05|""|"S"
-513|1|1|"McGough, Mr. James Robert"|"male"|36|0|0|"PC 17473"|26.2875|"E25"|"S"
-514|1|1|"Rothschild, Mrs. Martin (Elizabeth L. Barrett)"|"female"|54|1|0|"PC 17603"|59.4|""|"C"
-515|0|3|"Coleff, Mr. Satio"|"male"|24|0|0|"349209"|7.4958|""|"S"
-516|0|1|"Walker, Mr. William Anderson"|"male"|47|0|0|"36967"|34.0208|"D46"|"S"
-517|1|2|"Lemore, Mrs. (Amelia Milley)"|"female"|34|0|0|"C.A. 34260"|10.5|"F33"|"S"
-518|0|3|"Ryan, Mr. Patrick"|"male"|NA|0|0|"371110"|24.15|""|"Q"
-519|1|2|"Angle, Mrs. William A (Florence \Mary\ Agnes Hughes)"|"female"|36|1|0|"226875"|26|""|"S"
-520|0|3|"Pavlovic, Mr. Stefo"|"male"|32|0|0|"349242"|7.8958|""|"S"
-521|1|1|"Perreault, Miss. Anne"|"female"|30|0|0|"12749"|93.5|"B73"|"S"
-522|0|3|"Vovk, Mr. Janko"|"male"|22|0|0|"349252"|7.8958|""|"S"
-523|0|3|"Lahoud, Mr. Sarkis"|"male"|NA|0|0|"2624"|7.225|""|"C"
-524|1|1|"Hippach, Mrs. Louis Albert (Ida Sophia Fischer)"|"female"|44|0|1|"111361"|57.9792|"B18"|"C"
-525|0|3|"Kassem, Mr. Fared"|"male"|NA|0|0|"2700"|7.2292|""|"C"
-526|0|3|"Farrell, Mr. James"|"male"|40.5|0|0|"367232"|7.75|""|"Q"
-527|1|2|"Ridsdale, Miss. Lucy"|"female"|50|0|0|"W./C. 14258"|10.5|""|"S"
-528|0|1|"Farthing, Mr. John"|"male"|NA|0|0|"PC 17483"|221.7792|"C95"|"S"
-529|0|3|"Salonen, Mr. Johan Werner"|"male"|39|0|0|"3101296"|7.925|""|"S"
-530|0|2|"Hocking, Mr. Richard George"|"male"|23|2|1|"29104"|11.5|""|"S"
-531|1|2|"Quick, Miss. Phyllis May"|"female"|2|1|1|"26360"|26|""|"S"
-532|0|3|"Toufik, Mr. Nakli"|"male"|NA|0|0|"2641"|7.2292|""|"C"
-533|0|3|"Elias, Mr. Joseph Jr"|"male"|17|1|1|"2690"|7.2292|""|"C"
-534|1|3|"Peter, Mrs. Catherine (Catherine Rizk)"|"female"|NA|0|2|"2668"|22.3583|""|"C"
-535|0|3|"Cacic, Miss. Marija"|"female"|30|0|0|"315084"|8.6625|""|"S"
-536|1|2|"Hart, Miss. Eva Miriam"|"female"|7|0|2|"F.C.C. 13529"|26.25|""|"S"
-537|0|1|"Butt, Major. Archibald Willingham"|"male"|45|0|0|"113050"|26.55|"B38"|"S"
-538|1|1|"LeRoy, Miss. Bertha"|"female"|30|0|0|"PC 17761"|106.425|""|"C"
-539|0|3|"Risien, Mr. Samuel Beard"|"male"|NA|0|0|"364498"|14.5|""|"S"
-540|1|1|"Frolicher, Miss. Hedwig Margaritha"|"female"|22|0|2|"13568"|49.5|"B39"|"C"
-541|1|1|"Crosby, Miss. Harriet R"|"female"|36|0|2|"WE/P 5735"|71|"B22"|"S"
-542|0|3|"Andersson, Miss. Ingeborg Constanzia"|"female"|9|4|2|"347082"|31.275|""|"S"
-543|0|3|"Andersson, Miss. Sigrid Elisabeth"|"female"|11|4|2|"347082"|31.275|""|"S"
-544|1|2|"Beane, Mr. Edward"|"male"|32|1|0|"2908"|26|""|"S"
-545|0|1|"Douglas, Mr. Walter Donald"|"male"|50|1|0|"PC 17761"|106.425|"C86"|"C"
-546|0|1|"Nicholson, Mr. Arthur Ernest"|"male"|64|0|0|"693"|26|""|"S"
-547|1|2|"Beane, Mrs. Edward (Ethel Clarke)"|"female"|19|1|0|"2908"|26|""|"S"
-548|1|2|"Padro y Manent, Mr. Julian"|"male"|NA|0|0|"SC/PARIS 2146"|13.8625|""|"C"
-549|0|3|"Goldsmith, Mr. Frank John"|"male"|33|1|1|"363291"|20.525|""|"S"
-550|1|2|"Davies, Master. John Morgan Jr"|"male"|8|1|1|"C.A. 33112"|36.75|""|"S"
-551|1|1|"Thayer, Mr. John Borland Jr"|"male"|17|0|2|"17421"|110.8833|"C70"|"C"
-552|0|2|"Sharp, Mr. Percival James R"|"male"|27|0|0|"244358"|26|""|"S"
-553|0|3|"O'Brien, Mr. Timothy"|"male"|NA|0|0|"330979"|7.8292|""|"Q"
-554|1|3|"Leeni, Mr. Fahim (\Philip Zenni\)"|"male"|22|0|0|"2620"|7.225|""|"C"
-555|1|3|"Ohman, Miss. Velin"|"female"|22|0|0|"347085"|7.775|""|"S"
-556|0|1|"Wright, Mr. George"|"male"|62|0|0|"113807"|26.55|""|"S"
-557|1|1|"Duff Gordon, Lady. (Lucille Christiana Sutherland) (\Mrs Morgan\)"|"female"|48|1|0|"11755"|39.6|"A16"|"C"
-558|0|1|"Robbins, Mr. Victor"|"male"|NA|0|0|"PC 17757"|227.525|""|"C"
-559|1|1|"Taussig, Mrs. Emil (Tillie Mandelbaum)"|"female"|39|1|1|"110413"|79.65|"E67"|"S"
-560|1|3|"de Messemaeker, Mrs. Guillaume Joseph (Emma)"|"female"|36|1|0|"345572"|17.4|""|"S"
-561|0|3|"Morrow, Mr. Thomas Rowan"|"male"|NA|0|0|"372622"|7.75|""|"Q"
-562|0|3|"Sivic, Mr. Husein"|"male"|40|0|0|"349251"|7.8958|""|"S"
-563|0|2|"Norman, Mr. Robert Douglas"|"male"|28|0|0|"218629"|13.5|""|"S"
-564|0|3|"Simmons, Mr. John"|"male"|NA|0|0|"SOTON/OQ 392082"|8.05|""|"S"
-565|0|3|"Meanwell, Miss. (Marion Ogden)"|"female"|NA|0|0|"SOTON/O.Q. 392087"|8.05|""|"S"
-566|0|3|"Davies, Mr. Alfred J"|"male"|24|2|0|"A/4 48871"|24.15|""|"S"
-567|0|3|"Stoytcheff, Mr. Ilia"|"male"|19|0|0|"349205"|7.8958|""|"S"
-568|0|3|"Palsson, Mrs. Nils (Alma Cornelia Berglund)"|"female"|29|0|4|"349909"|21.075|""|"S"
-569|0|3|"Doharr, Mr. Tannous"|"male"|NA|0|0|"2686"|7.2292|""|"C"
-570|1|3|"Jonsson, Mr. Carl"|"male"|32|0|0|"350417"|7.8542|""|"S"
-571|1|2|"Harris, Mr. George"|"male"|62|0|0|"S.W./PP 752"|10.5|""|"S"
-572|1|1|"Appleton, Mrs. Edward Dale (Charlotte Lamson)"|"female"|53|2|0|"11769"|51.4792|"C101"|"S"
-573|1|1|"Flynn, Mr. John Irwin (\Irving\)"|"male"|36|0|0|"PC 17474"|26.3875|"E25"|"S"
-574|1|3|"Kelly, Miss. Mary"|"female"|NA|0|0|"14312"|7.75|""|"Q"
-575|0|3|"Rush, Mr. Alfred George John"|"male"|16|0|0|"A/4. 20589"|8.05|""|"S"
-576|0|3|"Patchett, Mr. George"|"male"|19|0|0|"358585"|14.5|""|"S"
-577|1|2|"Garside, Miss. Ethel"|"female"|34|0|0|"243880"|13|""|"S"
-578|1|1|"Silvey, Mrs. William Baird (Alice Munger)"|"female"|39|1|0|"13507"|55.9|"E44"|"S"
-579|0|3|"Caram, Mrs. Joseph (Maria Elias)"|"female"|NA|1|0|"2689"|14.4583|""|"C"
-580|1|3|"Jussila, Mr. Eiriik"|"male"|32|0|0|"STON/O 2. 3101286"|7.925|""|"S"
-581|1|2|"Christy, Miss. Julie Rachel"|"female"|25|1|1|"237789"|30|""|"S"
-582|1|1|"Thayer, Mrs. John Borland (Marian Longstreth Morris)"|"female"|39|1|1|"17421"|110.8833|"C68"|"C"
-583|0|2|"Downton, Mr. William James"|"male"|54|0|0|"28403"|26|""|"S"
-584|0|1|"Ross, Mr. John Hugo"|"male"|36|0|0|"13049"|40.125|"A10"|"C"
-585|0|3|"Paulner, Mr. Uscher"|"male"|NA|0|0|"3411"|8.7125|""|"C"
-586|1|1|"Taussig, Miss. Ruth"|"female"|18|0|2|"110413"|79.65|"E68"|"S"
-587|0|2|"Jarvis, Mr. John Denzil"|"male"|47|0|0|"237565"|15|""|"S"
-588|1|1|"Frolicher-Stehli, Mr. Maxmillian"|"male"|60|1|1|"13567"|79.2|"B41"|"C"
-589|0|3|"Gilinski, Mr. Eliezer"|"male"|22|0|0|"14973"|8.05|""|"S"
-590|0|3|"Murdlin, Mr. Joseph"|"male"|NA|0|0|"A./5. 3235"|8.05|""|"S"
-591|0|3|"Rintamaki, Mr. Matti"|"male"|35|0|0|"STON/O 2. 3101273"|7.125|""|"S"
-592|1|1|"Stephenson, Mrs. Walter Bertram (Martha Eustis)"|"female"|52|1|0|"36947"|78.2667|"D20"|"C"
-593|0|3|"Elsbury, Mr. William James"|"male"|47|0|0|"A/5 3902"|7.25|""|"S"
-594|0|3|"Bourke, Miss. Mary"|"female"|NA|0|2|"364848"|7.75|""|"Q"
-595|0|2|"Chapman, Mr. John Henry"|"male"|37|1|0|"SC/AH 29037"|26|""|"S"
-596|0|3|"Van Impe, Mr. Jean Baptiste"|"male"|36|1|1|"345773"|24.15|""|"S"
-597|1|2|"Leitch, Miss. Jessie Wills"|"female"|NA|0|0|"248727"|33|""|"S"
-598|0|3|"Johnson, Mr. Alfred"|"male"|49|0|0|"LINE"|0|""|"S"
-599|0|3|"Boulos, Mr. Hanna"|"male"|NA|0|0|"2664"|7.225|""|"C"
-600|1|1|"Duff Gordon, Sir. Cosmo Edmund (\Mr Morgan\)"|"male"|49|1|0|"PC 17485"|56.9292|"A20"|"C"
-601|1|2|"Jacobsohn, Mrs. Sidney Samuel (Amy Frances Christy)"|"female"|24|2|1|"243847"|27|""|"S"
-602|0|3|"Slabenoff, Mr. Petco"|"male"|NA|0|0|"349214"|7.8958|""|"S"
-603|0|1|"Harrington, Mr. Charles H"|"male"|NA|0|0|"113796"|42.4|""|"S"
-604|0|3|"Torber, Mr. Ernst William"|"male"|44|0|0|"364511"|8.05|""|"S"
-605|1|1|"Homer, Mr. Harry (\Mr E Haven\)"|"male"|35|0|0|"111426"|26.55|""|"C"
-606|0|3|"Lindell, Mr. Edvard Bengtsson"|"male"|36|1|0|"349910"|15.55|""|"S"
-607|0|3|"Karaic, Mr. Milan"|"male"|30|0|0|"349246"|7.8958|""|"S"
-608|1|1|"Daniel, Mr. Robert Williams"|"male"|27|0|0|"113804"|30.5|""|"S"
-609|1|2|"Laroche, Mrs. Joseph (Juliette Marie Louise Lafargue)"|"female"|22|1|2|"SC/Paris 2123"|41.5792|""|"C"
-610|1|1|"Shutes, Miss. Elizabeth W"|"female"|40|0|0|"PC 17582"|153.4625|"C125"|"S"
-611|0|3|"Andersson, Mrs. Anders Johan (Alfrida Konstantia Brogren)"|"female"|39|1|5|"347082"|31.275|""|"S"
-612|0|3|"Jardin, Mr. Jose Neto"|"male"|NA|0|0|"SOTON/O.Q. 3101305"|7.05|""|"S"
-613|1|3|"Murphy, Miss. Margaret Jane"|"female"|NA|1|0|"367230"|15.5|""|"Q"
-614|0|3|"Horgan, Mr. John"|"male"|NA|0|0|"370377"|7.75|""|"Q"
-615|0|3|"Brocklebank, Mr. William Alfred"|"male"|35|0|0|"364512"|8.05|""|"S"
-616|1|2|"Herman, Miss. Alice"|"female"|24|1|2|"220845"|65|""|"S"
-617|0|3|"Danbom, Mr. Ernst Gilbert"|"male"|34|1|1|"347080"|14.4|""|"S"
-618|0|3|"Lobb, Mrs. William Arthur (Cordelia K Stanlick)"|"female"|26|1|0|"A/5. 3336"|16.1|""|"S"
-619|1|2|"Becker, Miss. Marion Louise"|"female"|4|2|1|"230136"|39|"F4"|"S"
-620|0|2|"Gavey, Mr. Lawrence"|"male"|26|0|0|"31028"|10.5|""|"S"
-621|0|3|"Yasbeck, Mr. Antoni"|"male"|27|1|0|"2659"|14.4542|""|"C"
-622|1|1|"Kimball, Mr. Edwin Nelson Jr"|"male"|42|1|0|"11753"|52.5542|"D19"|"S"
-623|1|3|"Nakid, Mr. Sahid"|"male"|20|1|1|"2653"|15.7417|""|"C"
-624|0|3|"Hansen, Mr. Henry Damsgaard"|"male"|21|0|0|"350029"|7.8542|""|"S"
-625|0|3|"Bowen, Mr. David John \Dai\"|"male"|21|0|0|"54636"|16.1|""|"S"
-626|0|1|"Sutton, Mr. Frederick"|"male"|61|0|0|"36963"|32.3208|"D50"|"S"
-627|0|2|"Kirkland, Rev. Charles Leonard"|"male"|57|0|0|"219533"|12.35|""|"Q"
-628|1|1|"Longley, Miss. Gretchen Fiske"|"female"|21|0|0|"13502"|77.9583|"D9"|"S"
-629|0|3|"Bostandyeff, Mr. Guentcho"|"male"|26|0|0|"349224"|7.8958|""|"S"
-630|0|3|"O'Connell, Mr. Patrick D"|"male"|NA|0|0|"334912"|7.7333|""|"Q"
-631|1|1|"Barkworth, Mr. Algernon Henry Wilson"|"male"|80|0|0|"27042"|30|"A23"|"S"
-632|0|3|"Lundahl, Mr. Johan Svensson"|"male"|51|0|0|"347743"|7.0542|""|"S"
-633|1|1|"Stahelin-Maeglin, Dr. Max"|"male"|32|0|0|"13214"|30.5|"B50"|"C"
-634|0|1|"Parr, Mr. William Henry Marsh"|"male"|NA|0|0|"112052"|0|""|"S"
-635|0|3|"Skoog, Miss. Mabel"|"female"|9|3|2|"347088"|27.9|""|"S"
-636|1|2|"Davis, Miss. Mary"|"female"|28|0|0|"237668"|13|""|"S"
-637|0|3|"Leinonen, Mr. Antti Gustaf"|"male"|32|0|0|"STON/O 2. 3101292"|7.925|""|"S"
-638|0|2|"Collyer, Mr. Harvey"|"male"|31|1|1|"C.A. 31921"|26.25|""|"S"
-639|0|3|"Panula, Mrs. Juha (Maria Emilia Ojala)"|"female"|41|0|5|"3101295"|39.6875|""|"S"
-640|0|3|"Thorneycroft, Mr. Percival"|"male"|NA|1|0|"376564"|16.1|""|"S"
-641|0|3|"Jensen, Mr. Hans Peder"|"male"|20|0|0|"350050"|7.8542|""|"S"
-642|1|1|"Sagesser, Mlle. Emma"|"female"|24|0|0|"PC 17477"|69.3|"B35"|"C"
-643|0|3|"Skoog, Miss. Margit Elizabeth"|"female"|2|3|2|"347088"|27.9|""|"S"
-644|1|3|"Foo, Mr. Choong"|"male"|NA|0|0|"1601"|56.4958|""|"S"
-645|1|3|"Baclini, Miss. Eugenie"|"female"|0.75|2|1|"2666"|19.2583|""|"C"
-646|1|1|"Harper, Mr. Henry Sleeper"|"male"|48|1|0|"PC 17572"|76.7292|"D33"|"C"
-647|0|3|"Cor, Mr. Liudevit"|"male"|19|0|0|"349231"|7.8958|""|"S"
-648|1|1|"Simonius-Blumer, Col. Oberst Alfons"|"male"|56|0|0|"13213"|35.5|"A26"|"C"
-649|0|3|"Willey, Mr. Edward"|"male"|NA|0|0|"S.O./P.P. 751"|7.55|""|"S"
-650|1|3|"Stanley, Miss. Amy Zillah Elsie"|"female"|23|0|0|"CA. 2314"|7.55|""|"S"
-651|0|3|"Mitkoff, Mr. Mito"|"male"|NA|0|0|"349221"|7.8958|""|"S"
-652|1|2|"Doling, Miss. Elsie"|"female"|18|0|1|"231919"|23|""|"S"
-653|0|3|"Kalvik, Mr. Johannes Halvorsen"|"male"|21|0|0|"8475"|8.4333|""|"S"
-654|1|3|"O'Leary, Miss. Hanora \Norah\"|"female"|NA|0|0|"330919"|7.8292|""|"Q"
-655|0|3|"Hegarty, Miss. Hanora \Nora\"|"female"|18|0|0|"365226"|6.75|""|"Q"
-656|0|2|"Hickman, Mr. Leonard Mark"|"male"|24|2|0|"S.O.C. 14879"|73.5|""|"S"
-657|0|3|"Radeff, Mr. Alexander"|"male"|NA|0|0|"349223"|7.8958|""|"S"
-658|0|3|"Bourke, Mrs. John (Catherine)"|"female"|32|1|1|"364849"|15.5|""|"Q"
-659|0|2|"Eitemiller, Mr. George Floyd"|"male"|23|0|0|"29751"|13|""|"S"
-660|0|1|"Newell, Mr. Arthur Webster"|"male"|58|0|2|"35273"|113.275|"D48"|"C"
-661|1|1|"Frauenthal, Dr. Henry William"|"male"|50|2|0|"PC 17611"|133.65|""|"S"
-662|0|3|"Badt, Mr. Mohamed"|"male"|40|0|0|"2623"|7.225|""|"C"
-663|0|1|"Colley, Mr. Edward Pomeroy"|"male"|47|0|0|"5727"|25.5875|"E58"|"S"
-664|0|3|"Coleff, Mr. Peju"|"male"|36|0|0|"349210"|7.4958|""|"S"
-665|1|3|"Lindqvist, Mr. Eino William"|"male"|20|1|0|"STON/O 2. 3101285"|7.925|""|"S"
-666|0|2|"Hickman, Mr. Lewis"|"male"|32|2|0|"S.O.C. 14879"|73.5|""|"S"
-667|0|2|"Butler, Mr. Reginald Fenton"|"male"|25|0|0|"234686"|13|""|"S"
-668|0|3|"Rommetvedt, Mr. Knud Paust"|"male"|NA|0|0|"312993"|7.775|""|"S"
-669|0|3|"Cook, Mr. Jacob"|"male"|43|0|0|"A/5 3536"|8.05|""|"S"
-670|1|1|"Taylor, Mrs. Elmer Zebley (Juliet Cummins Wright)"|"female"|NA|1|0|"19996"|52|"C126"|"S"
-671|1|2|"Brown, Mrs. Thomas William Solomon (Elizabeth Catherine Ford)"|"female"|40|1|1|"29750"|39|""|"S"
-672|0|1|"Davidson, Mr. Thornton"|"male"|31|1|0|"F.C. 12750"|52|"B71"|"S"
-673|0|2|"Mitchell, Mr. Henry Michael"|"male"|70|0|0|"C.A. 24580"|10.5|""|"S"
-674|1|2|"Wilhelms, Mr. Charles"|"male"|31|0|0|"244270"|13|""|"S"
-675|0|2|"Watson, Mr. Ennis Hastings"|"male"|NA|0|0|"239856"|0|""|"S"
-676|0|3|"Edvardsson, Mr. Gustaf Hjalmar"|"male"|18|0|0|"349912"|7.775|""|"S"
-677|0|3|"Sawyer, Mr. Frederick Charles"|"male"|24.5|0|0|"342826"|8.05|""|"S"
-678|1|3|"Turja, Miss. Anna Sofia"|"female"|18|0|0|"4138"|9.8417|""|"S"
-679|0|3|"Goodwin, Mrs. Frederick (Augusta Tyler)"|"female"|43|1|6|"CA 2144"|46.9|""|"S"
-680|1|1|"Cardeza, Mr. Thomas Drake Martinez"|"male"|36|0|1|"PC 17755"|512.3292|"B51 B53 B55"|"C"
-681|0|3|"Peters, Miss. Katie"|"female"|NA|0|0|"330935"|8.1375|""|"Q"
-682|1|1|"Hassab, Mr. Hammad"|"male"|27|0|0|"PC 17572"|76.7292|"D49"|"C"
-683|0|3|"Olsvigen, Mr. Thor Anderson"|"male"|20|0|0|"6563"|9.225|""|"S"
-684|0|3|"Goodwin, Mr. Charles Edward"|"male"|14|5|2|"CA 2144"|46.9|""|"S"
-685|0|2|"Brown, Mr. Thomas William Solomon"|"male"|60|1|1|"29750"|39|""|"S"
-686|0|2|"Laroche, Mr. Joseph Philippe Lemercier"|"male"|25|1|2|"SC/Paris 2123"|41.5792|""|"C"
-687|0|3|"Panula, Mr. Jaako Arnold"|"male"|14|4|1|"3101295"|39.6875|""|"S"
-688|0|3|"Dakic, Mr. Branko"|"male"|19|0|0|"349228"|10.1708|""|"S"
-689|0|3|"Fischer, Mr. Eberhard Thelander"|"male"|18|0|0|"350036"|7.7958|""|"S"
-690|1|1|"Madill, Miss. Georgette Alexandra"|"female"|15|0|1|"24160"|211.3375|"B5"|"S"
-691|1|1|"Dick, Mr. Albert Adrian"|"male"|31|1|0|"17474"|57|"B20"|"S"
-692|1|3|"Karun, Miss. Manca"|"female"|4|0|1|"349256"|13.4167|""|"C"
-693|1|3|"Lam, Mr. Ali"|"male"|NA|0|0|"1601"|56.4958|""|"S"
-694|0|3|"Saad, Mr. Khalil"|"male"|25|0|0|"2672"|7.225|""|"C"
-695|0|1|"Weir, Col. John"|"male"|60|0|0|"113800"|26.55|""|"S"
-696|0|2|"Chapman, Mr. Charles Henry"|"male"|52|0|0|"248731"|13.5|""|"S"
-697|0|3|"Kelly, Mr. James"|"male"|44|0|0|"363592"|8.05|""|"S"
-698|1|3|"Mullens, Miss. Katherine \Katie\"|"female"|NA|0|0|"35852"|7.7333|""|"Q"
-699|0|1|"Thayer, Mr. John Borland"|"male"|49|1|1|"17421"|110.8833|"C68"|"C"
-700|0|3|"Humblen, Mr. Adolf Mathias Nicolai Olsen"|"male"|42|0|0|"348121"|7.65|"F G63"|"S"
-701|1|1|"Astor, Mrs. John Jacob (Madeleine Talmadge Force)"|"female"|18|1|0|"PC 17757"|227.525|"C62 C64"|"C"
-702|1|1|"Silverthorne, Mr. Spencer Victor"|"male"|35|0|0|"PC 17475"|26.2875|"E24"|"S"
-703|0|3|"Barbara, Miss. Saiide"|"female"|18|0|1|"2691"|14.4542|""|"C"
-704|0|3|"Gallagher, Mr. Martin"|"male"|25|0|0|"36864"|7.7417|""|"Q"
-705|0|3|"Hansen, Mr. Henrik Juul"|"male"|26|1|0|"350025"|7.8542|""|"S"
-706|0|2|"Morley, Mr. Henry Samuel (\Mr Henry Marshall\)"|"male"|39|0|0|"250655"|26|""|"S"
-707|1|2|"Kelly, Mrs. Florence \Fannie\"|"female"|45|0|0|"223596"|13.5|""|"S"
-708|1|1|"Calderhead, Mr. Edward Pennington"|"male"|42|0|0|"PC 17476"|26.2875|"E24"|"S"
-709|1|1|"Cleaver, Miss. Alice"|"female"|22|0|0|"113781"|151.55|""|"S"
-710|1|3|"Moubarek, Master. Halim Gonios (\William George\)"|"male"|NA|1|1|"2661"|15.2458|""|"C"
-711|1|1|"Mayne, Mlle. Berthe Antonine (\Mrs de Villiers\)"|"female"|24|0|0|"PC 17482"|49.5042|"C90"|"C"
-712|0|1|"Klaber, Mr. Herman"|"male"|NA|0|0|"113028"|26.55|"C124"|"S"
-713|1|1|"Taylor, Mr. Elmer Zebley"|"male"|48|1|0|"19996"|52|"C126"|"S"
-714|0|3|"Larsson, Mr. August Viktor"|"male"|29|0|0|"7545"|9.4833|""|"S"
-715|0|2|"Greenberg, Mr. Samuel"|"male"|52|0|0|"250647"|13|""|"S"
-716|0|3|"Soholt, Mr. Peter Andreas Lauritz Andersen"|"male"|19|0|0|"348124"|7.65|"F G73"|"S"
-717|1|1|"Endres, Miss. Caroline Louise"|"female"|38|0|0|"PC 17757"|227.525|"C45"|"C"
-718|1|2|"Troutt, Miss. Edwina Celia \Winnie\"|"female"|27|0|0|"34218"|10.5|"E101"|"S"
-719|0|3|"McEvoy, Mr. Michael"|"male"|NA|0|0|"36568"|15.5|""|"Q"
-720|0|3|"Johnson, Mr. Malkolm Joackim"|"male"|33|0|0|"347062"|7.775|""|"S"
-721|1|2|"Harper, Miss. Annie Jessie \Nina\"|"female"|6|0|1|"248727"|33|""|"S"
-722|0|3|"Jensen, Mr. Svend Lauritz"|"male"|17|1|0|"350048"|7.0542|""|"S"
-723|0|2|"Gillespie, Mr. William Henry"|"male"|34|0|0|"12233"|13|""|"S"
-724|0|2|"Hodges, Mr. Henry Price"|"male"|50|0|0|"250643"|13|""|"S"
-725|1|1|"Chambers, Mr. Norman Campbell"|"male"|27|1|0|"113806"|53.1|"E8"|"S"
-726|0|3|"Oreskovic, Mr. Luka"|"male"|20|0|0|"315094"|8.6625|""|"S"
-727|1|2|"Renouf, Mrs. Peter Henry (Lillian Jefferys)"|"female"|30|3|0|"31027"|21|""|"S"
-728|1|3|"Mannion, Miss. Margareth"|"female"|NA|0|0|"36866"|7.7375|""|"Q"
-729|0|2|"Bryhl, Mr. Kurt Arnold Gottfrid"|"male"|25|1|0|"236853"|26|""|"S"
-730|0|3|"Ilmakangas, Miss. Pieta Sofia"|"female"|25|1|0|"STON/O2. 3101271"|7.925|""|"S"
-731|1|1|"Allen, Miss. Elisabeth Walton"|"female"|29|0|0|"24160"|211.3375|"B5"|"S"
-732|0|3|"Hassan, Mr. Houssein G N"|"male"|11|0|0|"2699"|18.7875|""|"C"
-733|0|2|"Knight, Mr. Robert J"|"male"|NA|0|0|"239855"|0|""|"S"
-734|0|2|"Berriman, Mr. William John"|"male"|23|0|0|"28425"|13|""|"S"
-735|0|2|"Troupiansky, Mr. Moses Aaron"|"male"|23|0|0|"233639"|13|""|"S"
-736|0|3|"Williams, Mr. Leslie"|"male"|28.5|0|0|"54636"|16.1|""|"S"
-737|0|3|"Ford, Mrs. Edward (Margaret Ann Watson)"|"female"|48|1|3|"W./C. 6608"|34.375|""|"S"
-738|1|1|"Lesurer, Mr. Gustave J"|"male"|35|0|0|"PC 17755"|512.3292|"B101"|"C"
-739|0|3|"Ivanoff, Mr. Kanio"|"male"|NA|0|0|"349201"|7.8958|""|"S"
-740|0|3|"Nankoff, Mr. Minko"|"male"|NA|0|0|"349218"|7.8958|""|"S"
-741|1|1|"Hawksford, Mr. Walter James"|"male"|NA|0|0|"16988"|30|"D45"|"S"
-742|0|1|"Cavendish, Mr. Tyrell William"|"male"|36|1|0|"19877"|78.85|"C46"|"S"
-743|1|1|"Ryerson, Miss. Susan Parker \Suzette\"|"female"|21|2|2|"PC 17608"|262.375|"B57 B59 B63 B66"|"C"
-744|0|3|"McNamee, Mr. Neal"|"male"|24|1|0|"376566"|16.1|""|"S"
-745|1|3|"Stranden, Mr. Juho"|"male"|31|0|0|"STON/O 2. 3101288"|7.925|""|"S"
-746|0|1|"Crosby, Capt. Edward Gifford"|"male"|70|1|1|"WE/P 5735"|71|"B22"|"S"
-747|0|3|"Abbott, Mr. Rossmore Edward"|"male"|16|1|1|"C.A. 2673"|20.25|""|"S"
-748|1|2|"Sinkkonen, Miss. Anna"|"female"|30|0|0|"250648"|13|""|"S"
-749|0|1|"Marvin, Mr. Daniel Warner"|"male"|19|1|0|"113773"|53.1|"D30"|"S"
-750|0|3|"Connaghton, Mr. Michael"|"male"|31|0|0|"335097"|7.75|""|"Q"
-751|1|2|"Wells, Miss. Joan"|"female"|4|1|1|"29103"|23|""|"S"
-752|1|3|"Moor, Master. Meier"|"male"|6|0|1|"392096"|12.475|"E121"|"S"
-753|0|3|"Vande Velde, Mr. Johannes Joseph"|"male"|33|0|0|"345780"|9.5|""|"S"
-754|0|3|"Jonkoff, Mr. Lalio"|"male"|23|0|0|"349204"|7.8958|""|"S"
-755|1|2|"Herman, Mrs. Samuel (Jane Laver)"|"female"|48|1|2|"220845"|65|""|"S"
-756|1|2|"Hamalainen, Master. Viljo"|"male"|0.67|1|1|"250649"|14.5|""|"S"
-757|0|3|"Carlsson, Mr. August Sigfrid"|"male"|28|0|0|"350042"|7.7958|""|"S"
-758|0|2|"Bailey, Mr. Percy Andrew"|"male"|18|0|0|"29108"|11.5|""|"S"
-759|0|3|"Theobald, Mr. Thomas Leonard"|"male"|34|0|0|"363294"|8.05|""|"S"
-760|1|1|"Rothes, the Countess. of (Lucy Noel Martha Dyer-Edwards)"|"female"|33|0|0|"110152"|86.5|"B77"|"S"
-761|0|3|"Garfirth, Mr. John"|"male"|NA|0|0|"358585"|14.5|""|"S"
-762|0|3|"Nirva, Mr. Iisakki Antino Aijo"|"male"|41|0|0|"SOTON/O2 3101272"|7.125|""|"S"
-763|1|3|"Barah, Mr. Hanna Assi"|"male"|20|0|0|"2663"|7.2292|""|"C"
-764|1|1|"Carter, Mrs. William Ernest (Lucile Polk)"|"female"|36|1|2|"113760"|120|"B96 B98"|"S"
-765|0|3|"Eklund, Mr. Hans Linus"|"male"|16|0|0|"347074"|7.775|""|"S"
-766|1|1|"Hogeboom, Mrs. John C (Anna Andrews)"|"female"|51|1|0|"13502"|77.9583|"D11"|"S"
-767|0|1|"Brewe, Dr. Arthur Jackson"|"male"|NA|0|0|"112379"|39.6|""|"C"
-768|0|3|"Mangan, Miss. Mary"|"female"|30.5|0|0|"364850"|7.75|""|"Q"
-769|0|3|"Moran, Mr. Daniel J"|"male"|NA|1|0|"371110"|24.15|""|"Q"
-770|0|3|"Gronnestad, Mr. Daniel Danielsen"|"male"|32|0|0|"8471"|8.3625|""|"S"
-771|0|3|"Lievens, Mr. Rene Aime"|"male"|24|0|0|"345781"|9.5|""|"S"
-772|0|3|"Jensen, Mr. Niels Peder"|"male"|48|0|0|"350047"|7.8542|""|"S"
-773|0|2|"Mack, Mrs. (Mary)"|"female"|57|0|0|"S.O./P.P. 3"|10.5|"E77"|"S"
-774|0|3|"Elias, Mr. Dibo"|"male"|NA|0|0|"2674"|7.225|""|"C"
-775|1|2|"Hocking, Mrs. Elizabeth (Eliza Needs)"|"female"|54|1|3|"29105"|23|""|"S"
-776|0|3|"Myhrman, Mr. Pehr Fabian Oliver Malkolm"|"male"|18|0|0|"347078"|7.75|""|"S"
-777|0|3|"Tobin, Mr. Roger"|"male"|NA|0|0|"383121"|7.75|"F38"|"Q"
-778|1|3|"Emanuel, Miss. Virginia Ethel"|"female"|5|0|0|"364516"|12.475|""|"S"
-779|0|3|"Kilgannon, Mr. Thomas J"|"male"|NA|0|0|"36865"|7.7375|""|"Q"
-780|1|1|"Robert, Mrs. Edward Scott (Elisabeth Walton McMillan)"|"female"|43|0|1|"24160"|211.3375|"B3"|"S"
-781|1|3|"Ayoub, Miss. Banoura"|"female"|13|0|0|"2687"|7.2292|""|"C"
-782|1|1|"Dick, Mrs. Albert Adrian (Vera Gillespie)"|"female"|17|1|0|"17474"|57|"B20"|"S"
-783|0|1|"Long, Mr. Milton Clyde"|"male"|29|0|0|"113501"|30|"D6"|"S"
-784|0|3|"Johnston, Mr. Andrew G"|"male"|NA|1|2|"W./C. 6607"|23.45|""|"S"
-785|0|3|"Ali, Mr. William"|"male"|25|0|0|"SOTON/O.Q. 3101312"|7.05|""|"S"
-786|0|3|"Harmer, Mr. Abraham (David Lishin)"|"male"|25|0|0|"374887"|7.25|""|"S"
-787|1|3|"Sjoblom, Miss. Anna Sofia"|"female"|18|0|0|"3101265"|7.4958|""|"S"
-788|0|3|"Rice, Master. George Hugh"|"male"|8|4|1|"382652"|29.125|""|"Q"
-789|1|3|"Dean, Master. Bertram Vere"|"male"|1|1|2|"C.A. 2315"|20.575|""|"S"
-790|0|1|"Guggenheim, Mr. Benjamin"|"male"|46|0|0|"PC 17593"|79.2|"B82 B84"|"C"
-791|0|3|"Keane, Mr. Andrew \Andy\"|"male"|NA|0|0|"12460"|7.75|""|"Q"
-792|0|2|"Gaskell, Mr. Alfred"|"male"|16|0|0|"239865"|26|""|"S"
-793|0|3|"Sage, Miss. Stella Anna"|"female"|NA|8|2|"CA. 2343"|69.55|""|"S"
-794|0|1|"Hoyt, Mr. William Fisher"|"male"|NA|0|0|"PC 17600"|30.6958|""|"C"
-795|0|3|"Dantcheff, Mr. Ristiu"|"male"|25|0|0|"349203"|7.8958|""|"S"
-796|0|2|"Otter, Mr. Richard"|"male"|39|0|0|"28213"|13|""|"S"
-797|1|1|"Leader, Dr. Alice (Farnham)"|"female"|49|0|0|"17465"|25.9292|"D17"|"S"
-798|1|3|"Osman, Mrs. Mara"|"female"|31|0|0|"349244"|8.6833|""|"S"
-799|0|3|"Ibrahim Shawah, Mr. Yousseff"|"male"|30|0|0|"2685"|7.2292|""|"C"
-800|0|3|"Van Impe, Mrs. Jean Baptiste (Rosalie Paula Govaert)"|"female"|30|1|1|"345773"|24.15|""|"S"
-801|0|2|"Ponesell, Mr. Martin"|"male"|34|0|0|"250647"|13|""|"S"
-802|1|2|"Collyer, Mrs. Harvey (Charlotte Annie Tate)"|"female"|31|1|1|"C.A. 31921"|26.25|""|"S"
-803|1|1|"Carter, Master. William Thornton II"|"male"|11|1|2|"113760"|120|"B96 B98"|"S"
-804|1|3|"Thomas, Master. Assad Alexander"|"male"|0.42|0|1|"2625"|8.5167|""|"C"
-805|1|3|"Hedman, Mr. Oskar Arvid"|"male"|27|0|0|"347089"|6.975|""|"S"
-806|0|3|"Johansson, Mr. Karl Johan"|"male"|31|0|0|"347063"|7.775|""|"S"
-807|0|1|"Andrews, Mr. Thomas Jr"|"male"|39|0|0|"112050"|0|"A36"|"S"
-808|0|3|"Pettersson, Miss. Ellen Natalia"|"female"|18|0|0|"347087"|7.775|""|"S"
-809|0|2|"Meyer, Mr. August"|"male"|39|0|0|"248723"|13|""|"S"
-810|1|1|"Chambers, Mrs. Norman Campbell (Bertha Griggs)"|"female"|33|1|0|"113806"|53.1|"E8"|"S"
-811|0|3|"Alexander, Mr. William"|"male"|26|0|0|"3474"|7.8875|""|"S"
-812|0|3|"Lester, Mr. James"|"male"|39|0|0|"A/4 48871"|24.15|""|"S"
-813|0|2|"Slemen, Mr. Richard James"|"male"|35|0|0|"28206"|10.5|""|"S"
-814|0|3|"Andersson, Miss. Ebba Iris Alfrida"|"female"|6|4|2|"347082"|31.275|""|"S"
-815|0|3|"Tomlin, Mr. Ernest Portage"|"male"|30.5|0|0|"364499"|8.05|""|"S"
-816|0|1|"Fry, Mr. Richard"|"male"|NA|0|0|"112058"|0|"B102"|"S"
-817|0|3|"Heininen, Miss. Wendla Maria"|"female"|23|0|0|"STON/O2. 3101290"|7.925|""|"S"
-818|0|2|"Mallet, Mr. Albert"|"male"|31|1|1|"S.C./PARIS 2079"|37.0042|""|"C"
-819|0|3|"Holm, Mr. John Fredrik Alexander"|"male"|43|0|0|"C 7075"|6.45|""|"S"
-820|0|3|"Skoog, Master. Karl Thorsten"|"male"|10|3|2|"347088"|27.9|""|"S"
-821|1|1|"Hays, Mrs. Charles Melville (Clara Jennings Gregg)"|"female"|52|1|1|"12749"|93.5|"B69"|"S"
-822|1|3|"Lulic, Mr. Nikola"|"male"|27|0|0|"315098"|8.6625|""|"S"
-823|0|1|"Reuchlin, Jonkheer. John George"|"male"|38|0|0|"19972"|0|""|"S"
-824|1|3|"Moor, Mrs. (Beila)"|"female"|27|0|1|"392096"|12.475|"E121"|"S"
-825|0|3|"Panula, Master. Urho Abraham"|"male"|2|4|1|"3101295"|39.6875|""|"S"
-826|0|3|"Flynn, Mr. John"|"male"|NA|0|0|"368323"|6.95|""|"Q"
-827|0|3|"Lam, Mr. Len"|"male"|NA|0|0|"1601"|56.4958|""|"S"
-828|1|2|"Mallet, Master. Andre"|"male"|1|0|2|"S.C./PARIS 2079"|37.0042|""|"C"
-829|1|3|"McCormack, Mr. Thomas Joseph"|"male"|NA|0|0|"367228"|7.75|""|"Q"
-830|1|1|"Stone, Mrs. George Nelson (Martha Evelyn)"|"female"|62|0|0|"113572"|80|"B28"|""
-831|1|3|"Yasbeck, Mrs. Antoni (Selini Alexander)"|"female"|15|1|0|"2659"|14.4542|""|"C"
-832|1|2|"Richards, Master. George Sibley"|"male"|0.83|1|1|"29106"|18.75|""|"S"
-833|0|3|"Saad, Mr. Amin"|"male"|NA|0|0|"2671"|7.2292|""|"C"
-834|0|3|"Augustsson, Mr. Albert"|"male"|23|0|0|"347468"|7.8542|""|"S"
-835|0|3|"Allum, Mr. Owen George"|"male"|18|0|0|"2223"|8.3|""|"S"
-836|1|1|"Compton, Miss. Sara Rebecca"|"female"|39|1|1|"PC 17756"|83.1583|"E49"|"C"
-837|0|3|"Pasic, Mr. Jakob"|"male"|21|0|0|"315097"|8.6625|""|"S"
-838|0|3|"Sirota, Mr. Maurice"|"male"|NA|0|0|"392092"|8.05|""|"S"
-839|1|3|"Chip, Mr. Chang"|"male"|32|0|0|"1601"|56.4958|""|"S"
-840|1|1|"Marechal, Mr. Pierre"|"male"|NA|0|0|"11774"|29.7|"C47"|"C"
-841|0|3|"Alhomaki, Mr. Ilmari Rudolf"|"male"|20|0|0|"SOTON/O2 3101287"|7.925|""|"S"
-842|0|2|"Mudd, Mr. Thomas Charles"|"male"|16|0|0|"S.O./P.P. 3"|10.5|""|"S"
-843|1|1|"Serepeca, Miss. Augusta"|"female"|30|0|0|"113798"|31|""|"C"
-844|0|3|"Lemberopolous, Mr. Peter L"|"male"|34.5|0|0|"2683"|6.4375|""|"C"
-845|0|3|"Culumovic, Mr. Jeso"|"male"|17|0|0|"315090"|8.6625|""|"S"
-846|0|3|"Abbing, Mr. Anthony"|"male"|42|0|0|"C.A. 5547"|7.55|""|"S"
-847|0|3|"Sage, Mr. Douglas Bullen"|"male"|NA|8|2|"CA. 2343"|69.55|""|"S"
-848|0|3|"Markoff, Mr. Marin"|"male"|35|0|0|"349213"|7.8958|""|"C"
-849|0|2|"Harper, Rev. John"|"male"|28|0|1|"248727"|33|""|"S"
-850|1|1|"Goldenberg, Mrs. Samuel L (Edwiga Grabowska)"|"female"|NA|1|0|"17453"|89.1042|"C92"|"C"
-851|0|3|"Andersson, Master. Sigvard Harald Elias"|"male"|4|4|2|"347082"|31.275|""|"S"
-852|0|3|"Svensson, Mr. Johan"|"male"|74|0|0|"347060"|7.775|""|"S"
-853|0|3|"Boulos, Miss. Nourelain"|"female"|9|1|1|"2678"|15.2458|""|"C"
-854|1|1|"Lines, Miss. Mary Conover"|"female"|16|0|1|"PC 17592"|39.4|"D28"|"S"
-855|0|2|"Carter, Mrs. Ernest Courtenay (Lilian Hughes)"|"female"|44|1|0|"244252"|26|""|"S"
-856|1|3|"Aks, Mrs. Sam (Leah Rosen)"|"female"|18|0|1|"392091"|9.35|""|"S"
-857|1|1|"Wick, Mrs. George Dennick (Mary Hitchcock)"|"female"|45|1|1|"36928"|164.8667|""|"S"
-858|1|1|"Daly, Mr. Peter Denis "|"male"|51|0|0|"113055"|26.55|"E17"|"S"
-859|1|3|"Baclini, Mrs. Solomon (Latifa Qurban)"|"female"|24|0|3|"2666"|19.2583|""|"C"
-860|0|3|"Razi, Mr. Raihed"|"male"|NA|0|0|"2629"|7.2292|""|"C"
-861|0|3|"Hansen, Mr. Claus Peter"|"male"|41|2|0|"350026"|14.1083|""|"S"
-862|0|2|"Giles, Mr. Frederick Edward"|"male"|21|1|0|"28134"|11.5|""|"S"
-863|1|1|"Swift, Mrs. Frederick Joel (Margaret Welles Barron)"|"female"|48|0|0|"17466"|25.9292|"D17"|"S"
-864|0|3|"Sage, Miss. Dorothy Edith \Dolly\"|"female"|NA|8|2|"CA. 2343"|69.55|""|"S"
-865|0|2|"Gill, Mr. John William"|"male"|24|0|0|"233866"|13|""|"S"
-866|1|2|"Bystrom, Mrs. (Karolina)"|"female"|42|0|0|"236852"|13|""|"S"
-867|1|2|"Duran y More, Miss. Asuncion"|"female"|27|1|0|"SC/PARIS 2149"|13.8583|""|"C"
-868|0|1|"Roebling, Mr. Washington Augustus II"|"male"|31|0|0|"PC 17590"|50.4958|"A24"|"S"
-869|0|3|"van Melkebeke, Mr. Philemon"|"male"|NA|0|0|"345777"|9.5|""|"S"
-870|1|3|"Johnson, Master. Harold Theodor"|"male"|4|1|1|"347742"|11.1333|""|"S"
-871|0|3|"Balkic, Mr. Cerin"|"male"|26|0|0|"349248"|7.8958|""|"S"
-872|1|1|"Beckwith, Mrs. Richard Leonard (Sallie Monypeny)"|"female"|47|1|1|"11751"|52.5542|"D35"|"S"
-873|0|1|"Carlsson, Mr. Frans Olof"|"male"|33|0|0|"695"|5|"B51 B53 B55"|"S"
-874|0|3|"Vander Cruyssen, Mr. Victor"|"male"|47|0|0|"345765"|9|""|"S"
-875|1|2|"Abelson, Mrs. Samuel (Hannah Wizosky)"|"female"|28|1|0|"P/PP 3381"|24|""|"C"
-876|1|3|"Najib, Miss. Adele Kiamie \Jane\"|"female"|15|0|0|"2667"|7.225|""|"C"
-877|0|3|"Gustafsson, Mr. Alfred Ossian"|"male"|20|0|0|"7534"|9.8458|""|"S"
-878|0|3|"Petroff, Mr. Nedelio"|"male"|19|0|0|"349212"|7.8958|""|"S"
-879|0|3|"Laleff, Mr. Kristo"|"male"|NA|0|0|"349217"|7.8958|""|"S"
-880|1|1|"Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)"|"female"|56|0|1|"11767"|83.1583|"C50"|"C"
-881|1|2|"Shelley, Mrs. William (Imanita Parrish Hall)"|"female"|25|0|1|"230433"|26|""|"S"
-882|0|3|"Markun, Mr. Johann"|"male"|33|0|0|"349257"|7.8958|""|"S"
-883|0|3|"Dahlberg, Miss. Gerda Ulrika"|"female"|22|0|0|"7552"|10.5167|""|"S"
-884|0|2|"Banfield, Mr. Frederick James"|"male"|28|0|0|"C.A./SOTON 34068"|10.5|""|"S"
-885|0|3|"Sutehall, Mr. Henry Jr"|"male"|25|0|0|"SOTON/OQ 392076"|7.05|""|"S"
-886|0|3|"Rice, Mrs. William (Margaret Norton)"|"female"|39|0|5|"382652"|29.125|""|"Q"
-887|0|2|"Montvila, Rev. Juozas"|"male"|27|0|0|"211536"|13|""|"S"
-888|1|1|"Graham, Miss. Margaret Edith"|"female"|19|0|0|"112053"|30|"B42"|"S"
-889|0|3|"Johnston, Miss. Catherine Helen \Carrie\"|"female"|NA|1|2|"W./C. 6607"|23.45|""|"S"
-890|1|1|"Behr, Mr. Karl Howell"|"male"|26|0|0|"111369"|30|"C148"|"C"
-891|0|3|"Dooley, Mr. Patrick"|"male"|32|0|0|"370376"|7.75|""|"Q"
+"PASSENGERID"|"SURVIVED"|"PCLASS"|"NAME"|"SEX"|"AGE"|"SIBSP"|"PARCH"|"TICKET"|"FARE"|"CABIN"|"EMBARKED"
+1|0|3|"Braund, Mr. Owen Harris"|"male"|22|1|0|"A/5 21171"|7.25|""|"S"
+2|1|1|"Cumings, Mrs. John Bradley (Florence Briggs Thayer)"|"female"|38|1|0|"PC 17599"|71.2833|"C85"|"C"
+3|1|3|"Heikkinen, Miss. Laina"|"female"|26|0|0|"STON/O2. 3101282"|7.925|""|"S"
+4|1|1|"Futrelle, Mrs. Jacques Heath (Lily May Peel)"|"female"|35|1|0|"113803"|53.1|"C123"|"S"
+5|0|3|"Allen, Mr. William Henry"|"male"|35|0|0|"373450"|8.05|""|"S"
+6|0|3|"Moran, Mr. James"|"male"|NA|0|0|"330877"|8.4583|""|"Q"
+7|0|1|"McCarthy, Mr. Timothy J"|"male"|54|0|0|"17463"|51.8625|"E46"|"S"
+8|0|3|"Palsson, Master. Gosta Leonard"|"male"|2|3|1|"349909"|21.075|""|"S"
+9|1|3|"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)"|"female"|27|0|2|"347742"|11.1333|""|"S"
+10|1|2|"Nasser, Mrs. Nicholas (Adele Achem)"|"female"|14|1|0|"237736"|30.0708|""|"C"
+11|1|3|"Sandstrom, Miss. Marguerite Rut"|"female"|4|1|1|"PP 9549"|16.7|"G6"|"S"
+12|1|1|"Bonnell, Miss. Elizabeth"|"female"|58|0|0|"113783"|26.55|"C103"|"S"
+13|0|3|"Saundercock, Mr. William Henry"|"male"|20|0|0|"A/5. 2151"|8.05|""|"S"
+14|0|3|"Andersson, Mr. Anders Johan"|"male"|39|1|5|"347082"|31.275|""|"S"
+15|0|3|"Vestrom, Miss. Hulda Amanda Adolfina"|"female"|14|0|0|"350406"|7.8542|""|"S"
+16|1|2|"Hewlett, Mrs. (Mary D Kingcome) "|"female"|55|0|0|"248706"|16|""|"S"
+17|0|3|"Rice, Master. Eugene"|"male"|2|4|1|"382652"|29.125|""|"Q"
+18|1|2|"Williams, Mr. Charles Eugene"|"male"|NA|0|0|"244373"|13|""|"S"
+19|0|3|"Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)"|"female"|31|1|0|"345763"|18|""|"S"
+20|1|3|"Masselmani, Mrs. Fatima"|"female"|NA|0|0|"2649"|7.225|""|"C"
+21|0|2|"Fynney, Mr. Joseph J"|"male"|35|0|0|"239865"|26|""|"S"
+22|1|2|"Beesley, Mr. Lawrence"|"male"|34|0|0|"248698"|13|"D56"|"S"
+23|1|3|"McGowan, Miss. Anna \Annie\"|"female"|15|0|0|"330923"|8.0292|""|"Q"
+24|1|1|"Sloper, Mr. William Thompson"|"male"|28|0|0|"113788"|35.5|"A6"|"S"
+25|0|3|"Palsson, Miss. Torborg Danira"|"female"|8|3|1|"349909"|21.075|""|"S"
+26|1|3|"Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)"|"female"|38|1|5|"347077"|31.3875|""|"S"
+27|0|3|"Emir, Mr. Farred Chehab"|"male"|NA|0|0|"2631"|7.225|""|"C"
+28|0|1|"Fortune, Mr. Charles Alexander"|"male"|19|3|2|"19950"|263|"C23 C25 C27"|"S"
+29|1|3|"O'Dwyer, Miss. Ellen \Nellie\"|"female"|NA|0|0|"330959"|7.8792|""|"Q"
+30|0|3|"Todoroff, Mr. Lalio"|"male"|NA|0|0|"349216"|7.8958|""|"S"
+31|0|1|"Uruchurtu, Don. Manuel E"|"male"|40|0|0|"PC 17601"|27.7208|""|"C"
+32|1|1|"Spencer, Mrs. William Augustus (Marie Eugenie)"|"female"|NA|1|0|"PC 17569"|146.5208|"B78"|"C"
+33|1|3|"Glynn, Miss. Mary Agatha"|"female"|NA|0|0|"335677"|7.75|""|"Q"
+34|0|2|"Wheadon, Mr. Edward H"|"male"|66|0|0|"C.A. 24579"|10.5|""|"S"
+35|0|1|"Meyer, Mr. Edgar Joseph"|"male"|28|1|0|"PC 17604"|82.1708|""|"C"
+36|0|1|"Holverson, Mr. Alexander Oskar"|"male"|42|1|0|"113789"|52|""|"S"
+37|1|3|"Mamee, Mr. Hanna"|"male"|NA|0|0|"2677"|7.2292|""|"C"
+38|0|3|"Cann, Mr. Ernest Charles"|"male"|21|0|0|"A./5. 2152"|8.05|""|"S"
+39|0|3|"Vander Planke, Miss. Augusta Maria"|"female"|18|2|0|"345764"|18|""|"S"
+40|1|3|"Nicola-Yarred, Miss. Jamila"|"female"|14|1|0|"2651"|11.2417|""|"C"
+41|0|3|"Ahlin, Mrs. Johan (Johanna Persdotter Larsson)"|"female"|40|1|0|"7546"|9.475|""|"S"
+42|0|2|"Turpin, Mrs. William John Robert (Dorothy Ann Wonnacott)"|"female"|27|1|0|"11668"|21|""|"S"
+43|0|3|"Kraeff, Mr. Theodor"|"male"|NA|0|0|"349253"|7.8958|""|"C"
+44|1|2|"Laroche, Miss. Simonne Marie Anne Andree"|"female"|3|1|2|"SC/Paris 2123"|41.5792|""|"C"
+45|1|3|"Devaney, Miss. Margaret Delia"|"female"|19|0|0|"330958"|7.8792|""|"Q"
+46|0|3|"Rogers, Mr. William John"|"male"|NA|0|0|"S.C./A.4. 23567"|8.05|""|"S"
+47|0|3|"Lennon, Mr. Denis"|"male"|NA|1|0|"370371"|15.5|""|"Q"
+48|1|3|"O'Driscoll, Miss. Bridget"|"female"|NA|0|0|"14311"|7.75|""|"Q"
+49|0|3|"Samaan, Mr. Youssef"|"male"|NA|2|0|"2662"|21.6792|""|"C"
+50|0|3|"Arnold-Franchi, Mrs. Josef (Josefine Franchi)"|"female"|18|1|0|"349237"|17.8|""|"S"
+51|0|3|"Panula, Master. Juha Niilo"|"male"|7|4|1|"3101295"|39.6875|""|"S"
+52|0|3|"Nosworthy, Mr. Richard Cater"|"male"|21|0|0|"A/4. 39886"|7.8|""|"S"
+53|1|1|"Harper, Mrs. Henry Sleeper (Myna Haxtun)"|"female"|49|1|0|"PC 17572"|76.7292|"D33"|"C"
+54|1|2|"Faunthorpe, Mrs. Lizzie (Elizabeth Anne Wilkinson)"|"female"|29|1|0|"2926"|26|""|"S"
+55|0|1|"Ostby, Mr. Engelhart Cornelius"|"male"|65|0|1|"113509"|61.9792|"B30"|"C"
+56|1|1|"Woolner, Mr. Hugh"|"male"|NA|0|0|"19947"|35.5|"C52"|"S"
+57|1|2|"Rugg, Miss. Emily"|"female"|21|0|0|"C.A. 31026"|10.5|""|"S"
+58|0|3|"Novel, Mr. Mansouer"|"male"|28.5|0|0|"2697"|7.2292|""|"C"
+59|1|2|"West, Miss. Constance Mirium"|"female"|5|1|2|"C.A. 34651"|27.75|""|"S"
+60|0|3|"Goodwin, Master. William Frederick"|"male"|11|5|2|"CA 2144"|46.9|""|"S"
+61|0|3|"Sirayanian, Mr. Orsen"|"male"|22|0|0|"2669"|7.2292|""|"C"
+62|1|1|"Icard, Miss. Amelie"|"female"|38|0|0|"113572"|80|"B28"|""
+63|0|1|"Harris, Mr. Henry Birkhardt"|"male"|45|1|0|"36973"|83.475|"C83"|"S"
+64|0|3|"Skoog, Master. Harald"|"male"|4|3|2|"347088"|27.9|""|"S"
+65|0|1|"Stewart, Mr. Albert A"|"male"|NA|0|0|"PC 17605"|27.7208|""|"C"
+66|1|3|"Moubarek, Master. Gerios"|"male"|NA|1|1|"2661"|15.2458|""|"C"
+67|1|2|"Nye, Mrs. (Elizabeth Ramell)"|"female"|29|0|0|"C.A. 29395"|10.5|"F33"|"S"
+68|0|3|"Crease, Mr. Ernest James"|"male"|19|0|0|"S.P. 3464"|8.1583|""|"S"
+69|1|3|"Andersson, Miss. Erna Alexandra"|"female"|17|4|2|"3101281"|7.925|""|"S"
+70|0|3|"Kink, Mr. Vincenz"|"male"|26|2|0|"315151"|8.6625|""|"S"
+71|0|2|"Jenkin, Mr. Stephen Curnow"|"male"|32|0|0|"C.A. 33111"|10.5|""|"S"
+72|0|3|"Goodwin, Miss. Lillian Amy"|"female"|16|5|2|"CA 2144"|46.9|""|"S"
+73|0|2|"Hood, Mr. Ambrose Jr"|"male"|21|0|0|"S.O.C. 14879"|73.5|""|"S"
+74|0|3|"Chronopoulos, Mr. Apostolos"|"male"|26|1|0|"2680"|14.4542|""|"C"
+75|1|3|"Bing, Mr. Lee"|"male"|32|0|0|"1601"|56.4958|""|"S"
+76|0|3|"Moen, Mr. Sigurd Hansen"|"male"|25|0|0|"348123"|7.65|"F G73"|"S"
+77|0|3|"Staneff, Mr. Ivan"|"male"|NA|0|0|"349208"|7.8958|""|"S"
+78|0|3|"Moutal, Mr. Rahamin Haim"|"male"|NA|0|0|"374746"|8.05|""|"S"
+79|1|2|"Caldwell, Master. Alden Gates"|"male"|0.83|0|2|"248738"|29|""|"S"
+80|1|3|"Dowdell, Miss. Elizabeth"|"female"|30|0|0|"364516"|12.475|""|"S"
+81|0|3|"Waelens, Mr. Achille"|"male"|22|0|0|"345767"|9|""|"S"
+82|1|3|"Sheerlinck, Mr. Jan Baptist"|"male"|29|0|0|"345779"|9.5|""|"S"
+83|1|3|"McDermott, Miss. Brigdet Delia"|"female"|NA|0|0|"330932"|7.7875|""|"Q"
+84|0|1|"Carrau, Mr. Francisco M"|"male"|28|0|0|"113059"|47.1|""|"S"
+85|1|2|"Ilett, Miss. Bertha"|"female"|17|0|0|"SO/C 14885"|10.5|""|"S"
+86|1|3|"Backstrom, Mrs. Karl Alfred (Maria Mathilda Gustafsson)"|"female"|33|3|0|"3101278"|15.85|""|"S"
+87|0|3|"Ford, Mr. William Neal"|"male"|16|1|3|"W./C. 6608"|34.375|""|"S"
+88|0|3|"Slocovski, Mr. Selman Francis"|"male"|NA|0|0|"SOTON/OQ 392086"|8.05|""|"S"
+89|1|1|"Fortune, Miss. Mabel Helen"|"female"|23|3|2|"19950"|263|"C23 C25 C27"|"S"
+90|0|3|"Celotti, Mr. Francesco"|"male"|24|0|0|"343275"|8.05|""|"S"
+91|0|3|"Christmann, Mr. Emil"|"male"|29|0|0|"343276"|8.05|""|"S"
+92|0|3|"Andreasson, Mr. Paul Edvin"|"male"|20|0|0|"347466"|7.8542|""|"S"
+93|0|1|"Chaffee, Mr. Herbert Fuller"|"male"|46|1|0|"W.E.P. 5734"|61.175|"E31"|"S"
+94|0|3|"Dean, Mr. Bertram Frank"|"male"|26|1|2|"C.A. 2315"|20.575|""|"S"
+95|0|3|"Coxon, Mr. Daniel"|"male"|59|0|0|"364500"|7.25|""|"S"
+96|0|3|"Shorney, Mr. Charles Joseph"|"male"|NA|0|0|"374910"|8.05|""|"S"
+97|0|1|"Goldschmidt, Mr. George B"|"male"|71|0|0|"PC 17754"|34.6542|"A5"|"C"
+98|1|1|"Greenfield, Mr. William Bertram"|"male"|23|0|1|"PC 17759"|63.3583|"D10 D12"|"C"
+99|1|2|"Doling, Mrs. John T (Ada Julia Bone)"|"female"|34|0|1|"231919"|23|""|"S"
+100|0|2|"Kantor, Mr. Sinai"|"male"|34|1|0|"244367"|26|""|"S"
+101|0|3|"Petranec, Miss. Matilda"|"female"|28|0|0|"349245"|7.8958|""|"S"
+102|0|3|"Petroff, Mr. Pastcho (\Pentcho\)"|"male"|NA|0|0|"349215"|7.8958|""|"S"
+103|0|1|"White, Mr. Richard Frasar"|"male"|21|0|1|"35281"|77.2875|"D26"|"S"
+104|0|3|"Johansson, Mr. Gustaf Joel"|"male"|33|0|0|"7540"|8.6542|""|"S"
+105|0|3|"Gustafsson, Mr. Anders Vilhelm"|"male"|37|2|0|"3101276"|7.925|""|"S"
+106|0|3|"Mionoff, Mr. Stoytcho"|"male"|28|0|0|"349207"|7.8958|""|"S"
+107|1|3|"Salkjelsvik, Miss. Anna Kristine"|"female"|21|0|0|"343120"|7.65|""|"S"
+108|1|3|"Moss, Mr. Albert Johan"|"male"|NA|0|0|"312991"|7.775|""|"S"
+109|0|3|"Rekic, Mr. Tido"|"male"|38|0|0|"349249"|7.8958|""|"S"
+110|1|3|"Moran, Miss. Bertha"|"female"|NA|1|0|"371110"|24.15|""|"Q"
+111|0|1|"Porter, Mr. Walter Chamberlain"|"male"|47|0|0|"110465"|52|"C110"|"S"
+112|0|3|"Zabour, Miss. Hileni"|"female"|14.5|1|0|"2665"|14.4542|""|"C"
+113|0|3|"Barton, Mr. David John"|"male"|22|0|0|"324669"|8.05|""|"S"
+114|0|3|"Jussila, Miss. Katriina"|"female"|20|1|0|"4136"|9.825|""|"S"
+115|0|3|"Attalah, Miss. Malake"|"female"|17|0|0|"2627"|14.4583|""|"C"
+116|0|3|"Pekoniemi, Mr. Edvard"|"male"|21|0|0|"STON/O 2. 3101294"|7.925|""|"S"
+117|0|3|"Connors, Mr. Patrick"|"male"|70.5|0|0|"370369"|7.75|""|"Q"
+118|0|2|"Turpin, Mr. William John Robert"|"male"|29|1|0|"11668"|21|""|"S"
+119|0|1|"Baxter, Mr. Quigg Edmond"|"male"|24|0|1|"PC 17558"|247.5208|"B58 B60"|"C"
+120|0|3|"Andersson, Miss. Ellis Anna Maria"|"female"|2|4|2|"347082"|31.275|""|"S"
+121|0|2|"Hickman, Mr. Stanley George"|"male"|21|2|0|"S.O.C. 14879"|73.5|""|"S"
+122|0|3|"Moore, Mr. Leonard Charles"|"male"|NA|0|0|"A4. 54510"|8.05|""|"S"
+123|0|2|"Nasser, Mr. Nicholas"|"male"|32.5|1|0|"237736"|30.0708|""|"C"
+124|1|2|"Webber, Miss. Susan"|"female"|32.5|0|0|"27267"|13|"E101"|"S"
+125|0|1|"White, Mr. Percival Wayland"|"male"|54|0|1|"35281"|77.2875|"D26"|"S"
+126|1|3|"Nicola-Yarred, Master. Elias"|"male"|12|1|0|"2651"|11.2417|""|"C"
+127|0|3|"McMahon, Mr. Martin"|"male"|NA|0|0|"370372"|7.75|""|"Q"
+128|1|3|"Madsen, Mr. Fridtjof Arne"|"male"|24|0|0|"C 17369"|7.1417|""|"S"
+129|1|3|"Peter, Miss. Anna"|"female"|NA|1|1|"2668"|22.3583|"F E69"|"C"
+130|0|3|"Ekstrom, Mr. Johan"|"male"|45|0|0|"347061"|6.975|""|"S"
+131|0|3|"Drazenoic, Mr. Jozef"|"male"|33|0|0|"349241"|7.8958|""|"C"
+132|0|3|"Coelho, Mr. Domingos Fernandeo"|"male"|20|0|0|"SOTON/O.Q. 3101307"|7.05|""|"S"
+133|0|3|"Robins, Mrs. Alexander A (Grace Charity Laury)"|"female"|47|1|0|"A/5. 3337"|14.5|""|"S"
+134|1|2|"Weisz, Mrs. Leopold (Mathilde Francoise Pede)"|"female"|29|1|0|"228414"|26|""|"S"
+135|0|2|"Sobey, Mr. Samuel James Hayden"|"male"|25|0|0|"C.A. 29178"|13|""|"S"
+136|0|2|"Richard, Mr. Emile"|"male"|23|0|0|"SC/PARIS 2133"|15.0458|""|"C"
+137|1|1|"Newsom, Miss. Helen Monypeny"|"female"|19|0|2|"11752"|26.2833|"D47"|"S"
+138|0|1|"Futrelle, Mr. Jacques Heath"|"male"|37|1|0|"113803"|53.1|"C123"|"S"
+139|0|3|"Osen, Mr. Olaf Elon"|"male"|16|0|0|"7534"|9.2167|""|"S"
+140|0|1|"Giglio, Mr. Victor"|"male"|24|0|0|"PC 17593"|79.2|"B86"|"C"
+141|0|3|"Boulos, Mrs. Joseph (Sultana)"|"female"|NA|0|2|"2678"|15.2458|""|"C"
+142|1|3|"Nysten, Miss. Anna Sofia"|"female"|22|0|0|"347081"|7.75|""|"S"
+143|1|3|"Hakkarainen, Mrs. Pekka Pietari (Elin Matilda Dolck)"|"female"|24|1|0|"STON/O2. 3101279"|15.85|""|"S"
+144|0|3|"Burke, Mr. Jeremiah"|"male"|19|0|0|"365222"|6.75|""|"Q"
+145|0|2|"Andrew, Mr. Edgardo Samuel"|"male"|18|0|0|"231945"|11.5|""|"S"
+146|0|2|"Nicholls, Mr. Joseph Charles"|"male"|19|1|1|"C.A. 33112"|36.75|""|"S"
+147|1|3|"Andersson, Mr. August Edvard (\Wennerstrom\)"|"male"|27|0|0|"350043"|7.7958|""|"S"
+148|0|3|"Ford, Miss. Robina Maggie \Ruby\"|"female"|9|2|2|"W./C. 6608"|34.375|""|"S"
+149|0|2|"Navratil, Mr. Michel (\Louis M Hoffman\)"|"male"|36.5|0|2|"230080"|26|"F2"|"S"
+150|0|2|"Byles, Rev. Thomas Roussel Davids"|"male"|42|0|0|"244310"|13|""|"S"
+151|0|2|"Bateman, Rev. Robert James"|"male"|51|0|0|"S.O.P. 1166"|12.525|""|"S"
+152|1|1|"Pears, Mrs. Thomas (Edith Wearne)"|"female"|22|1|0|"113776"|66.6|"C2"|"S"
+153|0|3|"Meo, Mr. Alfonzo"|"male"|55.5|0|0|"A.5. 11206"|8.05|""|"S"
+154|0|3|"van Billiard, Mr. Austin Blyler"|"male"|40.5|0|2|"A/5. 851"|14.5|""|"S"
+155|0|3|"Olsen, Mr. Ole Martin"|"male"|NA|0|0|"Fa 265302"|7.3125|""|"S"
+156|0|1|"Williams, Mr. Charles Duane"|"male"|51|0|1|"PC 17597"|61.3792|""|"C"
+157|1|3|"Gilnagh, Miss. Katherine \Katie\"|"female"|16|0|0|"35851"|7.7333|""|"Q"
+158|0|3|"Corn, Mr. Harry"|"male"|30|0|0|"SOTON/OQ 392090"|8.05|""|"S"
+159|0|3|"Smiljanic, Mr. Mile"|"male"|NA|0|0|"315037"|8.6625|""|"S"
+160|0|3|"Sage, Master. Thomas Henry"|"male"|NA|8|2|"CA. 2343"|69.55|""|"S"
+161|0|3|"Cribb, Mr. John Hatfield"|"male"|44|0|1|"371362"|16.1|""|"S"
+162|1|2|"Watt, Mrs. James (Elizabeth \Bessie\ Inglis Milne)"|"female"|40|0|0|"C.A. 33595"|15.75|""|"S"
+163|0|3|"Bengtsson, Mr. John Viktor"|"male"|26|0|0|"347068"|7.775|""|"S"
+164|0|3|"Calic, Mr. Jovo"|"male"|17|0|0|"315093"|8.6625|""|"S"
+165|0|3|"Panula, Master. Eino Viljami"|"male"|1|4|1|"3101295"|39.6875|""|"S"
+166|1|3|"Goldsmith, Master. Frank John William \Frankie\"|"male"|9|0|2|"363291"|20.525|""|"S"
+167|1|1|"Chibnall, Mrs. (Edith Martha Bowerman)"|"female"|NA|0|1|"113505"|55|"E33"|"S"
+168|0|3|"Skoog, Mrs. William (Anna Bernhardina Karlsson)"|"female"|45|1|4|"347088"|27.9|""|"S"
+169|0|1|"Baumann, Mr. John D"|"male"|NA|0|0|"PC 17318"|25.925|""|"S"
+170|0|3|"Ling, Mr. Lee"|"male"|28|0|0|"1601"|56.4958|""|"S"
+171|0|1|"Van der hoef, Mr. Wyckoff"|"male"|61|0|0|"111240"|33.5|"B19"|"S"
+172|0|3|"Rice, Master. Arthur"|"male"|4|4|1|"382652"|29.125|""|"Q"
+173|1|3|"Johnson, Miss. Eleanor Ileen"|"female"|1|1|1|"347742"|11.1333|""|"S"
+174|0|3|"Sivola, Mr. Antti Wilhelm"|"male"|21|0|0|"STON/O 2. 3101280"|7.925|""|"S"
+175|0|1|"Smith, Mr. James Clinch"|"male"|56|0|0|"17764"|30.6958|"A7"|"C"
+176|0|3|"Klasen, Mr. Klas Albin"|"male"|18|1|1|"350404"|7.8542|""|"S"
+177|0|3|"Lefebre, Master. Henry Forbes"|"male"|NA|3|1|"4133"|25.4667|""|"S"
+178|0|1|"Isham, Miss. Ann Elizabeth"|"female"|50|0|0|"PC 17595"|28.7125|"C49"|"C"
+179|0|2|"Hale, Mr. Reginald"|"male"|30|0|0|"250653"|13|""|"S"
+180|0|3|"Leonard, Mr. Lionel"|"male"|36|0|0|"LINE"|0|""|"S"
+181|0|3|"Sage, Miss. Constance Gladys"|"female"|NA|8|2|"CA. 2343"|69.55|""|"S"
+182|0|2|"Pernot, Mr. Rene"|"male"|NA|0|0|"SC/PARIS 2131"|15.05|""|"C"
+183|0|3|"Asplund, Master. Clarence Gustaf Hugo"|"male"|9|4|2|"347077"|31.3875|""|"S"
+184|1|2|"Becker, Master. Richard F"|"male"|1|2|1|"230136"|39|"F4"|"S"
+185|1|3|"Kink-Heilmann, Miss. Luise Gretchen"|"female"|4|0|2|"315153"|22.025|""|"S"
+186|0|1|"Rood, Mr. Hugh Roscoe"|"male"|NA|0|0|"113767"|50|"A32"|"S"
+187|1|3|"O'Brien, Mrs. Thomas (Johanna \Hannah\ Godfrey)"|"female"|NA|1|0|"370365"|15.5|""|"Q"
+188|1|1|"Romaine, Mr. Charles Hallace (\Mr C Rolmane\)"|"male"|45|0|0|"111428"|26.55|""|"S"
+189|0|3|"Bourke, Mr. John"|"male"|40|1|1|"364849"|15.5|""|"Q"
+190|0|3|"Turcin, Mr. Stjepan"|"male"|36|0|0|"349247"|7.8958|""|"S"
+191|1|2|"Pinsky, Mrs. (Rosa)"|"female"|32|0|0|"234604"|13|""|"S"
+192|0|2|"Carbines, Mr. William"|"male"|19|0|0|"28424"|13|""|"S"
+193|1|3|"Andersen-Jensen, Miss. Carla Christine Nielsine"|"female"|19|1|0|"350046"|7.8542|""|"S"
+194|1|2|"Navratil, Master. Michel M"|"male"|3|1|1|"230080"|26|"F2"|"S"
+195|1|1|"Brown, Mrs. James Joseph (Margaret Tobin)"|"female"|44|0|0|"PC 17610"|27.7208|"B4"|"C"
+196|1|1|"Lurette, Miss. Elise"|"female"|58|0|0|"PC 17569"|146.5208|"B80"|"C"
+197|0|3|"Mernagh, Mr. Robert"|"male"|NA|0|0|"368703"|7.75|""|"Q"
+198|0|3|"Olsen, Mr. Karl Siegwart Andreas"|"male"|42|0|1|"4579"|8.4042|""|"S"
+199|1|3|"Madigan, Miss. Margaret \Maggie\"|"female"|NA|0|0|"370370"|7.75|""|"Q"
+200|0|2|"Yrois, Miss. Henriette (\Mrs Harbeck\)"|"female"|24|0|0|"248747"|13|""|"S"
+201|0|3|"Vande Walle, Mr. Nestor Cyriel"|"male"|28|0|0|"345770"|9.5|""|"S"
+202|0|3|"Sage, Mr. Frederick"|"male"|NA|8|2|"CA. 2343"|69.55|""|"S"
+203|0|3|"Johanson, Mr. Jakob Alfred"|"male"|34|0|0|"3101264"|6.4958|""|"S"
+204|0|3|"Youseff, Mr. Gerious"|"male"|45.5|0|0|"2628"|7.225|""|"C"
+205|1|3|"Cohen, Mr. Gurshon \Gus\"|"male"|18|0|0|"A/5 3540"|8.05|""|"S"
+206|0|3|"Strom, Miss. Telma Matilda"|"female"|2|0|1|"347054"|10.4625|"G6"|"S"
+207|0|3|"Backstrom, Mr. Karl Alfred"|"male"|32|1|0|"3101278"|15.85|""|"S"
+208|1|3|"Albimona, Mr. Nassef Cassem"|"male"|26|0|0|"2699"|18.7875|""|"C"
+209|1|3|"Carr, Miss. Helen \Ellen\"|"female"|16|0|0|"367231"|7.75|""|"Q"
+210|1|1|"Blank, Mr. Henry"|"male"|40|0|0|"112277"|31|"A31"|"C"
+211|0|3|"Ali, Mr. Ahmed"|"male"|24|0|0|"SOTON/O.Q. 3101311"|7.05|""|"S"
+212|1|2|"Cameron, Miss. Clear Annie"|"female"|35|0|0|"F.C.C. 13528"|21|""|"S"
+213|0|3|"Perkin, Mr. John Henry"|"male"|22|0|0|"A/5 21174"|7.25|""|"S"
+214|0|2|"Givard, Mr. Hans Kristensen"|"male"|30|0|0|"250646"|13|""|"S"
+215|0|3|"Kiernan, Mr. Philip"|"male"|NA|1|0|"367229"|7.75|""|"Q"
+216|1|1|"Newell, Miss. Madeleine"|"female"|31|1|0|"35273"|113.275|"D36"|"C"
+217|1|3|"Honkanen, Miss. Eliina"|"female"|27|0|0|"STON/O2. 3101283"|7.925|""|"S"
+218|0|2|"Jacobsohn, Mr. Sidney Samuel"|"male"|42|1|0|"243847"|27|""|"S"
+219|1|1|"Bazzani, Miss. Albina"|"female"|32|0|0|"11813"|76.2917|"D15"|"C"
+220|0|2|"Harris, Mr. Walter"|"male"|30|0|0|"W/C 14208"|10.5|""|"S"
+221|1|3|"Sunderland, Mr. Victor Francis"|"male"|16|0|0|"SOTON/OQ 392089"|8.05|""|"S"
+222|0|2|"Bracken, Mr. James H"|"male"|27|0|0|"220367"|13|""|"S"
+223|0|3|"Green, Mr. George Henry"|"male"|51|0|0|"21440"|8.05|""|"S"
+224|0|3|"Nenkoff, Mr. Christo"|"male"|NA|0|0|"349234"|7.8958|""|"S"
+225|1|1|"Hoyt, Mr. Frederick Maxfield"|"male"|38|1|0|"19943"|90|"C93"|"S"
+226|0|3|"Berglund, Mr. Karl Ivar Sven"|"male"|22|0|0|"PP 4348"|9.35|""|"S"
+227|1|2|"Mellors, Mr. William John"|"male"|19|0|0|"SW/PP 751"|10.5|""|"S"
+228|0|3|"Lovell, Mr. John Hall (\Henry\)"|"male"|20.5|0|0|"A/5 21173"|7.25|""|"S"
+229|0|2|"Fahlstrom, Mr. Arne Jonas"|"male"|18|0|0|"236171"|13|""|"S"
+230|0|3|"Lefebre, Miss. Mathilde"|"female"|NA|3|1|"4133"|25.4667|""|"S"
+231|1|1|"Harris, Mrs. Henry Birkhardt (Irene Wallach)"|"female"|35|1|0|"36973"|83.475|"C83"|"S"
+232|0|3|"Larsson, Mr. Bengt Edvin"|"male"|29|0|0|"347067"|7.775|""|"S"
+233|0|2|"Sjostedt, Mr. Ernst Adolf"|"male"|59|0|0|"237442"|13.5|""|"S"
+234|1|3|"Asplund, Miss. Lillian Gertrud"|"female"|5|4|2|"347077"|31.3875|""|"S"
+235|0|2|"Leyson, Mr. Robert William Norman"|"male"|24|0|0|"C.A. 29566"|10.5|""|"S"
+236|0|3|"Harknett, Miss. Alice Phoebe"|"female"|NA|0|0|"W./C. 6609"|7.55|""|"S"
+237|0|2|"Hold, Mr. Stephen"|"male"|44|1|0|"26707"|26|""|"S"
+238|1|2|"Collyer, Miss. Marjorie \Lottie\"|"female"|8|0|2|"C.A. 31921"|26.25|""|"S"
+239|0|2|"Pengelly, Mr. Frederick William"|"male"|19|0|0|"28665"|10.5|""|"S"
+240|0|2|"Hunt, Mr. George Henry"|"male"|33|0|0|"SCO/W 1585"|12.275|""|"S"
+241|0|3|"Zabour, Miss. Thamine"|"female"|NA|1|0|"2665"|14.4542|""|"C"
+242|1|3|"Murphy, Miss. Katherine \Kate\"|"female"|NA|1|0|"367230"|15.5|""|"Q"
+243|0|2|"Coleridge, Mr. Reginald Charles"|"male"|29|0|0|"W./C. 14263"|10.5|""|"S"
+244|0|3|"Maenpaa, Mr. Matti Alexanteri"|"male"|22|0|0|"STON/O 2. 3101275"|7.125|""|"S"
+245|0|3|"Attalah, Mr. Sleiman"|"male"|30|0|0|"2694"|7.225|""|"C"
+246|0|1|"Minahan, Dr. William Edward"|"male"|44|2|0|"19928"|90|"C78"|"Q"
+247|0|3|"Lindahl, Miss. Agda Thorilda Viktoria"|"female"|25|0|0|"347071"|7.775|""|"S"
+248|1|2|"Hamalainen, Mrs. William (Anna)"|"female"|24|0|2|"250649"|14.5|""|"S"
+249|1|1|"Beckwith, Mr. Richard Leonard"|"male"|37|1|1|"11751"|52.5542|"D35"|"S"
+250|0|2|"Carter, Rev. Ernest Courtenay"|"male"|54|1|0|"244252"|26|""|"S"
+251|0|3|"Reed, Mr. James George"|"male"|NA|0|0|"362316"|7.25|""|"S"
+252|0|3|"Strom, Mrs. Wilhelm (Elna Matilda Persson)"|"female"|29|1|1|"347054"|10.4625|"G6"|"S"
+253|0|1|"Stead, Mr. William Thomas"|"male"|62|0|0|"113514"|26.55|"C87"|"S"
+254|0|3|"Lobb, Mr. William Arthur"|"male"|30|1|0|"A/5. 3336"|16.1|""|"S"
+255|0|3|"Rosblom, Mrs. Viktor (Helena Wilhelmina)"|"female"|41|0|2|"370129"|20.2125|""|"S"
+256|1|3|"Touma, Mrs. Darwis (Hanne Youssef Razi)"|"female"|29|0|2|"2650"|15.2458|""|"C"
+257|1|1|"Thorne, Mrs. Gertrude Maybelle"|"female"|NA|0|0|"PC 17585"|79.2|""|"C"
+258|1|1|"Cherry, Miss. Gladys"|"female"|30|0|0|"110152"|86.5|"B77"|"S"
+259|1|1|"Ward, Miss. Anna"|"female"|35|0|0|"PC 17755"|512.3292|""|"C"
+260|1|2|"Parrish, Mrs. (Lutie Davis)"|"female"|50|0|1|"230433"|26|""|"S"
+261|0|3|"Smith, Mr. Thomas"|"male"|NA|0|0|"384461"|7.75|""|"Q"
+262|1|3|"Asplund, Master. Edvin Rojj Felix"|"male"|3|4|2|"347077"|31.3875|""|"S"
+263|0|1|"Taussig, Mr. Emil"|"male"|52|1|1|"110413"|79.65|"E67"|"S"
+264|0|1|"Harrison, Mr. William"|"male"|40|0|0|"112059"|0|"B94"|"S"
+265|0|3|"Henry, Miss. Delia"|"female"|NA|0|0|"382649"|7.75|""|"Q"
+266|0|2|"Reeves, Mr. David"|"male"|36|0|0|"C.A. 17248"|10.5|""|"S"
+267|0|3|"Panula, Mr. Ernesti Arvid"|"male"|16|4|1|"3101295"|39.6875|""|"S"
+268|1|3|"Persson, Mr. Ernst Ulrik"|"male"|25|1|0|"347083"|7.775|""|"S"
+269|1|1|"Graham, Mrs. William Thompson (Edith Junkins)"|"female"|58|0|1|"PC 17582"|153.4625|"C125"|"S"
+270|1|1|"Bissette, Miss. Amelia"|"female"|35|0|0|"PC 17760"|135.6333|"C99"|"S"
+271|0|1|"Cairns, Mr. Alexander"|"male"|NA|0|0|"113798"|31|""|"S"
+272|1|3|"Tornquist, Mr. William Henry"|"male"|25|0|0|"LINE"|0|""|"S"
+273|1|2|"Mellinger, Mrs. (Elizabeth Anne Maidment)"|"female"|41|0|1|"250644"|19.5|""|"S"
+274|0|1|"Natsch, Mr. Charles H"|"male"|37|0|1|"PC 17596"|29.7|"C118"|"C"
+275|1|3|"Healy, Miss. Hanora \Nora\"|"female"|NA|0|0|"370375"|7.75|""|"Q"
+276|1|1|"Andrews, Miss. Kornelia Theodosia"|"female"|63|1|0|"13502"|77.9583|"D7"|"S"
+277|0|3|"Lindblom, Miss. Augusta Charlotta"|"female"|45|0|0|"347073"|7.75|""|"S"
+278|0|2|"Parkes, Mr. Francis \Frank\"|"male"|NA|0|0|"239853"|0|""|"S"
+279|0|3|"Rice, Master. Eric"|"male"|7|4|1|"382652"|29.125|""|"Q"
+280|1|3|"Abbott, Mrs. Stanton (Rosa Hunt)"|"female"|35|1|1|"C.A. 2673"|20.25|""|"S"
+281|0|3|"Duane, Mr. Frank"|"male"|65|0|0|"336439"|7.75|""|"Q"
+282|0|3|"Olsson, Mr. Nils Johan Goransson"|"male"|28|0|0|"347464"|7.8542|""|"S"
+283|0|3|"de Pelsmaeker, Mr. Alfons"|"male"|16|0|0|"345778"|9.5|""|"S"
+284|1|3|"Dorking, Mr. Edward Arthur"|"male"|19|0|0|"A/5. 10482"|8.05|""|"S"
+285|0|1|"Smith, Mr. Richard William"|"male"|NA|0|0|"113056"|26|"A19"|"S"
+286|0|3|"Stankovic, Mr. Ivan"|"male"|33|0|0|"349239"|8.6625|""|"C"
+287|1|3|"de Mulder, Mr. Theodore"|"male"|30|0|0|"345774"|9.5|""|"S"
+288|0|3|"Naidenoff, Mr. Penko"|"male"|22|0|0|"349206"|7.8958|""|"S"
+289|1|2|"Hosono, Mr. Masabumi"|"male"|42|0|0|"237798"|13|""|"S"
+290|1|3|"Connolly, Miss. Kate"|"female"|22|0|0|"370373"|7.75|""|"Q"
+291|1|1|"Barber, Miss. Ellen \Nellie\"|"female"|26|0|0|"19877"|78.85|""|"S"
+292|1|1|"Bishop, Mrs. Dickinson H (Helen Walton)"|"female"|19|1|0|"11967"|91.0792|"B49"|"C"
+293|0|2|"Levy, Mr. Rene Jacques"|"male"|36|0|0|"SC/Paris 2163"|12.875|"D"|"C"
+294|0|3|"Haas, Miss. Aloisia"|"female"|24|0|0|"349236"|8.85|""|"S"
+295|0|3|"Mineff, Mr. Ivan"|"male"|24|0|0|"349233"|7.8958|""|"S"
+296|0|1|"Lewy, Mr. Ervin G"|"male"|NA|0|0|"PC 17612"|27.7208|""|"C"
+297|0|3|"Hanna, Mr. Mansour"|"male"|23.5|0|0|"2693"|7.2292|""|"C"
+298|0|1|"Allison, Miss. Helen Loraine"|"female"|2|1|2|"113781"|151.55|"C22 C26"|"S"
+299|1|1|"Saalfeld, Mr. Adolphe"|"male"|NA|0|0|"19988"|30.5|"C106"|"S"
+300|1|1|"Baxter, Mrs. James (Helene DeLaudeniere Chaput)"|"female"|50|0|1|"PC 17558"|247.5208|"B58 B60"|"C"
+301|1|3|"Kelly, Miss. Anna Katherine \Annie Kate\"|"female"|NA|0|0|"9234"|7.75|""|"Q"
+302|1|3|"McCoy, Mr. Bernard"|"male"|NA|2|0|"367226"|23.25|""|"Q"
+303|0|3|"Johnson, Mr. William Cahoone Jr"|"male"|19|0|0|"LINE"|0|""|"S"
+304|1|2|"Keane, Miss. Nora A"|"female"|NA|0|0|"226593"|12.35|"E101"|"Q"
+305|0|3|"Williams, Mr. Howard Hugh \Harry\"|"male"|NA|0|0|"A/5 2466"|8.05|""|"S"
+306|1|1|"Allison, Master. Hudson Trevor"|"male"|0.92|1|2|"113781"|151.55|"C22 C26"|"S"
+307|1|1|"Fleming, Miss. Margaret"|"female"|NA|0|0|"17421"|110.8833|""|"C"
+308|1|1|"Penasco y Castellana, Mrs. Victor de Satode (Maria Josefa Perez de Soto y Vallejo)"|"female"|17|1|0|"PC 17758"|108.9|"C65"|"C"
+309|0|2|"Abelson, Mr. Samuel"|"male"|30|1|0|"P/PP 3381"|24|""|"C"
+310|1|1|"Francatelli, Miss. Laura Mabel"|"female"|30|0|0|"PC 17485"|56.9292|"E36"|"C"
+311|1|1|"Hays, Miss. Margaret Bechstein"|"female"|24|0|0|"11767"|83.1583|"C54"|"C"
+312|1|1|"Ryerson, Miss. Emily Borie"|"female"|18|2|2|"PC 17608"|262.375|"B57 B59 B63 B66"|"C"
+313|0|2|"Lahtinen, Mrs. William (Anna Sylfven)"|"female"|26|1|1|"250651"|26|""|"S"
+314|0|3|"Hendekovic, Mr. Ignjac"|"male"|28|0|0|"349243"|7.8958|""|"S"
+315|0|2|"Hart, Mr. Benjamin"|"male"|43|1|1|"F.C.C. 13529"|26.25|""|"S"
+316|1|3|"Nilsson, Miss. Helmina Josefina"|"female"|26|0|0|"347470"|7.8542|""|"S"
+317|1|2|"Kantor, Mrs. Sinai (Miriam Sternin)"|"female"|24|1|0|"244367"|26|""|"S"
+318|0|2|"Moraweck, Dr. Ernest"|"male"|54|0|0|"29011"|14|""|"S"
+319|1|1|"Wick, Miss. Mary Natalie"|"female"|31|0|2|"36928"|164.8667|"C7"|"S"
+320|1|1|"Spedden, Mrs. Frederic Oakley (Margaretta Corning Stone)"|"female"|40|1|1|"16966"|134.5|"E34"|"C"
+321|0|3|"Dennis, Mr. Samuel"|"male"|22|0|0|"A/5 21172"|7.25|""|"S"
+322|0|3|"Danoff, Mr. Yoto"|"male"|27|0|0|"349219"|7.8958|""|"S"
+323|1|2|"Slayter, Miss. Hilda Mary"|"female"|30|0|0|"234818"|12.35|""|"Q"
+324|1|2|"Caldwell, Mrs. Albert Francis (Sylvia Mae Harbaugh)"|"female"|22|1|1|"248738"|29|""|"S"
+325|0|3|"Sage, Mr. George John Jr"|"male"|NA|8|2|"CA. 2343"|69.55|""|"S"
+326|1|1|"Young, Miss. Marie Grice"|"female"|36|0|0|"PC 17760"|135.6333|"C32"|"C"
+327|0|3|"Nysveen, Mr. Johan Hansen"|"male"|61|0|0|"345364"|6.2375|""|"S"
+328|1|2|"Ball, Mrs. (Ada E Hall)"|"female"|36|0|0|"28551"|13|"D"|"S"
+329|1|3|"Goldsmith, Mrs. Frank John (Emily Alice Brown)"|"female"|31|1|1|"363291"|20.525|""|"S"
+330|1|1|"Hippach, Miss. Jean Gertrude"|"female"|16|0|1|"111361"|57.9792|"B18"|"C"
+331|1|3|"McCoy, Miss. Agnes"|"female"|NA|2|0|"367226"|23.25|""|"Q"
+332|0|1|"Partner, Mr. Austen"|"male"|45.5|0|0|"113043"|28.5|"C124"|"S"
+333|0|1|"Graham, Mr. George Edward"|"male"|38|0|1|"PC 17582"|153.4625|"C91"|"S"
+334|0|3|"Vander Planke, Mr. Leo Edmondus"|"male"|16|2|0|"345764"|18|""|"S"
+335|1|1|"Frauenthal, Mrs. Henry William (Clara Heinsheimer)"|"female"|NA|1|0|"PC 17611"|133.65|""|"S"
+336|0|3|"Denkoff, Mr. Mitto"|"male"|NA|0|0|"349225"|7.8958|""|"S"
+337|0|1|"Pears, Mr. Thomas Clinton"|"male"|29|1|0|"113776"|66.6|"C2"|"S"
+338|1|1|"Burns, Miss. Elizabeth Margaret"|"female"|41|0|0|"16966"|134.5|"E40"|"C"
+339|1|3|"Dahl, Mr. Karl Edwart"|"male"|45|0|0|"7598"|8.05|""|"S"
+340|0|1|"Blackwell, Mr. Stephen Weart"|"male"|45|0|0|"113784"|35.5|"T"|"S"
+341|1|2|"Navratil, Master. Edmond Roger"|"male"|2|1|1|"230080"|26|"F2"|"S"
+342|1|1|"Fortune, Miss. Alice Elizabeth"|"female"|24|3|2|"19950"|263|"C23 C25 C27"|"S"
+343|0|2|"Collander, Mr. Erik Gustaf"|"male"|28|0|0|"248740"|13|""|"S"
+344|0|2|"Sedgwick, Mr. Charles Frederick Waddington"|"male"|25|0|0|"244361"|13|""|"S"
+345|0|2|"Fox, Mr. Stanley Hubert"|"male"|36|0|0|"229236"|13|""|"S"
+346|1|2|"Brown, Miss. Amelia \Mildred\"|"female"|24|0|0|"248733"|13|"F33"|"S"
+347|1|2|"Smith, Miss. Marion Elsie"|"female"|40|0|0|"31418"|13|""|"S"
+348|1|3|"Davison, Mrs. Thomas Henry (Mary E Finck)"|"female"|NA|1|0|"386525"|16.1|""|"S"
+349|1|3|"Coutts, Master. William Loch \William\"|"male"|3|1|1|"C.A. 37671"|15.9|""|"S"
+350|0|3|"Dimic, Mr. Jovan"|"male"|42|0|0|"315088"|8.6625|""|"S"
+351|0|3|"Odahl, Mr. Nils Martin"|"male"|23|0|0|"7267"|9.225|""|"S"
+352|0|1|"Williams-Lambert, Mr. Fletcher Fellows"|"male"|NA|0|0|"113510"|35|"C128"|"S"
+353|0|3|"Elias, Mr. Tannous"|"male"|15|1|1|"2695"|7.2292|""|"C"
+354|0|3|"Arnold-Franchi, Mr. Josef"|"male"|25|1|0|"349237"|17.8|""|"S"
+355|0|3|"Yousif, Mr. Wazli"|"male"|NA|0|0|"2647"|7.225|""|"C"
+356|0|3|"Vanden Steen, Mr. Leo Peter"|"male"|28|0|0|"345783"|9.5|""|"S"
+357|1|1|"Bowerman, Miss. Elsie Edith"|"female"|22|0|1|"113505"|55|"E33"|"S"
+358|0|2|"Funk, Miss. Annie Clemmer"|"female"|38|0|0|"237671"|13|""|"S"
+359|1|3|"McGovern, Miss. Mary"|"female"|NA|0|0|"330931"|7.8792|""|"Q"
+360|1|3|"Mockler, Miss. Helen Mary \Ellie\"|"female"|NA|0|0|"330980"|7.8792|""|"Q"
+361|0|3|"Skoog, Mr. Wilhelm"|"male"|40|1|4|"347088"|27.9|""|"S"
+362|0|2|"del Carlo, Mr. Sebastiano"|"male"|29|1|0|"SC/PARIS 2167"|27.7208|""|"C"
+363|0|3|"Barbara, Mrs. (Catherine David)"|"female"|45|0|1|"2691"|14.4542|""|"C"
+364|0|3|"Asim, Mr. Adola"|"male"|35|0|0|"SOTON/O.Q. 3101310"|7.05|""|"S"
+365|0|3|"O'Brien, Mr. Thomas"|"male"|NA|1|0|"370365"|15.5|""|"Q"
+366|0|3|"Adahl, Mr. Mauritz Nils Martin"|"male"|30|0|0|"C 7076"|7.25|""|"S"
+367|1|1|"Warren, Mrs. Frank Manley (Anna Sophia Atkinson)"|"female"|60|1|0|"110813"|75.25|"D37"|"C"
+368|1|3|"Moussa, Mrs. (Mantoura Boulos)"|"female"|NA|0|0|"2626"|7.2292|""|"C"
+369|1|3|"Jermyn, Miss. Annie"|"female"|NA|0|0|"14313"|7.75|""|"Q"
+370|1|1|"Aubart, Mme. Leontine Pauline"|"female"|24|0|0|"PC 17477"|69.3|"B35"|"C"
+371|1|1|"Harder, Mr. George Achilles"|"male"|25|1|0|"11765"|55.4417|"E50"|"C"
+372|0|3|"Wiklund, Mr. Jakob Alfred"|"male"|18|1|0|"3101267"|6.4958|""|"S"
+373|0|3|"Beavan, Mr. William Thomas"|"male"|19|0|0|"323951"|8.05|""|"S"
+374|0|1|"Ringhini, Mr. Sante"|"male"|22|0|0|"PC 17760"|135.6333|""|"C"
+375|0|3|"Palsson, Miss. Stina Viola"|"female"|3|3|1|"349909"|21.075|""|"S"
+376|1|1|"Meyer, Mrs. Edgar Joseph (Leila Saks)"|"female"|NA|1|0|"PC 17604"|82.1708|""|"C"
+377|1|3|"Landergren, Miss. Aurora Adelia"|"female"|22|0|0|"C 7077"|7.25|""|"S"
+378|0|1|"Widener, Mr. Harry Elkins"|"male"|27|0|2|"113503"|211.5|"C82"|"C"
+379|0|3|"Betros, Mr. Tannous"|"male"|20|0|0|"2648"|4.0125|""|"C"
+380|0|3|"Gustafsson, Mr. Karl Gideon"|"male"|19|0|0|"347069"|7.775|""|"S"
+381|1|1|"Bidois, Miss. Rosalie"|"female"|42|0|0|"PC 17757"|227.525|""|"C"
+382|1|3|"Nakid, Miss. Maria (\Mary\)"|"female"|1|0|2|"2653"|15.7417|""|"C"
+383|0|3|"Tikkanen, Mr. Juho"|"male"|32|0|0|"STON/O 2. 3101293"|7.925|""|"S"
+384|1|1|"Holverson, Mrs. Alexander Oskar (Mary Aline Towner)"|"female"|35|1|0|"113789"|52|""|"S"
+385|0|3|"Plotcharsky, Mr. Vasil"|"male"|NA|0|0|"349227"|7.8958|""|"S"
+386|0|2|"Davies, Mr. Charles Henry"|"male"|18|0|0|"S.O.C. 14879"|73.5|""|"S"
+387|0|3|"Goodwin, Master. Sidney Leonard"|"male"|1|5|2|"CA 2144"|46.9|""|"S"
+388|1|2|"Buss, Miss. Kate"|"female"|36|0|0|"27849"|13|""|"S"
+389|0|3|"Sadlier, Mr. Matthew"|"male"|NA|0|0|"367655"|7.7292|""|"Q"
+390|1|2|"Lehmann, Miss. Bertha"|"female"|17|0|0|"SC 1748"|12|""|"C"
+391|1|1|"Carter, Mr. William Ernest"|"male"|36|1|2|"113760"|120|"B96 B98"|"S"
+392|1|3|"Jansson, Mr. Carl Olof"|"male"|21|0|0|"350034"|7.7958|""|"S"
+393|0|3|"Gustafsson, Mr. Johan Birger"|"male"|28|2|0|"3101277"|7.925|""|"S"
+394|1|1|"Newell, Miss. Marjorie"|"female"|23|1|0|"35273"|113.275|"D36"|"C"
+395|1|3|"Sandstrom, Mrs. Hjalmar (Agnes Charlotta Bengtsson)"|"female"|24|0|2|"PP 9549"|16.7|"G6"|"S"
+396|0|3|"Johansson, Mr. Erik"|"male"|22|0|0|"350052"|7.7958|""|"S"
+397|0|3|"Olsson, Miss. Elina"|"female"|31|0|0|"350407"|7.8542|""|"S"
+398|0|2|"McKane, Mr. Peter David"|"male"|46|0|0|"28403"|26|""|"S"
+399|0|2|"Pain, Dr. Alfred"|"male"|23|0|0|"244278"|10.5|""|"S"
+400|1|2|"Trout, Mrs. William H (Jessie L)"|"female"|28|0|0|"240929"|12.65|""|"S"
+401|1|3|"Niskanen, Mr. Juha"|"male"|39|0|0|"STON/O 2. 3101289"|7.925|""|"S"
+402|0|3|"Adams, Mr. John"|"male"|26|0|0|"341826"|8.05|""|"S"
+403|0|3|"Jussila, Miss. Mari Aina"|"female"|21|1|0|"4137"|9.825|""|"S"
+404|0|3|"Hakkarainen, Mr. Pekka Pietari"|"male"|28|1|0|"STON/O2. 3101279"|15.85|""|"S"
+405|0|3|"Oreskovic, Miss. Marija"|"female"|20|0|0|"315096"|8.6625|""|"S"
+406|0|2|"Gale, Mr. Shadrach"|"male"|34|1|0|"28664"|21|""|"S"
+407|0|3|"Widegren, Mr. Carl/Charles Peter"|"male"|51|0|0|"347064"|7.75|""|"S"
+408|1|2|"Richards, Master. William Rowe"|"male"|3|1|1|"29106"|18.75|""|"S"
+409|0|3|"Birkeland, Mr. Hans Martin Monsen"|"male"|21|0|0|"312992"|7.775|""|"S"
+410|0|3|"Lefebre, Miss. Ida"|"female"|NA|3|1|"4133"|25.4667|""|"S"
+411|0|3|"Sdycoff, Mr. Todor"|"male"|NA|0|0|"349222"|7.8958|""|"S"
+412|0|3|"Hart, Mr. Henry"|"male"|NA|0|0|"394140"|6.8583|""|"Q"
+413|1|1|"Minahan, Miss. Daisy E"|"female"|33|1|0|"19928"|90|"C78"|"Q"
+414|0|2|"Cunningham, Mr. Alfred Fleming"|"male"|NA|0|0|"239853"|0|""|"S"
+415|1|3|"Sundman, Mr. Johan Julian"|"male"|44|0|0|"STON/O 2. 3101269"|7.925|""|"S"
+416|0|3|"Meek, Mrs. Thomas (Annie Louise Rowley)"|"female"|NA|0|0|"343095"|8.05|""|"S"
+417|1|2|"Drew, Mrs. James Vivian (Lulu Thorne Christian)"|"female"|34|1|1|"28220"|32.5|""|"S"
+418|1|2|"Silven, Miss. Lyyli Karoliina"|"female"|18|0|2|"250652"|13|""|"S"
+419|0|2|"Matthews, Mr. William John"|"male"|30|0|0|"28228"|13|""|"S"
+420|0|3|"Van Impe, Miss. Catharina"|"female"|10|0|2|"345773"|24.15|""|"S"
+421|0|3|"Gheorgheff, Mr. Stanio"|"male"|NA|0|0|"349254"|7.8958|""|"C"
+422|0|3|"Charters, Mr. David"|"male"|21|0|0|"A/5. 13032"|7.7333|""|"Q"
+423|0|3|"Zimmerman, Mr. Leo"|"male"|29|0|0|"315082"|7.875|""|"S"
+424|0|3|"Danbom, Mrs. Ernst Gilbert (Anna Sigrid Maria Brogren)"|"female"|28|1|1|"347080"|14.4|""|"S"
+425|0|3|"Rosblom, Mr. Viktor Richard"|"male"|18|1|1|"370129"|20.2125|""|"S"
+426|0|3|"Wiseman, Mr. Phillippe"|"male"|NA|0|0|"A/4. 34244"|7.25|""|"S"
+427|1|2|"Clarke, Mrs. Charles V (Ada Maria Winfield)"|"female"|28|1|0|"2003"|26|""|"S"
+428|1|2|"Phillips, Miss. Kate Florence (\Mrs Kate Louise Phillips Marshall\)"|"female"|19|0|0|"250655"|26|""|"S"
+429|0|3|"Flynn, Mr. James"|"male"|NA|0|0|"364851"|7.75|""|"Q"
+430|1|3|"Pickard, Mr. Berk (Berk Trembisky)"|"male"|32|0|0|"SOTON/O.Q. 392078"|8.05|"E10"|"S"
+431|1|1|"Bjornstrom-Steffansson, Mr. Mauritz Hakan"|"male"|28|0|0|"110564"|26.55|"C52"|"S"
+432|1|3|"Thorneycroft, Mrs. Percival (Florence Kate White)"|"female"|NA|1|0|"376564"|16.1|""|"S"
+433|1|2|"Louch, Mrs. Charles Alexander (Alice Adelaide Slow)"|"female"|42|1|0|"SC/AH 3085"|26|""|"S"
+434|0|3|"Kallio, Mr. Nikolai Erland"|"male"|17|0|0|"STON/O 2. 3101274"|7.125|""|"S"
+435|0|1|"Silvey, Mr. William Baird"|"male"|50|1|0|"13507"|55.9|"E44"|"S"
+436|1|1|"Carter, Miss. Lucile Polk"|"female"|14|1|2|"113760"|120|"B96 B98"|"S"
+437|0|3|"Ford, Miss. Doolina Margaret \Daisy\"|"female"|21|2|2|"W./C. 6608"|34.375|""|"S"
+438|1|2|"Richards, Mrs. Sidney (Emily Hocking)"|"female"|24|2|3|"29106"|18.75|""|"S"
+439|0|1|"Fortune, Mr. Mark"|"male"|64|1|4|"19950"|263|"C23 C25 C27"|"S"
+440|0|2|"Kvillner, Mr. Johan Henrik Johannesson"|"male"|31|0|0|"C.A. 18723"|10.5|""|"S"
+441|1|2|"Hart, Mrs. Benjamin (Esther Ada Bloomfield)"|"female"|45|1|1|"F.C.C. 13529"|26.25|""|"S"
+442|0|3|"Hampe, Mr. Leon"|"male"|20|0|0|"345769"|9.5|""|"S"
+443|0|3|"Petterson, Mr. Johan Emil"|"male"|25|1|0|"347076"|7.775|""|"S"
+444|1|2|"Reynaldo, Ms. Encarnacion"|"female"|28|0|0|"230434"|13|""|"S"
+445|1|3|"Johannesen-Bratthammer, Mr. Bernt"|"male"|NA|0|0|"65306"|8.1125|""|"S"
+446|1|1|"Dodge, Master. Washington"|"male"|4|0|2|"33638"|81.8583|"A34"|"S"
+447|1|2|"Mellinger, Miss. Madeleine Violet"|"female"|13|0|1|"250644"|19.5|""|"S"
+448|1|1|"Seward, Mr. Frederic Kimber"|"male"|34|0|0|"113794"|26.55|""|"S"
+449|1|3|"Baclini, Miss. Marie Catherine"|"female"|5|2|1|"2666"|19.2583|""|"C"
+450|1|1|"Peuchen, Major. Arthur Godfrey"|"male"|52|0|0|"113786"|30.5|"C104"|"S"
+451|0|2|"West, Mr. Edwy Arthur"|"male"|36|1|2|"C.A. 34651"|27.75|""|"S"
+452|0|3|"Hagland, Mr. Ingvald Olai Olsen"|"male"|NA|1|0|"65303"|19.9667|""|"S"
+453|0|1|"Foreman, Mr. Benjamin Laventall"|"male"|30|0|0|"113051"|27.75|"C111"|"C"
+454|1|1|"Goldenberg, Mr. Samuel L"|"male"|49|1|0|"17453"|89.1042|"C92"|"C"
+455|0|3|"Peduzzi, Mr. Joseph"|"male"|NA|0|0|"A/5 2817"|8.05|""|"S"
+456|1|3|"Jalsevac, Mr. Ivan"|"male"|29|0|0|"349240"|7.8958|""|"C"
+457|0|1|"Millet, Mr. Francis Davis"|"male"|65|0|0|"13509"|26.55|"E38"|"S"
+458|1|1|"Kenyon, Mrs. Frederick R (Marion)"|"female"|NA|1|0|"17464"|51.8625|"D21"|"S"
+459|1|2|"Toomey, Miss. Ellen"|"female"|50|0|0|"F.C.C. 13531"|10.5|""|"S"
+460|0|3|"O'Connor, Mr. Maurice"|"male"|NA|0|0|"371060"|7.75|""|"Q"
+461|1|1|"Anderson, Mr. Harry"|"male"|48|0|0|"19952"|26.55|"E12"|"S"
+462|0|3|"Morley, Mr. William"|"male"|34|0|0|"364506"|8.05|""|"S"
+463|0|1|"Gee, Mr. Arthur H"|"male"|47|0|0|"111320"|38.5|"E63"|"S"
+464|0|2|"Milling, Mr. Jacob Christian"|"male"|48|0|0|"234360"|13|""|"S"
+465|0|3|"Maisner, Mr. Simon"|"male"|NA|0|0|"A/S 2816"|8.05|""|"S"
+466|0|3|"Goncalves, Mr. Manuel Estanslas"|"male"|38|0|0|"SOTON/O.Q. 3101306"|7.05|""|"S"
+467|0|2|"Campbell, Mr. William"|"male"|NA|0|0|"239853"|0|""|"S"
+468|0|1|"Smart, Mr. John Montgomery"|"male"|56|0|0|"113792"|26.55|""|"S"
+469|0|3|"Scanlan, Mr. James"|"male"|NA|0|0|"36209"|7.725|""|"Q"
+470|1|3|"Baclini, Miss. Helene Barbara"|"female"|0.75|2|1|"2666"|19.2583|""|"C"
+471|0|3|"Keefe, Mr. Arthur"|"male"|NA|0|0|"323592"|7.25|""|"S"
+472|0|3|"Cacic, Mr. Luka"|"male"|38|0|0|"315089"|8.6625|""|"S"
+473|1|2|"West, Mrs. Edwy Arthur (Ada Mary Worth)"|"female"|33|1|2|"C.A. 34651"|27.75|""|"S"
+474|1|2|"Jerwan, Mrs. Amin S (Marie Marthe Thuillard)"|"female"|23|0|0|"SC/AH Basle 541"|13.7917|"D"|"C"
+475|0|3|"Strandberg, Miss. Ida Sofia"|"female"|22|0|0|"7553"|9.8375|""|"S"
+476|0|1|"Clifford, Mr. George Quincy"|"male"|NA|0|0|"110465"|52|"A14"|"S"
+477|0|2|"Renouf, Mr. Peter Henry"|"male"|34|1|0|"31027"|21|""|"S"
+478|0|3|"Braund, Mr. Lewis Richard"|"male"|29|1|0|"3460"|7.0458|""|"S"
+479|0|3|"Karlsson, Mr. Nils August"|"male"|22|0|0|"350060"|7.5208|""|"S"
+480|1|3|"Hirvonen, Miss. Hildur E"|"female"|2|0|1|"3101298"|12.2875|""|"S"
+481|0|3|"Goodwin, Master. Harold Victor"|"male"|9|5|2|"CA 2144"|46.9|""|"S"
+482|0|2|"Frost, Mr. Anthony Wood \Archie\"|"male"|NA|0|0|"239854"|0|""|"S"
+483|0|3|"Rouse, Mr. Richard Henry"|"male"|50|0|0|"A/5 3594"|8.05|""|"S"
+484|1|3|"Turkula, Mrs. (Hedwig)"|"female"|63|0|0|"4134"|9.5875|""|"S"
+485|1|1|"Bishop, Mr. Dickinson H"|"male"|25|1|0|"11967"|91.0792|"B49"|"C"
+486|0|3|"Lefebre, Miss. Jeannie"|"female"|NA|3|1|"4133"|25.4667|""|"S"
+487|1|1|"Hoyt, Mrs. Frederick Maxfield (Jane Anne Forby)"|"female"|35|1|0|"19943"|90|"C93"|"S"
+488|0|1|"Kent, Mr. Edward Austin"|"male"|58|0|0|"11771"|29.7|"B37"|"C"
+489|0|3|"Somerton, Mr. Francis William"|"male"|30|0|0|"A.5. 18509"|8.05|""|"S"
+490|1|3|"Coutts, Master. Eden Leslie \Neville\"|"male"|9|1|1|"C.A. 37671"|15.9|""|"S"
+491|0|3|"Hagland, Mr. Konrad Mathias Reiersen"|"male"|NA|1|0|"65304"|19.9667|""|"S"
+492|0|3|"Windelov, Mr. Einar"|"male"|21|0|0|"SOTON/OQ 3101317"|7.25|""|"S"
+493|0|1|"Molson, Mr. Harry Markland"|"male"|55|0|0|"113787"|30.5|"C30"|"S"
+494|0|1|"Artagaveytia, Mr. Ramon"|"male"|71|0|0|"PC 17609"|49.5042|""|"C"
+495|0|3|"Stanley, Mr. Edward Roland"|"male"|21|0|0|"A/4 45380"|8.05|""|"S"
+496|0|3|"Yousseff, Mr. Gerious"|"male"|NA|0|0|"2627"|14.4583|""|"C"
+497|1|1|"Eustis, Miss. Elizabeth Mussey"|"female"|54|1|0|"36947"|78.2667|"D20"|"C"
+498|0|3|"Shellard, Mr. Frederick William"|"male"|NA|0|0|"C.A. 6212"|15.1|""|"S"
+499|0|1|"Allison, Mrs. Hudson J C (Bessie Waldo Daniels)"|"female"|25|1|2|"113781"|151.55|"C22 C26"|"S"
+500|0|3|"Svensson, Mr. Olof"|"male"|24|0|0|"350035"|7.7958|""|"S"
+501|0|3|"Calic, Mr. Petar"|"male"|17|0|0|"315086"|8.6625|""|"S"
+502|0|3|"Canavan, Miss. Mary"|"female"|21|0|0|"364846"|7.75|""|"Q"
+503|0|3|"O'Sullivan, Miss. Bridget Mary"|"female"|NA|0|0|"330909"|7.6292|""|"Q"
+504|0|3|"Laitinen, Miss. Kristina Sofia"|"female"|37|0|0|"4135"|9.5875|""|"S"
+505|1|1|"Maioni, Miss. Roberta"|"female"|16|0|0|"110152"|86.5|"B79"|"S"
+506|0|1|"Penasco y Castellana, Mr. Victor de Satode"|"male"|18|1|0|"PC 17758"|108.9|"C65"|"C"
+507|1|2|"Quick, Mrs. Frederick Charles (Jane Richards)"|"female"|33|0|2|"26360"|26|""|"S"
+508|1|1|"Bradley, Mr. George (\George Arthur Brayton\)"|"male"|NA|0|0|"111427"|26.55|""|"S"
+509|0|3|"Olsen, Mr. Henry Margido"|"male"|28|0|0|"C 4001"|22.525|""|"S"
+510|1|3|"Lang, Mr. Fang"|"male"|26|0|0|"1601"|56.4958|""|"S"
+511|1|3|"Daly, Mr. Eugene Patrick"|"male"|29|0|0|"382651"|7.75|""|"Q"
+512|0|3|"Webber, Mr. James"|"male"|NA|0|0|"SOTON/OQ 3101316"|8.05|""|"S"
+513|1|1|"McGough, Mr. James Robert"|"male"|36|0|0|"PC 17473"|26.2875|"E25"|"S"
+514|1|1|"Rothschild, Mrs. Martin (Elizabeth L. Barrett)"|"female"|54|1|0|"PC 17603"|59.4|""|"C"
+515|0|3|"Coleff, Mr. Satio"|"male"|24|0|0|"349209"|7.4958|""|"S"
+516|0|1|"Walker, Mr. William Anderson"|"male"|47|0|0|"36967"|34.0208|"D46"|"S"
+517|1|2|"Lemore, Mrs. (Amelia Milley)"|"female"|34|0|0|"C.A. 34260"|10.5|"F33"|"S"
+518|0|3|"Ryan, Mr. Patrick"|"male"|NA|0|0|"371110"|24.15|""|"Q"
+519|1|2|"Angle, Mrs. William A (Florence \Mary\ Agnes Hughes)"|"female"|36|1|0|"226875"|26|""|"S"
+520|0|3|"Pavlovic, Mr. Stefo"|"male"|32|0|0|"349242"|7.8958|""|"S"
+521|1|1|"Perreault, Miss. Anne"|"female"|30|0|0|"12749"|93.5|"B73"|"S"
+522|0|3|"Vovk, Mr. Janko"|"male"|22|0|0|"349252"|7.8958|""|"S"
+523|0|3|"Lahoud, Mr. Sarkis"|"male"|NA|0|0|"2624"|7.225|""|"C"
+524|1|1|"Hippach, Mrs. Louis Albert (Ida Sophia Fischer)"|"female"|44|0|1|"111361"|57.9792|"B18"|"C"
+525|0|3|"Kassem, Mr. Fared"|"male"|NA|0|0|"2700"|7.2292|""|"C"
+526|0|3|"Farrell, Mr. James"|"male"|40.5|0|0|"367232"|7.75|""|"Q"
+527|1|2|"Ridsdale, Miss. Lucy"|"female"|50|0|0|"W./C. 14258"|10.5|""|"S"
+528|0|1|"Farthing, Mr. John"|"male"|NA|0|0|"PC 17483"|221.7792|"C95"|"S"
+529|0|3|"Salonen, Mr. Johan Werner"|"male"|39|0|0|"3101296"|7.925|""|"S"
+530|0|2|"Hocking, Mr. Richard George"|"male"|23|2|1|"29104"|11.5|""|"S"
+531|1|2|"Quick, Miss. Phyllis May"|"female"|2|1|1|"26360"|26|""|"S"
+532|0|3|"Toufik, Mr. Nakli"|"male"|NA|0|0|"2641"|7.2292|""|"C"
+533|0|3|"Elias, Mr. Joseph Jr"|"male"|17|1|1|"2690"|7.2292|""|"C"
+534|1|3|"Peter, Mrs. Catherine (Catherine Rizk)"|"female"|NA|0|2|"2668"|22.3583|""|"C"
+535|0|3|"Cacic, Miss. Marija"|"female"|30|0|0|"315084"|8.6625|""|"S"
+536|1|2|"Hart, Miss. Eva Miriam"|"female"|7|0|2|"F.C.C. 13529"|26.25|""|"S"
+537|0|1|"Butt, Major. Archibald Willingham"|"male"|45|0|0|"113050"|26.55|"B38"|"S"
+538|1|1|"LeRoy, Miss. Bertha"|"female"|30|0|0|"PC 17761"|106.425|""|"C"
+539|0|3|"Risien, Mr. Samuel Beard"|"male"|NA|0|0|"364498"|14.5|""|"S"
+540|1|1|"Frolicher, Miss. Hedwig Margaritha"|"female"|22|0|2|"13568"|49.5|"B39"|"C"
+541|1|1|"Crosby, Miss. Harriet R"|"female"|36|0|2|"WE/P 5735"|71|"B22"|"S"
+542|0|3|"Andersson, Miss. Ingeborg Constanzia"|"female"|9|4|2|"347082"|31.275|""|"S"
+543|0|3|"Andersson, Miss. Sigrid Elisabeth"|"female"|11|4|2|"347082"|31.275|""|"S"
+544|1|2|"Beane, Mr. Edward"|"male"|32|1|0|"2908"|26|""|"S"
+545|0|1|"Douglas, Mr. Walter Donald"|"male"|50|1|0|"PC 17761"|106.425|"C86"|"C"
+546|0|1|"Nicholson, Mr. Arthur Ernest"|"male"|64|0|0|"693"|26|""|"S"
+547|1|2|"Beane, Mrs. Edward (Ethel Clarke)"|"female"|19|1|0|"2908"|26|""|"S"
+548|1|2|"Padro y Manent, Mr. Julian"|"male"|NA|0|0|"SC/PARIS 2146"|13.8625|""|"C"
+549|0|3|"Goldsmith, Mr. Frank John"|"male"|33|1|1|"363291"|20.525|""|"S"
+550|1|2|"Davies, Master. John Morgan Jr"|"male"|8|1|1|"C.A. 33112"|36.75|""|"S"
+551|1|1|"Thayer, Mr. John Borland Jr"|"male"|17|0|2|"17421"|110.8833|"C70"|"C"
+552|0|2|"Sharp, Mr. Percival James R"|"male"|27|0|0|"244358"|26|""|"S"
+553|0|3|"O'Brien, Mr. Timothy"|"male"|NA|0|0|"330979"|7.8292|""|"Q"
+554|1|3|"Leeni, Mr. Fahim (\Philip Zenni\)"|"male"|22|0|0|"2620"|7.225|""|"C"
+555|1|3|"Ohman, Miss. Velin"|"female"|22|0|0|"347085"|7.775|""|"S"
+556|0|1|"Wright, Mr. George"|"male"|62|0|0|"113807"|26.55|""|"S"
+557|1|1|"Duff Gordon, Lady. (Lucille Christiana Sutherland) (\Mrs Morgan\)"|"female"|48|1|0|"11755"|39.6|"A16"|"C"
+558|0|1|"Robbins, Mr. Victor"|"male"|NA|0|0|"PC 17757"|227.525|""|"C"
+559|1|1|"Taussig, Mrs. Emil (Tillie Mandelbaum)"|"female"|39|1|1|"110413"|79.65|"E67"|"S"
+560|1|3|"de Messemaeker, Mrs. Guillaume Joseph (Emma)"|"female"|36|1|0|"345572"|17.4|""|"S"
+561|0|3|"Morrow, Mr. Thomas Rowan"|"male"|NA|0|0|"372622"|7.75|""|"Q"
+562|0|3|"Sivic, Mr. Husein"|"male"|40|0|0|"349251"|7.8958|""|"S"
+563|0|2|"Norman, Mr. Robert Douglas"|"male"|28|0|0|"218629"|13.5|""|"S"
+564|0|3|"Simmons, Mr. John"|"male"|NA|0|0|"SOTON/OQ 392082"|8.05|""|"S"
+565|0|3|"Meanwell, Miss. (Marion Ogden)"|"female"|NA|0|0|"SOTON/O.Q. 392087"|8.05|""|"S"
+566|0|3|"Davies, Mr. Alfred J"|"male"|24|2|0|"A/4 48871"|24.15|""|"S"
+567|0|3|"Stoytcheff, Mr. Ilia"|"male"|19|0|0|"349205"|7.8958|""|"S"
+568|0|3|"Palsson, Mrs. Nils (Alma Cornelia Berglund)"|"female"|29|0|4|"349909"|21.075|""|"S"
+569|0|3|"Doharr, Mr. Tannous"|"male"|NA|0|0|"2686"|7.2292|""|"C"
+570|1|3|"Jonsson, Mr. Carl"|"male"|32|0|0|"350417"|7.8542|""|"S"
+571|1|2|"Harris, Mr. George"|"male"|62|0|0|"S.W./PP 752"|10.5|""|"S"
+572|1|1|"Appleton, Mrs. Edward Dale (Charlotte Lamson)"|"female"|53|2|0|"11769"|51.4792|"C101"|"S"
+573|1|1|"Flynn, Mr. John Irwin (\Irving\)"|"male"|36|0|0|"PC 17474"|26.3875|"E25"|"S"
+574|1|3|"Kelly, Miss. Mary"|"female"|NA|0|0|"14312"|7.75|""|"Q"
+575|0|3|"Rush, Mr. Alfred George John"|"male"|16|0|0|"A/4. 20589"|8.05|""|"S"
+576|0|3|"Patchett, Mr. George"|"male"|19|0|0|"358585"|14.5|""|"S"
+577|1|2|"Garside, Miss. Ethel"|"female"|34|0|0|"243880"|13|""|"S"
+578|1|1|"Silvey, Mrs. William Baird (Alice Munger)"|"female"|39|1|0|"13507"|55.9|"E44"|"S"
+579|0|3|"Caram, Mrs. Joseph (Maria Elias)"|"female"|NA|1|0|"2689"|14.4583|""|"C"
+580|1|3|"Jussila, Mr. Eiriik"|"male"|32|0|0|"STON/O 2. 3101286"|7.925|""|"S"
+581|1|2|"Christy, Miss. Julie Rachel"|"female"|25|1|1|"237789"|30|""|"S"
+582|1|1|"Thayer, Mrs. John Borland (Marian Longstreth Morris)"|"female"|39|1|1|"17421"|110.8833|"C68"|"C"
+583|0|2|"Downton, Mr. William James"|"male"|54|0|0|"28403"|26|""|"S"
+584|0|1|"Ross, Mr. John Hugo"|"male"|36|0|0|"13049"|40.125|"A10"|"C"
+585|0|3|"Paulner, Mr. Uscher"|"male"|NA|0|0|"3411"|8.7125|""|"C"
+586|1|1|"Taussig, Miss. Ruth"|"female"|18|0|2|"110413"|79.65|"E68"|"S"
+587|0|2|"Jarvis, Mr. John Denzil"|"male"|47|0|0|"237565"|15|""|"S"
+588|1|1|"Frolicher-Stehli, Mr. Maxmillian"|"male"|60|1|1|"13567"|79.2|"B41"|"C"
+589|0|3|"Gilinski, Mr. Eliezer"|"male"|22|0|0|"14973"|8.05|""|"S"
+590|0|3|"Murdlin, Mr. Joseph"|"male"|NA|0|0|"A./5. 3235"|8.05|""|"S"
+591|0|3|"Rintamaki, Mr. Matti"|"male"|35|0|0|"STON/O 2. 3101273"|7.125|""|"S"
+592|1|1|"Stephenson, Mrs. Walter Bertram (Martha Eustis)"|"female"|52|1|0|"36947"|78.2667|"D20"|"C"
+593|0|3|"Elsbury, Mr. William James"|"male"|47|0|0|"A/5 3902"|7.25|""|"S"
+594|0|3|"Bourke, Miss. Mary"|"female"|NA|0|2|"364848"|7.75|""|"Q"
+595|0|2|"Chapman, Mr. John Henry"|"male"|37|1|0|"SC/AH 29037"|26|""|"S"
+596|0|3|"Van Impe, Mr. Jean Baptiste"|"male"|36|1|1|"345773"|24.15|""|"S"
+597|1|2|"Leitch, Miss. Jessie Wills"|"female"|NA|0|0|"248727"|33|""|"S"
+598|0|3|"Johnson, Mr. Alfred"|"male"|49|0|0|"LINE"|0|""|"S"
+599|0|3|"Boulos, Mr. Hanna"|"male"|NA|0|0|"2664"|7.225|""|"C"
+600|1|1|"Duff Gordon, Sir. Cosmo Edmund (\Mr Morgan\)"|"male"|49|1|0|"PC 17485"|56.9292|"A20"|"C"
+601|1|2|"Jacobsohn, Mrs. Sidney Samuel (Amy Frances Christy)"|"female"|24|2|1|"243847"|27|""|"S"
+602|0|3|"Slabenoff, Mr. Petco"|"male"|NA|0|0|"349214"|7.8958|""|"S"
+603|0|1|"Harrington, Mr. Charles H"|"male"|NA|0|0|"113796"|42.4|""|"S"
+604|0|3|"Torber, Mr. Ernst William"|"male"|44|0|0|"364511"|8.05|""|"S"
+605|1|1|"Homer, Mr. Harry (\Mr E Haven\)"|"male"|35|0|0|"111426"|26.55|""|"C"
+606|0|3|"Lindell, Mr. Edvard Bengtsson"|"male"|36|1|0|"349910"|15.55|""|"S"
+607|0|3|"Karaic, Mr. Milan"|"male"|30|0|0|"349246"|7.8958|""|"S"
+608|1|1|"Daniel, Mr. Robert Williams"|"male"|27|0|0|"113804"|30.5|""|"S"
+609|1|2|"Laroche, Mrs. Joseph (Juliette Marie Louise Lafargue)"|"female"|22|1|2|"SC/Paris 2123"|41.5792|""|"C"
+610|1|1|"Shutes, Miss. Elizabeth W"|"female"|40|0|0|"PC 17582"|153.4625|"C125"|"S"
+611|0|3|"Andersson, Mrs. Anders Johan (Alfrida Konstantia Brogren)"|"female"|39|1|5|"347082"|31.275|""|"S"
+612|0|3|"Jardin, Mr. Jose Neto"|"male"|NA|0|0|"SOTON/O.Q. 3101305"|7.05|""|"S"
+613|1|3|"Murphy, Miss. Margaret Jane"|"female"|NA|1|0|"367230"|15.5|""|"Q"
+614|0|3|"Horgan, Mr. John"|"male"|NA|0|0|"370377"|7.75|""|"Q"
+615|0|3|"Brocklebank, Mr. William Alfred"|"male"|35|0|0|"364512"|8.05|""|"S"
+616|1|2|"Herman, Miss. Alice"|"female"|24|1|2|"220845"|65|""|"S"
+617|0|3|"Danbom, Mr. Ernst Gilbert"|"male"|34|1|1|"347080"|14.4|""|"S"
+618|0|3|"Lobb, Mrs. William Arthur (Cordelia K Stanlick)"|"female"|26|1|0|"A/5. 3336"|16.1|""|"S"
+619|1|2|"Becker, Miss. Marion Louise"|"female"|4|2|1|"230136"|39|"F4"|"S"
+620|0|2|"Gavey, Mr. Lawrence"|"male"|26|0|0|"31028"|10.5|""|"S"
+621|0|3|"Yasbeck, Mr. Antoni"|"male"|27|1|0|"2659"|14.4542|""|"C"
+622|1|1|"Kimball, Mr. Edwin Nelson Jr"|"male"|42|1|0|"11753"|52.5542|"D19"|"S"
+623|1|3|"Nakid, Mr. Sahid"|"male"|20|1|1|"2653"|15.7417|""|"C"
+624|0|3|"Hansen, Mr. Henry Damsgaard"|"male"|21|0|0|"350029"|7.8542|""|"S"
+625|0|3|"Bowen, Mr. David John \Dai\"|"male"|21|0|0|"54636"|16.1|""|"S"
+626|0|1|"Sutton, Mr. Frederick"|"male"|61|0|0|"36963"|32.3208|"D50"|"S"
+627|0|2|"Kirkland, Rev. Charles Leonard"|"male"|57|0|0|"219533"|12.35|""|"Q"
+628|1|1|"Longley, Miss. Gretchen Fiske"|"female"|21|0|0|"13502"|77.9583|"D9"|"S"
+629|0|3|"Bostandyeff, Mr. Guentcho"|"male"|26|0|0|"349224"|7.8958|""|"S"
+630|0|3|"O'Connell, Mr. Patrick D"|"male"|NA|0|0|"334912"|7.7333|""|"Q"
+631|1|1|"Barkworth, Mr. Algernon Henry Wilson"|"male"|80|0|0|"27042"|30|"A23"|"S"
+632|0|3|"Lundahl, Mr. Johan Svensson"|"male"|51|0|0|"347743"|7.0542|""|"S"
+633|1|1|"Stahelin-Maeglin, Dr. Max"|"male"|32|0|0|"13214"|30.5|"B50"|"C"
+634|0|1|"Parr, Mr. William Henry Marsh"|"male"|NA|0|0|"112052"|0|""|"S"
+635|0|3|"Skoog, Miss. Mabel"|"female"|9|3|2|"347088"|27.9|""|"S"
+636|1|2|"Davis, Miss. Mary"|"female"|28|0|0|"237668"|13|""|"S"
+637|0|3|"Leinonen, Mr. Antti Gustaf"|"male"|32|0|0|"STON/O 2. 3101292"|7.925|""|"S"
+638|0|2|"Collyer, Mr. Harvey"|"male"|31|1|1|"C.A. 31921"|26.25|""|"S"
+639|0|3|"Panula, Mrs. Juha (Maria Emilia Ojala)"|"female"|41|0|5|"3101295"|39.6875|""|"S"
+640|0|3|"Thorneycroft, Mr. Percival"|"male"|NA|1|0|"376564"|16.1|""|"S"
+641|0|3|"Jensen, Mr. Hans Peder"|"male"|20|0|0|"350050"|7.8542|""|"S"
+642|1|1|"Sagesser, Mlle. Emma"|"female"|24|0|0|"PC 17477"|69.3|"B35"|"C"
+643|0|3|"Skoog, Miss. Margit Elizabeth"|"female"|2|3|2|"347088"|27.9|""|"S"
+644|1|3|"Foo, Mr. Choong"|"male"|NA|0|0|"1601"|56.4958|""|"S"
+645|1|3|"Baclini, Miss. Eugenie"|"female"|0.75|2|1|"2666"|19.2583|""|"C"
+646|1|1|"Harper, Mr. Henry Sleeper"|"male"|48|1|0|"PC 17572"|76.7292|"D33"|"C"
+647|0|3|"Cor, Mr. Liudevit"|"male"|19|0|0|"349231"|7.8958|""|"S"
+648|1|1|"Simonius-Blumer, Col. Oberst Alfons"|"male"|56|0|0|"13213"|35.5|"A26"|"C"
+649|0|3|"Willey, Mr. Edward"|"male"|NA|0|0|"S.O./P.P. 751"|7.55|""|"S"
+650|1|3|"Stanley, Miss. Amy Zillah Elsie"|"female"|23|0|0|"CA. 2314"|7.55|""|"S"
+651|0|3|"Mitkoff, Mr. Mito"|"male"|NA|0|0|"349221"|7.8958|""|"S"
+652|1|2|"Doling, Miss. Elsie"|"female"|18|0|1|"231919"|23|""|"S"
+653|0|3|"Kalvik, Mr. Johannes Halvorsen"|"male"|21|0|0|"8475"|8.4333|""|"S"
+654|1|3|"O'Leary, Miss. Hanora \Norah\"|"female"|NA|0|0|"330919"|7.8292|""|"Q"
+655|0|3|"Hegarty, Miss. Hanora \Nora\"|"female"|18|0|0|"365226"|6.75|""|"Q"
+656|0|2|"Hickman, Mr. Leonard Mark"|"male"|24|2|0|"S.O.C. 14879"|73.5|""|"S"
+657|0|3|"Radeff, Mr. Alexander"|"male"|NA|0|0|"349223"|7.8958|""|"S"
+658|0|3|"Bourke, Mrs. John (Catherine)"|"female"|32|1|1|"364849"|15.5|""|"Q"
+659|0|2|"Eitemiller, Mr. George Floyd"|"male"|23|0|0|"29751"|13|""|"S"
+660|0|1|"Newell, Mr. Arthur Webster"|"male"|58|0|2|"35273"|113.275|"D48"|"C"
+661|1|1|"Frauenthal, Dr. Henry William"|"male"|50|2|0|"PC 17611"|133.65|""|"S"
+662|0|3|"Badt, Mr. Mohamed"|"male"|40|0|0|"2623"|7.225|""|"C"
+663|0|1|"Colley, Mr. Edward Pomeroy"|"male"|47|0|0|"5727"|25.5875|"E58"|"S"
+664|0|3|"Coleff, Mr. Peju"|"male"|36|0|0|"349210"|7.4958|""|"S"
+665|1|3|"Lindqvist, Mr. Eino William"|"male"|20|1|0|"STON/O 2. 3101285"|7.925|""|"S"
+666|0|2|"Hickman, Mr. Lewis"|"male"|32|2|0|"S.O.C. 14879"|73.5|""|"S"
+667|0|2|"Butler, Mr. Reginald Fenton"|"male"|25|0|0|"234686"|13|""|"S"
+668|0|3|"Rommetvedt, Mr. Knud Paust"|"male"|NA|0|0|"312993"|7.775|""|"S"
+669|0|3|"Cook, Mr. Jacob"|"male"|43|0|0|"A/5 3536"|8.05|""|"S"
+670|1|1|"Taylor, Mrs. Elmer Zebley (Juliet Cummins Wright)"|"female"|NA|1|0|"19996"|52|"C126"|"S"
+671|1|2|"Brown, Mrs. Thomas William Solomon (Elizabeth Catherine Ford)"|"female"|40|1|1|"29750"|39|""|"S"
+672|0|1|"Davidson, Mr. Thornton"|"male"|31|1|0|"F.C. 12750"|52|"B71"|"S"
+673|0|2|"Mitchell, Mr. Henry Michael"|"male"|70|0|0|"C.A. 24580"|10.5|""|"S"
+674|1|2|"Wilhelms, Mr. Charles"|"male"|31|0|0|"244270"|13|""|"S"
+675|0|2|"Watson, Mr. Ennis Hastings"|"male"|NA|0|0|"239856"|0|""|"S"
+676|0|3|"Edvardsson, Mr. Gustaf Hjalmar"|"male"|18|0|0|"349912"|7.775|""|"S"
+677|0|3|"Sawyer, Mr. Frederick Charles"|"male"|24.5|0|0|"342826"|8.05|""|"S"
+678|1|3|"Turja, Miss. Anna Sofia"|"female"|18|0|0|"4138"|9.8417|""|"S"
+679|0|3|"Goodwin, Mrs. Frederick (Augusta Tyler)"|"female"|43|1|6|"CA 2144"|46.9|""|"S"
+680|1|1|"Cardeza, Mr. Thomas Drake Martinez"|"male"|36|0|1|"PC 17755"|512.3292|"B51 B53 B55"|"C"
+681|0|3|"Peters, Miss. Katie"|"female"|NA|0|0|"330935"|8.1375|""|"Q"
+682|1|1|"Hassab, Mr. Hammad"|"male"|27|0|0|"PC 17572"|76.7292|"D49"|"C"
+683|0|3|"Olsvigen, Mr. Thor Anderson"|"male"|20|0|0|"6563"|9.225|""|"S"
+684|0|3|"Goodwin, Mr. Charles Edward"|"male"|14|5|2|"CA 2144"|46.9|""|"S"
+685|0|2|"Brown, Mr. Thomas William Solomon"|"male"|60|1|1|"29750"|39|""|"S"
+686|0|2|"Laroche, Mr. Joseph Philippe Lemercier"|"male"|25|1|2|"SC/Paris 2123"|41.5792|""|"C"
+687|0|3|"Panula, Mr. Jaako Arnold"|"male"|14|4|1|"3101295"|39.6875|""|"S"
+688|0|3|"Dakic, Mr. Branko"|"male"|19|0|0|"349228"|10.1708|""|"S"
+689|0|3|"Fischer, Mr. Eberhard Thelander"|"male"|18|0|0|"350036"|7.7958|""|"S"
+690|1|1|"Madill, Miss. Georgette Alexandra"|"female"|15|0|1|"24160"|211.3375|"B5"|"S"
+691|1|1|"Dick, Mr. Albert Adrian"|"male"|31|1|0|"17474"|57|"B20"|"S"
+692|1|3|"Karun, Miss. Manca"|"female"|4|0|1|"349256"|13.4167|""|"C"
+693|1|3|"Lam, Mr. Ali"|"male"|NA|0|0|"1601"|56.4958|""|"S"
+694|0|3|"Saad, Mr. Khalil"|"male"|25|0|0|"2672"|7.225|""|"C"
+695|0|1|"Weir, Col. John"|"male"|60|0|0|"113800"|26.55|""|"S"
+696|0|2|"Chapman, Mr. Charles Henry"|"male"|52|0|0|"248731"|13.5|""|"S"
+697|0|3|"Kelly, Mr. James"|"male"|44|0|0|"363592"|8.05|""|"S"
+698|1|3|"Mullens, Miss. Katherine \Katie\"|"female"|NA|0|0|"35852"|7.7333|""|"Q"
+699|0|1|"Thayer, Mr. John Borland"|"male"|49|1|1|"17421"|110.8833|"C68"|"C"
+700|0|3|"Humblen, Mr. Adolf Mathias Nicolai Olsen"|"male"|42|0|0|"348121"|7.65|"F G63"|"S"
+701|1|1|"Astor, Mrs. John Jacob (Madeleine Talmadge Force)"|"female"|18|1|0|"PC 17757"|227.525|"C62 C64"|"C"
+702|1|1|"Silverthorne, Mr. Spencer Victor"|"male"|35|0|0|"PC 17475"|26.2875|"E24"|"S"
+703|0|3|"Barbara, Miss. Saiide"|"female"|18|0|1|"2691"|14.4542|""|"C"
+704|0|3|"Gallagher, Mr. Martin"|"male"|25|0|0|"36864"|7.7417|""|"Q"
+705|0|3|"Hansen, Mr. Henrik Juul"|"male"|26|1|0|"350025"|7.8542|""|"S"
+706|0|2|"Morley, Mr. Henry Samuel (\Mr Henry Marshall\)"|"male"|39|0|0|"250655"|26|""|"S"
+707|1|2|"Kelly, Mrs. Florence \Fannie\"|"female"|45|0|0|"223596"|13.5|""|"S"
+708|1|1|"Calderhead, Mr. Edward Pennington"|"male"|42|0|0|"PC 17476"|26.2875|"E24"|"S"
+709|1|1|"Cleaver, Miss. Alice"|"female"|22|0|0|"113781"|151.55|""|"S"
+710|1|3|"Moubarek, Master. Halim Gonios (\William George\)"|"male"|NA|1|1|"2661"|15.2458|""|"C"
+711|1|1|"Mayne, Mlle. Berthe Antonine (\Mrs de Villiers\)"|"female"|24|0|0|"PC 17482"|49.5042|"C90"|"C"
+712|0|1|"Klaber, Mr. Herman"|"male"|NA|0|0|"113028"|26.55|"C124"|"S"
+713|1|1|"Taylor, Mr. Elmer Zebley"|"male"|48|1|0|"19996"|52|"C126"|"S"
+714|0|3|"Larsson, Mr. August Viktor"|"male"|29|0|0|"7545"|9.4833|""|"S"
+715|0|2|"Greenberg, Mr. Samuel"|"male"|52|0|0|"250647"|13|""|"S"
+716|0|3|"Soholt, Mr. Peter Andreas Lauritz Andersen"|"male"|19|0|0|"348124"|7.65|"F G73"|"S"
+717|1|1|"Endres, Miss. Caroline Louise"|"female"|38|0|0|"PC 17757"|227.525|"C45"|"C"
+718|1|2|"Troutt, Miss. Edwina Celia \Winnie\"|"female"|27|0|0|"34218"|10.5|"E101"|"S"
+719|0|3|"McEvoy, Mr. Michael"|"male"|NA|0|0|"36568"|15.5|""|"Q"
+720|0|3|"Johnson, Mr. Malkolm Joackim"|"male"|33|0|0|"347062"|7.775|""|"S"
+721|1|2|"Harper, Miss. Annie Jessie \Nina\"|"female"|6|0|1|"248727"|33|""|"S"
+722|0|3|"Jensen, Mr. Svend Lauritz"|"male"|17|1|0|"350048"|7.0542|""|"S"
+723|0|2|"Gillespie, Mr. William Henry"|"male"|34|0|0|"12233"|13|""|"S"
+724|0|2|"Hodges, Mr. Henry Price"|"male"|50|0|0|"250643"|13|""|"S"
+725|1|1|"Chambers, Mr. Norman Campbell"|"male"|27|1|0|"113806"|53.1|"E8"|"S"
+726|0|3|"Oreskovic, Mr. Luka"|"male"|20|0|0|"315094"|8.6625|""|"S"
+727|1|2|"Renouf, Mrs. Peter Henry (Lillian Jefferys)"|"female"|30|3|0|"31027"|21|""|"S"
+728|1|3|"Mannion, Miss. Margareth"|"female"|NA|0|0|"36866"|7.7375|""|"Q"
+729|0|2|"Bryhl, Mr. Kurt Arnold Gottfrid"|"male"|25|1|0|"236853"|26|""|"S"
+730|0|3|"Ilmakangas, Miss. Pieta Sofia"|"female"|25|1|0|"STON/O2. 3101271"|7.925|""|"S"
+731|1|1|"Allen, Miss. Elisabeth Walton"|"female"|29|0|0|"24160"|211.3375|"B5"|"S"
+732|0|3|"Hassan, Mr. Houssein G N"|"male"|11|0|0|"2699"|18.7875|""|"C"
+733|0|2|"Knight, Mr. Robert J"|"male"|NA|0|0|"239855"|0|""|"S"
+734|0|2|"Berriman, Mr. William John"|"male"|23|0|0|"28425"|13|""|"S"
+735|0|2|"Troupiansky, Mr. Moses Aaron"|"male"|23|0|0|"233639"|13|""|"S"
+736|0|3|"Williams, Mr. Leslie"|"male"|28.5|0|0|"54636"|16.1|""|"S"
+737|0|3|"Ford, Mrs. Edward (Margaret Ann Watson)"|"female"|48|1|3|"W./C. 6608"|34.375|""|"S"
+738|1|1|"Lesurer, Mr. Gustave J"|"male"|35|0|0|"PC 17755"|512.3292|"B101"|"C"
+739|0|3|"Ivanoff, Mr. Kanio"|"male"|NA|0|0|"349201"|7.8958|""|"S"
+740|0|3|"Nankoff, Mr. Minko"|"male"|NA|0|0|"349218"|7.8958|""|"S"
+741|1|1|"Hawksford, Mr. Walter James"|"male"|NA|0|0|"16988"|30|"D45"|"S"
+742|0|1|"Cavendish, Mr. Tyrell William"|"male"|36|1|0|"19877"|78.85|"C46"|"S"
+743|1|1|"Ryerson, Miss. Susan Parker \Suzette\"|"female"|21|2|2|"PC 17608"|262.375|"B57 B59 B63 B66"|"C"
+744|0|3|"McNamee, Mr. Neal"|"male"|24|1|0|"376566"|16.1|""|"S"
+745|1|3|"Stranden, Mr. Juho"|"male"|31|0|0|"STON/O 2. 3101288"|7.925|""|"S"
+746|0|1|"Crosby, Capt. Edward Gifford"|"male"|70|1|1|"WE/P 5735"|71|"B22"|"S"
+747|0|3|"Abbott, Mr. Rossmore Edward"|"male"|16|1|1|"C.A. 2673"|20.25|""|"S"
+748|1|2|"Sinkkonen, Miss. Anna"|"female"|30|0|0|"250648"|13|""|"S"
+749|0|1|"Marvin, Mr. Daniel Warner"|"male"|19|1|0|"113773"|53.1|"D30"|"S"
+750|0|3|"Connaghton, Mr. Michael"|"male"|31|0|0|"335097"|7.75|""|"Q"
+751|1|2|"Wells, Miss. Joan"|"female"|4|1|1|"29103"|23|""|"S"
+752|1|3|"Moor, Master. Meier"|"male"|6|0|1|"392096"|12.475|"E121"|"S"
+753|0|3|"Vande Velde, Mr. Johannes Joseph"|"male"|33|0|0|"345780"|9.5|""|"S"
+754|0|3|"Jonkoff, Mr. Lalio"|"male"|23|0|0|"349204"|7.8958|""|"S"
+755|1|2|"Herman, Mrs. Samuel (Jane Laver)"|"female"|48|1|2|"220845"|65|""|"S"
+756|1|2|"Hamalainen, Master. Viljo"|"male"|0.67|1|1|"250649"|14.5|""|"S"
+757|0|3|"Carlsson, Mr. August Sigfrid"|"male"|28|0|0|"350042"|7.7958|""|"S"
+758|0|2|"Bailey, Mr. Percy Andrew"|"male"|18|0|0|"29108"|11.5|""|"S"
+759|0|3|"Theobald, Mr. Thomas Leonard"|"male"|34|0|0|"363294"|8.05|""|"S"
+760|1|1|"Rothes, the Countess. of (Lucy Noel Martha Dyer-Edwards)"|"female"|33|0|0|"110152"|86.5|"B77"|"S"
+761|0|3|"Garfirth, Mr. John"|"male"|NA|0|0|"358585"|14.5|""|"S"
+762|0|3|"Nirva, Mr. Iisakki Antino Aijo"|"male"|41|0|0|"SOTON/O2 3101272"|7.125|""|"S"
+763|1|3|"Barah, Mr. Hanna Assi"|"male"|20|0|0|"2663"|7.2292|""|"C"
+764|1|1|"Carter, Mrs. William Ernest (Lucile Polk)"|"female"|36|1|2|"113760"|120|"B96 B98"|"S"
+765|0|3|"Eklund, Mr. Hans Linus"|"male"|16|0|0|"347074"|7.775|""|"S"
+766|1|1|"Hogeboom, Mrs. John C (Anna Andrews)"|"female"|51|1|0|"13502"|77.9583|"D11"|"S"
+767|0|1|"Brewe, Dr. Arthur Jackson"|"male"|NA|0|0|"112379"|39.6|""|"C"
+768|0|3|"Mangan, Miss. Mary"|"female"|30.5|0|0|"364850"|7.75|""|"Q"
+769|0|3|"Moran, Mr. Daniel J"|"male"|NA|1|0|"371110"|24.15|""|"Q"
+770|0|3|"Gronnestad, Mr. Daniel Danielsen"|"male"|32|0|0|"8471"|8.3625|""|"S"
+771|0|3|"Lievens, Mr. Rene Aime"|"male"|24|0|0|"345781"|9.5|""|"S"
+772|0|3|"Jensen, Mr. Niels Peder"|"male"|48|0|0|"350047"|7.8542|""|"S"
+773|0|2|"Mack, Mrs. (Mary)"|"female"|57|0|0|"S.O./P.P. 3"|10.5|"E77"|"S"
+774|0|3|"Elias, Mr. Dibo"|"male"|NA|0|0|"2674"|7.225|""|"C"
+775|1|2|"Hocking, Mrs. Elizabeth (Eliza Needs)"|"female"|54|1|3|"29105"|23|""|"S"
+776|0|3|"Myhrman, Mr. Pehr Fabian Oliver Malkolm"|"male"|18|0|0|"347078"|7.75|""|"S"
+777|0|3|"Tobin, Mr. Roger"|"male"|NA|0|0|"383121"|7.75|"F38"|"Q"
+778|1|3|"Emanuel, Miss. Virginia Ethel"|"female"|5|0|0|"364516"|12.475|""|"S"
+779|0|3|"Kilgannon, Mr. Thomas J"|"male"|NA|0|0|"36865"|7.7375|""|"Q"
+780|1|1|"Robert, Mrs. Edward Scott (Elisabeth Walton McMillan)"|"female"|43|0|1|"24160"|211.3375|"B3"|"S"
+781|1|3|"Ayoub, Miss. Banoura"|"female"|13|0|0|"2687"|7.2292|""|"C"
+782|1|1|"Dick, Mrs. Albert Adrian (Vera Gillespie)"|"female"|17|1|0|"17474"|57|"B20"|"S"
+783|0|1|"Long, Mr. Milton Clyde"|"male"|29|0|0|"113501"|30|"D6"|"S"
+784|0|3|"Johnston, Mr. Andrew G"|"male"|NA|1|2|"W./C. 6607"|23.45|""|"S"
+785|0|3|"Ali, Mr. William"|"male"|25|0|0|"SOTON/O.Q. 3101312"|7.05|""|"S"
+786|0|3|"Harmer, Mr. Abraham (David Lishin)"|"male"|25|0|0|"374887"|7.25|""|"S"
+787|1|3|"Sjoblom, Miss. Anna Sofia"|"female"|18|0|0|"3101265"|7.4958|""|"S"
+788|0|3|"Rice, Master. George Hugh"|"male"|8|4|1|"382652"|29.125|""|"Q"
+789|1|3|"Dean, Master. Bertram Vere"|"male"|1|1|2|"C.A. 2315"|20.575|""|"S"
+790|0|1|"Guggenheim, Mr. Benjamin"|"male"|46|0|0|"PC 17593"|79.2|"B82 B84"|"C"
+791|0|3|"Keane, Mr. Andrew \Andy\"|"male"|NA|0|0|"12460"|7.75|""|"Q"
+792|0|2|"Gaskell, Mr. Alfred"|"male"|16|0|0|"239865"|26|""|"S"
+793|0|3|"Sage, Miss. Stella Anna"|"female"|NA|8|2|"CA. 2343"|69.55|""|"S"
+794|0|1|"Hoyt, Mr. William Fisher"|"male"|NA|0|0|"PC 17600"|30.6958|""|"C"
+795|0|3|"Dantcheff, Mr. Ristiu"|"male"|25|0|0|"349203"|7.8958|""|"S"
+796|0|2|"Otter, Mr. Richard"|"male"|39|0|0|"28213"|13|""|"S"
+797|1|1|"Leader, Dr. Alice (Farnham)"|"female"|49|0|0|"17465"|25.9292|"D17"|"S"
+798|1|3|"Osman, Mrs. Mara"|"female"|31|0|0|"349244"|8.6833|""|"S"
+799|0|3|"Ibrahim Shawah, Mr. Yousseff"|"male"|30|0|0|"2685"|7.2292|""|"C"
+800|0|3|"Van Impe, Mrs. Jean Baptiste (Rosalie Paula Govaert)"|"female"|30|1|1|"345773"|24.15|""|"S"
+801|0|2|"Ponesell, Mr. Martin"|"male"|34|0|0|"250647"|13|""|"S"
+802|1|2|"Collyer, Mrs. Harvey (Charlotte Annie Tate)"|"female"|31|1|1|"C.A. 31921"|26.25|""|"S"
+803|1|1|"Carter, Master. William Thornton II"|"male"|11|1|2|"113760"|120|"B96 B98"|"S"
+804|1|3|"Thomas, Master. Assad Alexander"|"male"|0.42|0|1|"2625"|8.5167|""|"C"
+805|1|3|"Hedman, Mr. Oskar Arvid"|"male"|27|0|0|"347089"|6.975|""|"S"
+806|0|3|"Johansson, Mr. Karl Johan"|"male"|31|0|0|"347063"|7.775|""|"S"
+807|0|1|"Andrews, Mr. Thomas Jr"|"male"|39|0|0|"112050"|0|"A36"|"S"
+808|0|3|"Pettersson, Miss. Ellen Natalia"|"female"|18|0|0|"347087"|7.775|""|"S"
+809|0|2|"Meyer, Mr. August"|"male"|39|0|0|"248723"|13|""|"S"
+810|1|1|"Chambers, Mrs. Norman Campbell (Bertha Griggs)"|"female"|33|1|0|"113806"|53.1|"E8"|"S"
+811|0|3|"Alexander, Mr. William"|"male"|26|0|0|"3474"|7.8875|""|"S"
+812|0|3|"Lester, Mr. James"|"male"|39|0|0|"A/4 48871"|24.15|""|"S"
+813|0|2|"Slemen, Mr. Richard James"|"male"|35|0|0|"28206"|10.5|""|"S"
+814|0|3|"Andersson, Miss. Ebba Iris Alfrida"|"female"|6|4|2|"347082"|31.275|""|"S"
+815|0|3|"Tomlin, Mr. Ernest Portage"|"male"|30.5|0|0|"364499"|8.05|""|"S"
+816|0|1|"Fry, Mr. Richard"|"male"|NA|0|0|"112058"|0|"B102"|"S"
+817|0|3|"Heininen, Miss. Wendla Maria"|"female"|23|0|0|"STON/O2. 3101290"|7.925|""|"S"
+818|0|2|"Mallet, Mr. Albert"|"male"|31|1|1|"S.C./PARIS 2079"|37.0042|""|"C"
+819|0|3|"Holm, Mr. John Fredrik Alexander"|"male"|43|0|0|"C 7075"|6.45|""|"S"
+820|0|3|"Skoog, Master. Karl Thorsten"|"male"|10|3|2|"347088"|27.9|""|"S"
+821|1|1|"Hays, Mrs. Charles Melville (Clara Jennings Gregg)"|"female"|52|1|1|"12749"|93.5|"B69"|"S"
+822|1|3|"Lulic, Mr. Nikola"|"male"|27|0|0|"315098"|8.6625|""|"S"
+823|0|1|"Reuchlin, Jonkheer. John George"|"male"|38|0|0|"19972"|0|""|"S"
+824|1|3|"Moor, Mrs. (Beila)"|"female"|27|0|1|"392096"|12.475|"E121"|"S"
+825|0|3|"Panula, Master. Urho Abraham"|"male"|2|4|1|"3101295"|39.6875|""|"S"
+826|0|3|"Flynn, Mr. John"|"male"|NA|0|0|"368323"|6.95|""|"Q"
+827|0|3|"Lam, Mr. Len"|"male"|NA|0|0|"1601"|56.4958|""|"S"
+828|1|2|"Mallet, Master. Andre"|"male"|1|0|2|"S.C./PARIS 2079"|37.0042|""|"C"
+829|1|3|"McCormack, Mr. Thomas Joseph"|"male"|NA|0|0|"367228"|7.75|""|"Q"
+830|1|1|"Stone, Mrs. George Nelson (Martha Evelyn)"|"female"|62|0|0|"113572"|80|"B28"|""
+831|1|3|"Yasbeck, Mrs. Antoni (Selini Alexander)"|"female"|15|1|0|"2659"|14.4542|""|"C"
+832|1|2|"Richards, Master. George Sibley"|"male"|0.83|1|1|"29106"|18.75|""|"S"
+833|0|3|"Saad, Mr. Amin"|"male"|NA|0|0|"2671"|7.2292|""|"C"
+834|0|3|"Augustsson, Mr. Albert"|"male"|23|0|0|"347468"|7.8542|""|"S"
+835|0|3|"Allum, Mr. Owen George"|"male"|18|0|0|"2223"|8.3|""|"S"
+836|1|1|"Compton, Miss. Sara Rebecca"|"female"|39|1|1|"PC 17756"|83.1583|"E49"|"C"
+837|0|3|"Pasic, Mr. Jakob"|"male"|21|0|0|"315097"|8.6625|""|"S"
+838|0|3|"Sirota, Mr. Maurice"|"male"|NA|0|0|"392092"|8.05|""|"S"
+839|1|3|"Chip, Mr. Chang"|"male"|32|0|0|"1601"|56.4958|""|"S"
+840|1|1|"Marechal, Mr. Pierre"|"male"|NA|0|0|"11774"|29.7|"C47"|"C"
+841|0|3|"Alhomaki, Mr. Ilmari Rudolf"|"male"|20|0|0|"SOTON/O2 3101287"|7.925|""|"S"
+842|0|2|"Mudd, Mr. Thomas Charles"|"male"|16|0|0|"S.O./P.P. 3"|10.5|""|"S"
+843|1|1|"Serepeca, Miss. Augusta"|"female"|30|0|0|"113798"|31|""|"C"
+844|0|3|"Lemberopolous, Mr. Peter L"|"male"|34.5|0|0|"2683"|6.4375|""|"C"
+845|0|3|"Culumovic, Mr. Jeso"|"male"|17|0|0|"315090"|8.6625|""|"S"
+846|0|3|"Abbing, Mr. Anthony"|"male"|42|0|0|"C.A. 5547"|7.55|""|"S"
+847|0|3|"Sage, Mr. Douglas Bullen"|"male"|NA|8|2|"CA. 2343"|69.55|""|"S"
+848|0|3|"Markoff, Mr. Marin"|"male"|35|0|0|"349213"|7.8958|""|"C"
+849|0|2|"Harper, Rev. John"|"male"|28|0|1|"248727"|33|""|"S"
+850|1|1|"Goldenberg, Mrs. Samuel L (Edwiga Grabowska)"|"female"|NA|1|0|"17453"|89.1042|"C92"|"C"
+851|0|3|"Andersson, Master. Sigvard Harald Elias"|"male"|4|4|2|"347082"|31.275|""|"S"
+852|0|3|"Svensson, Mr. Johan"|"male"|74|0|0|"347060"|7.775|""|"S"
+853|0|3|"Boulos, Miss. Nourelain"|"female"|9|1|1|"2678"|15.2458|""|"C"
+854|1|1|"Lines, Miss. Mary Conover"|"female"|16|0|1|"PC 17592"|39.4|"D28"|"S"
+855|0|2|"Carter, Mrs. Ernest Courtenay (Lilian Hughes)"|"female"|44|1|0|"244252"|26|""|"S"
+856|1|3|"Aks, Mrs. Sam (Leah Rosen)"|"female"|18|0|1|"392091"|9.35|""|"S"
+857|1|1|"Wick, Mrs. George Dennick (Mary Hitchcock)"|"female"|45|1|1|"36928"|164.8667|""|"S"
+858|1|1|"Daly, Mr. Peter Denis "|"male"|51|0|0|"113055"|26.55|"E17"|"S"
+859|1|3|"Baclini, Mrs. Solomon (Latifa Qurban)"|"female"|24|0|3|"2666"|19.2583|""|"C"
+860|0|3|"Razi, Mr. Raihed"|"male"|NA|0|0|"2629"|7.2292|""|"C"
+861|0|3|"Hansen, Mr. Claus Peter"|"male"|41|2|0|"350026"|14.1083|""|"S"
+862|0|2|"Giles, Mr. Frederick Edward"|"male"|21|1|0|"28134"|11.5|""|"S"
+863|1|1|"Swift, Mrs. Frederick Joel (Margaret Welles Barron)"|"female"|48|0|0|"17466"|25.9292|"D17"|"S"
+864|0|3|"Sage, Miss. Dorothy Edith \Dolly\"|"female"|NA|8|2|"CA. 2343"|69.55|""|"S"
+865|0|2|"Gill, Mr. John William"|"male"|24|0|0|"233866"|13|""|"S"
+866|1|2|"Bystrom, Mrs. (Karolina)"|"female"|42|0|0|"236852"|13|""|"S"
+867|1|2|"Duran y More, Miss. Asuncion"|"female"|27|1|0|"SC/PARIS 2149"|13.8583|""|"C"
+868|0|1|"Roebling, Mr. Washington Augustus II"|"male"|31|0|0|"PC 17590"|50.4958|"A24"|"S"
+869|0|3|"van Melkebeke, Mr. Philemon"|"male"|NA|0|0|"345777"|9.5|""|"S"
+870|1|3|"Johnson, Master. Harold Theodor"|"male"|4|1|1|"347742"|11.1333|""|"S"
+871|0|3|"Balkic, Mr. Cerin"|"male"|26|0|0|"349248"|7.8958|""|"S"
+872|1|1|"Beckwith, Mrs. Richard Leonard (Sallie Monypeny)"|"female"|47|1|1|"11751"|52.5542|"D35"|"S"
+873|0|1|"Carlsson, Mr. Frans Olof"|"male"|33|0|0|"695"|5|"B51 B53 B55"|"S"
+874|0|3|"Vander Cruyssen, Mr. Victor"|"male"|47|0|0|"345765"|9|""|"S"
+875|1|2|"Abelson, Mrs. Samuel (Hannah Wizosky)"|"female"|28|1|0|"P/PP 3381"|24|""|"C"
+876|1|3|"Najib, Miss. Adele Kiamie \Jane\"|"female"|15|0|0|"2667"|7.225|""|"C"
+877|0|3|"Gustafsson, Mr. Alfred Ossian"|"male"|20|0|0|"7534"|9.8458|""|"S"
+878|0|3|"Petroff, Mr. Nedelio"|"male"|19|0|0|"349212"|7.8958|""|"S"
+879|0|3|"Laleff, Mr. Kristo"|"male"|NA|0|0|"349217"|7.8958|""|"S"
+880|1|1|"Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)"|"female"|56|0|1|"11767"|83.1583|"C50"|"C"
+881|1|2|"Shelley, Mrs. William (Imanita Parrish Hall)"|"female"|25|0|1|"230433"|26|""|"S"
+882|0|3|"Markun, Mr. Johann"|"male"|33|0|0|"349257"|7.8958|""|"S"
+883|0|3|"Dahlberg, Miss. Gerda Ulrika"|"female"|22|0|0|"7552"|10.5167|""|"S"
+884|0|2|"Banfield, Mr. Frederick James"|"male"|28|0|0|"C.A./SOTON 34068"|10.5|""|"S"
+885|0|3|"Sutehall, Mr. Henry Jr"|"male"|25|0|0|"SOTON/OQ 392076"|7.05|""|"S"
+886|0|3|"Rice, Mrs. William (Margaret Norton)"|"female"|39|0|5|"382652"|29.125|""|"Q"
+887|0|2|"Montvila, Rev. Juozas"|"male"|27|0|0|"211536"|13|""|"S"
+888|1|1|"Graham, Miss. Margaret Edith"|"female"|19|0|0|"112053"|30|"B42"|"S"
+889|0|3|"Johnston, Miss. Catherine Helen \Carrie\"|"female"|NA|1|2|"W./C. 6607"|23.45|""|"S"
+890|1|1|"Behr, Mr. Karl Howell"|"male"|26|0|0|"111369"|30|"C148"|"C"
+891|0|3|"Dooley, Mr. Patrick"|"male"|32|0|0|"370376"|7.75|""|"Q"
```

### Comparing `nzpyida-0.2.2.6/nzpyida/sql.py` & `nzpyida-0.3.3/nzpyida/sql.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,354 +1,356 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-""" Utility functions """
-
-# Python 2 compatibility
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-from builtins import int
-from future import standard_library
-standard_library.install_aliases()
-
-import decimal
-import numpy as np
-import os
-
-import pandas as pd
-from pandas.io.sql import read_sql
-
-def _prepare_query(query_string, silent = False):
-    """
-    Return a formatted query string and print query if verbose mode activated
-
-    Parameters
-    ----------
-    query_string : str
-        String to be printed
-    silent : bool, default: False
-        If True, the query will not be printed at all
-
-    Returns
-    -------
-    querystring : str
-    """
-    if silent is False:
-        if os.getenv('VERBOSE') == 'True':
-            print("> " + query_string)
-    return query_string
-
-def _prepare_and_execute(idaobject, query, autocommit = True, silent = False):
-    """
-    See IdaDataBase._prepare_and_execute
-    """
-    # Open a cursor
-    cursor = idaobject._con.cursor()
-
-    try:
-        query = _prepare_query(query, silent)
-        #print(query)
-        cursor.execute(query)
-        if autocommit is True:
-            idaobject._autocommit()
-    except:
-        if idaobject._is_netezza_system():
-            idaobject.reconnect()
-        raise
-    else:
-        return True
-    finally:
-        cursor.close()
-
-def ida_query(idadb, query, silent=False, first_row_only=False, autocommit = False):
-    """
-    See IdaDataBase.ida_query
-    
-    Notes
-    -----
-    This method calls as appropriate either 
-    _ida_query_ODBC(), or 
-    _ida_query_JDBC()
-    """
-    if idadb._con_type == 'odbc':
-        return _ida_query_ODBC_new(idadb, query, silent, first_row_only, autocommit)
-    elif idadb._con_type == 'nzpy':
-        return _ida_query_NZPY(idadb, query, silent, first_row_only, autocommit)
-    else:
-        return _ida_query_JDBC(idadb, query, silent, first_row_only, autocommit)
-
-def _ida_query_ODBC_new(idadb, query, silent, first_row_only, autocommit):
-    """
-       For ODBC connections no further work needs to be done regarding
-       CLOB retrieval because it's fixed with a configuration keyword at
-       connection creation point. See IdaDataBase.__init__
-       """
-
-    if (query.strip()[:6].upper() == "SELECT") & first_row_only is True:
-            query = "select * from (" + query + ") as T LIMIT 1"
-
-    query = _prepare_query(query, silent)
-
-    try:
-        result = read_sql(query, idadb._con)
-        if first_row_only is True:
-            if result.shape[0] > 0:
-                tuple_as_list = list(result.values[0])
-                for index, element in enumerate(tuple_as_list):
-                    if element is None:
-                        tuple_as_list[index] = np.nan
-                    if isinstance(element, decimal.Decimal):
-                        tuple_as_list[index] = int(element)
-                result = tuple(tuple_as_list)
-            else:
-                #first_row_only is True but the query retuned nothing
-                result = tuple()
-        else:
-            if result.shape[1] == 1:
-                #convert to Series if only one column
-                # Note: This may solve problem in case the interface does not
-                # get properly the column names. This is just a suggestion.
-                # Uncomment it if you feel you need it, but so far it worked well without
-                # result.columns = [column[0] for column in cursor.description]
-                result = result[result.columns[0]]
-
-        return result
-    except Exception as e:
-
-        str_to_check = "\'NoneType\' object is not iterable"
-
-        if str(e) == str_to_check:
-            # query is not a select-statement, but its execution succeeded
-            if autocommit is True:
-                idadb.commit()
-                return None
-        else:
-            raise e
-
-
-def _ida_query_ODBC(idadb, query, silent, first_row_only, autocommit):
-    """
-    For ODBC connections no further work needs to be done regarding
-    CLOB retrieval because it's fixed with a configuration keyword at
-    connection creation point. See IdaDataBase.__init__
-    """
-    cursor = idadb._con.cursor()
-    try:
-        query = _prepare_query(query, silent)
-        cursor.execute(query)
-
-        if autocommit is True:
-            idadb._autocommit()            
-        try:
-            firstRow = cursor.fetchone()
-        except:
-            return None #non-SELECT query, didn't return anything
-        else:
-            #query with SELECT statement, mind that resultset might be empty
-            if first_row_only is True:
-                if firstRow is not None:
-                    #this following processing was proposed by Edouard
-                    tuple_as_list = list(tuple(firstRow))
-                    for index, element in enumerate(tuple_as_list):
-                        if element is None:
-                            tuple_as_list[index] = np.nan
-                        if isinstance(element, decimal.Decimal):
-                            tuple_as_list[index] = int(element)
-                    result = tuple(tuple_as_list)
-                else:
-                    #first_row_only is True but the query retuned nothing
-                    return tuple()
-            else:
-                result = read_sql(query, idadb._con) 
-                #convert to Series if only one column   
-                
-                # Note: This may solve problem in case the interface does not
-                # get properly the column names. This is just a suggestion. 
-                # Uncomment it if you feel you need it, but so far it worked well without
-                # result.columns = [column[0] for column in cursor.description]
-                
-                if len(result.columns) == 1:
-                    result = result[result.columns[0]]                        
-            return result
-    except:
-        raise
-    finally:
-        cursor.close()
-
-def _ida_query_NZPY(idadb, query, silent, first_row_only, autocommit):
-    """
-    """
-    cursor = idadb._con.cursor()
-    try:
-        query = _prepare_query(query, silent)
-        cursor.execute(query)
-
-        if autocommit is True:
-            idadb._autocommit()
-        try:
-            firstRow = cursor.fetchone()
-        except:
-            return None #non-SELECT query, didn't return anything
-        else:
-            #query with SELECT statement, mind that resultset might be empty
-            if first_row_only is True:
-                if firstRow is not None:
-                    #this following processing was proposed by Edouard
-                    tuple_as_list = list(tuple(firstRow))
-                    for index, element in enumerate(tuple_as_list):
-                        if element is None:
-                            tuple_as_list[index] = np.nan
-                        if isinstance(element, decimal.Decimal):
-                            tuple_as_list[index] = int(element)
-                    result = tuple(tuple_as_list)
-                else:
-                    #first_row_only is True but the query retuned nothing
-                    return tuple()
-            else:
-                colnames=[]
-                if type(cursor.description[0][0]) == bytes:
-                    colnames = [column[0].decode() for column in cursor.description]
-                else:
-                    colnames = [column[0] for column in cursor.description]
-
-                data = [firstRow]
-                data.extend(cursor.fetchall())
-                result = pd.DataFrame(data, columns= colnames)
-                #convert to Series if only one column
-                if len(result.columns) == 1:
-                    result = result[result.columns[0]]
-            return result
-    except:
-        raise
-    finally:
-        cursor.close()
-
-def _ida_query_JDBC(idadb, query, silent, first_row_only, autocommit):
-    """
-    For JDBC connections, the CLOBs are retrieved as handles from which
-    strings need to be manually extracted. 
-
-    The retrieval is done row by row due to progressiveStreaming feature of 
-    Db2 Warehouse which closes a CLOB handle as soon as the cursor moves to
-    the next row.
-    
-    Efforts to disable the progressiveStreaming feature where done but had
-    no success. See IdaDataBase.__init__
-    Were it possible to disable the feature, no separate method for ida_query 
-    for JDBC would be needed, as the CLOB would be retrieved as actual strings
-    instead of handles.
-    
-    If there are no CLOB columns, Pandas' read_sql method is used
-    """
-    cursor = idadb._con.cursor()
-    try:
-        query = _prepare_query(query, silent)
-        cursor.execute(query)
-        if autocommit is True:
-            idadb._autocommit()
-        try:
-            firstRow = cursor.fetchone()
-        except:
-            return None #non-SELECT query, didn't return anything
-        else:        
-            #query with SELECT statement, mind that resultset might be empty
-            colNumbersWithCLOBs = []
-            if firstRow is not None:
-               #identify CLOB columns               
-               for index, col in enumerate(firstRow):
-                   if hasattr(col, "getSubString") and hasattr(col, "length"):
-                       colNumbersWithCLOBs.append(index)
-               
-               firstRow = list(firstRow)
-               #replace CLOB's (if any) in the first row
-               if colNumbersWithCLOBs:
-                   for colNum in colNumbersWithCLOBs:
-                       firstRow[colNum] = firstRow[colNum].getSubString(1, firstRow[colNum].length())
-            if first_row_only is True:
-                if firstRow is None:
-                    return tuple()
-                else:
-                    #this following processing was proposed by Edouard
-                    for index, element in enumerate(firstRow):
-                        if element is None:
-                            firstRow[index] = np.nan
-                        if isinstance(element, decimal.Decimal):
-                            firstRow[index] = int(element)
-                    result = tuple(firstRow)
-            else:
-                #first_row_only is False
-                #get the column names for the DataFrame
-                colNames = [column[0] for column in cursor.description]
-                data = [firstRow]
-                if firstRow is None:
-                    # return an empty data frame
-                    result = pd.DataFrame(columns=colNames)
-                elif colNumbersWithCLOBs:
-                    #use the already retrieved row and retrieve the remaining
-                    data = []
-                    row = firstRow
-                    while row is not None:
-                        data.append(row)
-                        row = cursor.fetchone() #this returns a tuple
-                        if row is not None:
-                            row = list(row)
-                            for colNum in colNumbersWithCLOBs:
-                                try:
-                                    # Check needed because some DB2GSE functions
-                                    # return Null, which is then interpreted as
-                                    # None, which doesn't have getSubString method
-                                    row[colNum] = row[colNum].getSubString(
-                                                        1, row[colNum].length())
-                                except:
-                                    pass
-                    result = pd.DataFrame(data, columns = colNames)
-                else:
-                    # fetch the remaining rows
-                    data.extend(cursor.fetchall())
-                    result = pd.DataFrame(data, columns= colNames)
-
-                #convert to Series if only one column
-                if len(result.columns) == 1:
-                    result = result[result.columns[0]]
-            return result                       
-            
-    except:
-        if idadb._is_netezza_system():
-            idadb.reconnect()
-        raise
-    finally:
-        cursor.close()
-            
-def ida_scalar_query(idadb, query, silent = False, autocommit = False):
-    """
-    See IdaDataBase.ida_scalar_query
-    """
-    # Open a cursor
-    cursor = idadb._con.cursor()
-
-    try:
-        query = _prepare_query(query, silent)
-        cursor.execute(query)
-        
-        if autocommit is True:
-            idadb._autocommit()
-            
-        result = cursor.fetchone()[0]
-        if result is None:
-            result = np.nan
-    except:
-        if idadb._is_netezza_system():
-            idadb.reconnect()
-        raise
-    finally:
-        cursor.close()
-    return result
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+""" Utility functions """
+
+# Python 2 compatibility
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+from builtins import int
+from future import standard_library
+standard_library.install_aliases()
+
+import decimal
+import numpy as np
+import os
+
+import pandas as pd
+from pandas.io.sql import read_sql
+
+def _prepare_query(query_string, silent = False):
+    """
+    Return a formatted query string and print query if verbose mode activated
+
+    Parameters
+    ----------
+    query_string : str
+        String to be printed
+    silent : bool, default: False
+        If True, the query will not be printed at all
+
+    Returns
+    -------
+    querystring : str
+    """
+    if silent is False:
+        if os.getenv('VERBOSE') == 'True':
+            print("> " + query_string)
+    return query_string
+
+def _prepare_and_execute(idaobject, query, autocommit = True, silent = False):
+    """
+    See IdaDataBase._prepare_and_execute
+    """
+    # Open a cursor
+    cursor = idaobject._con.cursor()
+
+    try:
+        query = _prepare_query(query, silent)
+        #print(query)
+        cursor.execute(query)
+        if autocommit is True:
+            idaobject._autocommit()
+    except:
+        if idaobject._is_netezza_system():
+            idaobject.reconnect()
+        raise
+    else:
+        return True
+    finally:
+        cursor.close()
+
+def ida_query(idadb, query, silent=False, first_row_only=False, autocommit = False):
+    """
+    See IdaDataBase.ida_query
+    
+    Notes
+    -----
+    This method calls as appropriate either 
+    _ida_query_ODBC(), or 
+    _ida_query_JDBC()
+    """
+    if idadb._con_type == 'odbc':
+        return _ida_query_ODBC_new(idadb, query, silent, first_row_only, autocommit)
+    elif idadb._con_type == 'nzpy':
+        return _ida_query_NZPY(idadb, query, silent, first_row_only, autocommit)
+    else:
+        return _ida_query_JDBC(idadb, query, silent, first_row_only, autocommit)
+
+def _ida_query_ODBC_new(idadb, query, silent, first_row_only, autocommit):
+    """
+       For ODBC connections no further work needs to be done regarding
+       CLOB retrieval because it's fixed with a configuration keyword at
+       connection creation point. See IdaDataBase.__init__
+       """
+
+    if (query.strip()[:6].upper() == "SELECT") & first_row_only is True:
+            query = "select * from (" + query + ") as T LIMIT 1"
+
+    query = _prepare_query(query, silent)
+
+    try:
+        result = read_sql(query, idadb._con)
+        if first_row_only is True:
+            if result.shape[0] > 0:
+                tuple_as_list = list(result.values[0])
+                for index, element in enumerate(tuple_as_list):
+                    if element is None:
+                        tuple_as_list[index] = np.nan
+                    if isinstance(element, decimal.Decimal):
+                        tuple_as_list[index] = int(element)
+                result = tuple(tuple_as_list)
+            else:
+                #first_row_only is True but the query retuned nothing
+                result = tuple()
+        else:
+            if result.shape[1] == 1:
+                #convert to Series if only one column
+                # Note: This may solve problem in case the interface does not
+                # get properly the column names. This is just a suggestion.
+                # Uncomment it if you feel you need it, but so far it worked well without
+                # result.columns = [column[0] for column in cursor.description]
+                result = result[result.columns[0]]
+
+        return result
+    except Exception as e:
+
+        str_to_check = "\'NoneType\' object is not iterable"
+
+        if str(e) == str_to_check:
+            # query is not a select-statement, but its execution succeeded
+            if autocommit is True:
+                idadb.commit()
+                return None
+        else:
+            raise e
+
+
+def _ida_query_ODBC(idadb, query, silent, first_row_only, autocommit):
+    """
+    For ODBC connections no further work needs to be done regarding
+    CLOB retrieval because it's fixed with a configuration keyword at
+    connection creation point. See IdaDataBase.__init__
+    """
+    cursor = idadb._con.cursor()
+    try:
+        query = _prepare_query(query, silent)
+        cursor.execute(query)
+
+        if autocommit is True:
+            idadb._autocommit()            
+        try:
+            firstRow = cursor.fetchone()
+        except:
+            return None #non-SELECT query, didn't return anything
+        else:
+            #query with SELECT statement, mind that resultset might be empty
+            if first_row_only is True:
+                if firstRow is not None:
+                    #this following processing was proposed by Edouard
+                    tuple_as_list = list(tuple(firstRow))
+                    for index, element in enumerate(tuple_as_list):
+                        if element is None:
+                            tuple_as_list[index] = np.nan
+                        if isinstance(element, decimal.Decimal):
+                            tuple_as_list[index] = int(element)
+                    result = tuple(tuple_as_list)
+                else:
+                    #first_row_only is True but the query retuned nothing
+                    return tuple()
+            else:
+                result = read_sql(query, idadb._con) 
+                #convert to Series if only one column   
+                
+                # Note: This may solve problem in case the interface does not
+                # get properly the column names. This is just a suggestion. 
+                # Uncomment it if you feel you need it, but so far it worked well without
+                # result.columns = [column[0] for column in cursor.description]
+                
+                if len(result.columns) == 1:
+                    result = result[result.columns[0]]                        
+            return result
+    except:
+        raise
+    finally:
+        cursor.close()
+
+def _ida_query_NZPY(idadb, query, silent, first_row_only, autocommit):
+    """
+    """
+    cursor = idadb._con.cursor()
+    try:
+        query = _prepare_query(query, silent)
+        cursor.execute(query)
+
+        if autocommit is True:
+            idadb._autocommit()
+        try:
+            firstRow = cursor.fetchone()
+        except:
+            return None #non-SELECT query, didn't return anything
+        else:
+            #query with SELECT statement, mind that resultset might be empty
+            if first_row_only is True:
+                if firstRow is not None:
+                    #this following processing was proposed by Edouard
+                    tuple_as_list = list(tuple(firstRow))
+                    for index, element in enumerate(tuple_as_list):
+                        if element is None:
+                            tuple_as_list[index] = np.nan
+                        if isinstance(element, decimal.Decimal):
+                            tuple_as_list[index] = int(element)
+                    result = tuple(tuple_as_list)
+                else:
+                    #first_row_only is True but the query retuned nothing
+                    return tuple()
+            else:
+                colnames=[]
+                if type(cursor.description[0][0]) == bytes:
+                    colnames = [column[0].decode() for column in cursor.description]
+                else:
+                    colnames = [column[0] for column in cursor.description]
+                if firstRow is None:
+                    result = pd.DataFrame([], columns= colnames)
+                else:
+                    data = [firstRow]
+                    data.extend(cursor.fetchall())
+                    result = pd.DataFrame(data, columns= colnames)
+                #convert to Series if only one column
+                if len(result.columns) == 1:
+                    result = result[result.columns[0]]
+            return result
+    except:
+        raise
+    finally:
+        cursor.close()
+
+def _ida_query_JDBC(idadb, query, silent, first_row_only, autocommit):
+    """
+    For JDBC connections, the CLOBs are retrieved as handles from which
+    strings need to be manually extracted. 
+
+    The retrieval is done row by row due to progressiveStreaming feature of 
+    Db2 Warehouse which closes a CLOB handle as soon as the cursor moves to
+    the next row.
+    
+    Efforts to disable the progressiveStreaming feature where done but had
+    no success. See IdaDataBase.__init__
+    Were it possible to disable the feature, no separate method for ida_query 
+    for JDBC would be needed, as the CLOB would be retrieved as actual strings
+    instead of handles.
+    
+    If there are no CLOB columns, Pandas' read_sql method is used
+    """
+    cursor = idadb._con.cursor()
+    try:
+        query = _prepare_query(query, silent)
+        cursor.execute(query)
+        if autocommit is True:
+            idadb._autocommit()
+        try:
+            firstRow = cursor.fetchone()
+        except:
+            return None #non-SELECT query, didn't return anything
+        else:        
+            #query with SELECT statement, mind that resultset might be empty
+            colNumbersWithCLOBs = []
+            if firstRow is not None:
+               #identify CLOB columns               
+               for index, col in enumerate(firstRow):
+                   if hasattr(col, "getSubString") and hasattr(col, "length"):
+                       colNumbersWithCLOBs.append(index)
+               
+               firstRow = list(firstRow)
+               #replace CLOB's (if any) in the first row
+               if colNumbersWithCLOBs:
+                   for colNum in colNumbersWithCLOBs:
+                       firstRow[colNum] = firstRow[colNum].getSubString(1, firstRow[colNum].length())
+            if first_row_only is True:
+                if firstRow is None:
+                    return tuple()
+                else:
+                    #this following processing was proposed by Edouard
+                    for index, element in enumerate(firstRow):
+                        if element is None:
+                            firstRow[index] = np.nan
+                        if isinstance(element, decimal.Decimal):
+                            firstRow[index] = int(element)
+                    result = tuple(firstRow)
+            else:
+                #first_row_only is False
+                #get the column names for the DataFrame
+                colNames = [column[0] for column in cursor.description]
+                data = [firstRow]
+                if firstRow is None:
+                    # return an empty data frame
+                    result = pd.DataFrame(columns=colNames)
+                elif colNumbersWithCLOBs:
+                    #use the already retrieved row and retrieve the remaining
+                    data = []
+                    row = firstRow
+                    while row is not None:
+                        data.append(row)
+                        row = cursor.fetchone() #this returns a tuple
+                        if row is not None:
+                            row = list(row)
+                            for colNum in colNumbersWithCLOBs:
+                                try:
+                                    # Check needed because some DB2GSE functions
+                                    # return Null, which is then interpreted as
+                                    # None, which doesn't have getSubString method
+                                    row[colNum] = row[colNum].getSubString(
+                                                        1, row[colNum].length())
+                                except:
+                                    pass
+                    result = pd.DataFrame(data, columns = colNames)
+                else:
+                    # fetch the remaining rows
+                    data.extend(cursor.fetchall())
+                    result = pd.DataFrame(data, columns= colNames)
+
+                #convert to Series if only one column
+                if len(result.columns) == 1:
+                    result = result[result.columns[0]]
+            return result                       
+            
+    except:
+        if idadb._is_netezza_system():
+            idadb.reconnect()
+        raise
+    finally:
+        cursor.close()
+            
+def ida_scalar_query(idadb, query, silent = False, autocommit = False):
+    """
+    See IdaDataBase.ida_scalar_query
+    """
+    # Open a cursor
+    cursor = idadb._con.cursor()
+
+    try:
+        query = _prepare_query(query, silent)
+        cursor.execute(query)
+        
+        if autocommit is True:
+            idadb._autocommit()
+            
+        result = cursor.fetchone()[0]
+        if result is None:
+            result = np.nan
+    except:
+        if idadb._is_netezza_system():
+            idadb.reconnect()
+        raise
+    finally:
+        cursor.close()
+    return result
```

### Comparing `nzpyida-0.2.2.6/nzpyida/statistics.py` & `nzpyida-0.3.3/nzpyida/statistics.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,1151 +1,1151 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-# -----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-# -----------------------------------------------------------------------------
-
-# Python 2 Compatibility
-from __future__ import print_function
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import absolute_import
-from builtins import dict
-from builtins import zip
-from builtins import str
-from builtins import int
-from future import standard_library
-
-standard_library.install_aliases()
-
-from collections import OrderedDict
-import itertools
-import math
-import warnings
-from numbers import Number
-
-import pandas as pd
-import numpy as np
-import six
-
-import nzpyida
-from nzpyida.utils import chunklist
-
-"""
-Statistics module for IdaDataFrames
-"""
-
-
-def _numeric_stats(idadf, stat, columns):
-    """
-    Compute various stats from one or several numerical columns of an IdaDataFrame.
-
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-        Data source.
-    stat : str
-        Name of the statistic to be computed.
-    columns : str or list of str
-        Name of the columns that belong to the IdaDataFrame.
-
-    Returns
-    -------
-    Tuple or float64
-        One value for each column. For a one column input a float64 value is returned except for median
-
-    Notes
-    -----
-    Currently, the following functions are supported: count, mean, median, std,
-    var, min, max, sum. Should return a tuple. Only available for numerical
-    columns.
-    """
-    # Calculate count, mean, median, std, var, min, max
-    if isinstance(columns, six.string_types):
-        columns = [columns]
-
-    if isinstance(stat, six.string_types):
-        if stat == "count":
-            select_string = 'COUNT(\"' + '\"), COUNT(\"'.join(columns) + '\")'
-        elif stat == "mean":
-            select_string = ('AVG(CAST(\"' +
-                             '\" AS FLOAT)), AVG(CAST(\"'.join(columns) +
-                             '\" AS FLOAT))')
-        elif stat == "median":
-            return _get_percentiles(idadf, 0.5, columns).values[0]
-        elif stat == "std":
-            tuple_count = _numeric_stats(idadf, 'count', columns)
-            # in case of only one column, ensure tuple_count is iterable
-            if len(columns) == 1:
-                tuple_count = [tuple_count]
-            count_dict = dict((x, y) for x, y in zip(columns, tuple_count))
-            agg_list = []
-            for column in columns:
-                if idadf._idadb._is_netezza_system():
-                    # workaround for error on Netezza related to "/"
-                    # ERROR:  Function 'QRT(INT4)' does not exist
-                    # sqrt_term = "SQRT(%s) * POW(SQRT(%s), -1)" %(count_dict[column], count_dict[column]-1)
-                    sqrt_term = 1
-                else:
-                    sqrt_term = "SQRT(%s)/SQRT(%s)" % (count_dict[column], count_dict[column] - 1)
-                # agg_list.append("STDDEV(\"%s\")*(SQRT(%s)/SQRT(%s))"
-                agg_list.append("STDDEV(\"%s\")*( %s )" % (column, sqrt_term))
-            select_string = ', '.join(agg_list)
-        elif stat == "var":
-            tuple_count = _numeric_stats(idadf, 'count', columns)
-            if len(columns) == 1:
-                tuple_count = [tuple_count]
-            count_dict = dict((x, int(y)) for x, y in zip(columns, tuple_count))
-            agg_list = []
-            for column in columns:
-                if idadf._idadb._is_netezza_system():
-                    # workaround for error on Netezza related to "/"
-                    # ERROR:  Unable to identify an operator '//' ..
-                    # div_term = "%s.0 * POW(%s.0, -1)" %(count_dict[column], count_dict[column]-1)
-                    div_term = 1
-                else:
-                    div_term = "%s.0/%s.0" % (count_dict[column], count_dict[column] - 1)
-                agg_list.append("VARIANCE(\"%s\")*(%s)" % (column, div_term))
-            select_string = ', '.join(agg_list)
-        elif stat == "min":
-            select_string = 'MIN(\"' + '\"), MIN(\"'.join(columns) + '\")'
-        elif stat == "max":
-            select_string = 'MAX(\"' + '\"), MAX(\"'.join(columns) + '\")'
-        elif stat == "sum":
-            select_string = 'SUM(\"' + '\"), SUM(\"'.join(columns) + '\")'
-
-        name = idadf.internal_state.current_state
-
-        return idadf.ida_query("SELECT %s FROM %s" % (select_string, name)).values[0]
-
-
-def _get_percentiles(idadf, percentiles, columns):
-    """
-    Return percentiles over all entries of a column or list of columns in the
-    IdaDataFrame.
-
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    percentiles: Float or list of floats.
-        All values in percentiles must be > 0  and < 1
-    columns: String or list of string
-        Name of columns belonging to the IdaDataFrame.
-
-    Returns
-    -------
-        DataFrame
-    """
-
-    if isinstance(columns, six.string_types):
-        columns = [columns]
-    if isinstance(percentiles, Number):
-        percentiles = [percentiles]
-
-    name = idadf.internal_state.current_state
-
-    # Get na values for each columns
-    tuple_na = _get_number_of_nas(idadf, columns)
-    nrow = idadf.shape[0]
-    data = pd.DataFrame()
-    for index_col, column in enumerate(columns):
-        nb_not_missing = nrow - tuple_na[index_col]
-        indexes = [float(x) * float(nb_not_missing - 1) + 1 for x in percentiles]
-        low = [math.floor(x) for x in indexes]
-        high = [math.ceil(x) for x in indexes]
-        tuplelist = []
-        i = 0
-        for flag in [((x + 1) == y) for x, y in zip(low, high)]:
-            if flag:
-                tuplelist.append((i, i + 1))
-                i += 2
-            else:
-                tuplelist.append((i, i))
-                i += 1
-        unique = low + high
-        unique = set(unique)
-        unique = sorted(unique)
-        unique = [str(x) for x in unique]
-        indexes_string = ",".join(unique)
-        df = idadf.ida_query("(SELECT \"" + column + "\" AS \"" + column + "\" FROM (SELECT " +
-                             "ROW_NUMBER() OVER(ORDER BY \"" + column + "\") as rn, \"" +
-                             column + "\" FROM (SELECT * FROM " + name +
-                             " WHERE \"" + column + "\" IS NOT NULL " +
-                             ") AS T1) AS T2 WHERE rn  in(" + indexes_string + "))")
-
-        # indexvalues = list(df[df.columns[0]])
-        indexvalues = list(df)
-        # import pdb ; pdb.set_trace()
-        # print(tuplelist)
-        # print(indexvalues)
-        indexfinal = [(float(str(indexvalues[x[0]])) + float(str(indexvalues[x[1]]))) / 2 for x in tuplelist]
-        new_data = pd.DataFrame(indexfinal)
-        data[column] = (new_data.T).values[0]
-
-    percentile_names = [x for x in percentiles]
-    data.index = percentile_names
-    return data
-
-
-def _categorical_stats(idadf, stat, columns):
-    # TODO:
-    """
-    Computes various stats from one or several categorical columns of the IdaDataFrame.
-    This is not implemented.
-
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    stat : str
-        Name of the statistic function to be computed.
-    columns : str or list of str
-        Name of columns belonging to the IdaDataFrame.
-
-    Returns
-    -------
-        Tuple.
-    """
-    # Calculates count, unique, top, freq
-    raise NotImplementedError("TODO")
-
-
-def _get_number_of_nas(idadf, columns):
-    """
-    Return the count of missing values for a list of columns in the IdaDataFrame.
-
-    Parameters
-    ----------
-    idadf : IdaDataFrame
-    columns : str or list
-        One column as a string or a list of columns in the idaDataFrame.
-
-    Returns
-    -------
-        Tuple
-    """
-    if isinstance(columns, six.string_types):
-        columns = [columns]
-
-    name = idadf.internal_state.current_state
-
-    query_list = list()
-    for column in columns:
-        string = ("(SELECT COUNT(*) AS \"" + column + "\" FROM " +
-                  name + " WHERE \"" + column + "\" IS NULL) AS T_" + column)
-        query_list.append(string)
-
-    query_string = ', '.join(query_list)
-
-    # TODO: Improvement idea : Get nrow (shape) and substract by count("COLUMN")
-    return idadf.ida_query("SELECT * FROM " + query_string, first_row_only=True)
-
-
-def _count_level(idadf, columnlist=None):
-    """
-    Count distinct levels across a list of columns of an IdaDataFrame grouped
-    by themselves.
-
-
-    Parameters
-    ----------
-    columnlist : list
-        List of column names that exist in the IdaDataFrame. By default, these
-        are all columns in IdaDataFrame.
-
-    Returns
-    -------
-        Tuple
-
-    Notes
-    -----
-    The function assumes the following:
-
-        * The columns given as parameter exists in the IdaDataframe.
-        * The parameter columnlist is an optional list.
-        * Columns are referenced by their own name (character string).
-    """
-    if columnlist is None:
-        columnlist = idadf.columns
-
-    name = idadf.internal_state.current_state
-
-    query_list = []
-    for column in columnlist:
-        # Here cast ?
-        query_list.append("(SELECT COUNT(*) AS \"" + column + "\" FROM (" +
-                          "SELECT \"" + column + "\" FROM " + name +
-                          " GROUP BY \"" + column + "\" ) AS T1_" + column + ") AS T2_" + column)
-        # query_list.append("(SELECT CAST(COUNT(*) AS BIGINT) AS \"" + column +"\" FROM (" +
-        #                  "SELECT \"" + column + "\" FROM " + name + " ))")
-
-    query_string = ', '.join(query_list)
-    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
-    return idadf.ida_query("SELECT " + column_string + " FROM " + query_string, first_row_only=True)
-
-
-def _count_level_groupby(idadf, columnlist=None):
-    """
-    Count distinct levels across a list of columns in the IdaDataFrame grouped
-    by themselves. This is used to get the dimension of the resulting cross table.
-
-    Parameters
-    ----------
-    columnlist : list
-        List of column names existing in the IdaDataFrame. By default, these
-        are columns of self
-
-    Returns
-    -------
-        Tuple
-
-    Notes
-    -----
-    The function assumes the follwing:
-
-        * The columns given as parameter exists in the IdaDataframe.
-        * The parameter columnlist is a optional and is a list.
-        * Columns are referenced by their own name (character string).
-    """
-    if columnlist is None:
-        columnlist = idadf.columns
-
-    name = idadf.internal_state.current_state
-
-    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
-    query = (("SELECT COUNT(*) FROM (SELECT %s, COUNT(*) as COUNT " +
-              "FROM %s GROUP BY %s ORDER BY %s, COUNT ASC) AS T ")
-             % (column_string, name, column_string, column_string))
-    return idadf.ida_query(query, first_row_only=True)
-
-
-# TODO: REFACTORING: factors function should maybe return a tuple ?
-def _factors_count(idadf, columnlist, valuelist=None):
-    """
-    Count non-missing values for all columns in a list (valuelist) over the
-    IdaDataFrame grouped by a list of columns(columnlist).
-
-    Parameters
-    ----------
-    columnlist : list
-        List of column names that exist in self.
-    valuelist : list
-        List of column names that exist in self.
-
-     Notes
-     -----
-     The function assumes the following:
-
-        * The columns given as parameter exists in the IdaDataframe
-        * The parameter columnlist is a optional and is a list
-        * Columns are referenced by their own name (character string)
-
-    Returns
-    -------
-        DataFrame
-    """
-    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
-
-    name = idadf.internal_state.current_state
-
-    if valuelist is None:
-        query = (("SELECT %s, COUNT(*) as COUNT FROM %s GROUP BY %s ORDER BY %s, COUNT ASC")
-                 % (column_string, name, column_string, column_string))
-    else:
-        agg_list = []
-        for value in valuelist:
-            query = "COUNT(\"%s\") as \"%s\"" % (value, value)
-            agg_list.append(query)
-
-        agg_string = ', '.join(agg_list)
-        value_string = '\"' + '", "'.join(valuelist) + '\"'
-
-        query = (("SELECT %s,%s FROM %s GROUP BY %s ORDER BY %s,%s ASC")
-                 % (column_string, agg_string, name, column_string, column_string, value_string))
-
-    return idadf.ida_query(query)
-
-
-def _factors_sum(idadf, columnlist, valuelist):
-    """
-    Compute the arithmetic sum over for all columns in a list (valuelist)
-    over the IdaDataFrame grouped by a list of columns (columnlist).
-
-    Parameters
-    ----------
-    columnlist : list
-        List of column names that exist in self.
-    valuelist : list
-        List of column names that exist in self.
-
-    Notes
-    -----
-    The function assumes the following:
-
-        * The columns given as parameter exists in the IdaDataframe
-        * The parameter columnlist is a optional and is a list
-        * Columns are referenced by their own name (character string)
-
-    Returns
-    -------
-        DataFrame
-    """
-    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
-
-    name = idadf.internal_state.current_state
-
-    agg_list = []
-    for value in valuelist:
-        query = "SUM(\"%s\") as \"%s\"" % (value, value)
-        agg_list.append(query)
-
-    agg_string = ', '.join(agg_list)
-    value_string = '\"' + '", "'.join(valuelist) + '\"'
-
-    query = (("SELECT %s,%s FROM %s GROUP BY %s ORDER BY %s,%s ASC")
-             % (column_string, agg_string, name, column_string, column_string, value_string))
-
-    return idadf.ida_query(query)
-
-
-def _factors_avg(idadf, columnlist, valuelist):
-    """
-    Compute the arithmetic average for all columns in a list (valuelist) over
-    the IdaDataFrame grouped by a list of columns (columnlist).
-
-    Parameters
-    ----------
-    columnlist : list
-        List of column names that exist in self.
-    valuelist : list
-        List of column names that exist in self.
-
-    Notes
-    -----
-    The function assumes the following:
-
-        * The columns given as parameter exists in the IdaDataframe
-        * The parameter columnlist and valuelist are array-like
-        * Columns are referenced by their own name (character string)
-
-    Returns
-    -------
-        DataFrame
-    """
-    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
-
-    name = idadf.internal_state.current_state
-
-    agg_list = []
-    for value in valuelist:
-        agg = (("CAST(AVG(CAST(\"%s\" AS DECIMAL(10,6))) AS DECIMAL(10,6)) \"%s\"")
-               % (value, value))
-        agg_list.append(agg)
-
-    agg_string = ', '.join(agg_list)
-    value_string = '\"' + '", "'.join(valuelist) + '\"'
-
-    query = (("SELECT %s,%s FROM %s GROUP BY %s ORDER BY %s,%s ASC")
-             % (column_string, agg_string, name, column_string, column_string, value_string))
-
-    return idadf.ida_query(query)
-
-
-###############################################################################
-### Pivot Table
-###############################################################################
-
-def pivot_table(idadf, values=None, columns=None, max_entries=1000, sort=None,
-                factor_threshold=None, interactive=False, aggfunc='count'):
-    """
-    See IdaDataFrame.pivot_table
-    """
-
-    # TODO : Support index
-
-    if aggfunc.lower() not in ['count', 'sum', 'avg', 'average', 'mean']:
-        print("For now only 'count' and 'sum' and 'mean' as aggregation function is supported")
-        return
-
-    if (columns is None) & (factor_threshold is None):
-        print("Please provide parameter factor_threshold for automatic selection of columns")
-        return
-
-    if isinstance(columns, six.string_types):
-        columns = [columns]
-
-    if isinstance(values, six.string_types):
-        values = [values]
-
-    if (values is None) and (aggfunc.lower() != "count"):
-        raise ValueError("Cannot aggregate using another function than count if" +
-                         "no value(s) was/were given")
-
-    ####### Identify automatically categorical fields #########
-    # Load distinct count for each and evaluate categorical or not
-    data = idadf._table_def(factor_threshold)  #
-    if columns is None:
-        factors = data.loc[data['VALTYPE'] == "CATEGORICAL", ['TYPENAME', 'FACTORS']]
-        if len(factors) == 0:
-            print("No categorical columns to tabulate")
-            return
-    else:
-        factors = data.loc[columns, ['TYPENAME', 'FACTORS']]
-
-    if sort == "alpha":
-        factors.sort_index(inplace=True, ascending=1)
-    elif sort == "factor":
-        factors.sort(['FACTORS'], inplace=True, ascending=1)
-
-    if columns is None:
-        print("Automatic selection of columns :", factors.index.values)
-        columns = factors.index.values
-
-    nb_row = _count_level_groupby(idadf, factors.index.values)[0] * len(columns)
-    nb_col = len(factors.index.values)
-
-    nb_entries = nb_row * nb_col
-
-    if nb_entries > max_entries:  # Overflow risk
-        print("Number of entries :", nb_entries)
-        print("Value counts for factors:")
-        factor_values = factors[['FACTORS']]
-        factor_values.columns = ['']
-        print(factor_values.T)
-        print("WARNING :Attempt to make a table with more than " +
-              str(max_entries) + " elements. Either increase max_entries " +
-              "parameter or remove columns with too many levels.")
-        return
-
-    print("Output dataframe has dimensions", nb_row, "x", (nb_col + 1))
-    if interactive is True:
-        display_yes = nzpyida.utils.query_yes_no("Do you want to download it in memory ?")
-        if not display_yes:
-            return
-
-    categorical_columns = list(factors.index)
-    if aggfunc.lower() == 'count':
-        dataframe = _factors_count(idadf, categorical_columns, values)  # Download dataframe
-    if aggfunc.lower() == 'sum':
-        dataframe = _factors_sum(idadf, categorical_columns, values)  # Download dataframe
-    if aggfunc.lower() in ['avg', 'average', 'mean']:
-        dataframe = _factors_avg(idadf, categorical_columns, values)  # Download dataframe
-
-    if values is not None:
-        agg_values = values
-    else:
-        agg_values = aggfunc.upper()
-
-    if isinstance(agg_values, six.string_types):
-        agg_values = [agg_values]
-    dataframe.columns = categorical_columns + agg_values  # Name the aggregate column
-
-    # Formatting result
-    if len(agg_values) == 1:
-        dataframe[None] = agg_values[0]
-    else:
-        catdataframe = dataframe[categorical_columns]
-        dataframe = catdataframe.join(dataframe[agg_values].stack().reset_index(1))
-        dataframe['level_1'] = pd.Categorical(dataframe['level_1'], agg_values)
-        dataframe = dataframe.rename(columns={'level_1': None})
-        dataframe = dataframe.sort([None] + categorical_columns)
-
-    dataframe.set_index([None] + categorical_columns, inplace=True)
-    dataframe = dataframe.astype(float)
-
-    result = pd.Series(dataframe[dataframe.columns[0]])
-    result.name = None
-
-    return result
-
-
-###############################################################################
-### Descriptive statistics
-###############################################################################
-
-def summary(idadf):
-    table_name = idadf.internal_state.current_state
-    outtable_name = idadf._idadb._get_valid_tablename(prefix="pyida_describe")
-    idadf._idadb._call_stored_procedure("SUMMARY1000 ", intable=table_name, outtable=outtable_name)
-    result_query = "SELECT * FROM " + outtable_name + " ORDER BY columnname; "
-    result_df = idadf.ida_query(result_query)
-    idadf._idadb._call_stored_procedure("DROP_SUMMARY1000", intable=outtable_name)
-    return result_df
-
-
-def describe(idadf, percentiles=[0.25, 0.50, 0.75]):
-    """
-    See IdaDataFrame.describe
-    """
-    if percentiles is not None:
-        if isinstance(percentiles, Number):
-            percentiles = [percentiles]
-        if True in [(not isinstance(x, Number)) for x in percentiles]:
-            raise TypeError("Argument 'percentiles' should be either a number or " +
-                            "a list of numbers between 0 and 1")
-        elif True in [((x >= 1) | (x <= 0)) for x in percentiles]:
-            raise ValueError("Numbers in argument 'percentiles' should be between 0 and 1")
-
-    # Improvement idea : We could use dtypes instead of calculating this everytime
-    columns = idadf._get_numerical_columns()
-    data = []
-    if not columns:
-        columns = idadf._get_categorical_columns()
-        if not columns:
-            raise NotImplementedError("No numerical and no categorical columns")
-        else:
-            raise NotImplementedError("Categorical only idaDataFrame are not handled currently")
-            # TODO : Handle categorical columns
-            data.append(_categorical_stats(idadf, "count", columns))
-            data.append(_categorical_stats(idadf, "unique", columns))
-            data.append(_categorical_stats(idadf, "top", columns))
-            data.append(_categorical_stats(idadf, "freq", columns))
-    else:
-        data.append(_numeric_stats(idadf, "count", columns))
-        data.append(_numeric_stats(idadf, "mean", columns))
-        data.append(_numeric_stats(idadf, "std", columns))
-        data.append(_numeric_stats(idadf, "min", columns))
-        if percentiles is not None:
-            perc = (_get_percentiles(idadf, percentiles, columns))
-            for tup in perc.itertuples(index=False):
-                data.append(tup)
-        data.append(_numeric_stats(idadf, "max", columns))
-
-    data = pd.DataFrame(data)
-    data.columns = columns
-    if percentiles is not None:
-        percentile_names = [(str(int(x * 100)) + "%") for x in percentiles]
-    else:
-        percentile_names = []
-    data.index = ['count', 'mean', 'std', 'min'] + percentile_names + ['max']
-
-    # quick fix -> JDBC problems
-    # for column in data.columns:
-    #    data[[column]] = data[[column]].astype(float)
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        data = pd.Series(data[data.columns[0]])
-
-    return data
-
-
-def quantile(idadf, q=0.5):
-    """
-    See IdaDataFrame.quantile
-    """
-
-    if isinstance(q, Number):
-        q = [q]
-
-    # Sanity check
-    if True in [(not isinstance(x, Number)) for x in q]:
-        raise TypeError("Argument 'q' should be either a number or " +
-                        "a list of numbers between 0 and 1")
-    elif True in [((x >= 1) | (x <= 0)) for x in q]:
-        raise ValueError("Numbers in argument 'percentiles' should be between 0 and 1")
-
-    columns = idadf._get_numerical_columns()
-    if not columns:
-        print(idadf.name + " has no numeric columns")
-        return
-
-    result = _get_percentiles(idadf, q, columns)
-
-    if isinstance(q, list):
-        if len(q) > 1:
-            return result
-
-    result = result.T
-    result = result[result.columns[0]]
-    result.name = q[0]
-    result = result.astype('float')
-
-    if len(result) == 1:
-        result = result[0]
-
-    return result
-
-
-# Note : Not casting to double can lead to SQL overflow
-# TODO: Has to be modified in ibmdbR
-
-
-def cov(idadf, other=None):
-    if not idadf._idadb._is_netezza_system():
-        return cov_old(idadf, other)
-
-    numerical_columns = idadf._get_numerical_columns()
-    if len(numerical_columns) < 2:
-        print(idadf.name + " has less than two numeric columns")
-        return
-    column_string = ""
-    for column in numerical_columns:
-        column_string += "\"" + column + "\";"
-
-    result_df = pd.DataFrame(columns=numerical_columns, index=numerical_columns)
-
-    table_name = idadf.internal_state.current_state
-    outtable = idadf._idadb._get_valid_tablename(prefix="cov_")
-
-    idadf._idadb._call_stored_procedure("COVARIANCE1000MATRIX",
-                                        intable=table_name,
-                                        incolumn=column_string,
-                                        outtable=outtable)
-
-    # the calls of substring remove the surrounding double quotes
-    result_query = ("SELECT substring(VARXNAME,2,length(VARXNAME)-2) as VARXNAME, " +
-                    "substring(VARYNAME,2,length(VARYNAME)-2) as VARYNAME, " +
-                    "COVARIANCE, CNTX " +
-                    "FROM " + outtable + " ORDER BY varxname, varyname;")
-
-    cov_df = idadf.ida_query(result_query)
-
-    for index in cov_df.index.values:
-
-        col_list = []
-        for column in cov_df.columns.values:
-            col_list.append(cov_df.at[index, column])
-
-        result_df.at[col_list[0], col_list[1]] = col_list[2]
-
-    for column in result_df.columns:
-        result_df[column] = result_df[column].astype(float)
-    value = idadf._idadb.drop_table(outtable)
-
-    return result_df
-
-
-def cov_old(idadf, other=None):
-    """
-    See IdaDataFrame.cov
-    """
-    if isinstance(idadf, nzpyida.IdaSeries):
-        raise TypeError("cov() missing 1 required positional argument: 'other'")
-
-    # check if the covariance function is installed on the Netezza system
-    if idadf._idadb._is_netezza_system():
-        covar_query = "SELECT count(*) from _V_OBJECT  where OBJNAME like 'COVAR_SAMP#%' AND OBJDB = CURRENT_DB"
-        if idadf._idadb.ida_scalar_query(covar_query) == 0:
-            raise NotImplementedError("The COVAR_SAMP function is not installed on the Netezza database.")
-
-    columns = idadf._get_numerical_columns()
-    if len(columns) < 2:
-        print(idadf.name + " has less than two numeric columns")
-        return
-
-    tuple_count = _numeric_stats(idadf, 'count', columns)
-    count_dict = dict((x, int(y)) for x, y in zip(columns, tuple_count))
-
-    agg_list = []
-
-    combinations = [x for x in itertools.combinations_with_replacement(columns, 2)]
-    columns_set = [{x[0], x[1]} for x in combinations]
-
-    if idadf._idadb._is_netezza_system():
-        for column_pair in combinations:
-            agg_list.append("COVAR_SAMP(\"" + column_pair[0] + "\",\"" + column_pair[1] + "\")")
-    else:
-        for column_pair in combinations:
-            agg_list.append("COVARIANCE(\"" + column_pair[0] + "\",\"" +
-                            column_pair[1] + "\")*(" +
-                            str(min([count_dict[column_pair[0]],
-                                     count_dict[column_pair[1]]])) + ".0/" +
-                            str(min([count_dict[column_pair[0]],
-                                     count_dict[column_pair[1]]]) - 1) + ".0)")
-
-    agg_string = ', '.join(agg_list)
-
-    name = idadf.internal_state.current_state
-
-    data = idadf.ida_query("SELECT %s FROM %s" % (agg_string, name), first_row_only=True)
-
-    tuple_list = []
-
-    for column1 in columns:
-        list_value = []
-        for column2 in columns:
-            for index, column_set in enumerate(columns_set):
-                if {column1, column2} == column_set:
-                    list_value.append(data[index])
-                    break
-        tuple_list.append(tuple(list_value))
-
-    result = pd.DataFrame(tuple_list)
-    result.index = columns
-    result.columns = columns
-
-    if len(result) == 1:
-        result = result[0]
-
-    return result
-
-
-def corr(idadf):
-    if not idadf._idadb._is_netezza_system():
-        return corr_old(idadf)
-
-    numerical_columns = idadf._get_numerical_columns()
-    if len(numerical_columns) < 2:
-        print(idadf.name + " has less than two numeric columns")
-        return
-    column_string = ""
-    for column in numerical_columns:
-        column_string += "\"" + column + "\";"
-
-    result_df = pd.DataFrame(columns=numerical_columns, index=numerical_columns)
-
-    # print(result_df)
-
-    table_name = idadf.internal_state.current_state
-    outtable = idadf._idadb._get_valid_tablename(prefix="corr_")
-
-    idadf._idadb._call_stored_procedure("CORRELATION1000MATRIX ",
-                                        intable=table_name,
-                                        incolumn=column_string,
-                                        outtable=outtable)
-
-    # the calls of substring remove the surrounding double quotes
-    result_query = ("SELECT substring(VARXNAME,2,length(VARXNAME)-2) as VARXNAME, " +
-                    "substring(VARYNAME,2,length(VARYNAME)-2) as VARYNAME, " +
-                    "CORRELATION " +
-                    "FROM " + outtable + " ORDER BY varxname, varyname;")
-
-    corr_df = idadf.ida_query(result_query)
-
-    for index in corr_df.index.values:
-
-        col_list = []
-        for column in corr_df.columns.values:
-            col_list.append(corr_df.at[index, column])
-
-        result_df.at[col_list[0], col_list[1]] = col_list[2]
-
-    for column in result_df.columns:
-        result_df[column] = result_df[column].astype(float)
-    value = idadf._idadb.drop_table(outtable)
-    return result_df
-
-
-def train_test_split(idadf, train_table, test_table, id, fraction, seed):
-    table_name = idadf.internal_state.current_state
-
-    query = "CALL nza..SPLIT_DATA('intable = " + table_name + ",id = " + id + ",traintable = " + train_table + ",testtable= " + test_table + ",fraction= " + str(
-        fraction) + ",seed=" + str(seed) + ",outtabletype=table" + "');"
-
-    df = idadf.ida_query(query)
-    idadf.commit()
-
-    return df
-
-
-def corr_old(idadf, features=None, ignore_indexer=True):
-    """
-    See IdaDataFrame.corr
-    """
-    if isinstance(idadf, nzpyida.IdaSeries):
-        raise TypeError("corr() missing 1 required positional argument: 'other'")
-
-    # check if the corr function is installed on the Netezza system
-    if idadf._idadb._is_netezza_system():
-        corr_query = "SELECT count(*) from _V_OBJECT  where OBJNAME like 'CORR#%' AND OBJDB = CURRENT_DB"
-        if idadf._idadb.ida_scalar_query(corr_query) == 0:
-            raise NotImplementedError("The CORR function is not installed on the Netezza database.")
-
-    numerical_columns = idadf._get_numerical_columns()
-    if len(numerical_columns) < 2:
-        print(idadf.name + " has less than two numeric columns")
-        return
-
-    if ignore_indexer is True:
-        if idadf.indexer:
-            if idadf.indexer in numerical_columns:
-                numerical_columns.remove(idadf.indexer)
-
-    # print(features)
-    # target, features = ibmdbpy.utils._check_input(target, features)
-    if features is not None:
-        for feature in features:
-            if feature not in numerical_columns:
-                raise TypeError("Correlation-based measure not available for non-numerical columns %s" % feature)
-    else:
-        features = numerical_columns
-
-    # if target not in columns:
-    #    raise ValueError("%s is not a column of numerical type in %s"%(target, idadf.name))
-
-    values = OrderedDict()
-
-    combinations = [x for x in itertools.combinations(features, 2)]
-    # columns_set = [{x[0], x[1]} for x in combinations]
-
-    if idadf._idadb._is_netezza_system():
-        corr_function = "CORR"
-    else:
-        corr_function = "CORRELATION"
-
-    if len(features) < 64:  # the limit of variables for an SQL statement is 4096, i.e 64^2
-        agg_list = []
-        for column_pair in combinations:
-            agg = corr_function + "(\"%s\",\"%s\")" % (column_pair[0], column_pair[1])
-            agg_list.append(agg)
-
-        agg_string = ', '.join(agg_list)
-
-        name = idadf.internal_state.current_state
-
-        data = idadf.ida_query("SELECT %s FROM %s" % (agg_string, name), first_row_only=True)
-
-        for i, element in enumerate(combinations):
-            if element[0] not in values:
-                values[element[0]] = {}
-            if element[1] not in values:
-                values[element[1]] = {}
-            values[element[0]][element[1]] = data[i]
-            values[element[1]][element[0]] = data[i]
-
-        result = pd.DataFrame(values).fillna(1)
-    else:
-        chunkgen = chunklist(combinations, 100)
-
-        for chunk in chunkgen:
-            agg_list = []
-            for column_pair in chunk:
-                agg = corr_function + "(\"%s\",\"%s\")" % (column_pair[0], column_pair[1])
-                agg_list.append(agg)
-
-            agg_string = ', '.join(agg_list)
-
-            name = idadf.internal_state.current_state
-
-            data = idadf.ida_query("SELECT %s FROM %s" % (agg_string, name), first_row_only=True)
-
-            for i, element in enumerate(chunk):
-                if element[0] not in values:
-                    values[element[0]] = OrderedDict()
-                if element[1] not in values:
-                    values[element[1]] = OrderedDict()
-                values[element[0]][element[1]] = data[i]
-                values[element[1]][element[0]] = data[i]
-
-        result = pd.DataFrame(values).fillna(1)
-
-    result = result.reindex(result.columns)
-    if len(result) == 1:
-        result = result[0]
-
-    return result
-
-
-### corrwith
-
-
-def mad(idadf):
-    """
-    See IdaDataFrame.mad
-    """
-    columns = idadf._get_numerical_columns()
-    if len(columns) < 2:
-        print(idadf.name + " has less than two numeric columns")
-        return
-
-    mean_tuple = _numeric_stats(idadf, "mean", columns)
-    absmean_dict = dict((x, abs(y)) for x, y in zip(columns, mean_tuple))
-    tuple_na = _get_number_of_nas(idadf, columns)
-
-    agg_list = []
-    for index_col, column in enumerate(columns):
-        if idadf._idadb._is_netezza_system():
-            # workaround for error on Netezza related to "/"
-            # ERROR:  Unable to identify an operator '//'
-            div_term = "* pow(" + str(idadf.shape[0] - tuple_na[index_col]) + ", -1)"
-        else:
-            div_term = "/" + str(idadf.shape[0] - tuple_na[index_col])
-        agg_list.append("SUM(ABS(\"" + column + "\" -" +
-                        str(absmean_dict[column]) + "))" +
-                        div_term)
-
-    agg_string = ', '.join(agg_list)
-
-    name = idadf.internal_state.current_state
-
-    mad_tuple = idadf.ida_query("SELECT %s FROM %s" % (agg_string, name))
-    result = pd.Series(mad_tuple.values[0])
-    result.index = columns
-    result = result.astype('float')
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        result = result[0]
-
-    return result
-
-
-def ida_min(idadf):
-    """
-    See idadataFrame.min
-    """
-    na_tuple = _get_number_of_nas(idadf, idadf.columns)
-    min_tuple = _numeric_stats(idadf, "min", idadf.columns)
-    if not hasattr(min_tuple, "__iter__"): min_tuple = (min_tuple,)  # dirty fix
-    min_list = [np.nan if ((y > 0) and not isinstance(x, Number))
-                else x for x, y in zip(min_tuple, na_tuple)]
-    min_tuple = tuple(min_list)
-    result = pd.Series(min_tuple)
-    result.index = idadf.columns
-
-    # if isinstance(idadf, ibmdbpy.IdaSeries):
-    #   result = result[0]
-
-    return result
-
-
-def ida_max(idadf):
-    """
-    See idadataFrame.max
-    """
-    na_tuple = _get_number_of_nas(idadf, idadf.columns)
-    max_tuple = _numeric_stats(idadf, "max", idadf.columns)
-    if not hasattr(max_tuple, "__iter__"): max_tuple = (max_tuple,)  # dirty fix
-    max_list = [np.nan if ((y > 0) and not isinstance(x, Number))
-                else x for x, y in zip(max_tuple, na_tuple)]
-    max_tuple = tuple(max_list)
-    result = pd.Series(max_tuple)
-    result.index = idadf.columns
-
-    # if isinstance(idadf, ibmdbpy.IdaSeries):
-    #  result = result[0]
-
-    return result
-
-
-def count(idadf):
-    """
-    See IdaDataFrame.count
-    """
-    count_tuple = _numeric_stats(idadf, "count", idadf.columns)
-    result = pd.Series(count_tuple)
-    result.index = idadf.columns
-    result = result.astype(int)
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        result = result[0]
-
-    return result
-
-
-def count_distinct(idadf):
-    """
-    See IdaDataFrame.count_distinct
-    """
-    result = pd.Series(_count_level(idadf))
-    result.index = idadf.columns
-    result = result.astype(int)
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        result = result[0]
-
-    return result
-
-
-def std(idadf):
-    """
-    See IdaDataFrame.std
-    """
-    columns = idadf._get_numerical_columns()
-    if not columns:
-        warnings.warn("%s has no numeric columns" % idadf.name)
-        return pd.Series()
-
-    std_tuple = _numeric_stats(idadf, "std", columns)
-
-    result = pd.Series(std_tuple)
-    result.index = columns
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        result = result[0]
-
-    return result
-
-
-def var(idadf):
-    """
-    See IdaDataFrame.var
-    """
-    columns = idadf._get_numerical_columns()
-    if not columns:
-        warnings.warn("%s has no numeric columns" % idadf.name)
-        return pd.Series()
-
-    var_tuple = _numeric_stats(idadf, "var", columns)
-
-    result = pd.Series(var_tuple)
-    result.index = columns
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        result = result[0]
-
-    return result
-
-
-def mean(idadf):
-    """
-    See IdaDataFrame.mean
-    """
-    columns = idadf._get_numerical_columns()
-    if not columns:
-        warnings.warn("%s has no numeric columns" % idadf.name)
-        return pd.Series()
-
-    mean_tuple = _numeric_stats(idadf, "mean", columns)
-
-    result = pd.Series(mean_tuple)
-    result.index = columns
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        result = result[0]
-
-    return result
-
-
-def ida_sum(idadf):
-    """
-    See IdaDataFrame.sum
-    """
-    # Behave like having the option "numeric only" to true
-    columns = idadf._get_numerical_columns()
-    if not columns:
-        warnings.warn("%s has no numeric columns" % idadf.name)
-        return pd.Series()
-
-    sum_tuple = _numeric_stats(idadf, "sum", columns)
-
-    result = pd.Series(sum_tuple)
-    result.index = columns
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        result = result[0]
-
-    return result
-
-
-def median(idadf):
-    """
-    See IdaDataFrame.median
-    """
-    # Behave like having the option "numeric only" to true
-    columns = idadf._get_numerical_columns()
-    if not columns:
-        warnings.warn("%s has no numeric columns" % idadf.name)
-        return pd.Series()
-
-    median_tuple = _numeric_stats(idadf, "median", columns)
-
-    result = pd.Series(median_tuple)
-    result.index = columns
-
-    if isinstance(idadf, nzpyida.IdaSeries):
-        result = result[0]
-
-    return result
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+# -----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+# -----------------------------------------------------------------------------
+
+# Python 2 Compatibility
+from __future__ import print_function
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import absolute_import
+from builtins import dict
+from builtins import zip
+from builtins import str
+from builtins import int
+from future import standard_library
+
+standard_library.install_aliases()
+
+from collections import OrderedDict
+import itertools
+import math
+import warnings
+from numbers import Number
+
+import pandas as pd
+import numpy as np
+import six
+
+import nzpyida
+from nzpyida.utils import chunklist
+
+"""
+Statistics module for IdaDataFrames
+"""
+
+
+def _numeric_stats(idadf, stat, columns):
+    """
+    Compute various stats from one or several numerical columns of an IdaDataFrame.
+
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+        Data source.
+    stat : str
+        Name of the statistic to be computed.
+    columns : str or list of str
+        Name of the columns that belong to the IdaDataFrame.
+
+    Returns
+    -------
+    Tuple or float64
+        One value for each column. For a one column input a float64 value is returned except for median
+
+    Notes
+    -----
+    Currently, the following functions are supported: count, mean, median, std,
+    var, min, max, sum. Should return a tuple. Only available for numerical
+    columns.
+    """
+    # Calculate count, mean, median, std, var, min, max
+    if isinstance(columns, six.string_types):
+        columns = [columns]
+
+    if isinstance(stat, six.string_types):
+        if stat == "count":
+            select_string = 'COUNT(\"' + '\"), COUNT(\"'.join(columns) + '\")'
+        elif stat == "mean":
+            select_string = ('AVG(CAST(\"' +
+                             '\" AS FLOAT)), AVG(CAST(\"'.join(columns) +
+                             '\" AS FLOAT))')
+        elif stat == "median":
+            return _get_percentiles(idadf, 0.5, columns).values[0]
+        elif stat == "std":
+            tuple_count = _numeric_stats(idadf, 'count', columns)
+            # in case of only one column, ensure tuple_count is iterable
+            if len(columns) == 1:
+                tuple_count = [tuple_count]
+            count_dict = dict((x, y) for x, y in zip(columns, tuple_count))
+            agg_list = []
+            for column in columns:
+                if idadf._idadb._is_netezza_system():
+                    # workaround for error on Netezza related to "/"
+                    # ERROR:  Function 'QRT(INT4)' does not exist
+                    # sqrt_term = "SQRT(%s) * POW(SQRT(%s), -1)" %(count_dict[column], count_dict[column]-1)
+                    sqrt_term = 1
+                else:
+                    sqrt_term = "SQRT(%s)/SQRT(%s)" % (count_dict[column], count_dict[column] - 1)
+                # agg_list.append("STDDEV(\"%s\")*(SQRT(%s)/SQRT(%s))"
+                agg_list.append("STDDEV(\"%s\")*( %s )" % (column, sqrt_term))
+            select_string = ', '.join(agg_list)
+        elif stat == "var":
+            tuple_count = _numeric_stats(idadf, 'count', columns)
+            if len(columns) == 1:
+                tuple_count = [tuple_count]
+            count_dict = dict((x, int(y)) for x, y in zip(columns, tuple_count))
+            agg_list = []
+            for column in columns:
+                if idadf._idadb._is_netezza_system():
+                    # workaround for error on Netezza related to "/"
+                    # ERROR:  Unable to identify an operator '//' ..
+                    # div_term = "%s.0 * POW(%s.0, -1)" %(count_dict[column], count_dict[column]-1)
+                    div_term = 1
+                else:
+                    div_term = "%s.0/%s.0" % (count_dict[column], count_dict[column] - 1)
+                agg_list.append("VARIANCE(\"%s\")*(%s)" % (column, div_term))
+            select_string = ', '.join(agg_list)
+        elif stat == "min":
+            select_string = 'MIN(\"' + '\"), MIN(\"'.join(columns) + '\")'
+        elif stat == "max":
+            select_string = 'MAX(\"' + '\"), MAX(\"'.join(columns) + '\")'
+        elif stat == "sum":
+            select_string = 'SUM(\"' + '\"), SUM(\"'.join(columns) + '\")'
+
+        name = idadf.internal_state.current_state
+
+        return idadf.ida_query("SELECT %s FROM %s" % (select_string, name)).values[0]
+
+
+def _get_percentiles(idadf, percentiles, columns):
+    """
+    Return percentiles over all entries of a column or list of columns in the
+    IdaDataFrame.
+
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    percentiles: Float or list of floats.
+        All values in percentiles must be > 0  and < 1
+    columns: String or list of string
+        Name of columns belonging to the IdaDataFrame.
+
+    Returns
+    -------
+        DataFrame
+    """
+
+    if isinstance(columns, six.string_types):
+        columns = [columns]
+    if isinstance(percentiles, Number):
+        percentiles = [percentiles]
+
+    name = idadf.internal_state.current_state
+
+    # Get na values for each columns
+    tuple_na = _get_number_of_nas(idadf, columns)
+    nrow = idadf.shape[0]
+    data = pd.DataFrame()
+    for index_col, column in enumerate(columns):
+        nb_not_missing = nrow - tuple_na[index_col]
+        indexes = [float(x) * float(nb_not_missing - 1) + 1 for x in percentiles]
+        low = [math.floor(x) for x in indexes]
+        high = [math.ceil(x) for x in indexes]
+        tuplelist = []
+        i = 0
+        for flag in [((x + 1) == y) for x, y in zip(low, high)]:
+            if flag:
+                tuplelist.append((i, i + 1))
+                i += 2
+            else:
+                tuplelist.append((i, i))
+                i += 1
+        unique = low + high
+        unique = set(unique)
+        unique = sorted(unique)
+        unique = [str(x) for x in unique]
+        indexes_string = ",".join(unique)
+        df = idadf.ida_query("(SELECT \"" + column + "\" AS \"" + column + "\" FROM (SELECT " +
+                             "ROW_NUMBER() OVER(ORDER BY \"" + column + "\") as rn, \"" +
+                             column + "\" FROM (SELECT * FROM " + name +
+                             " WHERE \"" + column + "\" IS NOT NULL " +
+                             ") AS T1) AS T2 WHERE rn  in(" + indexes_string + "))")
+
+        # indexvalues = list(df[df.columns[0]])
+        indexvalues = list(df)
+        # import pdb ; pdb.set_trace()
+        # print(tuplelist)
+        # print(indexvalues)
+        indexfinal = [(float(str(indexvalues[x[0]])) + float(str(indexvalues[x[1]]))) / 2 for x in tuplelist]
+        new_data = pd.DataFrame(indexfinal)
+        data[column] = (new_data.T).values[0]
+
+    percentile_names = [x for x in percentiles]
+    data.index = percentile_names
+    return data
+
+
+def _categorical_stats(idadf, stat, columns):
+    # TODO:
+    """
+    Computes various stats from one or several categorical columns of the IdaDataFrame.
+    This is not implemented.
+
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    stat : str
+        Name of the statistic function to be computed.
+    columns : str or list of str
+        Name of columns belonging to the IdaDataFrame.
+
+    Returns
+    -------
+        Tuple.
+    """
+    # Calculates count, unique, top, freq
+    raise NotImplementedError("TODO")
+
+
+def _get_number_of_nas(idadf, columns):
+    """
+    Return the count of missing values for a list of columns in the IdaDataFrame.
+
+    Parameters
+    ----------
+    idadf : IdaDataFrame
+    columns : str or list
+        One column as a string or a list of columns in the idaDataFrame.
+
+    Returns
+    -------
+        Tuple
+    """
+    if isinstance(columns, six.string_types):
+        columns = [columns]
+
+    name = idadf.internal_state.current_state
+
+    query_list = list()
+    for column in columns:
+        string = ("(SELECT COUNT(*) AS \"" + column + "\" FROM " +
+                  name + " WHERE \"" + column + "\" IS NULL) AS T_" + column)
+        query_list.append(string)
+
+    query_string = ', '.join(query_list)
+
+    # TODO: Improvement idea : Get nrow (shape) and substract by count("COLUMN")
+    return idadf.ida_query("SELECT * FROM " + query_string, first_row_only=True)
+
+
+def _count_level(idadf, columnlist=None):
+    """
+    Count distinct levels across a list of columns of an IdaDataFrame grouped
+    by themselves.
+
+
+    Parameters
+    ----------
+    columnlist : list
+        List of column names that exist in the IdaDataFrame. By default, these
+        are all columns in IdaDataFrame.
+
+    Returns
+    -------
+        Tuple
+
+    Notes
+    -----
+    The function assumes the following:
+
+        * The columns given as parameter exists in the IdaDataframe.
+        * The parameter columnlist is an optional list.
+        * Columns are referenced by their own name (character string).
+    """
+    if columnlist is None:
+        columnlist = idadf.columns
+
+    name = idadf.internal_state.current_state
+
+    query_list = []
+    for column in columnlist:
+        # Here cast ?
+        query_list.append("(SELECT COUNT(*) AS \"" + column + "\" FROM (" +
+                          "SELECT \"" + column + "\" FROM " + name +
+                          " GROUP BY \"" + column + "\" ) AS T1_" + column + ") AS T2_" + column)
+        # query_list.append("(SELECT CAST(COUNT(*) AS BIGINT) AS \"" + column +"\" FROM (" +
+        #                  "SELECT \"" + column + "\" FROM " + name + " ))")
+
+    query_string = ', '.join(query_list)
+    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
+    return idadf.ida_query("SELECT " + column_string + " FROM " + query_string, first_row_only=True)
+
+
+def _count_level_groupby(idadf, columnlist=None):
+    """
+    Count distinct levels across a list of columns in the IdaDataFrame grouped
+    by themselves. This is used to get the dimension of the resulting cross table.
+
+    Parameters
+    ----------
+    columnlist : list
+        List of column names existing in the IdaDataFrame. By default, these
+        are columns of self
+
+    Returns
+    -------
+        Tuple
+
+    Notes
+    -----
+    The function assumes the follwing:
+
+        * The columns given as parameter exists in the IdaDataframe.
+        * The parameter columnlist is a optional and is a list.
+        * Columns are referenced by their own name (character string).
+    """
+    if columnlist is None:
+        columnlist = idadf.columns
+
+    name = idadf.internal_state.current_state
+
+    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
+    query = (("SELECT COUNT(*) FROM (SELECT %s, COUNT(*) as COUNT " +
+              "FROM %s GROUP BY %s ORDER BY %s, COUNT ASC) AS T ")
+             % (column_string, name, column_string, column_string))
+    return idadf.ida_query(query, first_row_only=True)
+
+
+# TODO: REFACTORING: factors function should maybe return a tuple ?
+def _factors_count(idadf, columnlist, valuelist=None):
+    """
+    Count non-missing values for all columns in a list (valuelist) over the
+    IdaDataFrame grouped by a list of columns(columnlist).
+
+    Parameters
+    ----------
+    columnlist : list
+        List of column names that exist in self.
+    valuelist : list
+        List of column names that exist in self.
+
+     Notes
+     -----
+     The function assumes the following:
+
+        * The columns given as parameter exists in the IdaDataframe
+        * The parameter columnlist is a optional and is a list
+        * Columns are referenced by their own name (character string)
+
+    Returns
+    -------
+        DataFrame
+    """
+    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
+
+    name = idadf.internal_state.current_state
+
+    if valuelist is None:
+        query = (("SELECT %s, COUNT(*) as COUNT FROM %s GROUP BY %s ORDER BY %s, COUNT ASC")
+                 % (column_string, name, column_string, column_string))
+    else:
+        agg_list = []
+        for value in valuelist:
+            query = "COUNT(\"%s\") as \"%s\"" % (value, value)
+            agg_list.append(query)
+
+        agg_string = ', '.join(agg_list)
+        value_string = '\"' + '", "'.join(valuelist) + '\"'
+
+        query = (("SELECT %s,%s FROM %s GROUP BY %s ORDER BY %s,%s ASC")
+                 % (column_string, agg_string, name, column_string, column_string, value_string))
+
+    return idadf.ida_query(query)
+
+
+def _factors_sum(idadf, columnlist, valuelist):
+    """
+    Compute the arithmetic sum over for all columns in a list (valuelist)
+    over the IdaDataFrame grouped by a list of columns (columnlist).
+
+    Parameters
+    ----------
+    columnlist : list
+        List of column names that exist in self.
+    valuelist : list
+        List of column names that exist in self.
+
+    Notes
+    -----
+    The function assumes the following:
+
+        * The columns given as parameter exists in the IdaDataframe
+        * The parameter columnlist is a optional and is a list
+        * Columns are referenced by their own name (character string)
+
+    Returns
+    -------
+        DataFrame
+    """
+    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
+
+    name = idadf.internal_state.current_state
+
+    agg_list = []
+    for value in valuelist:
+        query = "SUM(\"%s\") as \"%s\"" % (value, value)
+        agg_list.append(query)
+
+    agg_string = ', '.join(agg_list)
+    value_string = '\"' + '", "'.join(valuelist) + '\"'
+
+    query = (("SELECT %s,%s FROM %s GROUP BY %s ORDER BY %s,%s ASC")
+             % (column_string, agg_string, name, column_string, column_string, value_string))
+
+    return idadf.ida_query(query)
+
+
+def _factors_avg(idadf, columnlist, valuelist):
+    """
+    Compute the arithmetic average for all columns in a list (valuelist) over
+    the IdaDataFrame grouped by a list of columns (columnlist).
+
+    Parameters
+    ----------
+    columnlist : list
+        List of column names that exist in self.
+    valuelist : list
+        List of column names that exist in self.
+
+    Notes
+    -----
+    The function assumes the following:
+
+        * The columns given as parameter exists in the IdaDataframe
+        * The parameter columnlist and valuelist are array-like
+        * Columns are referenced by their own name (character string)
+
+    Returns
+    -------
+        DataFrame
+    """
+    column_string = '\"' + '\", \"'.join(columnlist) + '\"'
+
+    name = idadf.internal_state.current_state
+
+    agg_list = []
+    for value in valuelist:
+        agg = (("CAST(AVG(CAST(\"%s\" AS DECIMAL(10,6))) AS DECIMAL(10,6)) \"%s\"")
+               % (value, value))
+        agg_list.append(agg)
+
+    agg_string = ', '.join(agg_list)
+    value_string = '\"' + '", "'.join(valuelist) + '\"'
+
+    query = (("SELECT %s,%s FROM %s GROUP BY %s ORDER BY %s,%s ASC")
+             % (column_string, agg_string, name, column_string, column_string, value_string))
+
+    return idadf.ida_query(query)
+
+
+###############################################################################
+### Pivot Table
+###############################################################################
+
+def pivot_table(idadf, values=None, columns=None, max_entries=1000, sort=None,
+                factor_threshold=None, interactive=False, aggfunc='count'):
+    """
+    See IdaDataFrame.pivot_table
+    """
+
+    # TODO : Support index
+
+    if aggfunc.lower() not in ['count', 'sum', 'avg', 'average', 'mean']:
+        print("For now only 'count' and 'sum' and 'mean' as aggregation function is supported")
+        return
+
+    if (columns is None) & (factor_threshold is None):
+        print("Please provide parameter factor_threshold for automatic selection of columns")
+        return
+
+    if isinstance(columns, six.string_types):
+        columns = [columns]
+
+    if isinstance(values, six.string_types):
+        values = [values]
+
+    if (values is None) and (aggfunc.lower() != "count"):
+        raise ValueError("Cannot aggregate using another function than count if" +
+                         "no value(s) was/were given")
+
+    ####### Identify automatically categorical fields #########
+    # Load distinct count for each and evaluate categorical or not
+    data = idadf._table_def(factor_threshold)  #
+    if columns is None:
+        factors = data.loc[data['VALTYPE'] == "CATEGORICAL", ['TYPENAME', 'FACTORS']]
+        if len(factors) == 0:
+            print("No categorical columns to tabulate")
+            return
+    else:
+        factors = data.loc[columns, ['TYPENAME', 'FACTORS']]
+
+    if sort == "alpha":
+        factors.sort_index(inplace=True, ascending=1)
+    elif sort == "factor":
+        factors.sort(['FACTORS'], inplace=True, ascending=1)
+
+    if columns is None:
+        print("Automatic selection of columns :", factors.index.values)
+        columns = factors.index.values
+
+    nb_row = _count_level_groupby(idadf, factors.index.values)[0] * len(columns)
+    nb_col = len(factors.index.values)
+
+    nb_entries = nb_row * nb_col
+
+    if nb_entries > max_entries:  # Overflow risk
+        print("Number of entries :", nb_entries)
+        print("Value counts for factors:")
+        factor_values = factors[['FACTORS']]
+        factor_values.columns = ['']
+        print(factor_values.T)
+        print("WARNING :Attempt to make a table with more than " +
+              str(max_entries) + " elements. Either increase max_entries " +
+              "parameter or remove columns with too many levels.")
+        return
+
+    print("Output dataframe has dimensions", nb_row, "x", (nb_col + 1))
+    if interactive is True:
+        display_yes = nzpyida.utils.query_yes_no("Do you want to download it in memory ?")
+        if not display_yes:
+            return
+
+    categorical_columns = list(factors.index)
+    if aggfunc.lower() == 'count':
+        dataframe = _factors_count(idadf, categorical_columns, values)  # Download dataframe
+    if aggfunc.lower() == 'sum':
+        dataframe = _factors_sum(idadf, categorical_columns, values)  # Download dataframe
+    if aggfunc.lower() in ['avg', 'average', 'mean']:
+        dataframe = _factors_avg(idadf, categorical_columns, values)  # Download dataframe
+
+    if values is not None:
+        agg_values = values
+    else:
+        agg_values = aggfunc.upper()
+
+    if isinstance(agg_values, six.string_types):
+        agg_values = [agg_values]
+    dataframe.columns = categorical_columns + agg_values  # Name the aggregate column
+
+    # Formatting result
+    if len(agg_values) == 1:
+        dataframe[None] = agg_values[0]
+    else:
+        catdataframe = dataframe[categorical_columns]
+        dataframe = catdataframe.join(dataframe[agg_values].stack().reset_index(1))
+        dataframe['level_1'] = pd.Categorical(dataframe['level_1'], agg_values)
+        dataframe = dataframe.rename(columns={'level_1': None})
+        dataframe = dataframe.sort([None] + categorical_columns)
+
+    dataframe.set_index([None] + categorical_columns, inplace=True)
+    dataframe = dataframe.astype(float)
+
+    result = pd.Series(dataframe[dataframe.columns[0]])
+    result.name = None
+
+    return result
+
+
+###############################################################################
+### Descriptive statistics
+###############################################################################
+
+def summary(idadf):
+    table_name = idadf.internal_state.current_state
+    outtable_name = idadf._idadb._get_valid_tablename(prefix="pyida_describe")
+    idadf._idadb._call_stored_procedure("SUMMARY1000 ", intable=table_name, outtable=outtable_name)
+    result_query = "SELECT * FROM " + outtable_name + " ORDER BY columnname; "
+    result_df = idadf.ida_query(result_query)
+    idadf._idadb._call_stored_procedure("DROP_SUMMARY1000", intable=outtable_name)
+    return result_df
+
+
+def describe(idadf, percentiles=[0.25, 0.50, 0.75]):
+    """
+    See IdaDataFrame.describe
+    """
+    if percentiles is not None:
+        if isinstance(percentiles, Number):
+            percentiles = [percentiles]
+        if True in [(not isinstance(x, Number)) for x in percentiles]:
+            raise TypeError("Argument 'percentiles' should be either a number or " +
+                            "a list of numbers between 0 and 1")
+        elif True in [((x >= 1) | (x <= 0)) for x in percentiles]:
+            raise ValueError("Numbers in argument 'percentiles' should be between 0 and 1")
+
+    # Improvement idea : We could use dtypes instead of calculating this everytime
+    columns = idadf._get_numerical_columns()
+    data = []
+    if not columns:
+        columns = idadf._get_categorical_columns()
+        if not columns:
+            raise NotImplementedError("No numerical and no categorical columns")
+        else:
+            raise NotImplementedError("Categorical only idaDataFrame are not handled currently")
+            # TODO : Handle categorical columns
+            data.append(_categorical_stats(idadf, "count", columns))
+            data.append(_categorical_stats(idadf, "unique", columns))
+            data.append(_categorical_stats(idadf, "top", columns))
+            data.append(_categorical_stats(idadf, "freq", columns))
+    else:
+        data.append(_numeric_stats(idadf, "count", columns))
+        data.append(_numeric_stats(idadf, "mean", columns))
+        data.append(_numeric_stats(idadf, "std", columns))
+        data.append(_numeric_stats(idadf, "min", columns))
+        if percentiles is not None:
+            perc = (_get_percentiles(idadf, percentiles, columns))
+            for tup in perc.itertuples(index=False):
+                data.append(tup)
+        data.append(_numeric_stats(idadf, "max", columns))
+
+    data = pd.DataFrame(data)
+    data.columns = columns
+    if percentiles is not None:
+        percentile_names = [(str(int(x * 100)) + "%") for x in percentiles]
+    else:
+        percentile_names = []
+    data.index = ['count', 'mean', 'std', 'min'] + percentile_names + ['max']
+
+    # quick fix -> JDBC problems
+    # for column in data.columns:
+    #    data[[column]] = data[[column]].astype(float)
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        data = pd.Series(data[data.columns[0]])
+
+    return data
+
+
+def quantile(idadf, q=0.5):
+    """
+    See IdaDataFrame.quantile
+    """
+
+    if isinstance(q, Number):
+        q = [q]
+
+    # Sanity check
+    if True in [(not isinstance(x, Number)) for x in q]:
+        raise TypeError("Argument 'q' should be either a number or " +
+                        "a list of numbers between 0 and 1")
+    elif True in [((x >= 1) | (x <= 0)) for x in q]:
+        raise ValueError("Numbers in argument 'percentiles' should be between 0 and 1")
+
+    columns = idadf._get_numerical_columns()
+    if not columns:
+        print(idadf.name + " has no numeric columns")
+        return
+
+    result = _get_percentiles(idadf, q, columns)
+
+    if isinstance(q, list):
+        if len(q) > 1:
+            return result
+
+    result = result.T
+    result = result[result.columns[0]]
+    result.name = q[0]
+    result = result.astype('float')
+
+    if len(result) == 1:
+        result = result[0]
+
+    return result
+
+
+# Note : Not casting to double can lead to SQL overflow
+# TODO: Has to be modified in ibmdbR
+
+
+def cov(idadf, other=None):
+    if not idadf._idadb._is_netezza_system():
+        return cov_old(idadf, other)
+
+    numerical_columns = idadf._get_numerical_columns()
+    if len(numerical_columns) < 2:
+        print(idadf.name + " has less than two numeric columns")
+        return
+    column_string = ""
+    for column in numerical_columns:
+        column_string += "\"" + column + "\";"
+
+    result_df = pd.DataFrame(columns=numerical_columns, index=numerical_columns)
+
+    table_name = idadf.internal_state.current_state
+    outtable = idadf._idadb._get_valid_tablename(prefix="cov_")
+
+    idadf._idadb._call_stored_procedure("COVARIANCE1000MATRIX",
+                                        intable=table_name,
+                                        incolumn=column_string,
+                                        outtable=outtable)
+
+    # the calls of substring remove the surrounding double quotes
+    result_query = ("SELECT substring(VARXNAME,2,length(VARXNAME)-2) as VARXNAME, " +
+                    "substring(VARYNAME,2,length(VARYNAME)-2) as VARYNAME, " +
+                    "COVARIANCE, CNTX " +
+                    "FROM " + outtable + " ORDER BY varxname, varyname;")
+
+    cov_df = idadf.ida_query(result_query)
+
+    for index in cov_df.index.values:
+
+        col_list = []
+        for column in cov_df.columns.values:
+            col_list.append(cov_df.at[index, column])
+
+        result_df.at[col_list[0], col_list[1]] = col_list[2]
+
+    for column in result_df.columns:
+        result_df[column] = result_df[column].astype(float)
+    value = idadf._idadb.drop_table(outtable)
+
+    return result_df
+
+
+def cov_old(idadf, other=None):
+    """
+    See IdaDataFrame.cov
+    """
+    if isinstance(idadf, nzpyida.IdaSeries):
+        raise TypeError("cov() missing 1 required positional argument: 'other'")
+
+    # check if the covariance function is installed on the Netezza system
+    if idadf._idadb._is_netezza_system():
+        covar_query = "SELECT count(*) from _V_OBJECT  where OBJNAME like 'COVAR_SAMP#%' AND OBJDB = CURRENT_DB"
+        if idadf._idadb.ida_scalar_query(covar_query) == 0:
+            raise NotImplementedError("The COVAR_SAMP function is not installed on the Netezza database.")
+
+    columns = idadf._get_numerical_columns()
+    if len(columns) < 2:
+        print(idadf.name + " has less than two numeric columns")
+        return
+
+    tuple_count = _numeric_stats(idadf, 'count', columns)
+    count_dict = dict((x, int(y)) for x, y in zip(columns, tuple_count))
+
+    agg_list = []
+
+    combinations = [x for x in itertools.combinations_with_replacement(columns, 2)]
+    columns_set = [{x[0], x[1]} for x in combinations]
+
+    if idadf._idadb._is_netezza_system():
+        for column_pair in combinations:
+            agg_list.append("COVAR_SAMP(\"" + column_pair[0] + "\",\"" + column_pair[1] + "\")")
+    else:
+        for column_pair in combinations:
+            agg_list.append("COVARIANCE(\"" + column_pair[0] + "\",\"" +
+                            column_pair[1] + "\")*(" +
+                            str(min([count_dict[column_pair[0]],
+                                     count_dict[column_pair[1]]])) + ".0/" +
+                            str(min([count_dict[column_pair[0]],
+                                     count_dict[column_pair[1]]]) - 1) + ".0)")
+
+    agg_string = ', '.join(agg_list)
+
+    name = idadf.internal_state.current_state
+
+    data = idadf.ida_query("SELECT %s FROM %s" % (agg_string, name), first_row_only=True)
+
+    tuple_list = []
+
+    for column1 in columns:
+        list_value = []
+        for column2 in columns:
+            for index, column_set in enumerate(columns_set):
+                if {column1, column2} == column_set:
+                    list_value.append(data[index])
+                    break
+        tuple_list.append(tuple(list_value))
+
+    result = pd.DataFrame(tuple_list)
+    result.index = columns
+    result.columns = columns
+
+    if len(result) == 1:
+        result = result[0]
+
+    return result
+
+
+def corr(idadf):
+    if not idadf._idadb._is_netezza_system():
+        return corr_old(idadf)
+
+    numerical_columns = idadf._get_numerical_columns()
+    if len(numerical_columns) < 2:
+        print(idadf.name + " has less than two numeric columns")
+        return
+    column_string = ""
+    for column in numerical_columns:
+        column_string += "\"" + column + "\";"
+
+    result_df = pd.DataFrame(columns=numerical_columns, index=numerical_columns)
+
+    # print(result_df)
+
+    table_name = idadf.internal_state.current_state
+    outtable = idadf._idadb._get_valid_tablename(prefix="corr_")
+
+    idadf._idadb._call_stored_procedure("CORRELATION1000MATRIX ",
+                                        intable=table_name,
+                                        incolumn=column_string,
+                                        outtable=outtable)
+
+    # the calls of substring remove the surrounding double quotes
+    result_query = ("SELECT substring(VARXNAME,2,length(VARXNAME)-2) as VARXNAME, " +
+                    "substring(VARYNAME,2,length(VARYNAME)-2) as VARYNAME, " +
+                    "CORRELATION " +
+                    "FROM " + outtable + " ORDER BY varxname, varyname;")
+
+    corr_df = idadf.ida_query(result_query)
+
+    for index in corr_df.index.values:
+
+        col_list = []
+        for column in corr_df.columns.values:
+            col_list.append(corr_df.at[index, column])
+
+        result_df.at[col_list[0], col_list[1]] = col_list[2]
+
+    for column in result_df.columns:
+        result_df[column] = result_df[column].astype(float)
+    value = idadf._idadb.drop_table(outtable)
+    return result_df
+
+
+def train_test_split(idadf, train_table, test_table, id, fraction, seed):
+    table_name = idadf.internal_state.current_state
+
+    query = "CALL nza..SPLIT_DATA('intable = " + table_name + ",id = " + id + ",traintable = " + train_table + ",testtable= " + test_table + ",fraction= " + str(
+        fraction) + ",seed=" + str(seed) + ",outtabletype=table" + "');"
+
+    df = idadf.ida_query(query)
+    idadf.commit()
+
+    return df
+
+
+def corr_old(idadf, features=None, ignore_indexer=True):
+    """
+    See IdaDataFrame.corr
+    """
+    if isinstance(idadf, nzpyida.IdaSeries):
+        raise TypeError("corr() missing 1 required positional argument: 'other'")
+
+    # check if the corr function is installed on the Netezza system
+    if idadf._idadb._is_netezza_system():
+        corr_query = "SELECT count(*) from _V_OBJECT  where OBJNAME like 'CORR#%' AND OBJDB = CURRENT_DB"
+        if idadf._idadb.ida_scalar_query(corr_query) == 0:
+            raise NotImplementedError("The CORR function is not installed on the Netezza database.")
+
+    numerical_columns = idadf._get_numerical_columns()
+    if len(numerical_columns) < 2:
+        print(idadf.name + " has less than two numeric columns")
+        return
+
+    if ignore_indexer is True:
+        if idadf.indexer:
+            if idadf.indexer in numerical_columns:
+                numerical_columns.remove(idadf.indexer)
+
+    # print(features)
+    # target, features = ibmdbpy.utils._check_input(target, features)
+    if features is not None:
+        for feature in features:
+            if feature not in numerical_columns:
+                raise TypeError("Correlation-based measure not available for non-numerical columns %s" % feature)
+    else:
+        features = numerical_columns
+
+    # if target not in columns:
+    #    raise ValueError("%s is not a column of numerical type in %s"%(target, idadf.name))
+
+    values = OrderedDict()
+
+    combinations = [x for x in itertools.combinations(features, 2)]
+    # columns_set = [{x[0], x[1]} for x in combinations]
+
+    if idadf._idadb._is_netezza_system():
+        corr_function = "CORR"
+    else:
+        corr_function = "CORRELATION"
+
+    if len(features) < 64:  # the limit of variables for an SQL statement is 4096, i.e 64^2
+        agg_list = []
+        for column_pair in combinations:
+            agg = corr_function + "(\"%s\",\"%s\")" % (column_pair[0], column_pair[1])
+            agg_list.append(agg)
+
+        agg_string = ', '.join(agg_list)
+
+        name = idadf.internal_state.current_state
+
+        data = idadf.ida_query("SELECT %s FROM %s" % (agg_string, name), first_row_only=True)
+
+        for i, element in enumerate(combinations):
+            if element[0] not in values:
+                values[element[0]] = {}
+            if element[1] not in values:
+                values[element[1]] = {}
+            values[element[0]][element[1]] = data[i]
+            values[element[1]][element[0]] = data[i]
+
+        result = pd.DataFrame(values).fillna(1)
+    else:
+        chunkgen = chunklist(combinations, 100)
+
+        for chunk in chunkgen:
+            agg_list = []
+            for column_pair in chunk:
+                agg = corr_function + "(\"%s\",\"%s\")" % (column_pair[0], column_pair[1])
+                agg_list.append(agg)
+
+            agg_string = ', '.join(agg_list)
+
+            name = idadf.internal_state.current_state
+
+            data = idadf.ida_query("SELECT %s FROM %s" % (agg_string, name), first_row_only=True)
+
+            for i, element in enumerate(chunk):
+                if element[0] not in values:
+                    values[element[0]] = OrderedDict()
+                if element[1] not in values:
+                    values[element[1]] = OrderedDict()
+                values[element[0]][element[1]] = data[i]
+                values[element[1]][element[0]] = data[i]
+
+        result = pd.DataFrame(values).fillna(1)
+
+    result = result.reindex(result.columns)
+    if len(result) == 1:
+        result = result[0]
+
+    return result
+
+
+### corrwith
+
+
+def mad(idadf):
+    """
+    See IdaDataFrame.mad
+    """
+    columns = idadf._get_numerical_columns()
+    if len(columns) < 2:
+        print(idadf.name + " has less than two numeric columns")
+        return
+
+    mean_tuple = _numeric_stats(idadf, "mean", columns)
+    absmean_dict = dict((x, abs(y)) for x, y in zip(columns, mean_tuple))
+    tuple_na = _get_number_of_nas(idadf, columns)
+
+    agg_list = []
+    for index_col, column in enumerate(columns):
+        if idadf._idadb._is_netezza_system():
+            # workaround for error on Netezza related to "/"
+            # ERROR:  Unable to identify an operator '//'
+            div_term = "* pow(" + str(idadf.shape[0] - tuple_na[index_col]) + ", -1)"
+        else:
+            div_term = "/" + str(idadf.shape[0] - tuple_na[index_col])
+        agg_list.append("SUM(ABS(\"" + column + "\" -" +
+                        str(absmean_dict[column]) + "))" +
+                        div_term)
+
+    agg_string = ', '.join(agg_list)
+
+    name = idadf.internal_state.current_state
+
+    mad_tuple = idadf.ida_query("SELECT %s FROM %s" % (agg_string, name))
+    result = pd.Series(mad_tuple.values[0])
+    result.index = columns
+    result = result.astype('float')
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        result = result[0]
+
+    return result
+
+
+def ida_min(idadf):
+    """
+    See idadataFrame.min
+    """
+    na_tuple = _get_number_of_nas(idadf, idadf.columns)
+    min_tuple = _numeric_stats(idadf, "min", idadf.columns)
+    if not hasattr(min_tuple, "__iter__"): min_tuple = (min_tuple,)  # dirty fix
+    min_list = [np.nan if ((y > 0) and not isinstance(x, Number))
+                else x for x, y in zip(min_tuple, na_tuple)]
+    min_tuple = tuple(min_list)
+    result = pd.Series(min_tuple)
+    result.index = idadf.columns
+
+    # if isinstance(idadf, ibmdbpy.IdaSeries):
+    #   result = result[0]
+
+    return result
+
+
+def ida_max(idadf):
+    """
+    See idadataFrame.max
+    """
+    na_tuple = _get_number_of_nas(idadf, idadf.columns)
+    max_tuple = _numeric_stats(idadf, "max", idadf.columns)
+    if not hasattr(max_tuple, "__iter__"): max_tuple = (max_tuple,)  # dirty fix
+    max_list = [np.nan if ((y > 0) and not isinstance(x, Number))
+                else x for x, y in zip(max_tuple, na_tuple)]
+    max_tuple = tuple(max_list)
+    result = pd.Series(max_tuple)
+    result.index = idadf.columns
+
+    # if isinstance(idadf, ibmdbpy.IdaSeries):
+    #  result = result[0]
+
+    return result
+
+
+def count(idadf):
+    """
+    See IdaDataFrame.count
+    """
+    count_tuple = _numeric_stats(idadf, "count", idadf.columns)
+    result = pd.Series(count_tuple)
+    result.index = idadf.columns
+    result = result.astype(int)
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        result = result[0]
+
+    return result
+
+
+def count_distinct(idadf):
+    """
+    See IdaDataFrame.count_distinct
+    """
+    result = pd.Series(_count_level(idadf))
+    result.index = idadf.columns
+    result = result.astype(int)
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        result = result[0]
+
+    return result
+
+
+def std(idadf):
+    """
+    See IdaDataFrame.std
+    """
+    columns = idadf._get_numerical_columns()
+    if not columns:
+        warnings.warn("%s has no numeric columns" % idadf.name)
+        return pd.Series()
+
+    std_tuple = _numeric_stats(idadf, "std", columns)
+
+    result = pd.Series(std_tuple)
+    result.index = columns
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        result = result[0]
+
+    return result
+
+
+def var(idadf):
+    """
+    See IdaDataFrame.var
+    """
+    columns = idadf._get_numerical_columns()
+    if not columns:
+        warnings.warn("%s has no numeric columns" % idadf.name)
+        return pd.Series()
+
+    var_tuple = _numeric_stats(idadf, "var", columns)
+
+    result = pd.Series(var_tuple)
+    result.index = columns
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        result = result[0]
+
+    return result
+
+
+def mean(idadf):
+    """
+    See IdaDataFrame.mean
+    """
+    columns = idadf._get_numerical_columns()
+    if not columns:
+        warnings.warn("%s has no numeric columns" % idadf.name)
+        return pd.Series()
+
+    mean_tuple = _numeric_stats(idadf, "mean", columns)
+
+    result = pd.Series(mean_tuple)
+    result.index = columns
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        result = result[0]
+
+    return result
+
+
+def ida_sum(idadf):
+    """
+    See IdaDataFrame.sum
+    """
+    # Behave like having the option "numeric only" to true
+    columns = idadf._get_numerical_columns()
+    if not columns:
+        warnings.warn("%s has no numeric columns" % idadf.name)
+        return pd.Series()
+
+    sum_tuple = _numeric_stats(idadf, "sum", columns)
+
+    result = pd.Series(sum_tuple)
+    result.index = columns
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        result = result[0]
+
+    return result
+
+
+def median(idadf):
+    """
+    See IdaDataFrame.median
+    """
+    # Behave like having the option "numeric only" to true
+    columns = idadf._get_numerical_columns()
+    if not columns:
+        warnings.warn("%s has no numeric columns" % idadf.name)
+        return pd.Series()
+
+    median_tuple = _numeric_stats(idadf, "median", columns)
+
+    result = pd.Series(median_tuple)
+    result.index = columns
+
+    if isinstance(idadf, nzpyida.IdaSeries):
+        result = result[0]
+
+    return result
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_association_rules.py` & `nzpyida-0.3.3/nzpyida/tests/test_association_rules.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,53 +1,53 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for ibmdbpy.learn.assocation_rules
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-class Test_AssociationRulesInitiateModel(object):
-
-    def test_arules_instance(self, idadf):
-        pass
-
-    def test_arules_get_parameters(self, idadf):
-        pass
-
-    def test_arules_set_parameters(self, idadf):
-        pass
-
-class Test_AssociationRulesFitandPredict(object):
-
-    def test_arules_fit(self, idadf):
-        pass
-
-    def test_arules_predict(self, idadf):
-        pass
-
-    def test_arules_fit_and_predict(self, idadf):
-        pass
-
-class Test_AssociationRulesExploreResult(object):
-
-    def test_arules_describe(self, idadf):
-        pass
-
-    def test_arules_get_labels(self, idadf):
-        pass
-
-    def test_arules_retrieve_KMeans_Model(self, idadf):
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for ibmdbpy.learn.assocation_rules
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+class Test_AssociationRulesInitiateModel(object):
+
+    def test_arules_instance(self, idadf):
+        pass
+
+    def test_arules_get_parameters(self, idadf):
+        pass
+
+    def test_arules_set_parameters(self, idadf):
+        pass
+
+class Test_AssociationRulesFitandPredict(object):
+
+    def test_arules_fit(self, idadf):
+        pass
+
+    def test_arules_predict(self, idadf):
+        pass
+
+    def test_arules_fit_and_predict(self, idadf):
+        pass
+
+class Test_AssociationRulesExploreResult(object):
+
+    def test_arules_describe(self, idadf):
+        pass
+
+    def test_arules_get_labels(self, idadf):
+        pass
+
+    def test_arules_retrieve_KMeans_Model(self, idadf):
         pass
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_base.py` & `nzpyida-0.3.3/nzpyida/tests/test_base.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,201 +1,201 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-import pandas
-import pytest
-import six
-from nzpyida.learn import KMeans
-from flaky import flaky
-
-class Test_DataBaseExploration(object):
-
-    def test_idadb_current_schema(self, idadb):
-        assert isinstance(idadb.current_schema, six.string_types)
-
-    def test_idadb_show_tables(self, idadb):
-        test = idadb.show_tables()
-        assert isinstance(test, pandas.core.frame.DataFrame)
-        assert (list(test.columns) == ['TABSCHEMA','TABNAME','OWNER','TYPE'])
-
-    def test_idadb_show_tables_all(self, idadb):
-        test = idadb.show_tables(True)
-        assert isinstance(test, pandas.core.frame.DataFrame)
-        assert (list(test.columns) == ['TABSCHEMA','TABNAME','OWNER','TYPE'])
-
-    def test_idadb_show_models(self, idadb):
-        df = idadb.show_models()
-        assert isinstance(df, pandas.core.frame.DataFrame)
-        if not df.empty:
-            show_model_columns = ['MODELNAME', 'OWNER', 'CREATED', 'STATE',
-                                  'MININGFUNCTION', 'ALGORITHM', 'USERCATEGORY']
-            if not idadb._is_netezza_system():
-                show_model_columns =  ['MODELSCHEMA'] + show_model_columns
-            assert (list(df.columns) == show_model_columns)
-
-    def test_idadb_exists_table_or_view_positive0(self, idadb, idadf, idaview):
-        assert(idadb.exists_table_or_view(idadf.name) == 1)
-        assert(idadb.exists_table_or_view(idaview.name) == 1)
-
-    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-    def test_idadb_exists_table_or_view_positive(self, idadb, idadf, idaview, idageodf_county_view):
-        assert(idadb.exists_table_or_view(idadf.name) == 1)
-        assert(idadb.exists_table_or_view(idaview.name) == 1)
-        assert(idadb.exists_table_or_view(idageodf_county_view.name) == 1)
-
-    def test_idadb_exists_table_or_view_negative(self, idadb):
-        assert(idadb.exists_table_or_view("NOT_EXISTING_DATA_FRAME_130530496_4860385960") == 0)
-        assert(idadb.exists_table_or_view("NOT_EXISTING_130530496_4860385960") == 0)
-
-    #def test_idadb_exists_table_or_view_error(self, idadb):
-    #    with pytest.raises(TypeError):
-    #        idadb.exists_table_or_view("ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE")
-    # Note: We cannot assume that "ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE" exists and is of type "A"
-
-    def test_idadb_exists_table_positive(self, idadb, idadf):
-        assert(idadb.exists_table(idadf.name) == 1)
-
-    def test_idadb_exists_table_negative(self, idadb):
-        assert(idadb.exists_table("NOT_EXISTING_FRAME_130530496_4860385960") == 0)
-
-    def test_idadb_exists_table_error(self, idadb, idaview):
-        with pytest.raises(TypeError):
-            idadb.exists_table(idaview.name)
-
-    def test_idadb_exists_view_positive(self, idadb, idaview):
-        assert(idadb.exists_view(idaview.name) == 1)
-
-    def test_idadb_exists_view_negative(self, idadb):
-        assert(idadb.exists_view("NOT_EXISTING_VIEW_130530496_4860385960") == 0)
-
-    def test_idadb_exists_view_error(self, idadb, idadf):
-        with pytest.raises(TypeError):
-            idadb.exists_view(idadf.name)
-
-    def test_idadb_exists_model_positive(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, destructive=True)
-        # Create a simple KMEANS model
-        kmeans = KMeans(n_clusters=3, modelname="MODEL_58979457385")
-        kmeans.fit(idadf_tmp)
-        assert(idadb.exists_model("MODEL_58979457385") == 1)
-        try :
-            idadb.drop_model(kmeans.modelname)
-        except : pass
-
-    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-    def test_idadb_exists_model_with_schema_positive(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, destructive=True)
-        # Create a simple KMEANS model
-        kmeans = KMeans(n_clusters=3, modelname="MYSCHEMA.MODEL_45738558979")
-        kmeans.fit(idadf_tmp)
-        assert(idadb.exists_model("MYSCHEMA.MODEL_45738558979") == 1)
-        try :
-            idadb.drop_model(kmeans.modelname)
-        except : pass
-
-    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-    def test_idadb_exists_model_with_schema_positive_mixed_case(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, destructive=True)
-        # Create a simple KMEANS model
-        kmeans = KMeans(n_clusters=3, modelname="mySchema.Model_85584573979")
-        kmeans.fit(idadf_tmp)
-        assert(idadb.exists_model("mySchema.Model_85584573979") == 1)
-        assert(idadb.exists_model("MYSCHEMA.MODEL_85584573979") == 1)
-        assert(idadb.exists_model("myschema.model_85584573979") == 1)
-        try :
-            idadb.drop_model(kmeans.modelname)
-        except : pass
-
-    @flaky
-    def test_idadb_exists_model_negative(self, idadb):
-        if idadb.exists_model("MODEL_58979457385"):
-            idadb.drop_model("MODEL_58979457385")
-        assert(idadb.exists_model("MODEL_58979457385") == 0)
-
-    def test_idadb_exists_model_with_schema_negative(self, idadb):
-        if idadb.exists_model("SCHEMA_58979457385.MODEL_58979457385"):
-            idadb.drop_model("SCHEMA_58979457385.MODEL_58979457385")
-        assert(idadb.exists_model("SCHEMA_58979457385.MODEL_58979457385") == 0)
-
-    def test_idadb_exists_model_error(self, idadb, idadf):
-        with pytest.raises(TypeError):
-            idadb.exists_model(idadf.name)
-
-    def test_idadb_is_table_or_view_positive(self, idadb, idadf, idaview):
-        assert(idadb.is_table_or_view(idadf.name) == 1)
-        assert(idadb.is_table_or_view(idaview.name) == 1)
-
-    #def test_idadb_is_table_or_view_negative(self, idadb):
-    #    assert(idadb.is_table_or_view("ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE") == 0)
-
-    def test_idadb_is_table_or_view_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb.is_table_or_view("NOT_EXISTING_DATA_FRAME_130530496_4860385960")
-
-    def test_idadb_is_table_positive(self, idadb, idadf):
-        assert(idadb.is_table(idadf.name) == 1)
-
-    def test_idadb_is_table_negative(self, idadb, idaview):
-        assert(idadb.is_table(idaview.name) == 0)
-
-    def test_idadb_is_table_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb.is_table("NOT_EXISTING_DATA_FRAME_130530496_4860385960")
-
-    def test_idadb_is_view_positive(self, idadb, idaview):
-        assert(idadb.is_view(idaview.name) == 1)
-
-    def test_idadb_is_view_negative(self, idadb, idadf):
-        assert(idadb.is_view(idadf.name) == 0)
-
-    def test_idadb_is_view_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb.is_view("NOT_EXISTING_VIEW_130530496_4860385960")
-
-    def test_idadb_is_model_positive(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, destructive = True)
-        # Create a simple KMEANS model
-        kmeans = KMeans(n_clusters = 3)
-        kmeans.fit(idadf_tmp)
-        assert(idadb.is_model(kmeans.modelname) == 1)
-        try : idadb.drop_model(kmeans.modelname)
-        except : pass
-
-    def test_idadb_is_model_negative(self, idadb, idadf, idaview):
-        assert(idadb.is_model(idadf.name) == 0)
-        assert(idadb.is_model(idaview.name) == 0)
-        if not idadb._is_netezza_system():
-            assert(idadb.is_model("ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE") == 0)
-
-    def test_idadb_is_model_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb.is_model("NOTEXISTINGOBJECT_496070383095079384063739509")
-
-# List of functions that do not need to be tested :
-# i.e. the execution of everything here rely on it
-# _prepare_and_execute
-# _call_stored_procedure
-# _autocommit
-# _check_connection
-# _exists
-# _is
-# _drop
-
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+import pandas
+import pytest
+import six
+from nzpyida.learn import KMeans
+from flaky import flaky
+
+class Test_DataBaseExploration(object):
+
+    def test_idadb_current_schema(self, idadb):
+        assert isinstance(idadb.current_schema, six.string_types)
+
+    def test_idadb_show_tables(self, idadb):
+        test = idadb.show_tables()
+        assert isinstance(test, pandas.core.frame.DataFrame)
+        assert (list(test.columns) == ['TABSCHEMA','TABNAME','OWNER','TYPE'])
+
+    def test_idadb_show_tables_all(self, idadb):
+        test = idadb.show_tables(True)
+        assert isinstance(test, pandas.core.frame.DataFrame)
+        assert (list(test.columns) == ['TABSCHEMA','TABNAME','OWNER','TYPE'])
+
+    def test_idadb_show_models(self, idadb):
+        df = idadb.show_models()
+        assert isinstance(df, pandas.core.frame.DataFrame)
+        if not df.empty:
+            show_model_columns = ['MODELNAME', 'OWNER', 'CREATED', 'STATE',
+                                  'MININGFUNCTION', 'ALGORITHM', 'USERCATEGORY']
+            if not idadb._is_netezza_system():
+                show_model_columns =  ['MODELSCHEMA'] + show_model_columns
+            assert (list(df.columns) == show_model_columns)
+
+    def test_idadb_exists_table_or_view_positive0(self, idadb, idadf, idaview):
+        assert(idadb.exists_table_or_view(idadf.name) == 1)
+        assert(idadb.exists_table_or_view(idaview.name) == 1)
+
+    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+    def test_idadb_exists_table_or_view_positive(self, idadb, idadf, idaview, idageodf_county_view):
+        assert(idadb.exists_table_or_view(idadf.name) == 1)
+        assert(idadb.exists_table_or_view(idaview.name) == 1)
+        assert(idadb.exists_table_or_view(idageodf_county_view.name) == 1)
+
+    def test_idadb_exists_table_or_view_negative(self, idadb):
+        assert(idadb.exists_table_or_view("NOT_EXISTING_DATA_FRAME_130530496_4860385960") == 0)
+        assert(idadb.exists_table_or_view("NOT_EXISTING_130530496_4860385960") == 0)
+
+    #def test_idadb_exists_table_or_view_error(self, idadb):
+    #    with pytest.raises(TypeError):
+    #        idadb.exists_table_or_view("ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE")
+    # Note: We cannot assume that "ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE" exists and is of type "A"
+
+    def test_idadb_exists_table_positive(self, idadb, idadf):
+        assert(idadb.exists_table(idadf.name) == 1)
+
+    def test_idadb_exists_table_negative(self, idadb):
+        assert(idadb.exists_table("NOT_EXISTING_FRAME_130530496_4860385960") == 0)
+
+    def test_idadb_exists_table_error(self, idadb, idaview):
+        with pytest.raises(TypeError):
+            idadb.exists_table(idaview.name)
+
+    def test_idadb_exists_view_positive(self, idadb, idaview):
+        assert(idadb.exists_view(idaview.name) == 1)
+
+    def test_idadb_exists_view_negative(self, idadb):
+        assert(idadb.exists_view("NOT_EXISTING_VIEW_130530496_4860385960") == 0)
+
+    def test_idadb_exists_view_error(self, idadb, idadf):
+        with pytest.raises(TypeError):
+            idadb.exists_view(idadf.name)
+
+    def test_idadb_exists_model_positive(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, destructive=True)
+        # Create a simple KMEANS model
+        kmeans = KMeans(n_clusters=3, modelname="MODEL_58979457385")
+        kmeans.fit(idadf_tmp)
+        assert(idadb.exists_model("MODEL_58979457385") == 1)
+        try :
+            idadb.drop_model(kmeans.modelname)
+        except : pass
+
+    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+    def test_idadb_exists_model_with_schema_positive(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, destructive=True)
+        # Create a simple KMEANS model
+        kmeans = KMeans(n_clusters=3, modelname="MYSCHEMA.MODEL_45738558979")
+        kmeans.fit(idadf_tmp)
+        assert(idadb.exists_model("MYSCHEMA.MODEL_45738558979") == 1)
+        try :
+            idadb.drop_model(kmeans.modelname)
+        except : pass
+
+    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+    def test_idadb_exists_model_with_schema_positive_mixed_case(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, destructive=True)
+        # Create a simple KMEANS model
+        kmeans = KMeans(n_clusters=3, modelname="mySchema.Model_85584573979")
+        kmeans.fit(idadf_tmp)
+        assert(idadb.exists_model("mySchema.Model_85584573979") == 1)
+        assert(idadb.exists_model("MYSCHEMA.MODEL_85584573979") == 1)
+        assert(idadb.exists_model("myschema.model_85584573979") == 1)
+        try :
+            idadb.drop_model(kmeans.modelname)
+        except : pass
+
+    @flaky
+    def test_idadb_exists_model_negative(self, idadb):
+        if idadb.exists_model("MODEL_58979457385"):
+            idadb.drop_model("MODEL_58979457385")
+        assert(idadb.exists_model("MODEL_58979457385") == 0)
+
+    def test_idadb_exists_model_with_schema_negative(self, idadb):
+        if idadb.exists_model("SCHEMA_58979457385.MODEL_58979457385"):
+            idadb.drop_model("SCHEMA_58979457385.MODEL_58979457385")
+        assert(idadb.exists_model("SCHEMA_58979457385.MODEL_58979457385") == 0)
+
+    def test_idadb_exists_model_error(self, idadb, idadf):
+        with pytest.raises(TypeError):
+            idadb.exists_model(idadf.name)
+
+    def test_idadb_is_table_or_view_positive(self, idadb, idadf, idaview):
+        assert(idadb.is_table_or_view(idadf.name) == 1)
+        assert(idadb.is_table_or_view(idaview.name) == 1)
+
+    #def test_idadb_is_table_or_view_negative(self, idadb):
+    #    assert(idadb.is_table_or_view("ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE") == 0)
+
+    def test_idadb_is_table_or_view_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb.is_table_or_view("NOT_EXISTING_DATA_FRAME_130530496_4860385960")
+
+    def test_idadb_is_table_positive(self, idadb, idadf):
+        assert(idadb.is_table(idadf.name) == 1)
+
+    def test_idadb_is_table_negative(self, idadb, idaview):
+        assert(idadb.is_table(idaview.name) == 0)
+
+    def test_idadb_is_table_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb.is_table("NOT_EXISTING_DATA_FRAME_130530496_4860385960")
+
+    def test_idadb_is_view_positive(self, idadb, idaview):
+        assert(idadb.is_view(idaview.name) == 1)
+
+    def test_idadb_is_view_negative(self, idadb, idadf):
+        assert(idadb.is_view(idadf.name) == 0)
+
+    def test_idadb_is_view_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb.is_view("NOT_EXISTING_VIEW_130530496_4860385960")
+
+    def test_idadb_is_model_positive(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, destructive = True)
+        # Create a simple KMEANS model
+        kmeans = KMeans(n_clusters = 3)
+        kmeans.fit(idadf_tmp)
+        assert(idadb.is_model(kmeans.modelname) == 1)
+        try : idadb.drop_model(kmeans.modelname)
+        except : pass
+
+    def test_idadb_is_model_negative(self, idadb, idadf, idaview):
+        assert(idadb.is_model(idadf.name) == 0)
+        assert(idadb.is_model(idaview.name) == 0)
+        if not idadb._is_netezza_system():
+            assert(idadb.is_model("ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE") == 0)
+
+    def test_idadb_is_model_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb.is_model("NOTEXISTINGOBJECT_496070383095079384063739509")
+
+# List of functions that do not need to be tested :
+# i.e. the execution of everything here rely on it
+# _prepare_and_execute
+# _call_stored_procedure
+# _autocommit
+# _check_connection
+# _exists
+# _is
+# _drop
+
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_base_connexion.py` & `nzpyida-0.3.3/nzpyida/tests/test_base_connexion.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,154 +1,154 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-from numbers import Number
-
-import pandas
-import pytest
-import six
-
-from nzpyida import IdaDataBase, IdaDataFrame
-from nzpyida.exceptions import IdaDataBaseError
-
-class Test_ConnectToDB(object):
-
-    def test_idadb_attr(self, idadb):
-
-        if idadb._con_type == 'nzpy':
-            assert isinstance(idadb.data_source_name, dict)
-        else:
-            assert isinstance(idadb.data_source_name, six.string_types)
-        assert isinstance(idadb._con_type, six.string_types)
-        assert isinstance(idadb._idadfs, list)
-        if idadb._con_type =='odbc':
-            assert(isinstance(idadb._connection_string, six.string_types))
-        elif idadb._con_type == 'jdbc':
-            assert(isinstance(idadb._connection_string, six.string_types) or isinstance(idadb._connection_string, list))
-        elif idadb._con_type == 'nzpy':
-            assert(isinstance(idadb._connection_string, dict))
-        assert(type(idadb._con).__name__ in ['Connection','instance'])
-        assert(idadb._con_type in ["odbc", "jdbc", "nzpy"])
-
-    def test_idadb_instance_fail(self, idadb):
-
-        if idadb._con_type == 'odbc':
-            with pytest.raises(IdaDataBaseError):
-                IdaDataBase(dsn = 'NOTEXISTING_DATASOURCE')
-
-        if idadb._con_type == 'jdbc':
-            with pytest.raises(IdaDataBaseError):
-                IdaDataBase(dsn='jdbc:NOTEXISTING_DATASOURCE')
-            with pytest.raises(IdaDataBaseError):
-                IdaDataBase(dsn = 'jdbc:db2://awh-yp-small03.services.dal.bluemix.net:50000/BLUDB:user=XXXXXXXXXX;password=XXXXXXXXXXXX',
-                        uid="hello", pwd="world")
-            with pytest.raises(IdaDataBaseError):
-                IdaDataBase(dsn = 'jdbc:db2://awh-yp-small03.services.dal.bluemix.net:50000/BLUDB')
-
-
-class Test_ConnexionManagement(object):
-
-    def test_idadb_commit(self, idadb, df):
-        try : idadb.drop_table("TEST_COMMIT_5869703824607040")
-        except : pass
-        idadb._create_table(df, "TEST_COMMIT_5869703824607040")
-        idadb.commit()
-        #idadb.rback()
-        assert(idadb.exists_table("TEST_COMMIT_5869703824607040") == 1)
-        idadb.drop_table("TEST_COMMIT_5869703824607040")
-        idadb.commit()
-
-    def test_idadb_rollback(self, idadb, df):
-        # TODO: Does not work for JDBC -> Why no rollback for jaydebeapi ?
-        if idadb._con_type == 'odbc':
-            try:
-                idadb.drop_table("TEST_ROLLBACK_59673030586849305074")
-            except:
-                pass
-            idadb.commit()
-            idadb._create_table(df, "TEST_ROLLBACK_59673030586849305074")
-            idadb.rollback()
-            idadb.commit()
-            assert(idadb.exists_table("TEST_ROLLBACK_59673030586849305074") == 0)
-
-    def test_idadb_close(self, idadb_tmp):
-        idadb_tmp.close()
-        with pytest.raises(IdaDataBaseError):
-            idadb_tmp.show_tables()
-
-    def test_reconnect(self, idadb_tmp):
-        idadb_tmp.close()
-        with pytest.raises(IdaDataBaseError):
-            idadb_tmp.show_tables()
-        idadb_tmp.reconnect()
-        assert(isinstance(idadb_tmp.show_tables(), pandas.DataFrame))
-
-
-class Test_UploadDataFrame(object):
-
-    def test_idadb_as_idadataframe(self, idadb, df):
-        ida = idadb.as_idadataframe(df, "TEST_AS_IDADF_18729493954_23849590", clear_existing = True)
-        assert(all(ida.columns == df.columns))
-        assert(list(ida.index) == list(df.index))
-        assert(ida.shape == df.shape)
-        idadb.drop_table("TEST_AS_IDADF_18729493954_23849590")
-
-    # test schema support
-    def test_idadb_as_idadataframe_with_schema(self, idadb, df):
-        ida = idadb.as_idadataframe(df, "DUMMY.TEST_AS_IDADF_18729493954_23849590", clear_existing = True)
-        assert(all(ida.columns == df.columns))
-        assert(list(ida.index) == list(df.index))
-        assert(ida.shape == df.shape)
-        idadb.drop_table("DUMMY.TEST_AS_IDADF_18729493954_23849590")
-
-    # test if the columns function returns the correct values
-    # if two tables exist with the same name but different schema
-    def test_idadb_as_idadataframe_same_tablename(self, idadb, df):
-        ida11 = idadb.as_idadataframe(df, "TEST_AS_IDADF_18729493954_23849590", clear_existing = True)
-        ida21 = idadb.as_idadataframe(df, "DUMMY.TEST_AS_IDADF_18729493954_23849590", clear_existing = True)
-        assert(all(ida11.columns == df.columns))
-        assert(all(ida21.columns == df.columns))
-        ida12 = IdaDataFrame(idadb, "TEST_AS_IDADF_18729493954_23849590")
-        ida22 = IdaDataFrame(idadb, "DUMMY.TEST_AS_IDADF_18729493954_23849590")
-        assert(all(ida12.columns == df.columns))
-        assert(all(ida22.columns == df.columns))
-        idadb.drop_table("TEST_AS_IDADF_18729493954_23849590")
-        idadb.drop_table("DUMMY.TEST_AS_IDADF_18729493954_23849590")
-
-    def test_idadb_ida_query(self, idadb, idadf):
-        query = "SELECT * FROM %s FETCH LIMIT 5"%idadf.name
-        df = idadb.ida_query(query)
-        assert(isinstance(df,pandas.DataFrame))
-        assert(len(idadf.columns) == len(df.columns))
-        assert(len(df) == 5)
-
-    def test_idadb_ida_query_first_row_only(self, idadb, idadf, df):
-        query = "SELECT * FROM %s FETCH LIMIT 5"%idadf.name
-        downloaded_df = idadb.ida_query(query, first_row_only=True)
-        assert(isinstance(downloaded_df,tuple))
-        assert(len(downloaded_df) == len(df.loc[0]))
-
-    def test_idadb_ida_scalar_query(self, idadb, idadf):
-        query = "SELECT * FROM %s FETCH LIMIT 5"%idadf.name
-        downloaded_df = idadb.ida_scalar_query(query)
-        assert(isinstance(downloaded_df,six.string_types)|isinstance(downloaded_df,Number))
-
-
-    ################# MORE TEST TO DO HERE
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+from numbers import Number
+
+import pandas
+import pytest
+import six
+
+from nzpyida import IdaDataBase, IdaDataFrame
+from nzpyida.exceptions import IdaDataBaseError
+
+class Test_ConnectToDB(object):
+
+    def test_idadb_attr(self, idadb):
+
+        if idadb._con_type == 'nzpy':
+            assert isinstance(idadb.data_source_name, dict)
+        else:
+            assert isinstance(idadb.data_source_name, six.string_types)
+        assert isinstance(idadb._con_type, six.string_types)
+        assert isinstance(idadb._idadfs, list)
+        if idadb._con_type =='odbc':
+            assert(isinstance(idadb._connection_string, six.string_types))
+        elif idadb._con_type == 'jdbc':
+            assert(isinstance(idadb._connection_string, six.string_types) or isinstance(idadb._connection_string, list))
+        elif idadb._con_type == 'nzpy':
+            assert(isinstance(idadb._connection_string, dict))
+        assert(type(idadb._con).__name__ in ['Connection','instance'])
+        assert(idadb._con_type in ["odbc", "jdbc", "nzpy"])
+
+    def test_idadb_instance_fail(self, idadb):
+
+        if idadb._con_type == 'odbc':
+            with pytest.raises(IdaDataBaseError):
+                IdaDataBase(dsn = 'NOTEXISTING_DATASOURCE')
+
+        if idadb._con_type == 'jdbc':
+            with pytest.raises(IdaDataBaseError):
+                IdaDataBase(dsn='jdbc:NOTEXISTING_DATASOURCE')
+            with pytest.raises(IdaDataBaseError):
+                IdaDataBase(dsn = 'jdbc:db2://awh-yp-small03.services.dal.bluemix.net:50000/BLUDB:user=XXXXXXXXXX;password=XXXXXXXXXXXX',
+                        uid="hello", pwd="world")
+            with pytest.raises(IdaDataBaseError):
+                IdaDataBase(dsn = 'jdbc:db2://awh-yp-small03.services.dal.bluemix.net:50000/BLUDB')
+
+
+class Test_ConnexionManagement(object):
+
+    def test_idadb_commit(self, idadb, df):
+        try : idadb.drop_table("TEST_COMMIT_5869703824607040")
+        except : pass
+        idadb._create_table(df, "TEST_COMMIT_5869703824607040")
+        idadb.commit()
+        #idadb.rback()
+        assert(idadb.exists_table("TEST_COMMIT_5869703824607040") == 1)
+        idadb.drop_table("TEST_COMMIT_5869703824607040")
+        idadb.commit()
+
+    def test_idadb_rollback(self, idadb, df):
+        # TODO: Does not work for JDBC -> Why no rollback for jaydebeapi ?
+        if idadb._con_type == 'odbc':
+            try:
+                idadb.drop_table("TEST_ROLLBACK_59673030586849305074")
+            except:
+                pass
+            idadb.commit()
+            idadb._create_table(df, "TEST_ROLLBACK_59673030586849305074")
+            idadb.rollback()
+            idadb.commit()
+            assert(idadb.exists_table("TEST_ROLLBACK_59673030586849305074") == 0)
+
+    def test_idadb_close(self, idadb_tmp):
+        idadb_tmp.close()
+        with pytest.raises(IdaDataBaseError):
+            idadb_tmp.show_tables()
+
+    def test_reconnect(self, idadb_tmp):
+        idadb_tmp.close()
+        with pytest.raises(IdaDataBaseError):
+            idadb_tmp.show_tables()
+        idadb_tmp.reconnect()
+        assert(isinstance(idadb_tmp.show_tables(), pandas.DataFrame))
+
+
+class Test_UploadDataFrame(object):
+
+    def test_idadb_as_idadataframe(self, idadb, df):
+        ida = idadb.as_idadataframe(df, "TEST_AS_IDADF_18729493954_23849590", clear_existing = True)
+        assert(all(ida.columns == df.columns))
+        assert(list(ida.index) == list(df.index))
+        assert(ida.shape == df.shape)
+        idadb.drop_table("TEST_AS_IDADF_18729493954_23849590")
+
+    # test schema support
+    def test_idadb_as_idadataframe_with_schema(self, idadb, df):
+        ida = idadb.as_idadataframe(df, "DUMMY.TEST_AS_IDADF_18729493954_23849590", clear_existing = True)
+        assert(all(ida.columns == df.columns))
+        assert(list(ida.index) == list(df.index))
+        assert(ida.shape == df.shape)
+        idadb.drop_table("DUMMY.TEST_AS_IDADF_18729493954_23849590")
+
+    # test if the columns function returns the correct values
+    # if two tables exist with the same name but different schema
+    def test_idadb_as_idadataframe_same_tablename(self, idadb, df):
+        ida11 = idadb.as_idadataframe(df, "TEST_AS_IDADF_18729493954_23849590", clear_existing = True)
+        ida21 = idadb.as_idadataframe(df, "DUMMY.TEST_AS_IDADF_18729493954_23849590", clear_existing = True)
+        assert(all(ida11.columns == df.columns))
+        assert(all(ida21.columns == df.columns))
+        ida12 = IdaDataFrame(idadb, "TEST_AS_IDADF_18729493954_23849590")
+        ida22 = IdaDataFrame(idadb, "DUMMY.TEST_AS_IDADF_18729493954_23849590")
+        assert(all(ida12.columns == df.columns))
+        assert(all(ida22.columns == df.columns))
+        idadb.drop_table("TEST_AS_IDADF_18729493954_23849590")
+        idadb.drop_table("DUMMY.TEST_AS_IDADF_18729493954_23849590")
+
+    def test_idadb_ida_query(self, idadb, idadf):
+        query = "SELECT * FROM %s FETCH LIMIT 5"%idadf.name
+        df = idadb.ida_query(query)
+        assert(isinstance(df,pandas.DataFrame))
+        assert(len(idadf.columns) == len(df.columns))
+        assert(len(df) == 5)
+
+    def test_idadb_ida_query_first_row_only(self, idadb, idadf, df):
+        query = "SELECT * FROM %s FETCH LIMIT 5"%idadf.name
+        downloaded_df = idadb.ida_query(query, first_row_only=True)
+        assert(isinstance(downloaded_df,tuple))
+        assert(len(downloaded_df) == len(df.loc[0]))
+
+    def test_idadb_ida_scalar_query(self, idadb, idadf):
+        query = "SELECT * FROM %s FETCH LIMIT 5"%idadf.name
+        downloaded_df = idadb.ida_scalar_query(query)
+        assert(isinstance(downloaded_df,six.string_types)|isinstance(downloaded_df,Number))
+
+
+    ################# MORE TEST TO DO HERE
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_base_private.py` & `nzpyida-0.3.3/nzpyida/tests/test_base_private.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,162 +1,162 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for private methods of IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from builtins import zip
-from builtins import str
-from future import standard_library
-standard_library.install_aliases()
-
-import pytest
-import six
-
-from nzpyida import IdaDataFrame
-from nzpyida.exceptions import IdaDataBaseError
-
-
-class Test_IdaDataBase_PrivateMethods(object):
-
-    def test_idadb_upper_columns(self, idadb, df):
-        df_upper = idadb._upper_columns(df)
-        for column in df_upper.columns:
-            assert(column == column.upper())
-
-    def test_idadb_get_name_and_schema(self, idadb):
-        tup = idadb._get_name_and_schema("SCHEMA.TABLE")
-        assert(tup[0] == "SCHEMA")
-        assert(tup[1] == "TABLE")
-
-    def test_idadb_get_name_and_schema_no_schema(self, idadb):
-        tup = idadb._get_name_and_schema("TABLE")
-        assert(tup[0] == idadb.current_schema)
-        assert(tup[1] == "TABLE")
-
-    def test_idadb_get_valid_tablename_default(self, idadb):
-        tablename = idadb._get_valid_tablename()
-        assert(isinstance(tablename, six.string_types))
-        assert("DATA_FRAME_" in tablename)
-
-    def test_idadb_get_valid_tablename_custom(self, idadb):
-        tablename = idadb._get_valid_tablename("MY_PERSONAL_PREFIX")
-        assert("MY_PERSONAL_PREFIX" in tablename)
-
-    def test_idadb_get_valid_tablename_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb._get_valid_tablename("INCORRECT_PREFIX_ß%$")
-
-    def test_idadb_get_valid_viewname_default(self, idadb):
-        viewname = idadb._get_valid_viewname()
-        assert(isinstance(viewname, six.string_types))
-        assert("VIEW_" in viewname)
-
-    def test_idadb_get_valid_viewname_custom(self, idadb):
-        viewname = idadb._get_valid_viewname("MY_PERSONAL_PREFIX")
-        assert("MY_PERSONAL_PREFIX" in viewname)
-
-    def test_idadb_get_valid_viewname_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb._get_valid_viewname("INCORRECT_PREFIX_ß%$")
-
-    def test_idadb_get_valid_modelname_default(self, idadb):
-        modelname = idadb._get_valid_modelname()
-        assert(isinstance(modelname, six.string_types))
-        assert("MODEL_" in modelname)
-
-    def test_idadb_get_valid_modelname_custom(self, idadb):
-        modelname = idadb._get_valid_modelname("MY_PERSONAL_PREFIX")
-        assert("MY_PERSONAL_PREFIX" in modelname)
-
-    def test_idadb_get_valid_modelname_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb._get_valid_modelname("INCORRECT_PREFIX_ß%$")
-
-    def test_idadb_create_table(self, idadb, df):
-        try : idadb.drop_table("CREATE_TABLE_TEST_585960708904")
-        except : pass
-        idadb._create_table(df, "CREATE_TABLE_TEST_585960708904")
-        ida = IdaDataFrame(idadb, "CREATE_TABLE_TEST_585960708904")
-        assert(all([str(x) == str(y) for x,y in zip(df.columns, ida.columns)]))
-        assert(idadb.exists_table("CREATE_TABLE_TEST_585960708904") == 1)
-        idadb.drop_table("CREATE_TABLE_TEST_585960708904")
-
-    def test_idadb_create_view(self, idadb, df):
-        try : idadb.drop_table("CREATE_VIEW_TEST_585960708904")
-        except: pass
-        try : idadb.drop_view("VIEW_TEST_585960708904")
-        except: pass
-        idadb._create_table(df, "CREATE_VIEW_TEST_585960708904")
-        ida = IdaDataFrame(idadb, "CREATE_VIEW_TEST_585960708904")
-        idadb._create_view(ida, "VIEW_TEST_585960708904")
-        idadb.drop_table("CREATE_VIEW_TEST_585960708904")
-        idadb.drop_view("VIEW_TEST_585960708904")
-
-    # Make test when it fails too
-    def test_idadb_insert_into_database(self, idadb, df):
-        try : idadb.drop_table("INSERT_TEST_585960708904")
-        except: pass
-        idadb._create_table(df, "INSERT_TEST_585960708904")
-        ida = IdaDataFrame(idadb, "INSERT_TEST_585960708904")
-        assert(ida.shape[0] == 0)
-        idadb._insert_into_database(df.loc[[1]], idadb.current_schema, "INSERT_TEST_585960708904")
-        del ida.shape
-        assert(ida.shape[0] == 1)
-        idadb.drop_table("INSERT_TEST_585960708904")
-
-    def test_idadb_check_procedure(self, idadb):
-        # This test work of course only if KMEANS is available
-        bol = idadb._check_procedure('KMEANS')
-        assert(bol is True)
-
-    def test_idadb_check_procedure_fail(self, idadb):
-        with pytest.raises(IdaDataBaseError):
-            idadb._check_procedure('NOTEXISTINGPROCEDURE_495906823812')
-
-    def test_idadb_reset_attributes(self, idadb):
-        idadb.useless_attribute = "test"
-        assert(hasattr(idadb, "useless_attribute"))
-        idadb._reset_attributes("useless_attribute")
-        assert(not hasattr(idadb, "useless_attribute"))
-        idadb.useless_attribute = "test"
-        assert(hasattr(idadb, "useless_attribute"))
-        idadb._reset_attributes(["useless_attribute"])
-        assert(not hasattr(idadb, "useless_attribute"))
-
-    def test_idadb_reset_attributes_not_existing(self, idadb):
-        assert(not hasattr(idadb, "NOTEXISTINGATTRIBUTE_374965038285"))
-        idadb._reset_attributes("NOTEXISTINGATTRIBUTE_374965038285")
-        assert(not hasattr(idadb, "NOTEXISTINGATTRIBUTE_374965038285"))
-
-    def test_idadb_reset_attributes_multiple(self, idadb):
-        idadb.useless_attribute1 = "test"
-        idadb.useless_attribute2 = "test"
-        assert(hasattr(idadb, "useless_attribute1"))
-        assert(hasattr(idadb, "useless_attribute2"))
-        assert(not hasattr(idadb, "useless_attribute3"))
-        idadb._reset_attributes(["useless_attribute1", "useless_attribute2", "useless_attribute3"])
-        # Note : One is actually not existing
-        assert(not hasattr(idadb, "useless_attribute1"))
-        assert(not hasattr(idadb, "useless_attribute2"))
-        assert(not hasattr(idadb, "useless_attribute3"))
-
-    def test_idadb_reset_retrieve_cache(self, idadb):
-        idadb.my_custom_cache = "test"
-        cache = idadb._retrieve_cache("my_custom_cache")
-        assert(cache == idadb.my_custom_cache)
-
-    def test_idadb_reset_retrieve_cache_not_existing(self, idadb):
-        cache = idadb._retrieve_cache("NOTEXISTINGCACHE_43845696794939")
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for private methods of IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from builtins import zip
+from builtins import str
+from future import standard_library
+standard_library.install_aliases()
+
+import pytest
+import six
+
+from nzpyida import IdaDataFrame
+from nzpyida.exceptions import IdaDataBaseError
+
+
+class Test_IdaDataBase_PrivateMethods(object):
+
+    def test_idadb_upper_columns(self, idadb, df):
+        df_upper = idadb._upper_columns(df)
+        for column in df_upper.columns:
+            assert(column == column.upper())
+
+    def test_idadb_get_name_and_schema(self, idadb):
+        tup = idadb._get_name_and_schema("SCHEMA.TABLE")
+        assert(tup[0] == "SCHEMA")
+        assert(tup[1] == "TABLE")
+
+    def test_idadb_get_name_and_schema_no_schema(self, idadb):
+        tup = idadb._get_name_and_schema("TABLE")
+        assert(tup[0] == idadb.current_schema)
+        assert(tup[1] == "TABLE")
+
+    def test_idadb_get_valid_tablename_default(self, idadb):
+        tablename = idadb._get_valid_tablename()
+        assert(isinstance(tablename, six.string_types))
+        assert("DATA_FRAME_" in tablename)
+
+    def test_idadb_get_valid_tablename_custom(self, idadb):
+        tablename = idadb._get_valid_tablename("MY_PERSONAL_PREFIX")
+        assert("MY_PERSONAL_PREFIX" in tablename)
+
+    def test_idadb_get_valid_tablename_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb._get_valid_tablename("INCORRECT_PREFIX_ß%$")
+
+    def test_idadb_get_valid_viewname_default(self, idadb):
+        viewname = idadb._get_valid_viewname()
+        assert(isinstance(viewname, six.string_types))
+        assert("VIEW_" in viewname)
+
+    def test_idadb_get_valid_viewname_custom(self, idadb):
+        viewname = idadb._get_valid_viewname("MY_PERSONAL_PREFIX")
+        assert("MY_PERSONAL_PREFIX" in viewname)
+
+    def test_idadb_get_valid_viewname_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb._get_valid_viewname("INCORRECT_PREFIX_ß%$")
+
+    def test_idadb_get_valid_modelname_default(self, idadb):
+        modelname = idadb._get_valid_modelname()
+        assert(isinstance(modelname, six.string_types))
+        assert("MODEL_" in modelname)
+
+    def test_idadb_get_valid_modelname_custom(self, idadb):
+        modelname = idadb._get_valid_modelname("MY_PERSONAL_PREFIX")
+        assert("MY_PERSONAL_PREFIX" in modelname)
+
+    def test_idadb_get_valid_modelname_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb._get_valid_modelname("INCORRECT_PREFIX_ß%$")
+
+    def test_idadb_create_table(self, idadb, df):
+        try : idadb.drop_table("CREATE_TABLE_TEST_585960708904")
+        except : pass
+        idadb._create_table(df, "CREATE_TABLE_TEST_585960708904")
+        ida = IdaDataFrame(idadb, "CREATE_TABLE_TEST_585960708904")
+        assert(all([str(x) == str(y) for x,y in zip(df.columns, ida.columns)]))
+        assert(idadb.exists_table("CREATE_TABLE_TEST_585960708904") == 1)
+        idadb.drop_table("CREATE_TABLE_TEST_585960708904")
+
+    def test_idadb_create_view(self, idadb, df):
+        try : idadb.drop_table("CREATE_VIEW_TEST_585960708904")
+        except: pass
+        try : idadb.drop_view("VIEW_TEST_585960708904")
+        except: pass
+        idadb._create_table(df, "CREATE_VIEW_TEST_585960708904")
+        ida = IdaDataFrame(idadb, "CREATE_VIEW_TEST_585960708904")
+        idadb._create_view(ida, "VIEW_TEST_585960708904")
+        idadb.drop_table("CREATE_VIEW_TEST_585960708904")
+        idadb.drop_view("VIEW_TEST_585960708904")
+
+    # Make test when it fails too
+    def test_idadb_insert_into_database(self, idadb, df):
+        try : idadb.drop_table("INSERT_TEST_585960708904")
+        except: pass
+        idadb._create_table(df, "INSERT_TEST_585960708904")
+        ida = IdaDataFrame(idadb, "INSERT_TEST_585960708904")
+        assert(ida.shape[0] == 0)
+        idadb._insert_into_database(df.loc[[1]], idadb.current_schema, "INSERT_TEST_585960708904")
+        del ida.shape
+        assert(ida.shape[0] == 1)
+        idadb.drop_table("INSERT_TEST_585960708904")
+
+    def test_idadb_check_procedure(self, idadb):
+        # This test work of course only if KMEANS is available
+        bol = idadb._check_procedure('KMEANS')
+        assert(bol is True)
+
+    def test_idadb_check_procedure_fail(self, idadb):
+        with pytest.raises(IdaDataBaseError):
+            idadb._check_procedure('NOTEXISTINGPROCEDURE_495906823812')
+
+    def test_idadb_reset_attributes(self, idadb):
+        idadb.useless_attribute = "test"
+        assert(hasattr(idadb, "useless_attribute"))
+        idadb._reset_attributes("useless_attribute")
+        assert(not hasattr(idadb, "useless_attribute"))
+        idadb.useless_attribute = "test"
+        assert(hasattr(idadb, "useless_attribute"))
+        idadb._reset_attributes(["useless_attribute"])
+        assert(not hasattr(idadb, "useless_attribute"))
+
+    def test_idadb_reset_attributes_not_existing(self, idadb):
+        assert(not hasattr(idadb, "NOTEXISTINGATTRIBUTE_374965038285"))
+        idadb._reset_attributes("NOTEXISTINGATTRIBUTE_374965038285")
+        assert(not hasattr(idadb, "NOTEXISTINGATTRIBUTE_374965038285"))
+
+    def test_idadb_reset_attributes_multiple(self, idadb):
+        idadb.useless_attribute1 = "test"
+        idadb.useless_attribute2 = "test"
+        assert(hasattr(idadb, "useless_attribute1"))
+        assert(hasattr(idadb, "useless_attribute2"))
+        assert(not hasattr(idadb, "useless_attribute3"))
+        idadb._reset_attributes(["useless_attribute1", "useless_attribute2", "useless_attribute3"])
+        # Note : One is actually not existing
+        assert(not hasattr(idadb, "useless_attribute1"))
+        assert(not hasattr(idadb, "useless_attribute2"))
+        assert(not hasattr(idadb, "useless_attribute3"))
+
+    def test_idadb_reset_retrieve_cache(self, idadb):
+        idadb.my_custom_cache = "test"
+        cache = idadb._retrieve_cache("my_custom_cache")
+        assert(cache == idadb.my_custom_cache)
+
+    def test_idadb_reset_retrieve_cache_not_existing(self, idadb):
+        cache = idadb._retrieve_cache("NOTEXISTINGCACHE_43845696794939")
         assert(cache is None)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_base_table_manipulation.py` & `nzpyida-0.3.3/nzpyida/tests/test_base_table_manipulation.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,220 +1,220 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-import pytest
-
-from nzpyida import IdaDataFrame
-from nzpyida.learn import KMeans
-
-class Test_DeleteDataBaseObjects(object):
-
-    def test_idadb_drop_table(self, idadb, idadf_tmp):
-        assert(idadb.exists_table(idadf_tmp.name) == 1)
-        idadb.drop_table(idadf_tmp.name)
-        assert(idadb.exists_table(idadf_tmp.name) == 0)
-
-    def test_idadb_drop_table_value_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb.drop_table("NOTEXISTINGOBJECT_496070383095079384063739509")
-
-    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-    def test_idadb_drop_table_type_error(self, idadb, idaview):
-        with pytest.raises(TypeError):
-            idadb.drop_table(idaview.name) # this is a view
-
-    def test_idadb_drop_view(self, idadb, idaview_tmp):
-        assert(idadb.exists_view(idaview_tmp.name) == 1)
-        idadb.drop_view(idaview_tmp.name)
-        assert(idadb.exists_view(idaview_tmp.name) == 0)
-
-    def test_idadb_drop_view_value_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb.drop_view("NOTEXISTINGOBJECT_496070383095079384063739509")
-
-    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-    def test_idadb_drop_view_type_error(self, idadb, idadf):
-        with pytest.raises(TypeError):
-            idadb.drop_view(idadf.name) # this is a table
-
-    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-    @pytest.mark.xfail(raises=ValueError)
-    def test_idadb_drop_model_positive(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, destructive = True)
-        # Create a simple KMEANS model
-        kmeans = KMeans(n_clusters = 3)
-        kmeans.fit(idadf_tmp)
-        assert(idadb.is_model(kmeans.modelname) == 1)
-        idadb.drop_model(kmeans.modelname)
-        idadb.commit()
-
-    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-    def test_idadb_drop_model_value_error(self, idadb):
-        with pytest.raises(ValueError):
-            idadb.is_model("NOTEXISTINGOBJECT_496070383095079384063739509")
-
-    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-    def test_idadb_drop_model_type_error(self, idadb, idadf, idaview):
-        with pytest.raises(TypeError):
-            idadb.drop_model(idadf.name)
-        with pytest.raises(TypeError):
-            idadb.drop_model(idaview.name)
-        #with pytest.raises(TypeError):
-        #    idadb.drop_model("ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE")
-
-class Test_TableManipulation(object):
-    def test_idadb_rename(self, idadb, idadf_tmp):
-        try:
-            idadb.drop_table("TEST_RENAMED")
-        except:
-            pass
-        original_name = idadf_tmp.tablename
-        idadb.rename(idadf_tmp, "TEST_RENAMED")
-        idadb.commit()
-        assert(idadf_tmp.name == "TEST_RENAMED")
-        assert(idadb.exists_table("TEST_RENAMED") == 1)
-        assert(idadb.exists_table(original_name) == 0)
-        idadb.rename(idadf_tmp, original_name)
-
-    def test_idadb_rename_value_error(self, idadb, idadf):
-        with pytest.raises(ValueError):
-            idadb.rename(idadf, "T569ß4359**4\4%")
-
-    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc')")
-    def test_idadb_rename_type_error(self, idadb, idaview):
-        with pytest.raises(TypeError):
-            idadb.rename(idaview, "TEST_VIEW_RENAME")
-
-    def test_idadb_rename_name_error(self, idadb, idadf_tmp):
-        with pytest.raises(NameError):
-            idadb.rename(idadf_tmp, idadf_tmp.tablename)
-
-    def test_idadb_add_column_id_destructive(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, "ID", destructive = True)
-        assert("ID" in idadf_tmp.columns)
-        assert("ID" == idadf_tmp.indexer)
-        assert("ID" in idadf_tmp._get_all_columns_in_table())
-        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name, indexer = "ID")
-        assert("ID" in idadf_tmp_new.columns)
-        assert("ID" == idadf_tmp_new.indexer)
-        assert("ID" in idadf_tmp_new._get_all_columns_in_table())
-
-    def test_idadb_add_column_id_non_destructive(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, "ID", destructive = False)
-        assert("ID" in idadf_tmp.columns)
-        assert("ID" == idadf_tmp.indexer)
-        assert("ID" not in idadf_tmp._get_all_columns_in_table())
-        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
-        assert("ID" not in idadf_tmp_new.columns)
-        assert("ID" not in idadf_tmp_new._get_all_columns_in_table())
-
-    def test_idadb_add_column_id_non_destructive_custom_name(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, "MY_CUSTOM_ID", destructive = False)
-        assert("MY_CUSTOM_ID" in idadf_tmp.columns)
-        assert("MY_CUSTOM_ID" == idadf_tmp.indexer)
-        assert("MY_CUSTOM_ID" not in idadf_tmp._get_all_columns_in_table())
-        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
-        assert("MY_CUSTOM_ID" not in idadf_tmp_new.columns)
-        assert("MY_CUSTOM_ID" not in idadf_tmp_new._get_all_columns_in_table())
-
-    def test_idadb_add_column_id_non_destructive_value_error(self, idadb, idadf_tmp):
-        with pytest.raises(ValueError):
-            idadb.add_column_id(idadf_tmp, idadf_tmp.columns[0], destructive = False)
-
-    def test_idadb_add_column_id_non_destructive_type_error(self, idadb, df):
-        with pytest.raises(TypeError):
-            idadb.add_column_id(df, "ID", destructive = False)
-
-    def test_idadb_delete_column_value_error(self, idadb, idadf):
-        with pytest.raises(ValueError):
-            idadb.delete_column(idadf, "NOTEXISTING_COLUMN_309599439")
-
-    def test_idadb_delete_column_type_error(self, idadb, idadf):
-        with pytest.raises(TypeError):
-            idadb.delete_column(idadf, idadf)
-
-    def test_idadb_delete_column_destructive(self, idadb, idadf_tmp):
-        to_delete = idadf_tmp.columns[0]
-        idadb.delete_column(idadf_tmp, to_delete, destructive = True)
-        assert(to_delete not in idadf_tmp.columns)
-        assert(to_delete not in idadf_tmp._get_all_columns_in_table())
-        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
-        assert(to_delete not in idadf_tmp_new.columns)
-        assert(to_delete not in idadf_tmp_new._get_all_columns_in_table())
-
-    def test_idadb_delete_column_destructive_real_indexer(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, "ID", destructive = True)
-        idadb.delete_column(idadf_tmp, "ID", destructive = True)
-        assert("ID" not in idadf_tmp.columns)
-        assert("ID" != idadf_tmp.indexer)
-        assert("ID" not in idadf_tmp._get_all_columns_in_table())
-        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
-        assert("ID" not in idadf_tmp_new.columns)
-        assert("ID" not in idadf_tmp_new._get_all_columns_in_table())
-
-    def test_idadb_delete_column_destructive_virtual_indexer(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, "ID", destructive = False)
-        idadb.delete_column(idadf_tmp, "ID", destructive = True)
-        assert("ID" not in idadf_tmp.columns)
-        assert("ID" != idadf_tmp.indexer)
-        assert("ID" not in idadf_tmp._get_all_columns_in_table())
-        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
-        assert("ID" not in idadf_tmp_new.columns)
-        assert("ID" not in idadf_tmp_new._get_all_columns_in_table())
-
-    def test_idadb_delete_column_non_destructive(self, idadb, idadf_tmp):
-        to_delete = idadf_tmp.columns[0]
-        idadb.delete_column(idadf_tmp, to_delete, destructive = False)
-        assert(to_delete not in idadf_tmp.columns)
-        assert(to_delete in idadf_tmp._get_all_columns_in_table())
-        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
-        assert(to_delete in idadf_tmp_new.columns)
-        assert(to_delete in idadf_tmp_new._get_all_columns_in_table())
-
-    def test_idadb_delete_column_non_destructive_real_indexer(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, "ID", destructive = True)
-        idadb.delete_column(idadf_tmp, "ID", destructive = False)
-        assert("ID" not in idadf_tmp.columns)
-        assert("ID" != idadf_tmp.indexer)
-        assert("ID" in idadf_tmp._get_all_columns_in_table())
-        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name, indexer = "ID")
-        assert("ID" in idadf_tmp_new.columns)
-        assert("ID" == idadf_tmp_new.indexer)
-        assert("ID" in idadf_tmp_new._get_all_columns_in_table())
-
-    def test_idadb_delete_column_non_destructive_virtual_indexer(self, idadb, idadf_tmp):
-        idadb.add_column_id(idadf_tmp, "ID", destructive = False)
-        idadb.delete_column(idadf_tmp, "ID", destructive = False)
-        assert("ID" not in idadf_tmp.columns)
-        assert("ID" != idadf_tmp.indexer)
-        assert("ID" not in idadf_tmp._get_all_columns_in_table())
-        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
-        assert("ID" not in idadf_tmp_new.columns)
-        assert("ID" not in idadf_tmp_new._get_all_columns_in_table())
-
-    def test_idadb_append(self, idadb, idadf_tmp, df):
-        nrow = idadf_tmp.shape[0]
-        ncol = idadf_tmp.shape[1]
-        idadb.append(idadf_tmp, df)
-        assert(idadf_tmp.shape[0] == nrow*2)
-        nrow = idadf_tmp.shape[0]
-        idadb.append(idadf_tmp, df.loc[0:4])
-        assert(idadf_tmp.shape[0] == nrow + 5)
-        assert(idadf_tmp.shape[1] == ncol)
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+import pytest
+
+from nzpyida import IdaDataFrame
+from nzpyida.learn import KMeans
+
+class Test_DeleteDataBaseObjects(object):
+
+    def test_idadb_drop_table(self, idadb, idadf_tmp):
+        assert(idadb.exists_table(idadf_tmp.name) == 1)
+        idadb.drop_table(idadf_tmp.name)
+        assert(idadb.exists_table(idadf_tmp.name) == 0)
+
+    def test_idadb_drop_table_value_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb.drop_table("NOTEXISTINGOBJECT_496070383095079384063739509")
+
+    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+    def test_idadb_drop_table_type_error(self, idadb, idaview):
+        with pytest.raises(TypeError):
+            idadb.drop_table(idaview.name) # this is a view
+
+    def test_idadb_drop_view(self, idadb, idaview_tmp):
+        assert(idadb.exists_view(idaview_tmp.name) == 1)
+        idadb.drop_view(idaview_tmp.name)
+        assert(idadb.exists_view(idaview_tmp.name) == 0)
+
+    def test_idadb_drop_view_value_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb.drop_view("NOTEXISTINGOBJECT_496070383095079384063739509")
+
+    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+    def test_idadb_drop_view_type_error(self, idadb, idadf):
+        with pytest.raises(TypeError):
+            idadb.drop_view(idadf.name) # this is a table
+
+    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+    @pytest.mark.xfail(raises=ValueError)
+    def test_idadb_drop_model_positive(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, destructive = True)
+        # Create a simple KMEANS model
+        kmeans = KMeans(n_clusters = 3)
+        kmeans.fit(idadf_tmp)
+        assert(idadb.is_model(kmeans.modelname) == 1)
+        idadb.drop_model(kmeans.modelname)
+        idadb.commit()
+
+    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+    def test_idadb_drop_model_value_error(self, idadb):
+        with pytest.raises(ValueError):
+            idadb.is_model("NOTEXISTINGOBJECT_496070383095079384063739509")
+
+    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+    def test_idadb_drop_model_type_error(self, idadb, idadf, idaview):
+        with pytest.raises(TypeError):
+            idadb.drop_model(idadf.name)
+        with pytest.raises(TypeError):
+            idadb.drop_model(idaview.name)
+        #with pytest.raises(TypeError):
+        #    idadb.drop_model("ST_INFORMTN_SCHEMA.ST_UNITS_OF_MEASURE")
+
+class Test_TableManipulation(object):
+    def test_idadb_rename(self, idadb, idadf_tmp):
+        try:
+            idadb.drop_table("TEST_RENAMED")
+        except:
+            pass
+        original_name = idadf_tmp.tablename
+        idadb.rename(idadf_tmp, "TEST_RENAMED")
+        idadb.commit()
+        assert(idadf_tmp.name == "TEST_RENAMED")
+        assert(idadb.exists_table("TEST_RENAMED") == 1)
+        assert(idadb.exists_table(original_name) == 0)
+        idadb.rename(idadf_tmp, original_name)
+
+    def test_idadb_rename_value_error(self, idadb, idadf):
+        with pytest.raises(ValueError):
+            idadb.rename(idadf, "T569ß4359**4\4%")
+
+    @pytest.mark.skipif("'netezza' in config.getvalue('jdbc')")
+    def test_idadb_rename_type_error(self, idadb, idaview):
+        with pytest.raises(TypeError):
+            idadb.rename(idaview, "TEST_VIEW_RENAME")
+
+    def test_idadb_rename_name_error(self, idadb, idadf_tmp):
+        with pytest.raises(NameError):
+            idadb.rename(idadf_tmp, idadf_tmp.tablename)
+
+    def test_idadb_add_column_id_destructive(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, "ID", destructive = True)
+        assert("ID" in idadf_tmp.columns)
+        assert("ID" == idadf_tmp.indexer)
+        assert("ID" in idadf_tmp._get_all_columns_in_table())
+        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name, indexer = "ID")
+        assert("ID" in idadf_tmp_new.columns)
+        assert("ID" == idadf_tmp_new.indexer)
+        assert("ID" in idadf_tmp_new._get_all_columns_in_table())
+
+    def test_idadb_add_column_id_non_destructive(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, "ID", destructive = False)
+        assert("ID" in idadf_tmp.columns)
+        assert("ID" == idadf_tmp.indexer)
+        assert("ID" not in idadf_tmp._get_all_columns_in_table())
+        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
+        assert("ID" not in idadf_tmp_new.columns)
+        assert("ID" not in idadf_tmp_new._get_all_columns_in_table())
+
+    def test_idadb_add_column_id_non_destructive_custom_name(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, "MY_CUSTOM_ID", destructive = False)
+        assert("MY_CUSTOM_ID" in idadf_tmp.columns)
+        assert("MY_CUSTOM_ID" == idadf_tmp.indexer)
+        assert("MY_CUSTOM_ID" not in idadf_tmp._get_all_columns_in_table())
+        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
+        assert("MY_CUSTOM_ID" not in idadf_tmp_new.columns)
+        assert("MY_CUSTOM_ID" not in idadf_tmp_new._get_all_columns_in_table())
+
+    def test_idadb_add_column_id_non_destructive_value_error(self, idadb, idadf_tmp):
+        with pytest.raises(ValueError):
+            idadb.add_column_id(idadf_tmp, idadf_tmp.columns[0], destructive = False)
+
+    def test_idadb_add_column_id_non_destructive_type_error(self, idadb, df):
+        with pytest.raises(TypeError):
+            idadb.add_column_id(df, "ID", destructive = False)
+
+    def test_idadb_delete_column_value_error(self, idadb, idadf):
+        with pytest.raises(ValueError):
+            idadb.delete_column(idadf, "NOTEXISTING_COLUMN_309599439")
+
+    def test_idadb_delete_column_type_error(self, idadb, idadf):
+        with pytest.raises(TypeError):
+            idadb.delete_column(idadf, idadf)
+
+    def test_idadb_delete_column_destructive(self, idadb, idadf_tmp):
+        to_delete = idadf_tmp.columns[0]
+        idadb.delete_column(idadf_tmp, to_delete, destructive = True)
+        assert(to_delete not in idadf_tmp.columns)
+        assert(to_delete not in idadf_tmp._get_all_columns_in_table())
+        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
+        assert(to_delete not in idadf_tmp_new.columns)
+        assert(to_delete not in idadf_tmp_new._get_all_columns_in_table())
+
+    def test_idadb_delete_column_destructive_real_indexer(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, "ID", destructive = True)
+        idadb.delete_column(idadf_tmp, "ID", destructive = True)
+        assert("ID" not in idadf_tmp.columns)
+        assert("ID" != idadf_tmp.indexer)
+        assert("ID" not in idadf_tmp._get_all_columns_in_table())
+        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
+        assert("ID" not in idadf_tmp_new.columns)
+        assert("ID" not in idadf_tmp_new._get_all_columns_in_table())
+
+    def test_idadb_delete_column_destructive_virtual_indexer(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, "ID", destructive = False)
+        idadb.delete_column(idadf_tmp, "ID", destructive = True)
+        assert("ID" not in idadf_tmp.columns)
+        assert("ID" != idadf_tmp.indexer)
+        assert("ID" not in idadf_tmp._get_all_columns_in_table())
+        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
+        assert("ID" not in idadf_tmp_new.columns)
+        assert("ID" not in idadf_tmp_new._get_all_columns_in_table())
+
+    def test_idadb_delete_column_non_destructive(self, idadb, idadf_tmp):
+        to_delete = idadf_tmp.columns[0]
+        idadb.delete_column(idadf_tmp, to_delete, destructive = False)
+        assert(to_delete not in idadf_tmp.columns)
+        assert(to_delete in idadf_tmp._get_all_columns_in_table())
+        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
+        assert(to_delete in idadf_tmp_new.columns)
+        assert(to_delete in idadf_tmp_new._get_all_columns_in_table())
+
+    def test_idadb_delete_column_non_destructive_real_indexer(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, "ID", destructive = True)
+        idadb.delete_column(idadf_tmp, "ID", destructive = False)
+        assert("ID" not in idadf_tmp.columns)
+        assert("ID" != idadf_tmp.indexer)
+        assert("ID" in idadf_tmp._get_all_columns_in_table())
+        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name, indexer = "ID")
+        assert("ID" in idadf_tmp_new.columns)
+        assert("ID" == idadf_tmp_new.indexer)
+        assert("ID" in idadf_tmp_new._get_all_columns_in_table())
+
+    def test_idadb_delete_column_non_destructive_virtual_indexer(self, idadb, idadf_tmp):
+        idadb.add_column_id(idadf_tmp, "ID", destructive = False)
+        idadb.delete_column(idadf_tmp, "ID", destructive = False)
+        assert("ID" not in idadf_tmp.columns)
+        assert("ID" != idadf_tmp.indexer)
+        assert("ID" not in idadf_tmp._get_all_columns_in_table())
+        idadf_tmp_new = IdaDataFrame(idadb, idadf_tmp._name)
+        assert("ID" not in idadf_tmp_new.columns)
+        assert("ID" not in idadf_tmp_new._get_all_columns_in_table())
+
+    def test_idadb_append(self, idadb, idadf_tmp, df):
+        nrow = idadf_tmp.shape[0]
+        ncol = idadf_tmp.shape[1]
+        idadb.append(idadf_tmp, df)
+        assert(idadf_tmp.shape[0] == nrow*2)
+        nrow = idadf_tmp.shape[0]
+        idadb.append(idadf_tmp, df.loc[0:4])
+        assert(idadf_tmp.shape[0] == nrow + 5)
+        assert(idadf_tmp.shape[1] == ncol)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_feature_selection.py` & `nzpyida-0.3.3/nzpyida/tests/test_feature_selection.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,478 +1,480 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from builtins import round
-from builtins import zip
-from future import standard_library
-standard_library.install_aliases()
-
-import pandas
-import pytest
-
-from nzpyida.feature_selection import pearson
-from nzpyida.feature_selection import spearman
-from nzpyida.feature_selection import ttest
-from nzpyida.feature_selection import chisquared
-from nzpyida.feature_selection import gini, gini_pairwise
-from nzpyida.feature_selection import entropy
-from nzpyida.feature_selection import info_gain, gain_ratio, su
-
-# Test symmetry
-
-class Test_PearsonCorrelation(object):
-
-    def test_pearson_default(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = pearson(idadf, features = columns)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == len(columns))
-            assert(len(result.index) == len(columns))
-            result2 = pearson(idadf)
-            assert(all(result == result2))
-            result = result.fillna(0) # np.nan values are not equal when compared
-            assert(all(result == result.T)) # symmetry
-                
-    def test_pearson_valueError(self, idadf):
-        if len(idadf.columns) > 0:
-            with pytest.raises(ValueError):
-                pearson(idadf, features = idadf.columns[0])
-            with pytest.raises(ValueError):
-                pearson(idadf, target= idadf.columns[0], features = idadf.columns[0])
-                
-    def test_pearson_TypeError(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] != "NUMERIC"].index)
-        if len(columns) > 1:
-            with pytest.raises(TypeError):
-                pearson(idadf, features = columns)
-                
-    def test_pearson_one_target(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = pearson(idadf, target = columns[0])
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == (len(columns)-1))
-            
-    def test_pearson_multiple_target(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = pearson(idadf, target = [columns[0],columns[1]])
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == 2)
-            assert(len(result.index) == len(columns))
-            
-    def test_pearson_one_target_one_feature(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = pearson(idadf, target = columns[0], features=[columns[1]])
-            assert(isinstance(result, float))
-            result2 = pearson(idadf, target = columns[1], features=[columns[0]])
-            assert(round(result,3) == round(result2,3)) # symmetry
-    
-class Test_SpearmanRankCorrelation(object):
-
-    def test_spearman_default(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = spearman(idadf, features = columns)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == len(columns))
-            assert(len(result.index) == len(columns))
-            result2 = spearman(idadf)
-            assert(all(result == result2))
-            result = result.fillna(0) # np.nan values are not equal when compared
-            assert(all(result == result.T)) # symmetry
-                
-    def test_spearman_valueError(self, idadf):
-        if len(idadf.columns) > 0:
-            with pytest.raises(ValueError):
-                spearman(idadf, features = idadf.columns[0])
-            with pytest.raises(ValueError):
-                spearman(idadf, target= idadf.columns[0], features = idadf.columns[0])
-                
-    def test_spearman_TypeError(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] != "NUMERIC"].index)
-        if len(columns) > 1:
-            with pytest.raises(TypeError):
-                spearman(idadf, features = columns)
-                
-    def test_spearman_one_target(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = spearman(idadf, target = columns[0])
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == (len(columns)-1))
-            
-    def test_spearman_multiple_target(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = spearman(idadf, target = [columns[0],columns[1]])
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == 2)
-            assert(len(result.index) == len(columns))
-            
-    def test_spearman_one_target_one_feature(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = spearman(idadf, target = columns[0], features=[columns[1]])
-            assert(isinstance(result, float))
-            result2 = spearman(idadf, target = columns[1], features=[columns[0]])
-            assert(round(result,3) == round(result2,3)) # symmetry
-    
-class Test_Tstatistics(object):
-
-    def test_ttest_default(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = ttest(idadf, features = columns)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == len(columns))
-            assert(len(result.index) == len(columns))
-            result = result.fillna(0) # np.nan values are not equal when compared
-            result = result[result.index]
-            assert(all(result != result.T)) # asymmetry
-            
-    def test_ttest_valueError(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] != "NUMERIC"].index)
-        if len(columns) > 0: # Raise no numerical features
-            with pytest.raises(ValueError):
-                ttest(idadf, features = columns)
-        if len(idadf.columns) > 0: 
-            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
-                ttest(idadf, features = idadf.columns[0])
-            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
-                ttest(idadf, target= idadf.columns[0], features = idadf.columns[0])
-                
-    def test_ttest_TypeError(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] != "NUMERIC"].index)
-        if len(columns) > 1:
-            with pytest.raises(TypeError):
-                ttest(idadf, features = columns)
-                
-    def test_ttest_one_target(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = ttest(idadf, target = columns[0])
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == (len(columns)-1))
-            
-    def test_ttest_multiple_target(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = ttest(idadf, target = [columns[0],columns[1]])
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == 2)
-            assert(len(result.index) == len(columns))
-            
-    def test_ttest_one_target_one_feature(self, idadf):
-        data = idadf._table_def() 
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        if len(columns) > 1:
-            result = ttest(idadf, target = columns[0], features=[columns[1]])
-            assert(isinstance(result, float))
-            
-    
-class Test_Chisquared(object):
-    
-    def test_chisquared_default(self, idadf):
-        if len(idadf.columns) > 1:
-            result = chisquared(idadf, features = idadf.columns)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == len(idadf.columns))
-            assert(len(result.index) == len(idadf.columns))
-            result2 = chisquared(idadf)
-            assert(all(result == result2))
-            result = result.fillna(0) # np.nan values are not equal when compared
-            assert(all(result == result.T)) # symmetry
-            
-    def test_chisquared_valueError(self, idadf):
-        if len(idadf.columns) > 0: 
-            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
-                chisquared(idadf, features = idadf.columns[0])
-            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
-                chisquared(idadf, target= idadf.columns[0], features = idadf.columns[0])
-                
-    def test_chisquared_one_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = chisquared(idadf, target = idadf.columns[0])
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == (len(idadf.columns)-1))
-            
-    def test_chisquared_multiple_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = chisquared(idadf, target = [idadf.columns[0],idadf.columns[1]])
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == 2)
-            assert(len(result.index) == len(idadf.columns))
-            
-    def test_chisquared_one_target_one_feature(self, idadf):
-        if len(idadf.columns) > 1:
-            result = chisquared(idadf, target = idadf.columns[0], features=[idadf.columns[1]])
-            assert(isinstance(result, float))
-            result2 = chisquared(idadf, target = idadf.columns[1], features=[idadf.columns[0]])
-            assert(round(result,3) == round(result2,3)) # symmetry
-    
-class Test_Gini_Index(object):
-
-    def test_gini_default(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gini(idadf, features = idadf.columns)
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result.index) == len(idadf.columns))
-            result2 = gini(idadf)
-            assert(all(result == result2))
-            
-    def test_gini_multiple_columns(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gini(idadf, features = [idadf.columns[0],idadf.columns[1]])
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == 2)
-            
-    def test_gini_one_column(self, idadf):
-        if len(idadf.columns) >= 1:
-            result = gini(idadf, features = idadf.columns[0])
-            assert(isinstance(result, float))
-            
-class Test_Gini_Pairwise(object):
-
-    def test_gini_pairwise_default(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gini_pairwise(idadf, features = idadf.columns)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == len(idadf.columns))
-            assert(len(result.index) == len(idadf.columns))
-            result2 = gini_pairwise(idadf)
-            assert(all(result == result2)) 
-            result = result.fillna(0) # np.nan values are not equal when compared
-            assert(any(result != result.T)) # asymmetry
-            
-    def test_gini_pairwise_valueError(self, idadf):
-        if len(idadf.columns) > 0: 
-            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
-                gini_pairwise(idadf, features = idadf.columns[0])
-            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
-                gini_pairwise(idadf, target= idadf.columns[0], features = idadf.columns[0])
-                
-    def test_gini_pairwise_one_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gini_pairwise(idadf, target = idadf.columns[0])
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == (len(idadf.columns)-1))
-            
-    def test_gini_pairwise_multiple_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gini_pairwise(idadf, target = [idadf.columns[0],idadf.columns[1]])
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == 2)
-            assert(len(result.index) == len(idadf.columns))
-            
-    def test_gini_pairwise_one_target_one_feature(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gini_pairwise(idadf, target = idadf.columns[0], features=[idadf.columns[1]])
-            assert(isinstance(result, float))
-    
-class Test_Entropy(object):
-
-    def test_entropy_default(self, idadf):
-        if len(idadf.columns) > 1:
-            result = entropy(idadf)
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result.index) == len(idadf.columns))
-            
-    def test_entropy_multiple_columns(self, idadf):
-        if len(idadf.columns) > 1:
-            result = entropy(idadf, target = [idadf.columns[0],idadf.columns[1]])
-            assert(isinstance(result, float))
-            
-    def test_entropy_one_column(self, idadf):
-        if len(idadf.columns) >= 1:
-            result = entropy(idadf, target = idadf.columns[0])
-            assert(isinstance(result, float))
-    
-class Test_Information_Gain(object):
-
-    def test_info_gain_default(self, idadf):
-        if len(idadf.columns) > 1:
-            result = info_gain(idadf, features = idadf.columns)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == len(idadf.columns))
-            assert(len(result.index) == len(idadf.columns))
-            result2 = info_gain(idadf)
-            assert(all(result == result2))
-            result = result.fillna(0) # np.nan values are not equal when compared
-            assert(all(result == result.T)) # symmetry
-            
-    def test_info_gain_valueError(self, idadf):
-        if len(idadf.columns) > 0: 
-            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
-                info_gain(idadf, features = idadf.columns[0])
-            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
-                info_gain(idadf, target= idadf.columns[0], features = idadf.columns[0])
-                
-    def test_info_gain_one_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = info_gain(idadf, target = idadf.columns[0])
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == (len(idadf.columns)-1))
-            
-    def test_info_gain_multiple_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = info_gain(idadf, target = [idadf.columns[0],idadf.columns[1]])
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == 2)
-            assert(len(result.index) == len(idadf.columns))
-            
-    def test_info_gain_one_target_one_feature(self, idadf):
-        if len(idadf.columns) > 1:
-            result = info_gain(idadf, target = idadf.columns[0], features=[idadf.columns[1]])
-            assert(isinstance(result, float))
-            result2 = info_gain(idadf, target = idadf.columns[1], features=[idadf.columns[0]])
-            assert(round(result,3) == round(result2,3)) # symmetry
-    
-class Test_Symmetric_Gain_Ratio(object):
-
-    def test_sym_gain_ratio_default(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gain_ratio(idadf, features = idadf.columns, symmetry=True)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == len(idadf.columns))
-            assert(len(result.index) == len(idadf.columns))
-            result2 = gain_ratio(idadf, symmetry=True)
-            assert(all(result == result2))
-            result = result.fillna(0) # np.nan values are not equal when compared
-            assert(all(result == result.T)) # symmetry
-            
-    def test_sym_gain_ratio_valueError(self, idadf):
-        if len(idadf.columns) > 0: 
-            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
-                gain_ratio(idadf, features = idadf.columns[0], symmetry=True)
-            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
-                gain_ratio(idadf, target= idadf.columns[0], features = idadf.columns[0], symmetry=True)
-                
-    def test_sym_gain_ratio_one_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gain_ratio(idadf, target = idadf.columns[0], symmetry=True)
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == (len(idadf.columns)-1))
-            
-    def test_sym_gain_ratio_multiple_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gain_ratio(idadf, target = [idadf.columns[0],idadf.columns[1]], symmetry=True)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == 2)
-            assert(len(result.index) == len(idadf.columns))
-            
-    def test_sym_gain_ratio_one_target_one_feature(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gain_ratio(idadf, target = idadf.columns[0], features=[idadf.columns[1]], symmetry=True)
-            assert(isinstance(result, float))
-            result2 = gain_ratio(idadf, target = idadf.columns[1], features=[idadf.columns[0]], symmetry=True)
-            assert(round(result,3) == round(result2,3)) # symmetry
-    
-class Test_Asymmetric_Gain_Ratio(object):
-
-    def test_asym_gain_ratio_default(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gain_ratio(idadf, features = idadf.columns, symmetry=False)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == len(idadf.columns))
-            assert(len(result.index) == len(idadf.columns))
-            result2 = gain_ratio(idadf, symmetry=False)
-            assert(all(result == result2))
-            result = result.fillna(0) # np.nan values are not equal when compared
-            assert(any(result != result.T)) # asymmetry
-            
-    def test_asym_gain_ratio_valueError(self, idadf):
-        if len(idadf.columns) > 0: 
-            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
-                gain_ratio(idadf, features = idadf.columns[0], symmetry=False)
-            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
-                gain_ratio(idadf, target= idadf.columns[0], features = idadf.columns[0], symmetry=False)
-                
-    def test_asym_gain_ratio_one_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gain_ratio(idadf, target = idadf.columns[0], symmetry=False)
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == (len(idadf.columns)-1))
-            
-    def test_asym_gain_ratio_multiple_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gain_ratio(idadf, target = [idadf.columns[0],idadf.columns[1]], symmetry=False)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == 2)
-            assert(len(result.index) == len(idadf.columns))
-            
-    def test_asym_gain_ratio_one_target_one_feature(self, idadf):
-        if len(idadf.columns) > 1:
-            result = gain_ratio(idadf, target = idadf.columns[0], features=[idadf.columns[1]], symmetry=False)
-            assert(isinstance(result, float))
-            
-class Test_Symmetric_Uncertainty(object):
-
-    def test_su_default(self, idadf):
-        if len(idadf.columns) > 1:
-            result = su(idadf, features = idadf.columns)
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == len(idadf.columns))
-            assert(len(result.index) == len(idadf.columns))
-            result2 = su(idadf)
-            assert(all(result == result2))
-            result = result.fillna(0) # np.nan values are not equal when compared
-            assert(all(result == result.T)) # symmetry
-            
-    def test_su_valueError(self, idadf):
-        if len(idadf.columns) > 0: 
-            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
-                su(idadf, features = idadf.columns[0])
-            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
-                su(idadf, target= idadf.columns[0], features = idadf.columns[0])
-                
-    def test_su_one_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = su(idadf, target = idadf.columns[0])
-            assert(isinstance(result, pandas.core.series.Series))
-            assert(len(result) == (len(idadf.columns)-1))
-            
-    def test_su_multiple_target(self, idadf):
-        if len(idadf.columns) > 1:
-            result = su(idadf, target = [idadf.columns[0],idadf.columns[1]])
-            assert(isinstance(result, pandas.core.frame.DataFrame))
-            assert(len(result.columns) == 2)
-            assert(len(result.index) == len(idadf.columns))
-            
-    def test_su_one_target_one_feature(self, idadf):
-        if len(idadf.columns) > 1:
-            result = su(idadf, target = idadf.columns[0], features=[idadf.columns[1]])
-            assert(isinstance(result, float))
-            result2 = su(idadf, target = idadf.columns[1], features=[idadf.columns[0]])
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from builtins import round
+from builtins import zip
+from future import standard_library
+standard_library.install_aliases()
+
+import pandas
+import pytest
+
+from nzpyida.feature_selection import pearson
+from nzpyida.feature_selection import spearman
+from nzpyida.feature_selection import ttest
+from nzpyida.feature_selection import chisquared
+from nzpyida.feature_selection import gini, gini_pairwise
+from nzpyida.feature_selection import entropy
+from nzpyida.feature_selection import info_gain, gain_ratio, su
+
+# Test symmetry
+
+@pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+class Test_PearsonCorrelation(object):
+
+    def test_pearson_default(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = pearson(idadf, features = columns)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == len(columns))
+            assert(len(result.index) == len(columns))
+            result2 = pearson(idadf)
+            assert(all(result == result2))
+            result = result.fillna(0) # np.nan values are not equal when compared
+            assert(all(result == result.T)) # symmetry
+                
+    def test_pearson_valueError(self, idadf):
+        if len(idadf.columns) > 0:
+            with pytest.raises(ValueError):
+                pearson(idadf, features = idadf.columns[0])
+            with pytest.raises(ValueError):
+                pearson(idadf, target= idadf.columns[0], features = idadf.columns[0])
+                
+    def test_pearson_TypeError(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] != "NUMERIC"].index)
+        if len(columns) > 1:
+            with pytest.raises(TypeError):
+                pearson(idadf, features = columns)
+                
+    def test_pearson_one_target(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = pearson(idadf, target = columns[0])
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == (len(columns)-1))
+            
+    def test_pearson_multiple_target(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = pearson(idadf, target = [columns[0],columns[1]])
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == 2)
+            assert(len(result.index) == len(columns))
+            
+    def test_pearson_one_target_one_feature(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = pearson(idadf, target = columns[0], features=[columns[1]])
+            assert(isinstance(result, float))
+            result2 = pearson(idadf, target = columns[1], features=[columns[0]])
+            assert(round(result,3) == round(result2,3)) # symmetry
+
+@pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")  
+class Test_SpearmanRankCorrelation(object):
+
+    def test_spearman_default(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = spearman(idadf, features = columns)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == len(columns))
+            assert(len(result.index) == len(columns))
+            result2 = spearman(idadf)
+            assert(all(result == result2))
+            result = result.fillna(0) # np.nan values are not equal when compared
+            assert(all(result == result.T)) # symmetry
+                
+    def test_spearman_valueError(self, idadf):
+        if len(idadf.columns) > 0:
+            with pytest.raises(ValueError):
+                spearman(idadf, features = idadf.columns[0])
+            with pytest.raises(ValueError):
+                spearman(idadf, target= idadf.columns[0], features = idadf.columns[0])
+                
+    def test_spearman_TypeError(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] != "NUMERIC"].index)
+        if len(columns) > 1:
+            with pytest.raises(TypeError):
+                spearman(idadf, features = columns)
+                
+    def test_spearman_one_target(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = spearman(idadf, target = columns[0])
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == (len(columns)-1))
+            
+    def test_spearman_multiple_target(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = spearman(idadf, target = [columns[0],columns[1]])
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == 2)
+            assert(len(result.index) == len(columns))
+            
+    def test_spearman_one_target_one_feature(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = spearman(idadf, target = columns[0], features=[columns[1]])
+            assert(isinstance(result, float))
+            result2 = spearman(idadf, target = columns[1], features=[columns[0]])
+            assert(round(result,3) == round(result2,3)) # symmetry
+    
+class Test_Tstatistics(object):
+
+    def test_ttest_default(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = ttest(idadf, features = columns)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == len(columns))
+            assert(len(result.index) == len(columns))
+            result = result.fillna(0) # np.nan values are not equal when compared
+            result = result[result.index]
+            assert(all(result != result.T)) # asymmetry
+            
+    def test_ttest_valueError(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] != "NUMERIC"].index)
+        if len(columns) > 0: # Raise no numerical features
+            with pytest.raises(ValueError):
+                ttest(idadf, features = columns)
+        if len(idadf.columns) > 0: 
+            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
+                ttest(idadf, features = idadf.columns[0])
+            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
+                ttest(idadf, target= idadf.columns[0], features = idadf.columns[0])
+                
+    def test_ttest_TypeError(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] != "NUMERIC"].index)
+        if len(columns) > 1:
+            with pytest.raises(TypeError):
+                ttest(idadf, features = columns)
+                
+    def test_ttest_one_target(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = ttest(idadf, target = columns[0])
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == (len(columns)-1))
+            
+    def test_ttest_multiple_target(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = ttest(idadf, target = [columns[0],columns[1]])
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == 2)
+            assert(len(result.index) == len(columns))
+            
+    def test_ttest_one_target_one_feature(self, idadf):
+        data = idadf._table_def() 
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        if len(columns) > 1:
+            result = ttest(idadf, target = columns[0], features=[columns[1]])
+            assert(isinstance(result, float))
+            
+    
+class Test_Chisquared(object):
+    
+    def test_chisquared_default(self, idadf):
+        if len(idadf.columns) > 1:
+            result = chisquared(idadf, features = idadf.columns)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == len(idadf.columns))
+            assert(len(result.index) == len(idadf.columns))
+            result2 = chisquared(idadf)
+            assert(all(result == result2))
+            result = result.fillna(0) # np.nan values are not equal when compared
+            assert(all(result == result.T)) # symmetry
+            
+    def test_chisquared_valueError(self, idadf):
+        if len(idadf.columns) > 0: 
+            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
+                chisquared(idadf, features = idadf.columns[0])
+            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
+                chisquared(idadf, target= idadf.columns[0], features = idadf.columns[0])
+                
+    def test_chisquared_one_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = chisquared(idadf, target = idadf.columns[0])
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == (len(idadf.columns)-1))
+            
+    def test_chisquared_multiple_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = chisquared(idadf, target = [idadf.columns[0],idadf.columns[1]])
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == 2)
+            assert(len(result.index) == len(idadf.columns))
+            
+    def test_chisquared_one_target_one_feature(self, idadf):
+        if len(idadf.columns) > 1:
+            result = chisquared(idadf, target = idadf.columns[0], features=[idadf.columns[1]])
+            assert(isinstance(result, float))
+            result2 = chisquared(idadf, target = idadf.columns[1], features=[idadf.columns[0]])
+            assert(round(result,3) == round(result2,3)) # symmetry
+    
+class Test_Gini_Index(object):
+
+    def test_gini_default(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gini(idadf, features = idadf.columns)
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result.index) == len(idadf.columns))
+            result2 = gini(idadf)
+            assert(all(result == result2))
+            
+    def test_gini_multiple_columns(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gini(idadf, features = [idadf.columns[0],idadf.columns[1]])
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == 2)
+            
+    def test_gini_one_column(self, idadf):
+        if len(idadf.columns) >= 1:
+            result = gini(idadf, features = idadf.columns[0])
+            assert(isinstance(result, float))
+            
+class Test_Gini_Pairwise(object):
+
+    def test_gini_pairwise_default(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gini_pairwise(idadf, features = idadf.columns)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == len(idadf.columns))
+            assert(len(result.index) == len(idadf.columns))
+            result2 = gini_pairwise(idadf)
+            assert(all(result == result2)) 
+            result = result.fillna(0) # np.nan values are not equal when compared
+            assert(any(result != result.T)) # asymmetry
+            
+    def test_gini_pairwise_valueError(self, idadf):
+        if len(idadf.columns) > 0: 
+            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
+                gini_pairwise(idadf, features = idadf.columns[0])
+            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
+                gini_pairwise(idadf, target= idadf.columns[0], features = idadf.columns[0])
+                
+    def test_gini_pairwise_one_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gini_pairwise(idadf, target = idadf.columns[0])
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == (len(idadf.columns)-1))
+            
+    def test_gini_pairwise_multiple_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gini_pairwise(idadf, target = [idadf.columns[0],idadf.columns[1]])
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == 2)
+            assert(len(result.index) == len(idadf.columns))
+            
+    def test_gini_pairwise_one_target_one_feature(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gini_pairwise(idadf, target = idadf.columns[0], features=[idadf.columns[1]])
+            assert(isinstance(result, float))
+    
+class Test_Entropy(object):
+
+    def test_entropy_default(self, idadf):
+        if len(idadf.columns) > 1:
+            result = entropy(idadf)
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result.index) == len(idadf.columns))
+            
+    def test_entropy_multiple_columns(self, idadf):
+        if len(idadf.columns) > 1:
+            result = entropy(idadf, target = [idadf.columns[0],idadf.columns[1]])
+            assert(isinstance(result, float))
+            
+    def test_entropy_one_column(self, idadf):
+        if len(idadf.columns) >= 1:
+            result = entropy(idadf, target = idadf.columns[0])
+            assert(isinstance(result, float))
+    
+class Test_Information_Gain(object):
+
+    def test_info_gain_default(self, idadf):
+        if len(idadf.columns) > 1:
+            result = info_gain(idadf, features = idadf.columns)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == len(idadf.columns))
+            assert(len(result.index) == len(idadf.columns))
+            result2 = info_gain(idadf)
+            assert(all(result == result2))
+            result = result.fillna(0) # np.nan values are not equal when compared
+            assert(all(result == result.T)) # symmetry
+            
+    def test_info_gain_valueError(self, idadf):
+        if len(idadf.columns) > 0: 
+            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
+                info_gain(idadf, features = idadf.columns[0])
+            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
+                info_gain(idadf, target= idadf.columns[0], features = idadf.columns[0])
+                
+    def test_info_gain_one_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = info_gain(idadf, target = idadf.columns[0])
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == (len(idadf.columns)-1))
+            
+    def test_info_gain_multiple_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = info_gain(idadf, target = [idadf.columns[0],idadf.columns[1]])
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == 2)
+            assert(len(result.index) == len(idadf.columns))
+            
+    def test_info_gain_one_target_one_feature(self, idadf):
+        if len(idadf.columns) > 1:
+            result = info_gain(idadf, target = idadf.columns[0], features=[idadf.columns[1]])
+            assert(isinstance(result, float))
+            result2 = info_gain(idadf, target = idadf.columns[1], features=[idadf.columns[0]])
+            assert(round(result,3) == round(result2,3)) # symmetry
+    
+class Test_Symmetric_Gain_Ratio(object):
+
+    def test_sym_gain_ratio_default(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gain_ratio(idadf, features = idadf.columns, symmetry=True)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == len(idadf.columns))
+            assert(len(result.index) == len(idadf.columns))
+            result2 = gain_ratio(idadf, symmetry=True)
+            assert(all(result == result2))
+            result = result.fillna(0) # np.nan values are not equal when compared
+            assert(all(result == result.T)) # symmetry
+            
+    def test_sym_gain_ratio_valueError(self, idadf):
+        if len(idadf.columns) > 0: 
+            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
+                gain_ratio(idadf, features = idadf.columns[0], symmetry=True)
+            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
+                gain_ratio(idadf, target= idadf.columns[0], features = idadf.columns[0], symmetry=True)
+                
+    def test_sym_gain_ratio_one_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gain_ratio(idadf, target = idadf.columns[0], symmetry=True)
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == (len(idadf.columns)-1))
+            
+    def test_sym_gain_ratio_multiple_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gain_ratio(idadf, target = [idadf.columns[0],idadf.columns[1]], symmetry=True)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == 2)
+            assert(len(result.index) == len(idadf.columns))
+            
+    def test_sym_gain_ratio_one_target_one_feature(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gain_ratio(idadf, target = idadf.columns[0], features=[idadf.columns[1]], symmetry=True)
+            assert(isinstance(result, float))
+            result2 = gain_ratio(idadf, target = idadf.columns[1], features=[idadf.columns[0]], symmetry=True)
+            assert(round(result,3) == round(result2,3)) # symmetry
+    
+class Test_Asymmetric_Gain_Ratio(object):
+
+    def test_asym_gain_ratio_default(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gain_ratio(idadf, features = idadf.columns, symmetry=False)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == len(idadf.columns))
+            assert(len(result.index) == len(idadf.columns))
+            result2 = gain_ratio(idadf, symmetry=False)
+            assert(all(result == result2))
+            result = result.fillna(0) # np.nan values are not equal when compared
+            assert(any(result != result.T)) # asymmetry
+            
+    def test_asym_gain_ratio_valueError(self, idadf):
+        if len(idadf.columns) > 0: 
+            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
+                gain_ratio(idadf, features = idadf.columns[0], symmetry=False)
+            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
+                gain_ratio(idadf, target= idadf.columns[0], features = idadf.columns[0], symmetry=False)
+                
+    def test_asym_gain_ratio_one_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gain_ratio(idadf, target = idadf.columns[0], symmetry=False)
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == (len(idadf.columns)-1))
+            
+    def test_asym_gain_ratio_multiple_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gain_ratio(idadf, target = [idadf.columns[0],idadf.columns[1]], symmetry=False)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == 2)
+            assert(len(result.index) == len(idadf.columns))
+            
+    def test_asym_gain_ratio_one_target_one_feature(self, idadf):
+        if len(idadf.columns) > 1:
+            result = gain_ratio(idadf, target = idadf.columns[0], features=[idadf.columns[1]], symmetry=False)
+            assert(isinstance(result, float))
+            
+class Test_Symmetric_Uncertainty(object):
+
+    def test_su_default(self, idadf):
+        if len(idadf.columns) > 1:
+            result = su(idadf, features = idadf.columns)
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == len(idadf.columns))
+            assert(len(result.index) == len(idadf.columns))
+            result2 = su(idadf)
+            assert(all(result == result2))
+            result = result.fillna(0) # np.nan values are not equal when compared
+            assert(all(result == result.T)) # symmetry
+            
+    def test_su_valueError(self, idadf):
+        if len(idadf.columns) > 0: 
+            with pytest.raises(ValueError): # Cannot compute correlation coefficients of only one column (...), need at least 2
+                su(idadf, features = idadf.columns[0])
+            with pytest.raises(ValueError): # The correlation value of two same columns is always maximal
+                su(idadf, target= idadf.columns[0], features = idadf.columns[0])
+                
+    def test_su_one_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = su(idadf, target = idadf.columns[0])
+            assert(isinstance(result, pandas.core.series.Series))
+            assert(len(result) == (len(idadf.columns)-1))
+            
+    def test_su_multiple_target(self, idadf):
+        if len(idadf.columns) > 1:
+            result = su(idadf, target = [idadf.columns[0],idadf.columns[1]])
+            assert(isinstance(result, pandas.core.frame.DataFrame))
+            assert(len(result.columns) == 2)
+            assert(len(result.index) == len(idadf.columns))
+            
+    def test_su_one_target_one_feature(self, idadf):
+        if len(idadf.columns) > 1:
+            result = su(idadf, target = idadf.columns[0], features=[idadf.columns[1]])
+            assert(isinstance(result, float))
+            result2 = su(idadf, target = idadf.columns[1], features=[idadf.columns[0]])
             assert(round(result,3) == round(result2,3)) # symmetry
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_frame.py` & `nzpyida-0.3.3/nzpyida/tests/test_frame.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,439 +1,439 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from builtins import zip
-from future import standard_library
-standard_library.install_aliases()
-
-import pandas
-import pytest
-import six
-
-import nzpyida
-from nzpyida import IdaDataBase
-
-
-class Test_OpenDataFrameObject(object):
-
-    def test_idadf_attr_idadb(self, idadf):
-        assert isinstance(idadf._idadb, IdaDataBase)
-
-    def test_idadf_attr_name(self, idadf, df):
-        assert isinstance(idadf.name, six.string_types)
-        assert idadf.name == idadf.schema + "." + "TEST_IBMDBPY"
-        assert idadf.name == idadf.schema + "." + idadf.tablename
-
-    def test_idadf_attr_schema(self, idadf):
-        assert isinstance(idadf.schema, six.string_types)
-
-    def test_idadf_attr_indexer(self, idadf):
-        assert (isinstance(idadf.indexer, six.string_types)|(idadf.indexer is None))
-        # TODO : Check more deeply the indexer
-
-    def test_idadf_attr_loc(self, idadf):
-        assert isinstance(idadf.loc, nzpyida.indexing.Loc)
-
-    def test_idadf_attr_internalstate(self, idadf):
-        assert isinstance(idadf.internal_state, nzpyida.internals.InternalState)
-
-    def test_idadf_attr_type(self, idadf):
-        assert isinstance(idadf.type, six.string_types)
-        assert idadf.type == "Table"
-
-    def test_idadf_atrr_dtypes(self, idadf, df):
-        assert isinstance(idadf.dtypes, pandas.core.frame.DataFrame)
-        assert len(idadf.dtypes) == len(idadf.columns)
-        assert len(idadf.dtypes) == len(df.columns)
-
-    def test_idadf_attr_index(self, idadf, df):
-        # Ok, but what do we do if too big ?
-        assert type(idadf.index) in [pandas.Int64Index, pandas.Index, pandas.RangeIndex]  # Not sure here
-        assert list(idadf.index) == list(df.index)
-
-    def test_idadf_attr_columns(self, idadf, df):
-        assert isinstance(idadf.columns, pandas.core.index.Index)
-        assert idadf.columns.equals(df.columns)
-
-    def test_idadf_attr_axes(self, idadf):
-        assert isinstance(idadf.axes, list)
-        assert len(idadf.axes) == 2
-        assert idadf.axes[1].equals(idadf.columns)
-        assert list(idadf.axes[0]) == list(idadf.index)
-
-    def test_idadf_attr_shape(self, idadf, df):
-        assert isinstance(idadf.shape, tuple)
-        assert len(idadf.shape) == 2
-        assert idadf.shape[0] == len(idadf.index)
-        assert idadf.shape[1] == len(idadf.columns)
-        assert idadf.shape == df.shape
-
-    def test_idadf_empty(self, idadb, df):
-        idadb._create_table(df, "TEST_EMPTY_3496593727406047264076")
-        to_test = nzpyida.IdaDataFrame(idadb, "TEST_EMPTY_3496593727406047264076")
-        assert(to_test.empty is True)
-        idadb.drop_table("TEST_EMPTY_3496593727406047264076")
-
-    def test_idadf_len(self, idadf, df):
-        assert(len(idadf) == len(df))
-
-    def test_idadf_iter(self, idadf, df):
-        for idacol, col in zip(idadf, df):
-            assert(idacol == col)
-
-
-class Test_IdaDataFrameBehavior(object):
-    def test_idadf_getitem_1_col_idadf(self, idadf):
-        if len(idadf.columns) >= 1:
-            newidadf = idadf[[idadf.columns[0]]]
-            assert(isinstance(newidadf, nzpyida.IdaDataFrame) is True)
-            assert(len(newidadf.columns) == 1)
-            assert(idadf.columns[0] == newidadf.columns[0])
-
-            # We don't check of it is actually the corresponding column
-
-            newidadf = idadf[[idadf.columns[-1]]]
-            assert(isinstance(newidadf, nzpyida.IdaDataFrame) is True)
-            assert(len(newidadf.columns) == 1)
-            assert(idadf.columns[-1] == newidadf.columns[0])
-
-    def test_idadf_getitem_1_col_idadf_keyerror(self, idadf):
-        with pytest.raises(KeyError):
-            idadf[["NOTEXISTING_COLUMN_455849820205"]]
-
-    def test_idadf_getitem_2_cols_idadf(self, idadf):
-        if len(idadf.columns) >= 2:
-            newidadf = idadf[[idadf.columns[0], idadf.columns[-1]]]
-            assert(isinstance(newidadf, nzpyida.IdaDataFrame) is True)
-            assert(len(newidadf.columns) == 2)
-            assert(idadf.columns[0] == newidadf.columns[0])
-            assert(idadf.columns[-1] == newidadf.columns[-1])
-
-
-    def test_idadf_getitem_2_cols_idadf_keyerror(self, idadf):
-        with pytest.raises(KeyError):
-            idadf[[idadf.columns[0], "NOTEXISTING_COLUMN_455849820205"]]
-
-    # TODO : FIX If you select twice the same columns, only one with be taken into account
-    # (This is because they are referenced in a dictionary, maybe force modifying the name of the columns)
-
-    def test_idadf_getitem_all_cols_idadf(self, idadf):
-        if len(idadf.columns) >= 1:
-            newidadf = idadf[list(idadf.columns)]
-            assert(isinstance(newidadf, nzpyida.IdaDataFrame) is True)
-            assert(len(newidadf.columns) == len(idadf.columns))
-            assert(newidadf.shape == idadf.shape)
-
-    def test_idadf_getitem_idaseries(self, idadf):
-        if len(idadf.columns) >= 1:
-            newidaseries = idadf[idadf.columns[0]]
-            assert(isinstance(newidaseries, nzpyida.IdaSeries))
-            assert(len(newidaseries.columns) == 1)
-            assert(idadf.columns[0] == newidaseries.columns[0])
-
-            newidaseries = idadf[idadf.columns[-1]]
-            assert(isinstance(newidaseries, nzpyida.IdaDataFrame))
-            assert(len(newidaseries.columns) == 1)
-            assert(idadf.columns[-1] == newidaseries.columns[0])
-
-    def test_idadf_getitem_idaseries_keyerror(self, idadf):
-        with pytest.raises(KeyError):
-            idadf["NOTEXISTING_COLUMN_455849820205"]
-
-    def test_idadf_getitem_idaseries_keyerror_several_columns(self, idadf):
-        if len(idadf.columns) >= 2:
-            with pytest.raises(KeyError):
-                idadf[idadf.columns[0], idadf.columns[1]]
-
-    def test_idadf_getitem_slice(self, idadb, idadf, idadf_tmp):
-        if len(idadf) > 10:
-            newidadf = idadf[0:9]
-            assert(len(newidadf) == 10)
-
-        if len(idadf_tmp) > 10:
-            idadb.add_column_id(idadf_tmp, destructive = True)
-            newidadf_1 = idadf_tmp[0:9]
-            newidadf_2 = idadf_tmp[0:9]
-            assert(all(newidadf_1.head(10) == newidadf_2.head(10)))
-
-    def test_idaseries_getitem_slice(self, idadb, idadf, idadf_tmp):
-        # Set them as series first and do the same test as above
-        if len(idadf.columns) >= 1:
-            idadf = idadf[idadf.columns[0]]
-            idadf_tmp = idadf_tmp[idadf_tmp.columns[0]]
-            assert(isinstance(idadf, nzpyida.IdaDataFrame))
-            assert(isinstance(idadf_tmp, nzpyida.IdaSeries))
-
-            if len(idadf) > 10:
-                newidadf = idadf[0:9]
-                assert(len(newidadf) == 10)
-
-    def test_idadf_setitem(self, idadf):
-        pass
-
-    def test_idadf_delitem(self, idadf):
-        pass
-
-    def test_idadf_filter_lt(self, idadf):
-        pass
-
-    def test_idadf_filter_le(self, idadf):
-        pass
-
-    def test_idadf_filter_eq(self, idadf):
-        pass
-
-    def test_idadf_filter_ne(self, idadf):
-        pass
-
-    def test_idadf_filter_ge(self, idadf):
-        pass
-
-    def test_idadf_filter_gt(self, idadf):
-        pass
-
-    def test_idadf_feature_add(self, idadf):
-        pass
-
-    def test_idadf_feature_radd(self, idadf):
-        pass
-
-    def test_idadf_feature_div(self, idadf):
-        pass
-
-    def test_idadf_feature_rdiv(self, idadf):
-        pass
-
-    def test_idadf_feature_floordiv(self, idadf):
-        pass
-
-    def test_idadf_feature_rfloordiv(self, idadf):
-        pass
-
-    def test_idadf_feature_mod(self, idadf):
-        pass
-
-    def test_idadf_feature_rmod(self, idadf):
-        pass
-
-    def test_idadf_feature_mul(self, idadf):
-        pass
-
-    def test_idadf_feature_rmul(self, idadf):
-        pass
-
-    def test_idadf_feature_neg(self, idadf):
-        pass
-
-    def test_idadf_feature_rpos(self, idadf):
-        pass
-
-    def test_idadf_feature_pow(self, idadf):
-        pass
-
-    def test_idadf_feature_rpow(self, idadf):
-        pass
-
-    def test_idadf_feature_sub(self, idadf):
-        pass
-
-    def test_idadf_feature_rsub(self, idadf):
-        pass
-
-class Test_DataBaseFeatures(object):
-
-    def test_idadf_exists(self, idadf):
-        assert(idadf.exists() is True)
-        pass
-
-    def test_idadf_is_view(self, idadf):
-        assert(idadf.is_view() is False)
-        pass
-
-    def test_idadf_is_table(self, idadf):
-        assert(idadf.exists() is True)
-        pass
-
-    def test_idadf_get_primary_key(self, idadf):
-        pass
-
-    def test_idadf_ida_query(self, idadf):
-        pass
-
-    def test_idadf_ida_scalar_query(self, idadf):
-        pass
-
-
-class Test_DataExploration(object):
-    ### head
-    # For head and tail we do not test if the rows match because
-    # the order is not guaranteed anyway
-    def test_idadf_head_default(self, idadb, idadf, df):
-        sortkey = idadf.columns[0]
-        if idadf._get_numerical_columns():
-            sortkey = idadf._get_numerical_columns()[0]
-
-        ida_head = idadf.head()
-        assert isinstance(ida_head, pandas.core.frame.DataFrame)
-        assert len(ida_head) == 5
-        df_head = df.sort_values(sortkey).head()
-        assert (ida_head[sortkey].tolist() == df_head[sortkey].tolist())
-
-    def test_idadf_head_10(self, idadb, idadf, df):
-        ida_head = idadf.head(10)
-        assert isinstance(ida_head, pandas.core.frame.DataFrame)
-        assert len(ida_head) == 10
-
-    def test_idadf_head_10_sort(self, idadb, idadf, df):
-        ida_head = idadf.head(10, sort=False)
-        assert isinstance(ida_head, pandas.core.frame.DataFrame)
-        assert len(ida_head) == 10
-
-    def test_idadf_head_with_indexer(self, idadb, idadf_indexer, df):
-        ida_head = idadf_indexer.head()
-        sortby = len(df.columns)-1
-        df_head = df.sort_values(df.columns[sortby]).head()
-        assert isinstance(ida_head, pandas.core.frame.DataFrame)
-        assert len(ida_head) == 5
-        assert(ida_head[idadf_indexer.columns[sortby]].tolist() ==
-                       df_head[df.columns[sortby]].tolist())
-
-    def test_idadf_head_projected_3col(self, idadf, df):
-        if len(idadf.columns) >= 4:
-            columns = idadf.columns[1:4].tolist()
-            newidadf = idadf[columns]
-
-            sortkey = newidadf.columns[0]
-            if newidadf._get_numerical_columns():
-                sortkey = newidadf._get_numerical_columns()[0]
-
-            ida_head = newidadf.head()
-
-            df_sorted = df.sort_values(sortkey)
-            df_head = df_sorted[columns].head()
-            ida_sortkeys = ida_head[sortkey].tolist()
-            df_sortkeys = df_head[sortkey].tolist()
-            assert isinstance(ida_head, pandas.core.frame.DataFrame)
-            assert len(ida_head) == 5
-            # assert(ida_head[sortkey].tolist() == df_head[sortkey].tolist())
-            assert([ round(x, 10) for x in  ida_sortkeys] == [ round(x, 10) for x in df_sortkeys ])
-
-    def test_idadf_head_sorted(self, idadf, df):
-        sortIdx = len(df.columns) - 1
-        sortkey = idadf.columns[sortIdx]
-        newidadf = idadf.sort(sortkey)
-        ida_head = newidadf.head()
-
-        df_head = df.sort_values(sortkey).head()
-
-        assert(" ORDER BY " in newidadf.internal_state.get_state())
-        assert isinstance(ida_head, pandas.core.frame.DataFrame)
-        assert len(ida_head) == 5
-        assert(ida_head[sortkey].tolist() == df_head[sortkey].tolist())
-
-    def test_idadf_head_0(self, idadf):
-        with pytest.raises(ValueError):
-            idadf.head(0)
-
-    def test_idadf_head_negative(self, idadf):
-        with pytest.raises(ValueError):
-            idadf.head(-1)
-
-    ### tail
-    def test_idadf_tail_default(self, idadb, idadf, df):
-        sortkey = idadf.columns[0]
-        if idadf._get_numerical_columns():
-            sortkey = idadf._get_numerical_columns()[0]
-        ida_tail = idadf.tail()
-        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
-        assert len(ida_tail) == 5
-        df_tail = df.sort_values(sortkey).tail()
-        assert (ida_tail[sortkey].tolist() == df_tail[sortkey].tolist())
-
-    def test_idadf_tail_10(self, idadb, idadf, df):
-        ida_tail = idadf.tail(10)
-        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
-        assert len(ida_tail) == 10
-
-    def test_idadf_tail_10_sort(self, idadb, idadf, df):
-        ida_tail = idadf.tail(10, sort=False)
-        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
-        assert len(ida_tail) == 10
-
-    def test_idadf_tail_with_indexer(self, idadb, idadf_indexer, df):
-        ida_tail = idadf_indexer.tail()
-        sortby = len(df.columns)-1
-        df_head = df.sort_values(df.columns[sortby]).tail()
-        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
-        assert len(ida_tail) == 5
-        assert(ida_tail[idadf_indexer.columns[sortby]].tolist() ==
-                       df_head[df.columns[sortby]].tolist())
-
-    def test_idadf_tail_projected_3col(self, idadf, df):
-        if len(idadf.columns) >= 4:
-            columns = idadf.columns[1:4].tolist()
-            newidadf = idadf[columns]
-
-            sortkey = newidadf.columns[0]
-            if newidadf._get_numerical_columns():
-                sortkey = newidadf._get_numerical_columns()[0]
-
-            ida_tail = newidadf.tail()
-
-            df_sorted = df.sort_values(sortkey)
-            df_tail = df_sorted[columns].tail()
-
-            ida_sortkeys = ida_tail[sortkey].tolist()
-            df_sortkeys = df_tail[sortkey].tolist()
-
-            assert isinstance(ida_tail, pandas.core.frame.DataFrame)
-            assert len(ida_tail) == 5
-            # assert(ida_tail[sortkey].tolist() == df_tail[sortkey].tolist())
-            assert([ round(x, 10) for x in  ida_sortkeys] == [ round(x, 10) for x in df_sortkeys ])
-
-    @pytest.mark.skip(reason="tail on sorted dataframe fails in general, needs fixing first")
-    def test_idadf_tail_sorted(self, idadf, df):
-        sortIdx = len(df.columns) - 1
-        sortkey = idadf.columns[sortIdx]
-        newidadf = idadf.sort(sortkey)
-        ida_tail = newidadf.tail()
-
-        df_tail = df.sort_values(sortkey).tail()
-
-        assert(" ORDER BY " in newidadf.internal_state.get_state())
-        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
-        assert len(ida_tail) == 5
-        assert(ida_tail[sortkey].tolist() == df_tail[sortkey].tolist())
-
-
-    def test_idadf_tail_0(self, idadf):
-        with pytest.raises(ValueError):
-            idadf.tail(0)
-
-    def test_idadf_tail_negative(self, idadf):
-        with pytest.raises(ValueError):
-            idadf.tail(-1)
-
-    def test_idadf_pivot_table(self, idadf):
-        pass
-
-    def test_idadf_sort(self, idadf):
-        pass
-
-# no test
-#__enter__
-#__exit__
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from builtins import zip
+from future import standard_library
+standard_library.install_aliases()
+
+import pandas
+import pytest
+import six
+
+import nzpyida
+from nzpyida import IdaDataBase
+
+
+class Test_OpenDataFrameObject(object):
+
+    def test_idadf_attr_idadb(self, idadf):
+        assert isinstance(idadf._idadb, IdaDataBase)
+
+    def test_idadf_attr_name(self, idadf, df):
+        assert isinstance(idadf.name, six.string_types)
+        assert idadf.name == idadf.schema + "." + "TEST_IBMDBPY"
+        assert idadf.name == idadf.schema + "." + idadf.tablename
+
+    def test_idadf_attr_schema(self, idadf):
+        assert isinstance(idadf.schema, six.string_types)
+
+    def test_idadf_attr_indexer(self, idadf):
+        assert (isinstance(idadf.indexer, six.string_types)|(idadf.indexer is None))
+        # TODO : Check more deeply the indexer
+
+    def test_idadf_attr_loc(self, idadf):
+        assert isinstance(idadf.loc, nzpyida.indexing.Loc)
+
+    def test_idadf_attr_internalstate(self, idadf):
+        assert isinstance(idadf.internal_state, nzpyida.internals.InternalState)
+
+    def test_idadf_attr_type(self, idadf):
+        assert isinstance(idadf.type, six.string_types)
+        assert idadf.type == "Table"
+
+    def test_idadf_atrr_dtypes(self, idadf, df):
+        assert isinstance(idadf.dtypes, pandas.core.frame.DataFrame)
+        assert len(idadf.dtypes) == len(idadf.columns)
+        assert len(idadf.dtypes) == len(df.columns)
+
+    def test_idadf_attr_index(self, idadf, df):
+        # Ok, but what do we do if too big ?
+        assert type(idadf.index) in [pandas.Index, pandas.RangeIndex]  # Not sure here
+        assert list(idadf.index) == list(df.index)
+
+    def test_idadf_attr_columns(self, idadf, df):
+        assert isinstance(idadf.columns, pandas.Index)
+        assert idadf.columns.equals(df.columns)
+
+    def test_idadf_attr_axes(self, idadf):
+        assert isinstance(idadf.axes, list)
+        assert len(idadf.axes) == 2
+        assert idadf.axes[1].equals(idadf.columns)
+        assert list(idadf.axes[0]) == list(idadf.index)
+
+    def test_idadf_attr_shape(self, idadf, df):
+        assert isinstance(idadf.shape, tuple)
+        assert len(idadf.shape) == 2
+        assert idadf.shape[0] == len(idadf.index)
+        assert idadf.shape[1] == len(idadf.columns)
+        assert idadf.shape == df.shape
+
+    def test_idadf_empty(self, idadb, df):
+        idadb._create_table(df, "TEST_EMPTY_3496593727406047264076")
+        to_test = nzpyida.IdaDataFrame(idadb, "TEST_EMPTY_3496593727406047264076")
+        assert(to_test.empty is True)
+        idadb.drop_table("TEST_EMPTY_3496593727406047264076")
+
+    def test_idadf_len(self, idadf, df):
+        assert(len(idadf) == len(df))
+
+    def test_idadf_iter(self, idadf, df):
+        for idacol, col in zip(idadf, df):
+            assert(idacol == col)
+
+
+class Test_IdaDataFrameBehavior(object):
+    def test_idadf_getitem_1_col_idadf(self, idadf):
+        if len(idadf.columns) >= 1:
+            newidadf = idadf[[idadf.columns[0]]]
+            assert(isinstance(newidadf, nzpyida.IdaDataFrame) is True)
+            assert(len(newidadf.columns) == 1)
+            assert(idadf.columns[0] == newidadf.columns[0])
+
+            # We don't check of it is actually the corresponding column
+
+            newidadf = idadf[[idadf.columns[-1]]]
+            assert(isinstance(newidadf, nzpyida.IdaDataFrame) is True)
+            assert(len(newidadf.columns) == 1)
+            assert(idadf.columns[-1] == newidadf.columns[0])
+
+    def test_idadf_getitem_1_col_idadf_keyerror(self, idadf):
+        with pytest.raises(KeyError):
+            idadf[["NOTEXISTING_COLUMN_455849820205"]]
+
+    def test_idadf_getitem_2_cols_idadf(self, idadf):
+        if len(idadf.columns) >= 2:
+            newidadf = idadf[[idadf.columns[0], idadf.columns[-1]]]
+            assert(isinstance(newidadf, nzpyida.IdaDataFrame) is True)
+            assert(len(newidadf.columns) == 2)
+            assert(idadf.columns[0] == newidadf.columns[0])
+            assert(idadf.columns[-1] == newidadf.columns[-1])
+
+
+    def test_idadf_getitem_2_cols_idadf_keyerror(self, idadf):
+        with pytest.raises(KeyError):
+            idadf[[idadf.columns[0], "NOTEXISTING_COLUMN_455849820205"]]
+
+    # TODO : FIX If you select twice the same columns, only one with be taken into account
+    # (This is because they are referenced in a dictionary, maybe force modifying the name of the columns)
+
+    def test_idadf_getitem_all_cols_idadf(self, idadf):
+        if len(idadf.columns) >= 1:
+            newidadf = idadf[list(idadf.columns)]
+            assert(isinstance(newidadf, nzpyida.IdaDataFrame) is True)
+            assert(len(newidadf.columns) == len(idadf.columns))
+            assert(newidadf.shape == idadf.shape)
+
+    def test_idadf_getitem_idaseries(self, idadf):
+        if len(idadf.columns) >= 1:
+            newidaseries = idadf[idadf.columns[0]]
+            assert(isinstance(newidaseries, nzpyida.IdaSeries))
+            assert(len(newidaseries.columns) == 1)
+            assert(idadf.columns[0] == newidaseries.columns[0])
+
+            newidaseries = idadf[idadf.columns[-1]]
+            assert(isinstance(newidaseries, nzpyida.IdaDataFrame))
+            assert(len(newidaseries.columns) == 1)
+            assert(idadf.columns[-1] == newidaseries.columns[0])
+
+    def test_idadf_getitem_idaseries_keyerror(self, idadf):
+        with pytest.raises(KeyError):
+            idadf["NOTEXISTING_COLUMN_455849820205"]
+
+    def test_idadf_getitem_idaseries_keyerror_several_columns(self, idadf):
+        if len(idadf.columns) >= 2:
+            with pytest.raises(KeyError):
+                idadf[idadf.columns[0], idadf.columns[1]]
+
+    def test_idadf_getitem_slice(self, idadb, idadf, idadf_tmp):
+        if len(idadf) > 10:
+            newidadf = idadf[0:9]
+            assert(len(newidadf) == 10)
+
+        if len(idadf_tmp) > 10:
+            idadb.add_column_id(idadf_tmp, destructive = True)
+            newidadf_1 = idadf_tmp[0:9]
+            newidadf_2 = idadf_tmp[0:9]
+            assert(all(newidadf_1.head(10) == newidadf_2.head(10)))
+
+    def test_idaseries_getitem_slice(self, idadb, idadf, idadf_tmp):
+        # Set them as series first and do the same test as above
+        if len(idadf.columns) >= 1:
+            idadf = idadf[idadf.columns[0]]
+            idadf_tmp = idadf_tmp[idadf_tmp.columns[0]]
+            assert(isinstance(idadf, nzpyida.IdaDataFrame))
+            assert(isinstance(idadf_tmp, nzpyida.IdaSeries))
+
+            if len(idadf) > 10:
+                newidadf = idadf[0:9]
+                assert(len(newidadf) == 10)
+
+    def test_idadf_setitem(self, idadf):
+        pass
+
+    def test_idadf_delitem(self, idadf):
+        pass
+
+    def test_idadf_filter_lt(self, idadf):
+        pass
+
+    def test_idadf_filter_le(self, idadf):
+        pass
+
+    def test_idadf_filter_eq(self, idadf):
+        pass
+
+    def test_idadf_filter_ne(self, idadf):
+        pass
+
+    def test_idadf_filter_ge(self, idadf):
+        pass
+
+    def test_idadf_filter_gt(self, idadf):
+        pass
+
+    def test_idadf_feature_add(self, idadf):
+        pass
+
+    def test_idadf_feature_radd(self, idadf):
+        pass
+
+    def test_idadf_feature_div(self, idadf):
+        pass
+
+    def test_idadf_feature_rdiv(self, idadf):
+        pass
+
+    def test_idadf_feature_floordiv(self, idadf):
+        pass
+
+    def test_idadf_feature_rfloordiv(self, idadf):
+        pass
+
+    def test_idadf_feature_mod(self, idadf):
+        pass
+
+    def test_idadf_feature_rmod(self, idadf):
+        pass
+
+    def test_idadf_feature_mul(self, idadf):
+        pass
+
+    def test_idadf_feature_rmul(self, idadf):
+        pass
+
+    def test_idadf_feature_neg(self, idadf):
+        pass
+
+    def test_idadf_feature_rpos(self, idadf):
+        pass
+
+    def test_idadf_feature_pow(self, idadf):
+        pass
+
+    def test_idadf_feature_rpow(self, idadf):
+        pass
+
+    def test_idadf_feature_sub(self, idadf):
+        pass
+
+    def test_idadf_feature_rsub(self, idadf):
+        pass
+
+class Test_DataBaseFeatures(object):
+
+    def test_idadf_exists(self, idadf):
+        assert(idadf.exists() is True)
+        pass
+
+    def test_idadf_is_view(self, idadf):
+        assert(idadf.is_view() is False)
+        pass
+
+    def test_idadf_is_table(self, idadf):
+        assert(idadf.exists() is True)
+        pass
+
+    def test_idadf_get_primary_key(self, idadf):
+        pass
+
+    def test_idadf_ida_query(self, idadf):
+        pass
+
+    def test_idadf_ida_scalar_query(self, idadf):
+        pass
+
+
+class Test_DataExploration(object):
+    ### head
+    # For head and tail we do not test if the rows match because
+    # the order is not guaranteed anyway
+    def test_idadf_head_default(self, idadb, idadf, df):
+        sortkey = idadf.columns[0]
+        if idadf._get_numerical_columns():
+            sortkey = idadf._get_numerical_columns()[0]
+
+        ida_head = idadf.head()
+        assert isinstance(ida_head, pandas.core.frame.DataFrame)
+        assert len(ida_head) == 5
+        df_head = df.sort_values(sortkey).head()
+        assert (ida_head[sortkey].tolist() == df_head[sortkey].tolist())
+
+    def test_idadf_head_10(self, idadb, idadf, df):
+        ida_head = idadf.head(10)
+        assert isinstance(ida_head, pandas.core.frame.DataFrame)
+        assert len(ida_head) == 10
+
+    def test_idadf_head_10_sort(self, idadb, idadf, df):
+        ida_head = idadf.head(10, sort=False)
+        assert isinstance(ida_head, pandas.core.frame.DataFrame)
+        assert len(ida_head) == 10
+
+    def test_idadf_head_with_indexer(self, idadb, idadf_indexer, df):
+        ida_head = idadf_indexer.head()
+        sortby = len(df.columns)-1
+        df_head = df.sort_values(df.columns[sortby]).head()
+        assert isinstance(ida_head, pandas.core.frame.DataFrame)
+        assert len(ida_head) == 5
+        assert(ida_head[idadf_indexer.columns[sortby]].tolist() ==
+                       df_head[df.columns[sortby]].tolist())
+
+    def test_idadf_head_projected_3col(self, idadf, df):
+        if len(idadf.columns) >= 4:
+            columns = idadf.columns[1:4].tolist()
+            newidadf = idadf[columns]
+
+            sortkey = newidadf.columns[0]
+            if newidadf._get_numerical_columns():
+                sortkey = newidadf._get_numerical_columns()[0]
+
+            ida_head = newidadf.head()
+
+            df_sorted = df.sort_values(sortkey)
+            df_head = df_sorted[columns].head()
+            ida_sortkeys = ida_head[sortkey].tolist()
+            df_sortkeys = df_head[sortkey].tolist()
+            assert isinstance(ida_head, pandas.core.frame.DataFrame)
+            assert len(ida_head) == 5
+            # assert(ida_head[sortkey].tolist() == df_head[sortkey].tolist())
+            assert([ round(x, 10) for x in  ida_sortkeys] == [ round(x, 10) for x in df_sortkeys ])
+
+    def test_idadf_head_sorted(self, idadf, df):
+        sortIdx = len(df.columns) - 1
+        sortkey = idadf.columns[sortIdx]
+        newidadf = idadf.sort(sortkey)
+        ida_head = newidadf.head()
+
+        df_head = df.sort_values(sortkey).head()
+
+        assert(" ORDER BY " in newidadf.internal_state.get_state())
+        assert isinstance(ida_head, pandas.core.frame.DataFrame)
+        assert len(ida_head) == 5
+        assert(ida_head[sortkey].tolist() == df_head[sortkey].tolist())
+
+    def test_idadf_head_0(self, idadf):
+        with pytest.raises(ValueError):
+            idadf.head(0)
+
+    def test_idadf_head_negative(self, idadf):
+        with pytest.raises(ValueError):
+            idadf.head(-1)
+
+    ### tail
+    def test_idadf_tail_default(self, idadb, idadf, df):
+        sortkey = idadf.columns[0]
+        if idadf._get_numerical_columns():
+            sortkey = idadf._get_numerical_columns()[0]
+        ida_tail = idadf.tail()
+        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
+        assert len(ida_tail) == 5
+        df_tail = df.sort_values(sortkey).tail()
+        assert (ida_tail[sortkey].tolist() == df_tail[sortkey].tolist())
+
+    def test_idadf_tail_10(self, idadb, idadf, df):
+        ida_tail = idadf.tail(10)
+        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
+        assert len(ida_tail) == 10
+
+    def test_idadf_tail_10_sort(self, idadb, idadf, df):
+        ida_tail = idadf.tail(10, sort=False)
+        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
+        assert len(ida_tail) == 10
+
+    def test_idadf_tail_with_indexer(self, idadb, idadf_indexer, df):
+        ida_tail = idadf_indexer.tail()
+        sortby = len(df.columns)-1
+        df_head = df.sort_values(df.columns[sortby]).tail()
+        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
+        assert len(ida_tail) == 5
+        assert(ida_tail[idadf_indexer.columns[sortby]].tolist() ==
+                       df_head[df.columns[sortby]].tolist())
+
+    def test_idadf_tail_projected_3col(self, idadf, df):
+        if len(idadf.columns) >= 4:
+            columns = idadf.columns[1:4].tolist()
+            newidadf = idadf[columns]
+
+            sortkey = newidadf.columns[0]
+            if newidadf._get_numerical_columns():
+                sortkey = newidadf._get_numerical_columns()[0]
+
+            ida_tail = newidadf.tail()
+
+            df_sorted = df.sort_values(sortkey)
+            df_tail = df_sorted[columns].tail()
+
+            ida_sortkeys = ida_tail[sortkey].tolist()
+            df_sortkeys = df_tail[sortkey].tolist()
+
+            assert isinstance(ida_tail, pandas.core.frame.DataFrame)
+            assert len(ida_tail) == 5
+            # assert(ida_tail[sortkey].tolist() == df_tail[sortkey].tolist())
+            assert([ round(x, 10) for x in  ida_sortkeys] == [ round(x, 10) for x in df_sortkeys ])
+
+    @pytest.mark.skip(reason="tail on sorted dataframe fails in general, needs fixing first")
+    def test_idadf_tail_sorted(self, idadf, df):
+        sortIdx = len(df.columns) - 1
+        sortkey = idadf.columns[sortIdx]
+        newidadf = idadf.sort(sortkey)
+        ida_tail = newidadf.tail()
+
+        df_tail = df.sort_values(sortkey).tail()
+
+        assert(" ORDER BY " in newidadf.internal_state.get_state())
+        assert isinstance(ida_tail, pandas.core.frame.DataFrame)
+        assert len(ida_tail) == 5
+        assert(ida_tail[sortkey].tolist() == df_tail[sortkey].tolist())
+
+
+    def test_idadf_tail_0(self, idadf):
+        with pytest.raises(ValueError):
+            idadf.tail(0)
+
+    def test_idadf_tail_negative(self, idadf):
+        with pytest.raises(ValueError):
+            idadf.tail(-1)
+
+    def test_idadf_pivot_table(self, idadf):
+        pass
+
+    def test_idadf_sort(self, idadf):
+        pass
+
+# no test
+#__enter__
+#__exit__
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_geoFrame.py` & `nzpyida-0.3.3/nzpyida/tests/test_geoFrame.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,268 +1,268 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaGeoDataFrame
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-import pandas
-import pytest
-import six
-
-from copy import deepcopy
-
-from nzpyida import IdaDataFrame
-from nzpyida import IdaSeries
-from nzpyida import IdaGeoDataFrame
-from nzpyida import IdaGeoSeries
-
-@pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-class Test_IdaGeoDataFrame(object):
-
-    def test_idageodf_set_geometry_error(self, idageodf_county):
-        with pytest.raises(KeyError):
-            idageodf_county.set_geometry('not a column in the Ida')
-        with pytest.raises(TypeError):
-            idageodf_county.set_geometry('OBJECTID')
-
-    def test_idageodf_set_geometry_success(self, idageodf_county):
-        geometry_colname = 'SHAPE'
-        idageodf_county.set_geometry(geometry_colname)
-        assert(isinstance(idageodf_county.geometry, IdaGeoSeries))
-        assert(idageodf_county.geometry.column == geometry_colname)
-
-    def test_idageodf_nondestructive_geometry_column_deletion(
-            self, idageodf_county):
-        geometry_colname = 'SHAPE'
-        idageodf_county.set_geometry(geometry_colname)
-        assert(idageodf_county.geometry.column == geometry_colname)
-        del(idageodf_county[geometry_colname])
-        with pytest.raises(AttributeError):
-            idageodf_county.geometry
-
-    def test_idageodf_geometry_not_set(self, idageodf_county):
-        idageodf_county._geometry_colname = None
-        with pytest.raises(AttributeError):
-            idageodf_county.geometry
-
-    def test_idageodf_fromIdaDataFrame(self, idadf):
-        newidageodf = IdaGeoDataFrame.from_IdaDataFrame(idadf)
-        assert(isinstance(newidageodf, IdaGeoDataFrame))
-
-    def test_idageodf_column_projection_IdaGeoSeries(self, idageodf_county):
-        indexer = 'OBJECTID'
-        idageodf_county.indexer = indexer
-        column = 'SHAPE'
-        ida = idageodf_county[column]
-        assert(isinstance(ida, IdaGeoSeries))
-        assert(ida.column == column)
-        assert(ida.indexer == indexer)
-
-    def test_idageodf_column_projection_IdaSeries(self, idageodf_county):
-        indexer = 'OBJECTID'
-        idageodf_county.indexer = indexer
-        column = 'NAME'
-        ida = idageodf_county[column]
-        assert(isinstance(ida, IdaSeries))
-        assert(ida.column == column)
-        assert(ida.indexer == indexer)
-
-    def test_idageodf_column_projection_IdaGeoDataFrame(self, idageodf_county):
-        geometry_colname = 'SHAPE'
-        idageodf_county.set_geometry(geometry_colname)
-        columns = ['SHAPE', 'OBJECTID']
-        ida = idageodf_county[columns]
-        assert(isinstance(ida, IdaGeoDataFrame))
-        assert(all(ida.columns) == all(columns))
-        assert(ida.geometry.column == geometry_colname)
-
-    def test_idageodf_column_projection_IdaDataFrame(self, idageodf_county):
-        geometry_colname = 'SHAPE'
-        idageodf_county.set_geometry(geometry_colname)
-        columns = ['NAME', 'OBJECTID']
-        ida = idageodf_county[columns]
-        assert(isinstance(ida, IdaDataFrame))
-        assert(all(ida.columns) == all(columns))
-        with pytest.raises(AttributeError):
-            ida.geometry
-
-    def test_idageodf_geospatial_method_call_carried_on_IdaGeoSeries(
-            self, idageodf_county):
-        geometry_colname = 'SHAPE'
-        idageodf_county.set_geometry(geometry_colname)
-        attribute = 'area'
-        assert(not hasattr(IdaGeoDataFrame, attribute))
-        assert(hasattr(IdaGeoSeries, attribute))
-        assert(idageodf_county.__getattr__(attribute))
-
-    def test_idageodf_getattr_unresolved(self, idageodf_county):
-        with pytest.raises(AttributeError):
-            idageodf_county.__getattr__('not_an_attribute')
-
-    def test_idageodf_equals(self, idageodf_customer, idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.equals(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_distance(self, idageodf_customer,idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.distance(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_crosses(self,idageodf_customer,idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.crosses(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_intersects(self, idageodf_customer, idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.intersects(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_overlaps(self, idageodf_customer, idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.overlaps(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_touches(self, idageodf_customer, idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.touches(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_disjoint(self, idageodf_customer, idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.disjoint(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_contains(self, idageodf_customer,idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.contains(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_within(self, idageodf_customer, idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.within(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_mbr_intersects(self, idageodf_customer, idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.mbr_intersects(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_difference(self, idageodf_customer,idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.difference(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_intersection(self, idageodf_customer,idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.intersection(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_union(self, idageodf_customer,idageodf_county):
-        idageodf = idageodf_customer
-        idageodf.set_geometry('SHAPE')
-        idageodf_county.set_geometry('SHAPE')
-        assert (isinstance(idageodf.union(idageodf_county), IdaGeoDataFrame))
-
-    def test_idageodf_binary_operation_handler_non_geometry_column(
-            self, idageodf_customer,idageodf_county):
-        idageodf = idageodf_customer  # ST_POLYGON
-        with pytest.raises(TypeError):
-            idageodf._binary_operation_handler(
-                idageodf_county,
-                db2gse_function='DB2GSE.ST_AGEOSPATIALFUNCTION',
-                valid_types=['ST_POINT'])
-
-    def test_idageodf_max_distance(self, idageodf_county):
-         idageodf = idageodf_county
-         idageodf.set_geometry('SHAPE')
-         ida1 = idageodf[idageodf['NAME'] == 'Austin']
-         ida2 = idageodf[idageodf['NAME'] == 'Kent']
-         res = ida1.distance(ida2, 'KILOMETER')
-         max_dist = res['RESULT'].max()
-         assert(int(max_dist) == 2531)
-
-    def test_idageodf_max_distance_lc(self, idageodf_county_view):
-         idageodf = idageodf_county_view
-         idageodf.set_geometry('shape')
-         ida1 = idageodf[idageodf['name'] == 'Austin']
-         ida2 = idageodf[idageodf['name'] == 'Kent']
-         res = ida1.distance(ida2, 'kilometer')
-         max_dist = res['RESULT'].max()
-         assert(int(max_dist) == 2531)
-
-
-    def test_idageodf_max_distance_mbr(self, idageodf_county):
-         idageodf = idageodf_county
-         idageodf.set_geometry('SHAPE')
-         idageodf['MBR'] = idageodf.mbr()
-         idageodf.set_geometry('MBR')
-         ida1 = idageodf[idageodf['NAME'] == 'Austin']
-         ida2 = idageodf[idageodf['NAME'] == 'Kent']
-         res = ida1.distance(ida2, 'KILOMETER')
-         max_dist_mbr = res['RESULT'].max()
-         assert(int(max_dist_mbr) == 2519)
-
-    def test_idageodf_max_distance_mbr_lc(self, idageodf_county_view):
-         idageodf = idageodf_county_view
-         idageodf.set_geometry('shape')
-         idageodf['mbr'] = idageodf.mbr()
-         idageodf.set_geometry('mbr')
-         ida1 = idageodf[idageodf['name'] == 'Austin']
-         ida2 = idageodf[idageodf['name'] == 'Kent']
-         res = ida1.distance(ida2, 'kilometer')
-         max_dist_mbr = res['RESULT'].max()
-         assert(int(max_dist_mbr) == 2519)
-
-    def test_idageodf_max_area_union(self, idageodf_county):
-         idageodf = idageodf_county
-         idageodf.set_geometry('SHAPE')
-         idageodf['MBR'] = idageodf.mbr()
-         idageodf.set_geometry('MBR')
-         ida1 = idageodf[idageodf['NAME'] == 'Austin']
-         ida2 = idageodf[idageodf['NAME'] == 'Kent']
-         ida12_union = ida1.union(ida2)
-         ida12_union.set_geometry('RESULT')
-         ida12_union['MBR_UNION_AREA'] = ida12_union.area('KILOMETER')
-         max_area_union = ida12_union['MBR_UNION_AREA'].max()
-         assert(int(max_area_union) == 6596)
-
-    def test_idageodf_max_area_union_lc(self, idageodf_county_view):
-         idageodf = idageodf_county_view
-         idageodf.set_geometry('shape')
-         idageodf['mbr'] = idageodf.mbr()
-         idageodf.set_geometry('mbr')
-         ida1 = idageodf[idageodf['name'] == 'Austin']
-         ida2 = idageodf[idageodf['name'] == 'Kent']
-         ida12_union = ida1.union(ida2)
-         ida12_union.set_geometry('RESULT')
-         ida12_union['mbr_union_area'] = ida12_union.area('kilometer')
-         max_area_union = ida12_union['mbr_union_area'].max()
-         assert(int(max_area_union) == 6596)
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaGeoDataFrame
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+import pandas
+import pytest
+import six
+
+from copy import deepcopy
+
+from nzpyida import IdaDataFrame
+from nzpyida import IdaSeries
+from nzpyida import IdaGeoDataFrame
+from nzpyida import IdaGeoSeries
+
+@pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+class Test_IdaGeoDataFrame(object):
+
+    def test_idageodf_set_geometry_error(self, idageodf_county):
+        with pytest.raises(KeyError):
+            idageodf_county.set_geometry('not a column in the Ida')
+        with pytest.raises(TypeError):
+            idageodf_county.set_geometry('OBJECTID')
+
+    def test_idageodf_set_geometry_success(self, idageodf_county):
+        geometry_colname = 'SHAPE'
+        idageodf_county.set_geometry(geometry_colname)
+        assert(isinstance(idageodf_county.geometry, IdaGeoSeries))
+        assert(idageodf_county.geometry.column == geometry_colname)
+
+    def test_idageodf_nondestructive_geometry_column_deletion(
+            self, idageodf_county):
+        geometry_colname = 'SHAPE'
+        idageodf_county.set_geometry(geometry_colname)
+        assert(idageodf_county.geometry.column == geometry_colname)
+        del(idageodf_county[geometry_colname])
+        with pytest.raises(AttributeError):
+            idageodf_county.geometry
+
+    def test_idageodf_geometry_not_set(self, idageodf_county):
+        idageodf_county._geometry_colname = None
+        with pytest.raises(AttributeError):
+            idageodf_county.geometry
+
+    def test_idageodf_fromIdaDataFrame(self, idadf):
+        newidageodf = IdaGeoDataFrame.from_IdaDataFrame(idadf)
+        assert(isinstance(newidageodf, IdaGeoDataFrame))
+
+    def test_idageodf_column_projection_IdaGeoSeries(self, idageodf_county):
+        indexer = 'OBJECTID'
+        idageodf_county.indexer = indexer
+        column = 'SHAPE'
+        ida = idageodf_county[column]
+        assert(isinstance(ida, IdaGeoSeries))
+        assert(ida.column == column)
+        assert(ida.indexer == indexer)
+
+    def test_idageodf_column_projection_IdaSeries(self, idageodf_county):
+        indexer = 'OBJECTID'
+        idageodf_county.indexer = indexer
+        column = 'NAME'
+        ida = idageodf_county[column]
+        assert(isinstance(ida, IdaSeries))
+        assert(ida.column == column)
+        assert(ida.indexer == indexer)
+
+    def test_idageodf_column_projection_IdaGeoDataFrame(self, idageodf_county):
+        geometry_colname = 'SHAPE'
+        idageodf_county.set_geometry(geometry_colname)
+        columns = ['SHAPE', 'OBJECTID']
+        ida = idageodf_county[columns]
+        assert(isinstance(ida, IdaGeoDataFrame))
+        assert(all(ida.columns) == all(columns))
+        assert(ida.geometry.column == geometry_colname)
+
+    def test_idageodf_column_projection_IdaDataFrame(self, idageodf_county):
+        geometry_colname = 'SHAPE'
+        idageodf_county.set_geometry(geometry_colname)
+        columns = ['NAME', 'OBJECTID']
+        ida = idageodf_county[columns]
+        assert(isinstance(ida, IdaDataFrame))
+        assert(all(ida.columns) == all(columns))
+        with pytest.raises(AttributeError):
+            ida.geometry
+
+    def test_idageodf_geospatial_method_call_carried_on_IdaGeoSeries(
+            self, idageodf_county):
+        geometry_colname = 'SHAPE'
+        idageodf_county.set_geometry(geometry_colname)
+        attribute = 'area'
+        assert(not hasattr(IdaGeoDataFrame, attribute))
+        assert(hasattr(IdaGeoSeries, attribute))
+        assert(idageodf_county.__getattr__(attribute))
+
+    def test_idageodf_getattr_unresolved(self, idageodf_county):
+        with pytest.raises(AttributeError):
+            idageodf_county.__getattr__('not_an_attribute')
+
+    def test_idageodf_equals(self, idageodf_customer, idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.equals(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_distance(self, idageodf_customer,idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.distance(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_crosses(self,idageodf_customer,idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.crosses(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_intersects(self, idageodf_customer, idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.intersects(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_overlaps(self, idageodf_customer, idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.overlaps(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_touches(self, idageodf_customer, idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.touches(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_disjoint(self, idageodf_customer, idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.disjoint(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_contains(self, idageodf_customer,idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.contains(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_within(self, idageodf_customer, idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.within(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_mbr_intersects(self, idageodf_customer, idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.mbr_intersects(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_difference(self, idageodf_customer,idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.difference(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_intersection(self, idageodf_customer,idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.intersection(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_union(self, idageodf_customer,idageodf_county):
+        idageodf = idageodf_customer
+        idageodf.set_geometry('SHAPE')
+        idageodf_county.set_geometry('SHAPE')
+        assert (isinstance(idageodf.union(idageodf_county), IdaGeoDataFrame))
+
+    def test_idageodf_binary_operation_handler_non_geometry_column(
+            self, idageodf_customer,idageodf_county):
+        idageodf = idageodf_customer  # ST_POLYGON
+        with pytest.raises(TypeError):
+            idageodf._binary_operation_handler(
+                idageodf_county,
+                db2gse_function='DB2GSE.ST_AGEOSPATIALFUNCTION',
+                valid_types=['ST_POINT'])
+
+    def test_idageodf_max_distance(self, idageodf_county):
+         idageodf = idageodf_county
+         idageodf.set_geometry('SHAPE')
+         ida1 = idageodf[idageodf['NAME'] == 'Austin']
+         ida2 = idageodf[idageodf['NAME'] == 'Kent']
+         res = ida1.distance(ida2, 'KILOMETER')
+         max_dist = res['RESULT'].max()
+         assert(int(max_dist) == 2531)
+
+    def test_idageodf_max_distance_lc(self, idageodf_county_view):
+         idageodf = idageodf_county_view
+         idageodf.set_geometry('shape')
+         ida1 = idageodf[idageodf['name'] == 'Austin']
+         ida2 = idageodf[idageodf['name'] == 'Kent']
+         res = ida1.distance(ida2, 'kilometer')
+         max_dist = res['RESULT'].max()
+         assert(int(max_dist) == 2531)
+
+
+    def test_idageodf_max_distance_mbr(self, idageodf_county):
+         idageodf = idageodf_county
+         idageodf.set_geometry('SHAPE')
+         idageodf['MBR'] = idageodf.mbr()
+         idageodf.set_geometry('MBR')
+         ida1 = idageodf[idageodf['NAME'] == 'Austin']
+         ida2 = idageodf[idageodf['NAME'] == 'Kent']
+         res = ida1.distance(ida2, 'KILOMETER')
+         max_dist_mbr = res['RESULT'].max()
+         assert(int(max_dist_mbr) == 2519)
+
+    def test_idageodf_max_distance_mbr_lc(self, idageodf_county_view):
+         idageodf = idageodf_county_view
+         idageodf.set_geometry('shape')
+         idageodf['mbr'] = idageodf.mbr()
+         idageodf.set_geometry('mbr')
+         ida1 = idageodf[idageodf['name'] == 'Austin']
+         ida2 = idageodf[idageodf['name'] == 'Kent']
+         res = ida1.distance(ida2, 'kilometer')
+         max_dist_mbr = res['RESULT'].max()
+         assert(int(max_dist_mbr) == 2519)
+
+    def test_idageodf_max_area_union(self, idageodf_county):
+         idageodf = idageodf_county
+         idageodf.set_geometry('SHAPE')
+         idageodf['MBR'] = idageodf.mbr()
+         idageodf.set_geometry('MBR')
+         ida1 = idageodf[idageodf['NAME'] == 'Austin']
+         ida2 = idageodf[idageodf['NAME'] == 'Kent']
+         ida12_union = ida1.union(ida2)
+         ida12_union.set_geometry('RESULT')
+         ida12_union['MBR_UNION_AREA'] = ida12_union.area('KILOMETER')
+         max_area_union = ida12_union['MBR_UNION_AREA'].max()
+         assert(int(max_area_union) == 6596)
+
+    def test_idageodf_max_area_union_lc(self, idageodf_county_view):
+         idageodf = idageodf_county_view
+         idageodf.set_geometry('shape')
+         idageodf['mbr'] = idageodf.mbr()
+         idageodf.set_geometry('mbr')
+         ida1 = idageodf[idageodf['name'] == 'Austin']
+         ida2 = idageodf[idageodf['name'] == 'Kent']
+         ida12_union = ida1.union(ida2)
+         ida12_union.set_geometry('RESULT')
+         ida12_union['mbr_union_area'] = ida12_union.area('kilometer')
+         max_area_union = ida12_union['mbr_union_area'].max()
+         assert(int(max_area_union) == 6596)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_geoSeries.py` & `nzpyida-0.3.3/nzpyida/tests/test_geoSeries.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,273 +1,273 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaGeoSeries
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-import pandas
-import pytest
-import six
-
-from nzpyida import IdaSeries
-from nzpyida import IdaGeoSeries
-from nzpyida.exceptions import IdaGeoDataFrameError
-
-@pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
-class Test_IdaGeoSeries(object):
-
-    def test_idageoseries_generalize(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        with pytest.raises(TypeError):
-            idageoseries.generalize('not a float')
-        with pytest.raises(ValueError):
-            idageoseries.generalize(-5)
-        assert(isinstance(idageoseries.generalize(1.0), IdaGeoSeries))
-
-    def test_idageoseries_buffer(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        with pytest.raises(TypeError):
-            idageoseries.buffer(distance='not a number')
-        assert(isinstance(idageoseries.buffer(
-                distance=2.3, unit="CLARKE'S FOOT"), IdaGeoSeries))
-
-    def test_idageoseries_centroid(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.centroid(), IdaGeoSeries))
-
-    def test_idageoseries_convex_hull(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.convex_hull(), IdaGeoSeries))
-
-    def test_idageoseries_boundary(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.boundary(), IdaGeoSeries))
-
-    def test_idageoseries_envelope(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.envelope(), IdaGeoSeries))
-
-    def test_idageoseries_exterior_ring(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE'].envelope() # get ST_POLYGON
-        assert(isinstance(idageoseries.exterior_ring(), IdaGeoSeries))
-
-    def test_idageoseries_mbr(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.mbr(), IdaGeoSeries))
-
-    def test_idageoseries_end_point(self, idageodf_county):
-        # TODO: add dataset with a column with ST_LINESTRING
-        pass
-
-    def test_idageoseries_mid_point(self, idageodf_county):
-        # TODO: add dataset with a column with ST_LINESTRING
-        pass
-
-    def test_idageoseries_start_point(self, idageodf_county):
-        # TODO: add dataset with a column with ST_LINESTRING
-        pass
-
-    def test_idageoseries_srid(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.srid(), IdaSeries))
-
-    def test_idageoseries_srs_name(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.srs_name(), IdaSeries))
-
-    def test_idageoseries_geometry_type(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.geometry_type(), IdaSeries))
-
-    def test_idageoseries_area(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.area(
-                unit="BRITISH FOOT (BENOIT 1895 A)"), IdaSeries))
-
-    def test_idageoseries_dimension(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.dimension(), IdaSeries))  
-
-    def test_idageoseries_length(self, idageodf_tornado):
-        idageoseries = idageodf_tornado['SHAPE']
-        assert(isinstance(idageoseries.length(
-                unit="CLARKE'S FOOT"), IdaSeries))
-
-    def test_idageoseries_perimeter(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.perimeter(
-                unit="CLARKE'S FOOT"), IdaSeries))
-
-    def test_idageoseries_num_geometries(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.num_geometries(), IdaSeries))
-
-    def test_idageoseries_num_interior_ring(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE'].envelope() # get ST_POLYGON
-        assert(isinstance(idageoseries.num_interior_ring(), IdaSeries))
-
-    def test_idageoseries_num_line_strings(self, idageodf_tornado):
-        idageoseries = idageodf_tornado['SHAPE']
-        assert(isinstance(idageoseries.num_line_strings(), IdaSeries))
-
-    def test_idageoseries_num_points(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.num_points(), IdaSeries))
-
-    def test_idageoseries_num_polygons(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.num_polygons(), IdaSeries))
-
-    def test_idageoseries_coord_dim(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.coord_dim(), IdaSeries))
-
-    def test_idageoseries_is_3d(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.is_3d(), IdaSeries))
-
-    def test_idageoseries_is_measured(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.is_measured(), IdaSeries))
-
-    def test_idageoseries_is_valid(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.is_valid(), IdaSeries))
-        
-    def test_idageoseries_max_m(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.max_m(), IdaSeries))
-
-    def test_idageoseries_max_x(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.max_x(), IdaSeries))
-
-    def test_idageoseries_max_y(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.max_y(), IdaSeries))
-
-    def test_idageoseries_max_z(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.max_z(), IdaSeries))
-        
-    def test_idageoseries_min_m(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.min_m(), IdaSeries))
-
-    def test_idageoseries_min_x(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.min_x(), IdaSeries))
-
-    def test_idageoseries_min_y(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.min_y(), IdaSeries))
-
-    def test_idageoseries_min_z(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.min_z(), IdaSeries))
-
-    def test_idageoseries_m(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE'].centroid()
-        assert(isinstance(idageoseries.m(), IdaSeries))
-
-    def test_idageoseries_x(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE'].centroid()
-        assert(isinstance(idageoseries.x(), IdaSeries))
-
-    def test_idageoseries_y(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE'].centroid()
-        assert(isinstance(idageoseries.y(), IdaSeries))
-
-    def test_idageoseries_z(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE'].centroid()
-        assert(isinstance(idageoseries.z(), IdaSeries))
-
-    def test_idageoseries_is_closed(self, idageodf_county):
-        # TODO: add dataset with a column with ST_LINESTRING
-        pass
-
-    def test_idageoseries_is_empty(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.is_empty(), IdaSeries))
-
-    def test_idageoseries_is_simple(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        assert(isinstance(idageoseries.is_simple(), IdaSeries))
-    
-    def test_idageoseries_check_linear_unit(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        with pytest.raises(TypeError):
-            idageoseries._check_linear_unit(10)
-        with pytest.raises(IdaGeoDataFrameError):
-            idageoseries._check_linear_unit('not a valid unit')
-        unit = 'meter'
-        ans = idageoseries._check_linear_unit(unit)
-        assert(ans == '\'METER\'')
-        unit = 'Yard (SEARS)' # parenthesis
-        ans = idageoseries._check_linear_unit(unit)
-        assert(ans == '\'YARD (SEARS)\'')
-        unit = 'Clarke\'s foot' # quotation mark
-        ans = idageoseries._check_linear_unit(unit)
-        assert(ans == '\'CLARKE\'\'S FOOT\'')
-        unit = 'bin width 37.5 METRES' #with dot    
-        ans = idageoseries._check_linear_unit(unit)
-        assert(ans == '\'BIN WIDTH 37.5 METRES\'')
-    
-    def test_idageoseries_linear_units(self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE']
-        units = idageoseries.linear_units
-        assert(not units.empty)
-
-    def test_idageoseries_unary_operation_handler_non_geometry_column(
-            self, idageodf_county):
-        idageoseries = idageodf_county['SHAPE'] # ST_POLYGON      
-        with pytest.raises(TypeError):
-            idageoseries._unary_operation_handler(
-                db2gse_function = 'DB2GSE.ST_AGEOSPATIALFUNCTION',
-                valid_types = ['ST_POINT'])
-
-    def test_idageoseries_area_km(self, idageodf_county):
-        idageodf = idageodf_county
-        idageodf.set_geometry('SHAPE')
-        idageodf['AREA_KM'] = idageodf.area(unit='KILOMETER')
-        max_area_km = idageodf['AREA_KM'].max()
-        assert(int(max_area_km)== 52073)
-
-    def test_idageoseries_area_km_lc(self, idageodf_county_view):
-        idageodf = idageodf_county_view
-        idageodf.set_geometry('shape')
-        idageodf['area_km'] = idageodf.area(unit='kilometer')
-        max_area_km = idageodf['area_km'].max()
-        assert(int(max_area_km)== 52073)
-
-    def test_idageoseries_area_km_mbr(self, idageodf_county):
-        idageodf = idageodf_county
-        idageodf.set_geometry('SHAPE')
-        idageodf['MBR'] = idageodf.mbr()
-        idageodf.set_geometry('MBR')
-        idageodf['AREA_KM_MBR'] = idageodf.area(unit='KILOMETER')
-        max_area_km_mbr = idageodf['AREA_KM_MBR'].max()
-        assert(int(max_area_km_mbr) == 100246)
-
-    def test_idageoseries_area_km_mbr_lc(self, idageodf_county_view):
-        idageodf = idageodf_county_view
-        idageodf.set_geometry('shape')
-        idageodf['mbr'] = idageodf.mbr()
-        idageodf.set_geometry('mbr')
-        idageodf['area_km_mbr'] = idageodf.area(unit='kilometer')
-        max_area_km_mbr = idageodf['area_km_mbr'].max()
-        assert(int(max_area_km_mbr) == 100246)
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaGeoSeries
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+import pandas
+import pytest
+import six
+
+from nzpyida import IdaSeries
+from nzpyida import IdaGeoSeries
+from nzpyida.exceptions import IdaGeoDataFrameError
+
+@pytest.mark.skipif("'netezza' in config.getvalue('jdbc') or config.getvalue('hostname') != ''")
+class Test_IdaGeoSeries(object):
+
+    def test_idageoseries_generalize(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        with pytest.raises(TypeError):
+            idageoseries.generalize('not a float')
+        with pytest.raises(ValueError):
+            idageoseries.generalize(-5)
+        assert(isinstance(idageoseries.generalize(1.0), IdaGeoSeries))
+
+    def test_idageoseries_buffer(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        with pytest.raises(TypeError):
+            idageoseries.buffer(distance='not a number')
+        assert(isinstance(idageoseries.buffer(
+                distance=2.3, unit="CLARKE'S FOOT"), IdaGeoSeries))
+
+    def test_idageoseries_centroid(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.centroid(), IdaGeoSeries))
+
+    def test_idageoseries_convex_hull(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.convex_hull(), IdaGeoSeries))
+
+    def test_idageoseries_boundary(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.boundary(), IdaGeoSeries))
+
+    def test_idageoseries_envelope(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.envelope(), IdaGeoSeries))
+
+    def test_idageoseries_exterior_ring(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE'].envelope() # get ST_POLYGON
+        assert(isinstance(idageoseries.exterior_ring(), IdaGeoSeries))
+
+    def test_idageoseries_mbr(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.mbr(), IdaGeoSeries))
+
+    def test_idageoseries_end_point(self, idageodf_county):
+        # TODO: add dataset with a column with ST_LINESTRING
+        pass
+
+    def test_idageoseries_mid_point(self, idageodf_county):
+        # TODO: add dataset with a column with ST_LINESTRING
+        pass
+
+    def test_idageoseries_start_point(self, idageodf_county):
+        # TODO: add dataset with a column with ST_LINESTRING
+        pass
+
+    def test_idageoseries_srid(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.srid(), IdaSeries))
+
+    def test_idageoseries_srs_name(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.srs_name(), IdaSeries))
+
+    def test_idageoseries_geometry_type(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.geometry_type(), IdaSeries))
+
+    def test_idageoseries_area(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.area(
+                unit="BRITISH FOOT (BENOIT 1895 A)"), IdaSeries))
+
+    def test_idageoseries_dimension(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.dimension(), IdaSeries))  
+
+    def test_idageoseries_length(self, idageodf_tornado):
+        idageoseries = idageodf_tornado['SHAPE']
+        assert(isinstance(idageoseries.length(
+                unit="CLARKE'S FOOT"), IdaSeries))
+
+    def test_idageoseries_perimeter(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.perimeter(
+                unit="CLARKE'S FOOT"), IdaSeries))
+
+    def test_idageoseries_num_geometries(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.num_geometries(), IdaSeries))
+
+    def test_idageoseries_num_interior_ring(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE'].envelope() # get ST_POLYGON
+        assert(isinstance(idageoseries.num_interior_ring(), IdaSeries))
+
+    def test_idageoseries_num_line_strings(self, idageodf_tornado):
+        idageoseries = idageodf_tornado['SHAPE']
+        assert(isinstance(idageoseries.num_line_strings(), IdaSeries))
+
+    def test_idageoseries_num_points(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.num_points(), IdaSeries))
+
+    def test_idageoseries_num_polygons(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.num_polygons(), IdaSeries))
+
+    def test_idageoseries_coord_dim(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.coord_dim(), IdaSeries))
+
+    def test_idageoseries_is_3d(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.is_3d(), IdaSeries))
+
+    def test_idageoseries_is_measured(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.is_measured(), IdaSeries))
+
+    def test_idageoseries_is_valid(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.is_valid(), IdaSeries))
+        
+    def test_idageoseries_max_m(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.max_m(), IdaSeries))
+
+    def test_idageoseries_max_x(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.max_x(), IdaSeries))
+
+    def test_idageoseries_max_y(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.max_y(), IdaSeries))
+
+    def test_idageoseries_max_z(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.max_z(), IdaSeries))
+        
+    def test_idageoseries_min_m(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.min_m(), IdaSeries))
+
+    def test_idageoseries_min_x(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.min_x(), IdaSeries))
+
+    def test_idageoseries_min_y(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.min_y(), IdaSeries))
+
+    def test_idageoseries_min_z(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.min_z(), IdaSeries))
+
+    def test_idageoseries_m(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE'].centroid()
+        assert(isinstance(idageoseries.m(), IdaSeries))
+
+    def test_idageoseries_x(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE'].centroid()
+        assert(isinstance(idageoseries.x(), IdaSeries))
+
+    def test_idageoseries_y(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE'].centroid()
+        assert(isinstance(idageoseries.y(), IdaSeries))
+
+    def test_idageoseries_z(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE'].centroid()
+        assert(isinstance(idageoseries.z(), IdaSeries))
+
+    def test_idageoseries_is_closed(self, idageodf_county):
+        # TODO: add dataset with a column with ST_LINESTRING
+        pass
+
+    def test_idageoseries_is_empty(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.is_empty(), IdaSeries))
+
+    def test_idageoseries_is_simple(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        assert(isinstance(idageoseries.is_simple(), IdaSeries))
+    
+    def test_idageoseries_check_linear_unit(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        with pytest.raises(TypeError):
+            idageoseries._check_linear_unit(10)
+        with pytest.raises(IdaGeoDataFrameError):
+            idageoseries._check_linear_unit('not a valid unit')
+        unit = 'meter'
+        ans = idageoseries._check_linear_unit(unit)
+        assert(ans == '\'METER\'')
+        unit = 'Yard (SEARS)' # parenthesis
+        ans = idageoseries._check_linear_unit(unit)
+        assert(ans == '\'YARD (SEARS)\'')
+        unit = 'Clarke\'s foot' # quotation mark
+        ans = idageoseries._check_linear_unit(unit)
+        assert(ans == '\'CLARKE\'\'S FOOT\'')
+        unit = 'bin width 37.5 METRES' #with dot    
+        ans = idageoseries._check_linear_unit(unit)
+        assert(ans == '\'BIN WIDTH 37.5 METRES\'')
+    
+    def test_idageoseries_linear_units(self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE']
+        units = idageoseries.linear_units
+        assert(not units.empty)
+
+    def test_idageoseries_unary_operation_handler_non_geometry_column(
+            self, idageodf_county):
+        idageoseries = idageodf_county['SHAPE'] # ST_POLYGON      
+        with pytest.raises(TypeError):
+            idageoseries._unary_operation_handler(
+                db2gse_function = 'DB2GSE.ST_AGEOSPATIALFUNCTION',
+                valid_types = ['ST_POINT'])
+
+    def test_idageoseries_area_km(self, idageodf_county):
+        idageodf = idageodf_county
+        idageodf.set_geometry('SHAPE')
+        idageodf['AREA_KM'] = idageodf.area(unit='KILOMETER')
+        max_area_km = idageodf['AREA_KM'].max()
+        assert(int(max_area_km)== 52073)
+
+    def test_idageoseries_area_km_lc(self, idageodf_county_view):
+        idageodf = idageodf_county_view
+        idageodf.set_geometry('shape')
+        idageodf['area_km'] = idageodf.area(unit='kilometer')
+        max_area_km = idageodf['area_km'].max()
+        assert(int(max_area_km)== 52073)
+
+    def test_idageoseries_area_km_mbr(self, idageodf_county):
+        idageodf = idageodf_county
+        idageodf.set_geometry('SHAPE')
+        idageodf['MBR'] = idageodf.mbr()
+        idageodf.set_geometry('MBR')
+        idageodf['AREA_KM_MBR'] = idageodf.area(unit='KILOMETER')
+        max_area_km_mbr = idageodf['AREA_KM_MBR'].max()
+        assert(int(max_area_km_mbr) == 100246)
+
+    def test_idageoseries_area_km_mbr_lc(self, idageodf_county_view):
+        idageodf = idageodf_county_view
+        idageodf.set_geometry('shape')
+        idageodf['mbr'] = idageodf.mbr()
+        idageodf.set_geometry('mbr')
+        idageodf['area_km_mbr'] = idageodf.area(unit='kilometer')
+        max_area_km_mbr = idageodf['area_km_mbr'].max()
+        assert(int(max_area_km_mbr) == 100246)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_internals.py` & `nzpyida-0.3.3/nzpyida/tests/test_internals.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-
-class Test_internals(object):
-    def test_internals_init(idadf):
-        pass
-
-    def test_internals_update(idadf):
-        pass
-
-    def test_internals_current_state(idadf):
-        pass
-
-    def test_internals_clone(idadf):
-        pass
-
-    def test_internals_stop_cumulating(idadf):
-        pass
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+
+class Test_internals(object):
+    def test_internals_init(idadf):
+        pass
+
+    def test_internals_update(idadf):
+        pass
+
+    def test_internals_current_state(idadf):
+        pass
+
+    def test_internals_clone(idadf):
+        pass
+
+    def test_internals_stop_cumulating(idadf):
+        pass
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_kmeans.py` & `nzpyida-0.3.3/nzpyida/tests/test_kmeans.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,53 +1,53 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for ibmdbpy.learn.KMeans
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-class Test_KMeansInitiateClustering(object):
-
-    def test_kmeans_instance(self, idadf):
-        pass
-
-    def test_kmeans_get_parameters(self, idadf):
-        pass
-
-    def test_kmeans_set_parameters(self, idadf):
-        pass
-
-class Test_KMeansFitandPredict(object):
-
-    def test_kmeans_fit(self, idadf):
-        pass
-
-    def test_kmeans_predict(self, idadf):
-        pass
-
-    def test_kmeans_fit_and_predict(self, idadf):
-        pass
-
-class Test_KMeansExploreResult(object):
-
-    def test_kmeans_describe(self, idadf):
-        pass
-
-    def test_kmeans_get_labels(self, idadf):
-        pass
-
-    def test_kmeans_retrieve_KMeans_Model(self, idadf):
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for ibmdbpy.learn.KMeans
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+class Test_KMeansInitiateClustering(object):
+
+    def test_kmeans_instance(self, idadf):
+        pass
+
+    def test_kmeans_get_parameters(self, idadf):
+        pass
+
+    def test_kmeans_set_parameters(self, idadf):
+        pass
+
+class Test_KMeansFitandPredict(object):
+
+    def test_kmeans_fit(self, idadf):
+        pass
+
+    def test_kmeans_predict(self, idadf):
+        pass
+
+    def test_kmeans_fit_and_predict(self, idadf):
+        pass
+
+class Test_KMeansExploreResult(object):
+
+    def test_kmeans_describe(self, idadf):
+        pass
+
+    def test_kmeans_get_labels(self, idadf):
+        pass
+
+    def test_kmeans_retrieve_KMeans_Model(self, idadf):
         pass
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_naive_bayes.py` & `nzpyida-0.3.3/nzpyida/tests/test_frame_connexion.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,53 +1,47 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for ibmdbpy.learn.naive_bayes
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-class Test_NaiveBayesInitiateModel(object):
-
-    def test_bayes_instance(self, idadf):
-        pass
-
-    def test_bayes_get_parameters(self, idadf):
-        pass
-
-    def test_bayes_set_parameters(self, idadf):
-        pass
-
-class Test_NaiveBayesFitandPredict(object):
-
-    def test_bayes_fit(self, idadf):
-        pass
-
-    def test_bayes_predict(self, idadf):
-        pass
-
-    def test_bayes_fit_and_predict(self, idadf):
-        pass
-
-class Test_NaiveBayesExploreResult(object):
-
-    def test_bayes_describe(self, idadf):
-        pass
-
-    def test_bayes_get_labels(self, idadf):
-        pass
-
-    def test_bayes_retrieve_KMeans_Model(self, idadf):
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for connexion related functions of IdaDataFrame
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+import pandas
+
+class Test_Import(object):
+
+    def test_idadf_as_dataframe(self, idadf):
+        tmp = idadf.as_dataframe() # For avoiding overhead off loading several time
+        assert isinstance(tmp, pandas.core.frame.DataFrame)
+        assert list(tmp.columns) == list(idadf.columns)
+        assert list(tmp.index) == list(idadf.index)
+        assert tmp.name == idadf.tablename
+
+class Test_ConnexionManagement(object):
+    def test_idadf_save_as(self, idadf):
+        pass
+
+    def test_idadf_commit(self, idadf):
+        pass
+
+    def test_idadf_rollback(self, idadf):
+        pass
+
+    def test_idadf_close(self, idadf):
+        pass
+
+    def test_idadf_reopen(self, idadf):
         pass
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_series.py` & `nzpyida-0.3.3/nzpyida/tests/test_series.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-class Test_IdaSeries(object):
-
-    def test_idaSeries_init(self):
-        pass
-
-    def test_idaSeries_cone(self):
-        pass
-
-    def test_idaSeries_return_format(self):
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+class Test_IdaSeries(object):
+
+    def test_idaSeries_init(self):
+        pass
+
+    def test_idaSeries_cone(self):
+        pass
+
+    def test_idaSeries_return_format(self):
         pass
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_sorting.py` & `nzpyida-0.3.3/nzpyida/tests/test_sorting.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,63 +1,63 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for sorting of IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-
-class Test_sorting(object):
-    def test_sort_1_row_asc_inplace(idadf):
-        pass
-
-    def test_sort_1_row_dsc_inplace(idadf):
-        pass
-
-    def test_sort_X_rows_asc_inplace(idadf):
-        pass
-
-    def test_sort_X_rows_dsc_inplace(idadf):
-        pass
-
-    def test_sort_cols_asc_inplace(idadf):
-        pass
-
-    def test_sort_cols_dsc_inplace(idadf):
-        pass
-
-    def test_sort_1_row_asc_not_inplace(idadf):
-        pass
-
-    def test_sort_1_row_dsc_not_inplace(idadf):
-        pass
-
-    def test_sort_X_rows_asc_not_inplace(idadf):
-        pass
-
-    def test_sort_X_rows_dsc_not_inplace(idadf):
-        pass
-
-    def test_sort_cols_asc_not_inplace(idadf):
-        pass
-
-    def test_sort_cols_dsc_not_inplace(idadf):
-        pass
-
-
-
-
-
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for sorting of IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+
+class Test_sorting(object):
+    def test_sort_1_row_asc_inplace(idadf):
+        pass
+
+    def test_sort_1_row_dsc_inplace(idadf):
+        pass
+
+    def test_sort_X_rows_asc_inplace(idadf):
+        pass
+
+    def test_sort_X_rows_dsc_inplace(idadf):
+        pass
+
+    def test_sort_cols_asc_inplace(idadf):
+        pass
+
+    def test_sort_cols_dsc_inplace(idadf):
+        pass
+
+    def test_sort_1_row_asc_not_inplace(idadf):
+        pass
+
+    def test_sort_1_row_dsc_not_inplace(idadf):
+        pass
+
+    def test_sort_X_rows_asc_not_inplace(idadf):
+        pass
+
+    def test_sort_X_rows_dsc_not_inplace(idadf):
+        pass
+
+    def test_sort_cols_asc_not_inplace(idadf):
+        pass
+
+    def test_sort_cols_dsc_not_inplace(idadf):
+        pass
+
+
+
+
+
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_statistics.py` & `nzpyida-0.3.3/nzpyida/tests/test_statistics.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,206 +1,203 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for statistics for IdaDataFrame Objects
-"""
-
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from builtins import object
-from builtins import str
-from future import standard_library
-standard_library.install_aliases()
-
-import numpy
-import pandas
-import pytest
-
-from nzpyida.statistics import _numeric_stats , _get_percentiles, _get_number_of_nas, _count_level, _count_level_groupby
-from nzpyida import IdaDataFrame as IDADF
-
-class Test_PrivateStatisticsMethods(object):
-
-    def test_idadf_numeric_stats_default(self, idadf):
-        data = idadf._table_def() # We necessarly have to put the test under this condition
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        assert isinstance(_numeric_stats(idadf, "count", columns), numpy.ndarray)
-        assert isinstance(_numeric_stats(idadf, "mean", columns), numpy.ndarray)
-        assert isinstance(_numeric_stats(idadf, "median", columns), numpy.ndarray)
-        assert isinstance(_numeric_stats(idadf, "std", columns), numpy.ndarray)
-        assert isinstance(_numeric_stats(idadf, "var", columns), numpy.ndarray)
-        assert isinstance(_numeric_stats(idadf, "min", columns), numpy.ndarray)
-        assert isinstance(_numeric_stats(idadf, "max", columns), numpy.ndarray)
-
-    def test_idadf_onecolumn_numeric_numeric_stats_one_column(self, idadf_onecolumn_numeric):
-        column = idadf_onecolumn_numeric.columns.tolist()
-        assert(isinstance(_numeric_stats(idadf_onecolumn_numeric, "count", column), numpy.float64) |
-               isinstance(_numeric_stats(idadf_onecolumn_numeric, "count", column), numpy.int64) )
-        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "mean", column), numpy.float64)
-        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "median", column), numpy.ndarray)
-        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "std", column), numpy.float64)
-        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "var", column), numpy.float64)
-        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "min", column), numpy.float64)
-        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "max", column), numpy.float64)
-
-    def test_idadf_numeric_stats_accuracy(self, idadf):
-        pass
-
-    def test_idadf_get_percentiles_default(self, idadf):
-        data = idadf._table_def() # We necessarly have to put the test under this condition
-        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
-        assert isinstance(_get_percentiles(idadf, 0.5,columns), pandas.core.frame.DataFrame)
-        assert isinstance(_get_percentiles(idadf, [0.5],columns), pandas.core.frame.DataFrame)
-        assert _get_percentiles(idadf, [0.5],columns).shape[0] == 1
-        assert isinstance(_get_percentiles(idadf, [0.3,0.4,0.5],columns), pandas.core.frame.DataFrame)
-        assert _get_percentiles(idadf, [0.3,0.4,0.5],columns).shape[0] == 3
-
-    def test_idadf_get_percentiles_accuracy(self, idadf):
-        pass
-
-    def test_idadf_get_categorical_stats(self, idadf):
-        # FUNCTION TO IMPLEMENT
-        pass
-
-    def test_idadf_get_number_of_nas(self, idadf):
-        assert isinstance(_get_number_of_nas(idadf, idadf.columns), tuple)
-        assert len(_get_number_of_nas(idadf, idadf.columns)) == len(idadf.columns)
-        assert len(_get_number_of_nas(idadf, idadf.columns[0])) == 1
-
-    def test_idadf_get_number_of_nas_accuracy(self, idadf):
-        pass
-
-
-
-    def test_idadf_count_level(self, idadf):
-        assert isinstance(_count_level(idadf), tuple)
-        assert len(_count_level(idadf)) == len(idadf.columns)
-        assert len(_count_level(idadf, idadf.columns)) == len(idadf.columns)
-
-    def test_idadf_count_level_accuracy(self, idadf):
-        pass
-
-    def test_idadf_count_level_groupby(self, idadf):
-        assert isinstance(_count_level_groupby(idadf), tuple)
-        assert(_count_level_groupby(idadf)[0] % 1 == 0)
-        assert len(_count_level_groupby(idadf)) == 1
-        assert len(_count_level_groupby(idadf, idadf.columns)) == 1
-
-    def test_idadf_count_level_group_by_accuracy(self, idadf):
-        pass
-
-    def test_idadf_factor_count(self, idadf):
-        pass
-
-    def test_idadf_factor_sum(self, idadf):
-        pass
-
-    def test_idadf_factor_avg(self, idadf):
-        pass
-
-class Test_DescriptiveStatistics(object):
-
-    def test_idadf_pivot_table(self, idadf):
-        pass
-
-    def test_idadf_describe_default(self, idadf):
-        to_assert = idadf.describe()
-        assert isinstance(to_assert, pandas.core.frame.DataFrame)
-        assert (len(to_assert) == 8) & (to_assert.shape[1] <= len(idadf.columns))
-        assert all([x in to_assert.index for x in ["count","mean","std", "min", "max"]])
-
-    def test_idadf_describe_custom_percentiles(self, idadf):
-        assert len(idadf.describe([0.2,0.3,0.4,0.5,0.6])) == 10
-        assert len(idadf.describe([0.5])) == 6
-
-    def test_idadf_describe_bad_parameter_value(self, idadf):
-        with pytest.raises(ValueError):
-            idadf.describe([0.2,0.3,0.4,0.5,-1])
-
-    def test_idadf_describe_bad_parameter_type(self, idadf):
-        with pytest.raises(TypeError):
-            idadf.describe("string")
-
-    def test_idadf_quantile_default(self, idadf, df):
-        assert str(idadf.quantile()) == str(df.quantile())
-
-    def test_idadf_quantile_custom(self, idadf, df):
-        assert all(idadf.quantile([0.2,0.4,0.6,0.8]) == df.quantile([0.2,0.4,0.6,0.8]))
-
-    def test_idadf_quantile_value_out_of_range(self, idadf):
-        with pytest.raises(ValueError):
-            idadf.quantile([5])
-        with pytest.raises(ValueError):
-            idadf.quantile([-0.4])
-        with pytest.raises(TypeError):
-            idadf.quantile(["lol"])
-        with pytest.raises(TypeError):
-            idadf.quantile("lol")
-        with pytest.raises(TypeError):
-            idadf.quantile(0.5,0.7)
-
-    def test_idadf_cov(self, idadf, df):
-        assert str(idadf.cov()) == str(df.cov())
-
-    def test_idadf_corr(self, idadf, df):
-        assert str(idadf.corr()) == str(df.corr())
-
-    def test_idadf_mad(self, idadf, df):
-        assert str(idadf.mad()) == str(df.mad())
-
-    # TODO : Fix min and max in python 2
-    def test_idadf_min(self, idadf, df):
-        assert str(idadf.min()) == str(df.min())
-
-    def test_idadf_max(self, idadf, df):
-        assert str(idadf.max()) == str(df.max())
-
-    def test_idadf_count(self, idadf, df):
-        assert all(idadf.count() == df.count())
-
-    def test_idadf_count_distinct(self, idadf, df):
-        pass
-
-    def test_idadf_std(self, idadf, df):
-        assert str(idadf.std()) == str(df.std())
-
-    def test_idadf_var(self, idadf, df):
-        assert str(idadf.var()) == str(df.var())
-
-    def test_idadf_mean(self, idadf, df):
-        assert str(idadf.mean()) == str(df.mean())
-
-    def test_idadf_sum(self, idadf, df):
-        assert str(idadf.sum()) == str(df.sum(numeric_only = True))
-
-    def test_idadf_median(self, idadf, df):
-        assert str(idadf.median()) == str(df.median())
-
-    @pytest.mark.parametrize("f",
-                             [IDADF.describe,
-                              IDADF.cov,
-                              IDADF.corr,
-                              IDADF.quantile,
-                              IDADF.mad,
-                              IDADF.min,
-                              IDADF.max,
-                              IDADF.count,
-                              IDADF.count_distinct,
-                              IDADF.std,
-                              IDADF.var,
-                              IDADF.mean,
-                              IDADF.sum,
-                              IDADF.median,
-                             ])
-    def test_idadf_statistics_one_column(self, idadf_onecolumn_numeric, f):
-        f(idadf_onecolumn_numeric)
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for statistics for IdaDataFrame Objects
+"""
+
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from builtins import object
+from builtins import str
+from future import standard_library
+standard_library.install_aliases()
+
+import numpy
+import pandas
+import pytest
+
+from nzpyida.statistics import _numeric_stats , _get_percentiles, _get_number_of_nas, _count_level, _count_level_groupby
+from nzpyida import IdaDataFrame as IDADF
+
+class Test_PrivateStatisticsMethods(object):
+
+    def test_idadf_numeric_stats_default(self, idadf):
+        data = idadf._table_def() # We necessarly have to put the test under this condition
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        assert isinstance(_numeric_stats(idadf, "count", columns), numpy.ndarray)
+        assert isinstance(_numeric_stats(idadf, "mean", columns), numpy.ndarray)
+        assert isinstance(_numeric_stats(idadf, "median", columns), numpy.ndarray)
+        assert isinstance(_numeric_stats(idadf, "std", columns), numpy.ndarray)
+        assert isinstance(_numeric_stats(idadf, "var", columns), numpy.ndarray)
+        assert isinstance(_numeric_stats(idadf, "min", columns), numpy.ndarray)
+        assert isinstance(_numeric_stats(idadf, "max", columns), numpy.ndarray)
+
+    def test_idadf_onecolumn_numeric_numeric_stats_one_column(self, idadf_onecolumn_numeric):
+        column = idadf_onecolumn_numeric.columns.tolist()
+        assert(isinstance(_numeric_stats(idadf_onecolumn_numeric, "count", column), numpy.float64) |
+               isinstance(_numeric_stats(idadf_onecolumn_numeric, "count", column), numpy.int64) )
+        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "mean", column), numpy.float64)
+        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "median", column), numpy.ndarray)
+        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "std", column), numpy.float64)
+        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "var", column), numpy.float64)
+        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "min", column), numpy.float64)
+        assert isinstance(_numeric_stats(idadf_onecolumn_numeric, "max", column), numpy.float64)
+
+    def test_idadf_numeric_stats_accuracy(self, idadf):
+        pass
+
+    def test_idadf_get_percentiles_default(self, idadf):
+        data = idadf._table_def() # We necessarly have to put the test under this condition
+        columns = list(data.loc[data['VALTYPE'] == "NUMERIC"].index)
+        assert isinstance(_get_percentiles(idadf, 0.5,columns), pandas.core.frame.DataFrame)
+        assert isinstance(_get_percentiles(idadf, [0.5],columns), pandas.core.frame.DataFrame)
+        assert _get_percentiles(idadf, [0.5],columns).shape[0] == 1
+        assert isinstance(_get_percentiles(idadf, [0.3,0.4,0.5],columns), pandas.core.frame.DataFrame)
+        assert _get_percentiles(idadf, [0.3,0.4,0.5],columns).shape[0] == 3
+
+    def test_idadf_get_percentiles_accuracy(self, idadf):
+        pass
+
+    def test_idadf_get_categorical_stats(self, idadf):
+        # FUNCTION TO IMPLEMENT
+        pass
+
+    def test_idadf_get_number_of_nas(self, idadf):
+        assert isinstance(_get_number_of_nas(idadf, idadf.columns), tuple)
+        assert len(_get_number_of_nas(idadf, idadf.columns)) == len(idadf.columns)
+        assert len(_get_number_of_nas(idadf, idadf.columns[0])) == 1
+
+    def test_idadf_get_number_of_nas_accuracy(self, idadf):
+        pass
+
+
+
+    def test_idadf_count_level(self, idadf):
+        assert isinstance(_count_level(idadf), tuple)
+        assert len(_count_level(idadf)) == len(idadf.columns)
+        assert len(_count_level(idadf, idadf.columns)) == len(idadf.columns)
+
+    def test_idadf_count_level_accuracy(self, idadf):
+        pass
+
+    def test_idadf_count_level_groupby(self, idadf):
+        assert isinstance(_count_level_groupby(idadf), tuple)
+        assert(_count_level_groupby(idadf)[0] % 1 == 0)
+        assert len(_count_level_groupby(idadf)) == 1
+        assert len(_count_level_groupby(idadf, idadf.columns)) == 1
+
+    def test_idadf_count_level_group_by_accuracy(self, idadf):
+        pass
+
+    def test_idadf_factor_count(self, idadf):
+        pass
+
+    def test_idadf_factor_sum(self, idadf):
+        pass
+
+    def test_idadf_factor_avg(self, idadf):
+        pass
+
+class Test_DescriptiveStatistics(object):
+
+    def test_idadf_pivot_table(self, idadf):
+        pass
+
+    def test_idadf_describe_default(self, idadf):
+        to_assert = idadf.describe()
+        assert isinstance(to_assert, pandas.core.frame.DataFrame)
+        assert (len(to_assert) == 8) & (to_assert.shape[1] <= len(idadf.columns))
+        assert all([x in to_assert.index for x in ["count","mean","std", "min", "max"]])
+
+    def test_idadf_describe_custom_percentiles(self, idadf):
+        assert len(idadf.describe([0.2,0.3,0.4,0.5,0.6])) == 10
+        assert len(idadf.describe([0.5])) == 6
+
+    def test_idadf_describe_bad_parameter_value(self, idadf):
+        with pytest.raises(ValueError):
+            idadf.describe([0.2,0.3,0.4,0.5,-1])
+
+    def test_idadf_describe_bad_parameter_type(self, idadf):
+        with pytest.raises(TypeError):
+            idadf.describe("string")
+
+    def test_idadf_quantile_default(self, idadf, df):
+        assert str(idadf.quantile()) == str(df.quantile(numeric_only=True))
+
+    def test_idadf_quantile_custom(self, idadf, df):
+        assert all(idadf.quantile([0.2,0.4,0.6,0.8]) == df.quantile([0.2,0.4,0.6,0.8], numeric_only=True))
+
+    def test_idadf_quantile_value_out_of_range(self, idadf):
+        with pytest.raises(ValueError):
+            idadf.quantile([5])
+        with pytest.raises(ValueError):
+            idadf.quantile([-0.4])
+        with pytest.raises(TypeError):
+            idadf.quantile(["lol"])
+        with pytest.raises(TypeError):
+            idadf.quantile("lol")
+        with pytest.raises(TypeError):
+            idadf.quantile(0.5,0.7)
+
+    def test_idadf_cov(self, idadf, df):
+        assert str(idadf.cov()) == str(df.cov(numeric_only=True))
+
+    def test_idadf_corr(self, idadf, df):
+        assert str(idadf.corr()) == str(df.corr(numeric_only=True))
+
+    # TODO : Fix min and max in python 2
+    def test_idadf_min(self, idadf, df):
+        assert str(idadf.min()) == str(df.min())
+
+    def test_idadf_max(self, idadf, df):
+        assert str(idadf.max()) == str(df.max())
+
+    def test_idadf_count(self, idadf, df):
+        assert all(idadf.count() == df.count())
+
+    def test_idadf_count_distinct(self, idadf, df):
+        pass
+
+    def test_idadf_std(self, idadf, df):
+        assert str(idadf.std()) == str(df.std(numeric_only=True))
+
+    def test_idadf_var(self, idadf, df):
+        assert str(idadf.var()) == str(df.var(numeric_only=True))
+
+    def test_idadf_mean(self, idadf, df):
+        assert str(idadf.mean()) == str(df.mean(numeric_only=True))
+
+    def test_idadf_sum(self, idadf, df):
+        assert str(idadf.sum()) == str(df.sum(numeric_only=True))
+
+    def test_idadf_median(self, idadf, df):
+        assert str(idadf.median()) == str(df.median(numeric_only=True))
+
+    @pytest.mark.parametrize("f",
+                             [IDADF.describe,
+                              IDADF.cov,
+                              IDADF.corr,
+                              IDADF.quantile,
+                              IDADF.mad,
+                              IDADF.min,
+                              IDADF.max,
+                              IDADF.count,
+                              IDADF.count_distinct,
+                              IDADF.std,
+                              IDADF.var,
+                              IDADF.mean,
+                              IDADF.sum,
+                              IDADF.median,
+                             ])
+    def test_idadf_statistics_one_column(self, idadf_onecolumn_numeric, f):
+        f(idadf_onecolumn_numeric)
```

### Comparing `nzpyida-0.2.2.6/nzpyida/tests/test_utils.py` & `nzpyida-0.3.3/nzpyida/tests/test_utils.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-Test module for IdaDataFrameObjects
-"""
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-standard_library.install_aliases()
-
-
-class Test_utils(object):
-    def test_set_verbose(idadb):
-        pass
-
-    def test_set_autocommit(idadb):
-        pass
-
-    def test_check_tablename(idadb):
-        pass
-
-    def test_check_viewname(idadb):
-        pass
-
-    def test_check_case(idadb):
-        pass
-
-    def test_reset_attributes(idadb):
-        pass
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+Test module for IdaDataFrameObjects
+"""
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+standard_library.install_aliases()
+
+
+class Test_utils(object):
+    def test_set_verbose(idadb):
+        pass
+
+    def test_set_autocommit(idadb):
+        pass
+
+    def test_check_tablename(idadb):
+        pass
+
+    def test_check_viewname(idadb):
+        pass
+
+    def test_check_case(idadb):
+        pass
+
+    def test_reset_attributes(idadb):
+        pass
```

### Comparing `nzpyida-0.2.2.6/nzpyida/utils.py` & `nzpyida-0.3.3/nzpyida/utils.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,345 +1,345 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-""" Utility functions """
-
-# Python 2 compatibility
-from __future__ import print_function
-from __future__ import unicode_literals
-from __future__ import division
-from __future__ import absolute_import
-from builtins import range
-from builtins import input
-from future import standard_library
-standard_library.install_aliases()
-
-import sys
-import os
-import warnings
-from time import time
-from functools import wraps
-
-import six
-import pandas as pd
-from copy import deepcopy
-
-#-----------------------------------------------------------------------------
-# Environment variable setter
-
-def set_verbose(is_verbose):
-    """
-    Set the environment variable “VERBOSE” to ‘TRUE’ or ‘FALSE’. If it is set 
-    to ‘TRUE’, all SQL request are printed in the console.
-    """
-    if is_verbose is True:
-        os.environ['VERBOSE'] = 'True'
-    elif is_verbose is False:
-        os.environ['VERBOSE'] = 'False'
-    else:
-        raise ValueError("is_verbose should be a boolean")
-
-def set_autocommit(is_autocommit):
-    """
-    Set the environment variable “AUTCOMMIT” to ‘TRUE’ or ‘FALSE’. If it is set 
-    to ‘TRUE’, all operations are committed automatically.
-    """
-    if is_autocommit is True:
-        os.environ['AUTOCOMMIT'] = 'True'
-    elif is_autocommit is False:
-        os.environ['AUTOCOMMIT'] = 'False'
-    else:
-        raise ValueError("is_autocommit should be a boolean")
-
-#-----------------------------------------------------------------------
-# Performance measurement wrapper
-
-def timed(function):
-    """
-    Measure the elapsed time of custom functions of the package. Should be used 
-    as a decorator.
-    """
-    @wraps(function)
-    def wrapper(*args, **kwds):
-        """Calculate elapsed time in seconds"""
-        start = time()
-        result = function(*args, **kwds)
-        elapsed = time() - start
-        if os.environ['VERBOSE'] == 'True':
-            print("Execution time: %s seconds." %elapsed)
-        return result
-    return wrapper
-
-def query_yes_no(question, default=None):
-    """
-    Ask a yes/no question via raw_input() and return its answer.
-
-    Parameters
-    ----------
-    question : str
-        Question to be asked to the user. Should be a yes/no question.
-    default : "yes"/"no"
-        The presumed answer if the user just hits <Enter>.
-
-    Returns
-    -------
-    bool
-    """
-    valid = {"yes": True, "y": True, "ye": True,
-             "no": False, "n": False}
-    if default is None:
-        prompt = " [y/n] "
-    elif default == "yes":
-        prompt = " [Y/n] "
-    elif default == "no":
-        prompt = " [y/N] "
-    else:
-        raise ValueError("invalid default answer: '%s'" % default)
-
-    while True:
-        sys.stdout.write(question + prompt)
-        choice = input().lower() # in Python 2.7, it is raw_input
-        if default is not None and choice == '':
-            return valid[default]
-        elif choice in valid:
-            return valid[choice]
-        else:
-            sys.stdout.write("Please respond with 'yes' or 'no' "
-                             "(or 'y' or 'n').\n")
-                             
-def chunklist(l, n):
-    """ Yield successive n-sized chunks from l."""
-    for i in range(0, len(l), n):
-        yield l[i:i+n]
-
-
-def silent(function):
-    """
-    Decorate that silent the function it decorates,
-    i.e SQL queries will not be printed.
-
-    Notes
-    -----
-    * Legacy from Benchmark submodule
-
-    """
-    @wraps(function)
-    def wrapper(self, *args, **kwds):
-        original_value = os.environ['VERBOSE']
-        set_verbose(False)
-        result = function(self, *args, **kwds)
-        os.environ['VERBOSE'] = original_value
-        return result
-    return wrapper
-
-def to_nK(dataframe, nKrow):
-    """
-    Create a version of the dataframe that has n Krows.
-
-    Parameters
-    ----------
-    dataframe : DataFrame
-        DataFrame to use as a basis
-    nKrow : int
-        Number of Krows the return dataframe should contain
-
-    Returns
-    -------
-    DataFrame
-        A Version of the inputed dataframe with n Krows
-
-    Notes
-    -----
-
-    * If the dataset is smaller than nKrow, it will be imputed randomly 
-      with some existing rows, otherwise a random sample is returned
-    * Legacy from Benchmark submodule
-
-    """
-    if nKrow < 1:
-        raise ValueError("n should be an integer with minimum value 1")
-    if not isinstance(nKrow, int):
-        raise ValueError("n should be an integer with minimum value 1")
-    def some(df, nrow):
-        """Return n random rows from the given dataframe"""
-        from numpy import random
-        return df.ix[random.choice(range(len(df)), nrow)]
-
-    if nKrow*1000 > dataframe.shape[0]:
-        df = pd.concat([dataframe, some(dataframe, nKrow*1000 - dataframe.shape[0])], ignore_index=True)
-    else:
-        df = pd.concat([some(dataframe, nKrow*1000)], ignore_index=True)
-    return df
-
-def extend_dataset(df, n):
-    """
-    Extend a dataframe horizontaly by duplicating its columns n times
-
-    Notes
-    -----
-
-    * Legacy from Benchmark submodule
-    """
-    df_2 = deepcopy(df)
-    df_out = deepcopy(df)
-    while n > 0:
-        df_2.columns = ["%s_extended_%s"%(column,n) for column in df.columns]
-        df_out = pd.concat([df_out,df_2], axis = 1)
-        n -= 1
-    return df_out
-
-def check_tablename(tablename):
-    """
-    Check if a string is upper case. This function converts the string to upper 
-    case and checks if it is a valid table name.
-
-    Parameters
-    ----------
-    tablename : str
-        string to be checked. 
-
-    Returns
-    -------
-    str
-        Checked and upper cased table name.
-
-    Notes
-    -----
-        Table names should consist of upper case characters or numbers that can
-        be separated by underscores (“_”) characters.
-    """
-    return _check_database_objectname(tablename, 'Table')
-
-def check_viewname(viewname):
-    """
-    Convenience function for checking view names, which have the same prerequisites as table names.
-    See the check_tablename documentation.
-    """
-    return _check_database_objectname(viewname, 'View')
-
-def check_modelname(modelname):
-    """
-    Convenience function for checking model names, which have the same prerequisites as table names.
-    See the check_tablename documentation.
-    """
-    return _check_database_objectname(modelname, 'Model')
-
-
-def _check_case(name):
-    """
-    Check if the name given as parameter is in upper case and convert it to 
-    upper cases.
-    """
-    if name != name.upper():
-        warnings.warn("Mixed case names are not supported in database object names.", UserWarning)
-    return name.upper()
-
-
-def _check_database_objectname(tablename, objecttype):
-    """
-    Check if a string is upper case. This function converts the string to upper
-    case and checks if it is a valid database object (table, view or model) name.
-
-    Parameters
-    ----------
-    tablename : str
-        string to be checked.
-
-    objecttype : str
-            ''
-
-    Returns
-    -------
-    str
-        Checked and upper cased database object name.
-
-    Notes
-    -----
-        Database object names should consist of upper case characters or numbers that can
-        be separated by underscores (“_”) characters.
-    """
-    tablename = _check_case(tablename)
-    if not all([(char.isalnum() | (char == '_') | (char == '.')) for char in tablename]):
-        raise ValueError("%s name '%s' is not valid, only alphanumeric characters and underscores are allowed."%(objecttype, tablename))
-    if tablename.count(".") > 1:
-        raise ValueError("%s name '%s' is not valid, only one '.' character is allowed."%(objecttype, tablename))
-    return tablename
-
-
-def _convert_dtypes(idadf, data):
-    """
-    DEPRECATED - CURRENTLY NOT IN USE
-    was used for formatting dataframe types
-
-    Convert datatypes in a dataframe to float or to int according to the 
-    corresponding type in database. This function only works if the dataframe 
-    that is given as a parameter has the same columns as the current 
-    IdaDataFrame.
-    """
-    # Note : Here I made the choice to convert every numeric attributes to
-    # float, the reason is that converting to int for an attribute that has
-    # missing values leads to an error, this is known as a pandas' gotcha
-    # We could use statistics._get_number_of_nas to know if there are
-    # missing values for each attributes, but I made the choice no to do it,
-    # this for reducing complexity and improve performance. However, it no
-    # efficient in terms of memory.
-    import pdb ; pdb.set_trace() ;
-    original_dtype = idadf.dtypes
-
-    int_attributes = ['SMALLINT', 'INTEGER', 'BIGINT', 'BOOLEAN']
-    float_attributes = ['REAL', 'DOUBLE', 'FLOAT', 'DECIMAL',
-                        'DECFLOAT', 'APPROXIMATE', 'NUMERIC']
-    for index, dtype in enumerate(original_dtype.values):
-        if dtype[0] in int_attributes + float_attributes:
-            data[[data.columns[index]]] = data[[data.columns[index]]].astype(float)
-        else:
-            data[[data.columns[index]]] = data[[data.columns[index]]].astype(object)
-    return data
-
-def _reset_attributes(idaobject, attributes):
-    """
-    Delete an attribute of a list of attributes of an object, if the attribute exists.
-    """
-    if (not hasattr(attributes, "__iter__"))|isinstance(attributes, six.string_types):
-        attributes = [attributes]
-    for attribute in attributes:
-        try:
-            delattr(idaobject, attribute)
-        except AttributeError:
-            pass
-        
-def _check_input(idadf, target, features):
-    """
-    Check if the input is valid. 
-    Target should be a string that exists as a column in the IdaDataFrame, or None.
-    Features should a string or a list of columns in the IdaDataFrame, or None.
-    If features is None, a list with all features different from target is returned
-    """
-    if target is not None:
-        if not isinstance(target, six.string_types):
-            raise ValueError("target should be a string")
-        if target not in idadf.columns:
-            raise ValueError("Unknown column %s"%target)
-    
-    if features is not None:
-        if isinstance(features, six.string_types):
-            if features not in idadf.columns:
-                raise ValueError("Unknown column %s"%features)
-            features = [features]
-        for x in features:
-            if x not in idadf.columns:
-                raise ValueError("Unknown column %s"%x)
-    else:
-        if target is not None:
-            features = [x for x in idadf.columns if x not in target]
-        else:
-            features = list(idadf.columns)
-            
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+""" Utility functions """
+
+# Python 2 compatibility
+from __future__ import print_function
+from __future__ import unicode_literals
+from __future__ import division
+from __future__ import absolute_import
+from builtins import range
+from builtins import input
+from future import standard_library
+standard_library.install_aliases()
+
+import sys
+import os
+import warnings
+from time import time
+from functools import wraps
+
+import six
+import pandas as pd
+from copy import deepcopy
+
+#-----------------------------------------------------------------------------
+# Environment variable setter
+
+def set_verbose(is_verbose):
+    """
+    Set the environment variable “VERBOSE” to ‘TRUE’ or ‘FALSE’. If it is set 
+    to ‘TRUE’, all SQL request are printed in the console.
+    """
+    if is_verbose is True:
+        os.environ['VERBOSE'] = 'True'
+    elif is_verbose is False:
+        os.environ['VERBOSE'] = 'False'
+    else:
+        raise ValueError("is_verbose should be a boolean")
+
+def set_autocommit(is_autocommit):
+    """
+    Set the environment variable “AUTCOMMIT” to ‘TRUE’ or ‘FALSE’. If it is set 
+    to ‘TRUE’, all operations are committed automatically.
+    """
+    if is_autocommit is True:
+        os.environ['AUTOCOMMIT'] = 'True'
+    elif is_autocommit is False:
+        os.environ['AUTOCOMMIT'] = 'False'
+    else:
+        raise ValueError("is_autocommit should be a boolean")
+
+#-----------------------------------------------------------------------
+# Performance measurement wrapper
+
+def timed(function):
+    """
+    Measure the elapsed time of custom functions of the package. Should be used 
+    as a decorator.
+    """
+    @wraps(function)
+    def wrapper(*args, **kwds):
+        """Calculate elapsed time in seconds"""
+        start = time()
+        result = function(*args, **kwds)
+        elapsed = time() - start
+        if os.environ['VERBOSE'] == 'True':
+            print("Execution time: %s seconds." %elapsed)
+        return result
+    return wrapper
+
+def query_yes_no(question, default=None):
+    """
+    Ask a yes/no question via raw_input() and return its answer.
+
+    Parameters
+    ----------
+    question : str
+        Question to be asked to the user. Should be a yes/no question.
+    default : "yes"/"no"
+        The presumed answer if the user just hits <Enter>.
+
+    Returns
+    -------
+    bool
+    """
+    valid = {"yes": True, "y": True, "ye": True,
+             "no": False, "n": False}
+    if default is None:
+        prompt = " [y/n] "
+    elif default == "yes":
+        prompt = " [Y/n] "
+    elif default == "no":
+        prompt = " [y/N] "
+    else:
+        raise ValueError("invalid default answer: '%s'" % default)
+
+    while True:
+        sys.stdout.write(question + prompt)
+        choice = input().lower() # in Python 2.7, it is raw_input
+        if default is not None and choice == '':
+            return valid[default]
+        elif choice in valid:
+            return valid[choice]
+        else:
+            sys.stdout.write("Please respond with 'yes' or 'no' "
+                             "(or 'y' or 'n').\n")
+                             
+def chunklist(l, n):
+    """ Yield successive n-sized chunks from l."""
+    for i in range(0, len(l), n):
+        yield l[i:i+n]
+
+
+def silent(function):
+    """
+    Decorate that silent the function it decorates,
+    i.e SQL queries will not be printed.
+
+    Notes
+    -----
+    * Legacy from Benchmark submodule
+
+    """
+    @wraps(function)
+    def wrapper(self, *args, **kwds):
+        original_value = os.environ['VERBOSE']
+        set_verbose(False)
+        result = function(self, *args, **kwds)
+        os.environ['VERBOSE'] = original_value
+        return result
+    return wrapper
+
+def to_nK(dataframe, nKrow):
+    """
+    Create a version of the dataframe that has n Krows.
+
+    Parameters
+    ----------
+    dataframe : DataFrame
+        DataFrame to use as a basis
+    nKrow : int
+        Number of Krows the return dataframe should contain
+
+    Returns
+    -------
+    DataFrame
+        A Version of the inputed dataframe with n Krows
+
+    Notes
+    -----
+
+    * If the dataset is smaller than nKrow, it will be imputed randomly 
+      with some existing rows, otherwise a random sample is returned
+    * Legacy from Benchmark submodule
+
+    """
+    if nKrow < 1:
+        raise ValueError("n should be an integer with minimum value 1")
+    if not isinstance(nKrow, int):
+        raise ValueError("n should be an integer with minimum value 1")
+    def some(df, nrow):
+        """Return n random rows from the given dataframe"""
+        from numpy import random
+        return df.ix[random.choice(range(len(df)), nrow)]
+
+    if nKrow*1000 > dataframe.shape[0]:
+        df = pd.concat([dataframe, some(dataframe, nKrow*1000 - dataframe.shape[0])], ignore_index=True)
+    else:
+        df = pd.concat([some(dataframe, nKrow*1000)], ignore_index=True)
+    return df
+
+def extend_dataset(df, n):
+    """
+    Extend a dataframe horizontaly by duplicating its columns n times
+
+    Notes
+    -----
+
+    * Legacy from Benchmark submodule
+    """
+    df_2 = deepcopy(df)
+    df_out = deepcopy(df)
+    while n > 0:
+        df_2.columns = ["%s_extended_%s"%(column,n) for column in df.columns]
+        df_out = pd.concat([df_out,df_2], axis = 1)
+        n -= 1
+    return df_out
+
+def check_tablename(tablename):
+    """
+    Check if a string is upper case. This function converts the string to upper 
+    case and checks if it is a valid table name.
+
+    Parameters
+    ----------
+    tablename : str
+        string to be checked. 
+
+    Returns
+    -------
+    str
+        Checked and upper cased table name.
+
+    Notes
+    -----
+        Table names should consist of upper case characters or numbers that can
+        be separated by underscores (“_”) characters.
+    """
+    return _check_database_objectname(tablename, 'Table')
+
+def check_viewname(viewname):
+    """
+    Convenience function for checking view names, which have the same prerequisites as table names.
+    See the check_tablename documentation.
+    """
+    return _check_database_objectname(viewname, 'View')
+
+def check_modelname(modelname):
+    """
+    Convenience function for checking model names, which have the same prerequisites as table names.
+    See the check_tablename documentation.
+    """
+    return _check_database_objectname(modelname, 'Model')
+
+
+def _check_case(name):
+    """
+    Check if the name given as parameter is in upper case and convert it to 
+    upper cases.
+    """
+    if name != name.upper():
+        warnings.warn("Mixed case names are not supported in database object names.", UserWarning)
+    return name.upper()
+
+
+def _check_database_objectname(tablename, objecttype):
+    """
+    Check if a string is upper case. This function converts the string to upper
+    case and checks if it is a valid database object (table, view or model) name.
+
+    Parameters
+    ----------
+    tablename : str
+        string to be checked.
+
+    objecttype : str
+            ''
+
+    Returns
+    -------
+    str
+        Checked and upper cased database object name.
+
+    Notes
+    -----
+        Database object names should consist of upper case characters or numbers that can
+        be separated by underscores (“_”) characters.
+    """
+    tablename = _check_case(tablename)
+    if not all([(char.isalnum() | (char == '_') | (char == '.')) for char in tablename]):
+        raise ValueError("%s name '%s' is not valid, only alphanumeric characters and underscores are allowed."%(objecttype, tablename))
+    if tablename.count(".") > 1:
+        raise ValueError("%s name '%s' is not valid, only one '.' character is allowed."%(objecttype, tablename))
+    return tablename
+
+
+def _convert_dtypes(idadf, data):
+    """
+    DEPRECATED - CURRENTLY NOT IN USE
+    was used for formatting dataframe types
+
+    Convert datatypes in a dataframe to float or to int according to the 
+    corresponding type in database. This function only works if the dataframe 
+    that is given as a parameter has the same columns as the current 
+    IdaDataFrame.
+    """
+    # Note : Here I made the choice to convert every numeric attributes to
+    # float, the reason is that converting to int for an attribute that has
+    # missing values leads to an error, this is known as a pandas' gotcha
+    # We could use statistics._get_number_of_nas to know if there are
+    # missing values for each attributes, but I made the choice no to do it,
+    # this for reducing complexity and improve performance. However, it no
+    # efficient in terms of memory.
+    import pdb ; pdb.set_trace() ;
+    original_dtype = idadf.dtypes
+
+    int_attributes = ['SMALLINT', 'INTEGER', 'BIGINT', 'BOOLEAN']
+    float_attributes = ['REAL', 'DOUBLE', 'FLOAT', 'DECIMAL',
+                        'DECFLOAT', 'APPROXIMATE', 'NUMERIC']
+    for index, dtype in enumerate(original_dtype.values):
+        if dtype[0] in int_attributes + float_attributes:
+            data[[data.columns[index]]] = data[[data.columns[index]]].astype(float)
+        else:
+            data[[data.columns[index]]] = data[[data.columns[index]]].astype(object)
+    return data
+
+def _reset_attributes(idaobject, attributes):
+    """
+    Delete an attribute of a list of attributes of an object, if the attribute exists.
+    """
+    if (not hasattr(attributes, "__iter__"))|isinstance(attributes, six.string_types):
+        attributes = [attributes]
+    for attribute in attributes:
+        try:
+            delattr(idaobject, attribute)
+        except AttributeError:
+            pass
+        
+def _check_input(idadf, target, features):
+    """
+    Check if the input is valid. 
+    Target should be a string that exists as a column in the IdaDataFrame, or None.
+    Features should a string or a list of columns in the IdaDataFrame, or None.
+    If features is None, a list with all features different from target is returned
+    """
+    if target is not None:
+        if not isinstance(target, six.string_types):
+            raise ValueError("target should be a string")
+        if target not in idadf.columns:
+            raise ValueError("Unknown column %s"%target)
+    
+    if features is not None:
+        if isinstance(features, six.string_types):
+            if features not in idadf.columns:
+                raise ValueError("Unknown column %s"%features)
+            features = [features]
+        for x in features:
+            if x not in idadf.columns:
+                raise ValueError("Unknown column %s"%x)
+    else:
+        if target is not None:
+            features = [x for x in idadf.columns if x not in target]
+        else:
+            features = list(idadf.columns)
+            
     return target, features
```

### Comparing `nzpyida-0.2.2.6/setup.py` & `nzpyida-0.3.3/setup.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,89 +1,86 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-#-----------------------------------------------------------------------------
-# Copyright (c) 2015, IBM Corp.
-# All rights reserved.
-#
-# Distributed under the terms of the BSD Simplified License.
-#
-# The full license is in the LICENSE file, distributed with this software.
-#-----------------------------------------------------------------------------
-
-"""
-setup.py
-"""
-
-# -*- coding: utf-8 -*-
-# Note: Always prefer setuptools over distutils
-from setuptools import setup, find_packages
-from codecs import open
-
-
-# Get the long description from the relevant file
-
-
-with open('README.rst', 'r', encoding='utf-8') as f:
-    longdesc = f.read()
-
-
-classifiers = [
-        # How mature is this project? Common values are
-        #   3 - Alpha
-        #   4 - Beta
-        #   5 - Production/Stable
-        'Development Status :: 4 - Beta',
-
-        # Indicate who your project is intended for
-        'Intended Audience :: Developers',
-        'Intended Audience :: Education',
-        'Intended Audience :: End Users/Desktop',
-        'Intended Audience :: Information Technology',
-        'Intended Audience :: Science/Research',
-        'Topic :: Software Development :: Build Tools',
-
-        # Pick your license as you wish (should match "license" above)
-        'License :: OSI Approved :: BSD License',
-
-        'Operating System :: MacOS :: MacOS X',
-        'Operating System :: Microsoft :: Windows',
-        'Operating System :: POSIX',
-
-        'Natural Language :: English',
-
-        # Specify the Python versions you support here. In particular, ensure
-        # that you indicate whether you support Python 2, Python 3 or both.
-        'Programming Language :: Python :: 2.7',
-        'Programming Language :: Python :: 3',
-        'Programming Language :: Python :: 3.2',
-        'Programming Language :: Python :: 3.3',
-        'Programming Language :: Python :: 3.4',
-        'Programming Language :: Python :: 3.5',
-        'Programming Language :: Python :: 3.6',
-        'Programming Language :: Python :: Implementation :: CPython',
-
-        'Topic :: Database',
-        'Topic :: Scientific/Engineering',
-        'Topic :: Software Development'
-      ]
-
-setup(name='nzpyida',
-      version='0.2.2.6',
-      install_requires=['pandas','numpy','future','six','pypyodbc','pyodbc', 'lazy', 'nzpy'],
-
-      extras_require={
-        'jdbc':['JayDeBeApi==1.*', 'Jpype1==0.6.3'],
-        'test':['pytest', 'flaky==3.4.0'],
-        'doc':['sphinx', 'ipython', 'numpydoc', 'sphinx_rtd_theme']
-      },
-      description='Supports Custom ML/Analytics Execution Inside Netezza',
-      long_description=longdesc,
-      long_description_content_type='text/x-rst',
-      author='IBM Corp.',
-      author_email='vinay.kasireddy@ibm.com,toni.bollinger@de.ibm.com',
-      license='BSD',
-      classifiers=classifiers,
-      keywords='data analytics database development ibm netezza pandas scikitlearn scalability machine-learning knowledge discovery',
-      packages=find_packages(exclude=['docs', 'tests*']),
-      package_data={
-        'nzpyida.sampledata': ['*.txt']}
-     )
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+#-----------------------------------------------------------------------------
+# Copyright (c) 2015, IBM Corp.
+# All rights reserved.
+#
+# Distributed under the terms of the BSD Simplified License.
+#
+# The full license is in the LICENSE file, distributed with this software.
+#-----------------------------------------------------------------------------
+
+"""
+setup.py
+"""
+
+# -*- coding: utf-8 -*-
+# Note: Always prefer setuptools over distutils
+from setuptools import setup, find_packages
+from codecs import open
+
+
+# Get the long description from the relevant file
+
+
+with open('README.md', 'r', encoding='utf-8') as f:
+    longdesc = f.read()
+
+
+classifiers = [
+        # How mature is this project? Common values are
+        #   3 - Alpha
+        #   4 - Beta
+        #   5 - Production/Stable
+        'Development Status :: 4 - Beta',
+
+        # Indicate who your project is intended for
+        'Intended Audience :: Developers',
+        'Intended Audience :: Education',
+        'Intended Audience :: End Users/Desktop',
+        'Intended Audience :: Information Technology',
+        'Intended Audience :: Science/Research',
+        'Topic :: Software Development :: Build Tools',
+
+        # Pick your license as you wish (should match "license" above)
+        'License :: OSI Approved :: BSD License',
+
+        'Operating System :: MacOS :: MacOS X',
+        'Operating System :: Microsoft :: Windows',
+        'Operating System :: POSIX',
+
+        'Natural Language :: English',
+
+        # Specify the Python versions you support here. In particular, ensure
+        # that you indicate whether you support Python 2, Python 3 or both.
+        'Programming Language :: Python :: 3.6',
+        'Programming Language :: Python :: 3.7',
+        'Programming Language :: Python :: 3.8',
+        'Programming Language :: Python :: 3.9',
+        'Programming Language :: Python :: Implementation :: CPython',
+
+        'Topic :: Database',
+        'Topic :: Scientific/Engineering',
+        'Topic :: Software Development'
+      ]
+
+setup(name='nzpyida',
+      version='0.3.3',
+      install_requires=['pandas','numpy','future','six','pypyodbc','pyodbc', 'lazy', 'nzpy'],
+
+      extras_require={
+        'jdbc':['JayDeBeApi==1.*', 'Jpype1==0.6.3'],
+        'test':['pytest', 'flaky==3.4.0'],
+        'doc':['sphinx', 'ipython', 'numpydoc', 'sphinx_rtd_theme']
+      },
+      description='Supports Custom ML/Analytics Execution Inside Netezza',
+      long_description=longdesc,
+      long_description_content_type='text/markdown',
+      author='IBM Corp.',
+      author_email='mlabenski@ibm.com,pawel.mroz1@ibm.com',
+      license='BSD',
+      classifiers=classifiers,
+      keywords='data analytics database development ibm netezza pandas scikitlearn scalability machine-learning knowledge discovery',
+      packages=find_packages(exclude=['docs', 'tests*']),
+      package_data={
+        'nzpyida.sampledata': ['*.txt']}
+     )
```

