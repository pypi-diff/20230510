# Comparing `tmp/shimoku_api_python-0.8.1-py2.py3-none-any.whl.zip` & `tmp/shimoku_api_python-0.9-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,26 +1,28 @@
-Zip file size: 47239 bytes, number of entries: 24
--rw-rw-r--  2.0 unx     1503 b- defN 22-Jul-31 08:33 shimoku_api_python/__init__.py
--rw-rw-r--  2.0 unx    13662 b- defN 22-May-02 12:25 shimoku_api_python/client.py
+Zip file size: 60706 bytes, number of entries: 26
+-rw-rw-r--  2.0 unx     1716 b- defN 22-Aug-17 08:48 shimoku_api_python/__init__.py
+-rw-rw-r--  2.0 unx    13742 b- defN 22-Aug-17 08:48 shimoku_api_python/client.py
 -rw-rw-r--  2.0 unx     7749 b- defN 22-Jan-03 14:16 shimoku_api_python/configuration.py
 -rw-rw-r--  2.0 unx      154 b- defN 22-Apr-28 19:55 shimoku_api_python/exceptions.py
--rw-rw-r--  2.0 unx     1167 b- defN 22-Jan-03 14:16 shimoku_api_python/api/app_metadata_api.py
+-rw-rw-r--  2.0 unx    10096 b- defN 22-Aug-17 08:48 shimoku_api_python/api/ai_api.py
+-rw-rw-r--  2.0 unx     2283 b- defN 22-Aug-17 08:48 shimoku_api_python/api/app_metadata_api.py
 -rw-rw-r--  2.0 unx     1861 b- defN 22-May-22 07:46 shimoku_api_python/api/app_type_metadata_api.py
 -rw-rw-r--  2.0 unx      692 b- defN 22-Jan-03 14:16 shimoku_api_python/api/business_metadata_api.py
--rw-rw-r--  2.0 unx    13659 b- defN 22-Jul-30 14:50 shimoku_api_python/api/data_managing_api.py
--rw-rw-r--  2.0 unx    50285 b- defN 22-Jul-31 08:33 shimoku_api_python/api/explorer_api.py
+-rw-rw-r--  2.0 unx    24412 b- defN 22-Aug-17 08:48 shimoku_api_python/api/data_managing_api.py
+-rw-rw-r--  2.0 unx    66227 b- defN 22-Aug-17 08:48 shimoku_api_python/api/explorer_api.py
+-rw-rw-r--  2.0 unx    17143 b- defN 22-Aug-17 08:48 shimoku_api_python/api/file_metadata_api.py
 -rw-rw-r--  2.0 unx      523 b- defN 22-Jan-03 14:16 shimoku_api_python/api/notifications_api.py
 -rw-rw-r--  2.0 unx     7382 b- defN 22-Jan-03 14:16 shimoku_api_python/api/path_metadata_api.py
 -rw-rw-r--  2.0 unx      763 b- defN 22-Jan-10 13:13 shimoku_api_python/api/ping_api.py
--rw-rw-r--  2.0 unx    98937 b- defN 22-Jul-31 09:02 shimoku_api_python/api/plot_api.py
--rw-rw-r--  2.0 unx    15738 b- defN 22-Jul-31 08:33 shimoku_api_python/api/report_metadata_api.py
+-rw-rw-r--  2.0 unx   118246 b- defN 22-Aug-17 08:48 shimoku_api_python/api/plot_api.py
+-rw-rw-r--  2.0 unx    15742 b- defN 22-Aug-17 08:48 shimoku_api_python/api/report_metadata_api.py
 -rw-rw-r--  2.0 unx    10202 b- defN 22-Mar-03 07:32 shimoku_api_python/api/suite_api.py
 -rw-rw-r--  2.0 unx      241 b- defN 22-Jan-03 14:16 shimoku_api_python/api/universe_metadata_api.py
 -rw-rw-r--  2.0 unx    15999 b- defN 22-Mar-03 07:32 shimoku_api_python/api/templates/charts_catalog.py
 -rw-rw-r--  2.0 unx     6577 b- defN 22-Jan-10 13:13 shimoku_api_python/api/templates/shimoku_backoffice.py
--rw-rw-r--  2.0 unx       91 b- defN 22-Jul-31 09:07 shimoku_api_python-0.8.1.dist-info/AUTHORS.rst
--rw-rw-r--  2.0 unx     1081 b- defN 22-Jul-31 09:07 shimoku_api_python-0.8.1.dist-info/LICENSE.txt
--rw-rw-r--  2.0 unx      658 b- defN 22-Jul-31 09:07 shimoku_api_python-0.8.1.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 22-Jul-31 09:07 shimoku_api_python-0.8.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       19 b- defN 22-Jul-31 09:07 shimoku_api_python-0.8.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2301 b- defN 22-Jul-31 09:07 shimoku_api_python-0.8.1.dist-info/RECORD
-24 files, 251354 bytes uncompressed, 43411 bytes compressed:  82.7%
+-rw-rw-r--  2.0 unx       91 b- defN 22-Aug-17 08:50 shimoku_api_python-0.9.dist-info/AUTHORS.rst
+-rw-rw-r--  2.0 unx     1081 b- defN 22-Aug-17 08:50 shimoku_api_python-0.9.dist-info/LICENSE.txt
+-rw-rw-r--  2.0 unx      656 b- defN 22-Aug-17 08:50 shimoku_api_python-0.9.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 22-Aug-17 08:50 shimoku_api_python-0.9.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       19 b- defN 22-Aug-17 08:50 shimoku_api_python-0.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2481 b- defN 22-Aug-17 08:50 shimoku_api_python-0.9.dist-info/RECORD
+26 files, 326188 bytes uncompressed, 56600 bytes compressed:  82.6%
```

## zipnote {}

```diff
@@ -6,14 +6,17 @@
 
 Filename: shimoku_api_python/configuration.py
 Comment: 
 
 Filename: shimoku_api_python/exceptions.py
 Comment: 
 
+Filename: shimoku_api_python/api/ai_api.py
+Comment: 
+
 Filename: shimoku_api_python/api/app_metadata_api.py
 Comment: 
 
 Filename: shimoku_api_python/api/app_type_metadata_api.py
 Comment: 
 
 Filename: shimoku_api_python/api/business_metadata_api.py
@@ -21,14 +24,17 @@
 
 Filename: shimoku_api_python/api/data_managing_api.py
 Comment: 
 
 Filename: shimoku_api_python/api/explorer_api.py
 Comment: 
 
+Filename: shimoku_api_python/api/file_metadata_api.py
+Comment: 
+
 Filename: shimoku_api_python/api/notifications_api.py
 Comment: 
 
 Filename: shimoku_api_python/api/path_metadata_api.py
 Comment: 
 
 Filename: shimoku_api_python/api/ping_api.py
@@ -48,26 +54,26 @@
 
 Filename: shimoku_api_python/api/templates/charts_catalog.py
 Comment: 
 
 Filename: shimoku_api_python/api/templates/shimoku_backoffice.py
 Comment: 
 
-Filename: shimoku_api_python-0.8.1.dist-info/AUTHORS.rst
+Filename: shimoku_api_python-0.9.dist-info/AUTHORS.rst
 Comment: 
 
-Filename: shimoku_api_python-0.8.1.dist-info/LICENSE.txt
+Filename: shimoku_api_python-0.9.dist-info/LICENSE.txt
 Comment: 
 
-Filename: shimoku_api_python-0.8.1.dist-info/METADATA
+Filename: shimoku_api_python-0.9.dist-info/METADATA
 Comment: 
 
-Filename: shimoku_api_python-0.8.1.dist-info/WHEEL
+Filename: shimoku_api_python-0.9.dist-info/WHEEL
 Comment: 
 
-Filename: shimoku_api_python-0.8.1.dist-info/top_level.txt
+Filename: shimoku_api_python-0.9.dist-info/top_level.txt
 Comment: 
 
-Filename: shimoku_api_python-0.8.1.dist-info/RECORD
+Filename: shimoku_api_python-0.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## shimoku_api_python/__init__.py

```diff
@@ -3,15 +3,17 @@
 # import apis into sdk package
 from shimoku_api_python.api.universe_metadata_api import UniverseMetadataApi
 from shimoku_api_python.api.business_metadata_api import BusinessMetadataApi
 from shimoku_api_python.api.app_metadata_api import AppMetadataApi
 from shimoku_api_python.api.app_type_metadata_api import AppTypeMetadataApi
 from shimoku_api_python.api.report_metadata_api import ReportMetadataApi
 from shimoku_api_python.api.data_managing_api import DataManagingApi
+from shimoku_api_python.api.file_metadata_api import FileMetadataApi
 from shimoku_api_python.api.plot_api import PlotApi
+from shimoku_api_python.api.ai_api import AiAPI
 from shimoku_api_python.api.ping_api import PingApi
 
 from shimoku_api_python.client import ApiClient
 # from shimoku_api_python.configuration import Configuration
 
 
 class Client(object):
@@ -26,11 +28,13 @@
 
         self.universe = UniverseMetadataApi(self._api_client)
         self.business = BusinessMetadataApi(self._api_client)
         self.app_type = AppTypeMetadataApi(self._api_client)
         self.app = AppMetadataApi(self._api_client)
         self.report = ReportMetadataApi(self._api_client)
         self.data = DataManagingApi(self._api_client)
+        self.file = FileMetadataApi(self._api_client)
         self.plt = PlotApi(self._api_client)
+        self.ai = AiAPI(self._api_client)
 
     def set_config(self, config={}):
         self.api_client.set_config(config)
```

## shimoku_api_python/client.py

```diff
@@ -259,14 +259,17 @@
             )
         else:
             raise ValueError(
                 "http method must be `GET`, `HEAD`, `OPTIONS`,"
                 " `POST`, `PATCH`, `PUT` or `DELETE`."
             )
 
+    def raw_request(self, **kwargs):
+        return requests.request(**kwargs)
+
     def sanitize_for_serialization(self, obj):
         """Builds a JSON POST object.
         If obj is None, return None.
         If obj is str, int, long, float, bool, return directly.
         If obj is datetime.datetime, datetime.date
             convert to string in iso8601 format.
         If obj is list, sanitize each element in the list.
```

## shimoku_api_python/api/app_metadata_api.py

```diff
@@ -1,18 +1,26 @@
 """"""
 
 from abc import ABC
 from typing import List, Dict, Union
 
-from shimoku_api_python.api.explorer_api import AppExplorerApi
+from shimoku_api_python.api.explorer_api import (
+    AppExplorerApi, MultiCreateApi,
+    CascadeExplorerAPI, CascadeCreateExplorerAPI,
+)
+from shimoku_api_python.exceptions import ApiClientError
 
 
 class AppMetadataApi(AppExplorerApi, ABC):
     """
     """
+    _create_app_type_and_app = MultiCreateApi.create_app_type_and_app
+    # TODO this a prior is in AppExplorerApi why if I remove this line it does not work?
+    _get_app_by_name = CascadeExplorerAPI.get_app_by_name
+    _create_app = CascadeCreateExplorerAPI.create_app
 
     def __init__(self, api_client):
         self.api_client = api_client
 
     def has_app_report(self, business_id: str, app_id: str) -> bool:
         """"""
         reports: List[str] = (
@@ -42,7 +50,24 @@
 
     def show_title(self, business_id: str, app_id: str) -> Dict:
         return self.hide_title(
             business_id=business_id,
             app_id=app_id,
             hide_title=False,
         )
+
+    def get_or_create_app_and_apptype(self, business_id: str, name: str) -> Dict:
+        """Try to create an App and AppType if they exist instead retrieve them"""
+        try:
+            d: Dict[str, Dict] = self._create_app_type_and_app(
+                business_id=business_id,
+                app_type_metadata={'name': name},
+                app_metadata={},
+            )
+            app: Dict = d['app']
+        except ApiClientError:  # Business admin user
+            app: Dict = self._get_app_by_name(business_id=business_id, name=name)
+            if not app:
+                app: Dict = self._create_app(
+                    business_id=business_id, name=name,
+                )
+        return app
```

## shimoku_api_python/api/data_managing_api.py

```diff
@@ -3,28 +3,34 @@
 from typing import List, Dict, Optional, Union, Set
 from itertools import chain
 
 import datetime as dt
 import pandas as pd
 from pandas import DataFrame
 
-from .explorer_api import GetExplorerAPI, DeleteExplorerApi, CreateExplorerAPI
+from .explorer_api import (
+    GetExplorerAPI, DeleteExplorerApi, CreateExplorerAPI,
+    DatasetExplorerApi, ReportDatasetExplorerApi,
+)
 from .report_metadata_api import ReportMetadataApi
 
 
 class DataExplorerApi:
     get_report = ReportMetadataApi.get_report
     _get_report_with_data = GetExplorerAPI._get_report_with_data
     get_report_data = ReportMetadataApi.get_report_data
     _update_report = ReportMetadataApi.update_report
+    _get_report_datasets = ReportDatasetExplorerApi.get_report_datasets
 
     _create_report_entries = CreateExplorerAPI._create_report_entries
-    _delete_report_entries = DeleteExplorerApi.delete_report_entries
+    _create_data_points = DatasetExplorerApi.create_data_points
+    _create_dataset = DatasetExplorerApi.create_dataset
+    _create_reportdataset = ReportDatasetExplorerApi.create_reportdataset
 
-    _get_report_by_external_id = ReportMetadataApi.get_reports_by_external_id
+    _delete_report_entries = DeleteExplorerApi.delete_report_entries
 
 
 class DataValidation:
 
     def __init__(self, api_client):
         self.api_client = api_client
 
@@ -42,18 +48,21 @@
                     'The data you passed is a list that must be '
                     'able to be converted into a pandas dataframe'
                 )
         elif isinstance(data, dict):
             try:
                 df_ = DataFrame(data)
             except Exception:
-                raise ValueError(
-                    'The data you passed is a dict that must be '
-                    'able to be converted into a pandas dataframe'
-                )
+                try:
+                    df_ = DataFrame(data, index=[0])
+                except Exception:
+                    raise ValueError(
+                        'The data you passed is a dict that must be '
+                        'able to be converted into a pandas dataframe'
+                    )
         elif isinstance(data, str):
             try:
                 d: List[Dict] = json.loads(data)
                 df_ = DataFrame(d)
             except Exception:
                 raise ValueError(
                     'The data you passed is a json that must be '
@@ -106,21 +115,43 @@
             raise ValueError('data must be either a list, dict or a json')
 
         try:
             assert sorted(data.keys()) == sorted(vals)
         except AssertionError:
             raise ValueError('data keys must be "name", "value" and "children"')
 
+    def _validate_input_form_data(self, data: Dict):
+        try:
+            assert type(data) == dict
+        except AssertionError:
+            raise ValueError('data must be a dict')
 
-class DataManagingApi(DataExplorerApi, DataValidation):
-    """
-    """
+        try:
+            assert 'fields' in data
+        except AssertionError:
+            raise ValueError('"fields" is not a key in the input data')
 
-    def __init__(self, api_client):
-        self.api_client = api_client
+        try:
+            assert type(data['fields']) == list
+        except AssertionError:
+            raise ValueError('fields must be a list')
+
+        try:
+            assert all(['fields' in field_ for field_ in data['fields']])
+        except AssertionError:
+            raise ValueError('"fields" are not keys in the input data')
+
+        try:
+            assert all([
+                'fieldName' in field__ and 'mapping' in field__
+                for field_ in data['fields']
+                for field__ in field_['fields']
+            ])
+        except AssertionError:
+            raise ValueError('"fieldName" and "mapping" are not keys in the input data')
 
     def _is_report_data_empty(
         self, report_data: Union[List[Dict], str, DataFrame, Dict, List],
     ) -> bool:
         if isinstance(report_data, DataFrame):
             if report_data.empty:
                 return True
@@ -139,14 +170,26 @@
                 return True
         else:
             raise ValueError(
                 f'Data must be a Dictionary, JSON or pandas DataFrame '
                 f'Provided: {type(report_data)}'
             )
 
+
+class DataManagingApi(DataExplorerApi, DataValidation):
+    """This is used for
+    - report.chartData
+    - reportEntry
+
+    For DataSet / Data see: DataSetManagingApi()
+    """
+
+    def __init__(self, api_client):
+        self.api_client = api_client
+
     def _transform_report_data_to_chart_data(
         self, report_data: Union[List[Dict], str, DataFrame, Dict],
     ) -> List[Dict]:
         if isinstance(report_data, DataFrame):
             chart_data: List[Dict] = report_data.to_dict(orient='records')
         elif isinstance(report_data, dict):
             df_: DataFrame = pd.DataFrame(report_data)
@@ -159,14 +202,99 @@
         else:
             raise ValueError(
                 f'Data must be a Dictionary, JSON or pandas DataFrame '
                 f'Provided: {type(report_data)}'
             )
         return chart_data
 
+    def _convert_input_data_to_db_items(
+        self, data: Union[List[Dict], Dict]
+    ) -> Union[List[Dict], Dict]:
+        """Given an input data, for all the keys of the data convert it to
+         a Shimoku body parameter for Data table
+
+          stringField1: String
+          stringField2: String
+          stringField3: String
+          stringField4: String
+          intField1: Float
+          intField2: Float
+          intField3: Float
+          intField4: Float
+          dateField1: AWSDateTime
+          customField1: AWSJSON
+
+        Example
+        ------------
+        input
+            data = [
+             {'product': 'Matcha Latte', '2015': 43.3, '2016': 85.8, '2017': 93.7},
+             {'product': 'Milk Tea', '2015': 83.1, '2016': 73.4, '2017': 55.1},
+             {'product': 'Cheese Cocoa', '2015': 86.4, '2016': 65.2, '2017': 82.5},
+             {'product': 'Walnut Brownie', '2015': 72.4, '2016': 53.9, '2017': 39.1}
+            ]
+
+        intermediate output
+            d = {
+                'product': 'stringField1',
+                '2015': 'intField1',
+                '2016': 'intField2',
+                '2017': 'intField3'
+            }
+
+        output
+            [
+                {'stringField1': 'Matcha Latte',
+                 'intField1': 43.3,
+                 'intField2': 85.8,
+                 'intField3': 93.7
+                 },
+                {'stringField1': 'Milk Tea',
+                 'intField1': 83.1,
+                 'intField2': 73.4,
+                 'intField3': 55.1
+                 },
+                {'stringField1': 'Cheese Cocoa',
+                 'intField1': 86.4,
+                 'intField2': 65.2,
+                 'intField3': 82.5
+                 },
+                {'stringField1': 'Walnut Brownie',
+                 'intField1': 72.4,
+                 'intField2': 53.9,
+                 'intField3': 39.1
+                 }
+            ]
+        """
+        if type(data) == dict:
+            return {'customField1': json.dumps(data)}
+        elif type(data) == list:
+            d = {}
+            str_counter = 0
+            float_counter = 0
+            date_counter = 0
+            for k, v in data[0].items():
+                type_v = type(v)
+                if type_v == str:
+                    str_counter += 1
+                    d.update({k: f'stringField{str_counter}'})
+                elif type_v == float or type_v == int:
+                    float_counter += 1
+                    d.update({k: f'intField{float_counter}'})
+                elif type_v == dt.date or type_v == dt.datetime or type_v == pd.Timestamp:
+                    date_counter += 1
+                    d.update({k: f'dateField{date_counter}'})
+                elif type_v == dict:
+                    d.update({k: f'customField1'})
+                else:
+                    raise ValueError(f'Unknown value type {v} | Type {type_v}')
+            return [{d[k]: v for k, v in datum.items()} for datum in data]
+        else:
+            assert type(data) == list or type(data) == dict
+
     def _convert_dataframe_to_report_entry(
         self, df: DataFrame,
         filter_map: Optional[Dict[str, str]] = None,
         filter_fields: Optional[Dict[str, List[str]]] = None,
         search_columns: Optional[List[str]] = None,
     ) -> List[Dict]:
         """
@@ -237,14 +365,15 @@
             return [
                 {**data_entry, **metadata_entry}
                 for data_entry, metadata_entry in zip(data_entries, metadata_entries)
             ]
         else:
             return data_entries
 
+# TODO pending add append_report_data to free Echarts
     def append_report_data(
         self, business_id: str, app_id: str,
         report_data: Union[List[Dict], str, DataFrame, Dict],
         report_id: Optional[str] = None,
         external_id: Optional[str] = None,
     ) -> None:
         """Having a dataframe of Report
@@ -345,15 +474,16 @@
                 business_id=business_id,
                 app_id=app_id,
                 report_id=report_id,
                 external_id=external_id,
             )
         )
 
-        if report.get('reportType'):
+        report_type: Optional[str] = report.get('reportType')
+        if report_type:
             chart_data_new: List[Dict] = (
                 self._transform_report_data_to_chart_data(report_data)
             )
 
             chart_data: Dict = report.get('chartData')
             if chart_data:
                 chart_data = chart_data + chart_data_new
@@ -391,7 +521,182 @@
 
             self._create_report_entries(
                 business_id=business_id,
                 app_id=app_id,
                 report_id=report_id,
                 items=report_data,
             )
+
+
+class DataSetManagingApi(DataExplorerApi, DataValidation):
+    """
+    """
+
+    def __init__(self, api_client):
+        self.api_client = api_client
+
+# TODO
+    def _convert_dataframe_to_dataset_data(
+        self, df: DataFrame,
+        filter_map: Optional[Dict[str, str]] = None,
+        filter_fields: Optional[Dict[str, List[str]]] = None,
+        search_columns: Optional[List[str]] = None,
+    ) -> List[Dict]:
+        """
+        :param df:
+        :param report_id:
+        :param filter_fields: Example: {
+                'stringField1': ['high', 'medium', 'low'],
+                'stringField2': ['probable', 'improbable'],
+            }
+        :param search_columns:
+        """
+        cols: List[str] = df.columns.tolist()
+
+        if filter_fields:
+            try:
+                assert len(filter_fields) <= 4
+            except AssertionError:
+                raise ValueError(
+                    f'At maximum a table may have 4 different filters | '
+                    f'You provided {len(filter_fields)} | '
+                    f'You provided {filter_fields}'
+                )
+
+            df_ = df.rename(columns=filter_map)
+            metadata_entries: Dict = df_[list(filter_map.values())].to_dict(orient='records')
+        elif search_columns:
+            df_ = df.rename(columns=filter_map)
+            filter_search_map: List = [
+                v for k, v in filter_map.items() if k in search_columns
+            ]
+            metadata_entries: Dict = df_[filter_search_map].to_dict(orient='records')
+        else:
+            data_columns: List[str] = cols
+            metadata_entries: List[Dict] = []
+
+        records: List[Dict] = df.to_dict(orient='records')
+        try:
+            data_entries: List[Dict] = [
+                {'data': json.dumps(d)}
+                for d in records
+            ]
+        except TypeError:
+            # If we have date or datetime values
+            # then we need to convert them to isoformat
+            for datum in records:
+                for k, v in datum.items():
+                    if isinstance(v, dt.date) or isinstance(v, dt.datetime):
+                        datum[k] = v.isoformat()
+
+            data_entries: List[Dict] = [
+                {'data': json.dumps(d)}
+                for d in records
+            ]
+
+        if metadata_entries:
+            try:
+                _ = json.dumps(metadata_entries)
+            except TypeError:
+                # If we have date or datetime values
+                # then we need to convert them to isoformat
+                for datum in metadata_entries:
+                    for k, v in datum.items():
+                        if isinstance(v, dt.date) or isinstance(v, dt.datetime):
+                            datum[k] = v.isoformat()
+
+            # Generate the list of single entries with all
+            # necessary information to be posted
+            return [
+                {**data_entry, **metadata_entry}
+                for data_entry, metadata_entry in zip(data_entries, metadata_entries)
+            ]
+        else:
+            return data_entries
+
+# TODO
+    def append_dataset_data(
+        self, business_id: str, app_id: str,
+        report_data: Union[List[Dict], str, DataFrame, Dict],
+        report_id: Optional[str] = None,
+        external_id: Optional[str] = None,
+    ) -> None:
+        """Having a dataframe of Report
+
+        It is an aggregation, meaning we preserve all previous
+        data and just concatenate the new ones
+        """
+        _ = self._validate_data_is_pandarable(report_data)
+
+        if self._is_report_data_empty(report_data):
+            return
+
+        report: Dict = (
+            self._get_report_with_data(
+                business_id=business_id,
+                app_id=app_id,
+                report_id=report_id,
+                external_id=external_id,
+            )
+        )
+
+        if report.get('reportType'):  # For non-table chart
+            chart_data_new: List[Dict] = (
+                self._transform_report_data_to_chart_data(report_data)
+            )
+
+            chart_data: Dict = report.get('chartData')
+
+            # Data resistance
+            #  check that the column names are the
+            #  same in the data we try to append
+            all_keys: Set[str] = set(chain.from_iterable(chart_data))
+            for d in chart_data_new:
+                try:
+                    assert set(sorted((d.keys()))) == all_keys
+                except AssertionError:
+                    KeyError(
+                        f'The provided data has not the same keys'
+                        f' that the data it tries to append | '
+                        f'Required keys: {all_keys}'
+                    )
+
+            if chart_data:
+                chart_data = chart_data + chart_data_new
+            else:
+                chart_data = chart_data_new
+
+            try:
+                report_data_ = {
+                    'chartData': json.dumps(chart_data),
+                }
+            except TypeError:
+                # If we have date or datetime values
+                # then we need to convert them to isoformat
+                for datum in chart_data:
+                    for k, v in datum.items():
+                        if isinstance(v, dt.date) or isinstance(v, dt.datetime):
+                            datum[k] = v.isoformat()
+
+                report_data_ = {
+                    'chartData': json.dumps(chart_data),
+                }
+
+            self._update_report(
+                business_id=business_id,
+                app_id=app_id,
+                report_id=report_id,
+                report_metadata=report_data_,
+            )
+        else:  # Then it is a table
+            item: Dict = {'reportId': report_id}
+
+            data: List[Dict] = (
+                self.convert_dataframe_to_report_entry(
+                    report_id=report_id, df=report_data,
+                )
+            )
+
+            # TODO we can store batches and go faster than one by one
+            for datum in data:
+                item.update(datum)
+                self.post_report_entry(item)
```

## shimoku_api_python/api/explorer_api.py

```diff
@@ -1,10 +1,10 @@
 """"""
 
-from typing import List, Dict, Optional, Any
+from typing import List, Dict, Optional, Any, Union
 import json
 from time import sleep
 
 from shimoku_api_python.exceptions import ApiClientError
 
 
 class GetExplorerAPI(object):
@@ -48,14 +48,15 @@
         app_data: Dict = (
             self.api_client.query_element(
                 method='GET', endpoint=endpoint, **kwargs
             )
         )
         return app_data
 
+# TODO add new data & dataset logic!
     def _get_report_with_data(
         self,
         business_id: Optional[str] = None,
         app_id: Optional[str] = None,
         report_id: Optional[str] = None,
         external_id: Optional[str] = None,
         **kwargs,
@@ -135,14 +136,53 @@
         )
         # we do not return the chartData in the get_report()
         #  use _get_report_with_data() instead
         if report_data.get('chartData'):
             report_data.pop('chartData')
         return report_data
 
+    def get_dataset(self, business_id: str, dataset_id: str, **kwargs) -> Dict:
+        """Retrieve an specific app_id metadata
+
+        :param business_id: business UUID
+        :param dataset_id: dataset UUID
+        """
+        endpoint: str = f'business/{business_id}/dataset/{dataset_id}'
+        dataset_data: Dict = (
+            self.api_client.query_element(
+                method='GET', endpoint=endpoint, **kwargs
+            )
+        )
+        return dataset_data
+
+    def get_reportdataset(
+            self, business_id: str, app_id: str, report_id: str,
+            reportdataset_id: str, **kwargs
+    ) -> Dict:
+        """Retrieve an specific app_id metadata
+
+        :param business_id: business UUID
+        :param app_id: app UUID
+        :param report_id: report UUID
+        :param reportdataset_id: reportDataSet UUID
+        """
+        endpoint: str = (
+            f'business/{business_id}/'
+            f'app/{app_id}/'
+            f'report/{report_id}/'
+            f'{reportdataset_id}'
+        )
+        dataset_data: Dict = (
+            self.api_client.query_element(
+                method='GET', endpoint=endpoint, **kwargs
+            )
+        )
+        return dataset_data
+
+# TODO add new data & dataset logic!
     def get_report_data(
         self, business_id: str,
         app_id: Optional[str] = None,
         report_id: Optional[str] = None,
         external_id: Optional[str] = None,
     ) -> List[Dict]:
         """"""
@@ -175,14 +215,51 @@
             report_entries: List = [
                 self.api_client.query_element(
                     method='GET', endpoint=endpoint,
                 )
             ]
             return report_entries[0]['items']
 
+    def get_file(
+            self, business_id: Optional[str] = None,
+            app_id: Optional[str] = None,
+            file_id: Optional[str] = None,
+    ) -> bytes:
+        """Retrieve an specific file from an app
+
+        :param business_id: business UUID
+        :param app_id: Shimoku app UUID (only required if the external_id is provided)
+        :param file_id: Shimoku report UUID
+        """
+        endpoint: str = f'business/{business_id}/app/{app_id}/file/{file_id}'
+        file_data: str = self.api_client.query_element(method='GET', endpoint=endpoint)
+
+        try:
+            url: str = file_data['url']
+        except KeyError:
+            raise KeyError(f'Could not GET file')
+
+        file_object = self.api_client.raw_request(
+            **dict(method='GET', url=url)
+        )
+        return file_object.content
+
+    def get_files(
+            self, business_id: Optional[str] = None,
+            app_id: Optional[str] = None,
+    ) -> List[Dict]:
+        """Retrieve an specific file from an app
+
+        :param business_id: business UUID
+        :param app_id: Shimoku app UUID (only required if the external_id is provided)
+        """
+        endpoint: str = f'business/{business_id}/app/{app_id}/files'
+        files: List[Dict] = self.api_client.query_element(method='GET', endpoint=endpoint)
+        return files
+
 
 class CascadeExplorerAPI(GetExplorerAPI):
 
     def __init__(self, api_client):
         super().__init__(api_client)
 
     def get_universe_businesses(self) -> List[Dict]:
@@ -271,14 +348,23 @@
         apps: Optional[List[Dict]] = (
             self.get_business_apps(
                 business_id=business_id,
             )
         )
         return [app['id'] for app in apps]
 
+    def get_business_all_files(self, business_id) -> List[Dict]:
+        """Given a business retrieve all files metadata
+        """
+        apps: List[Dict] = self.get_business_apps(business_id=business_id)
+        files: List[Dict] = list
+        for app in apps:
+            files = files + self.get_files(business_id=business_id, app_id=app['id'])
+        return files
+
     def find_app_by_name_filter(
         self, business_id: str, name: Optional[str] = None,
         normalized_name: Optional[str] = None,
     ) -> Dict:
         """"""
         apps_list: List[Dict] = self.get_business_apps(business_id=business_id)
 
@@ -347,14 +433,84 @@
             self.get_app_reports(
                 business_id=business_id,
                 app_id=app_id,
             )
         )
         return [report['id'] for report in reports]
 
+    def get_report_datasets(
+            self, business_id: str, app_id: str,  report_id: str,
+    ) -> List[Dict]:
+        endpoint: str = (
+            f'business/{business_id}/'
+            f'app/{app_id}/'
+            f'report/{report_id}/'
+            f'reportDataSets'
+        )
+        reportdatasets: List[Dict] = (
+            self.api_client.query_element(
+                endpoint=endpoint, method='GET',
+            )
+        )
+        reportdatasets = reportdatasets.get('items')
+        if not reportdatasets:
+            return []
+
+        data_sets: List[Dict] = []
+        for reportdataset in reportdatasets:
+            endpoint: str = (
+                f'business/{business_id}/'
+                f'dataSet/{reportdataset["dataSetId"]}'
+            )
+            data_set: Dict = (
+                self.api_client.query_element(
+                    endpoint=endpoint, method='GET',
+                )
+            )
+            if data_set.get('item'):
+                data_sets = data_sets + [data_set]
+        return data_sets
+
+    def get_dataset_data(
+            self, business_id: str, dataset_id: str,
+    ) -> List[Dict]:
+        endpoint: str = (
+            f'business/{business_id}/'
+            f'dataSet/{dataset_id}/'
+            f'datas'
+        )
+        datas_raw: List[Dict] = (
+            self.api_client.query_element(
+                endpoint=endpoint, method='GET',
+            )
+        )
+        datas = datas_raw.get('items')
+        if not datas:
+            return []
+        return datas
+
+    def get_report_dataset_data(
+            self, business_id: str, app_id: str, report_id: str,
+    ) -> List[Dict]:
+        """"""
+        report_datasets: List[Dict] = self.get_report_datasets(
+            business_id=business_id, app_id=app_id, report_id=report_id,
+        )
+        data: List = []
+        for report_dataset in report_datasets:
+            dataset_id: str = report_dataset['id']
+            datum: List[Dict] = (
+                self.get_dataset_data(
+                    business_id=business_id,
+                    dataset_id=dataset_id,
+                )
+            )
+            data = data + datum
+        return data
+
     # TODO pending
     def get_report_all_report_entries(self, report_id: str) -> List[str]:
         """Given a report retrieve all reportEntries
 
         :param report_id: app UUID
         """
         raise NotImplementedError
@@ -685,48 +841,24 @@
         else:
             item['hideTitle'] = 'true'
 
         return self.api_client.query_element(
             method='POST', endpoint=endpoint, **{'body_params': item},
         )
 
-    def create_app_from_app_type_normalized_name(self, app_type_name: str) -> Dict:
-        """Create AppType and App if required and return the App component
-        """
-        try:
-            app_type: Dict = self._create_app_type(name=app_type_name)
-        except ValueError:  # It already exists then
-            app_type: Dict = (
-                self._find_app_type_by_name_filter(name=app_type_name)
-            )
-
-        app_type_id: str = app_type['id']
-        apps: Dict = self._get_business_apps(business_id=self.business_id)
-        target_apps = [app for app in apps if app['appType']['id'] == app_type_id]
-
-        if not apps:
-            app: Dict = (
-                self._create_app(
-                    business_id=self.business_id,
-                    app_type_id=app_type_id,
-                )
-            )
-        else:
-            app: Dict = target_apps[0]
-        return app
-
     def create_report(
         self, business_id: str, app_id: str, report_metadata: Dict,
         real_time: bool = False,
     ) -> Dict:
         """Create new Report associated to an AppId
 
         :param business_id:
         :param app_id:
         :param report_metadata: A dict with all the values required to create a report
+        :param real_time: Whether it is real time or not
         """
         def append_fields(item: Dict, field_name: str) -> Dict:
             """Equivalent to
             grid: Optional[str] = report_metadata.get('grid')
             if grid:
                 item['grid'] = grid
             """
@@ -734,31 +866,30 @@
             if field_value is not None:
                 item[field_name] = field_value
             return item
 
         endpoint: str = f'business/{business_id}/app/{app_id}/report'
 
         # These are the mandatory fields
-        title: str = report_metadata['title']
+        # title: str = report_metadata['title']
 
         # These are the mandatory fields
-        item: Dict = {
-            'appId': app_id,
-            'title': title,
-        }
+        item: Dict = {'appId': app_id}
 
+        item: Dict = append_fields(item=item, field_name='title')
         item: Dict = append_fields(item=item, field_name='path')
         item: Dict = append_fields(item=item, field_name='pathOrder')
         item: Dict = append_fields(item=item, field_name='grid')
         item: Dict = append_fields(item=item, field_name='reportType')
         item: Dict = append_fields(item=item, field_name='order')
         item: Dict = append_fields(item=item, field_name='sizeColumns')
         item: Dict = append_fields(item=item, field_name='sizeRows')
         item: Dict = append_fields(item=item, field_name='padding')
         item: Dict = append_fields(item=item, field_name='bentobox')
+        item: Dict = append_fields(item=item, field_name='properties')
 
         if real_time:
             item['subscribe'] = True
 
         # Update items with kwargs
         item.update(report_metadata)
 
@@ -780,14 +911,85 @@
 
         return {
             k: v
             for k, v in report.items()
             if k not in ['chartData', 'owner', 'chartDataItem']  # we do not return the data
         }
 
+    def create_dataset(self, business_id: str) -> Dict:
+        """Create new DataSet associated to a business
+
+        :param business_id:
+        """
+        endpoint: str = f'business/{business_id}/dataSet'
+
+        return self.api_client.query_element(
+            method='POST', endpoint=endpoint, **{'body_params': {}},
+        )
+
+    def create_reportdataset(
+            self, business_id: str, app_id: str, report_id: str,
+            dataset_id: str, dataset_properties: str,
+    ) -> Dict:
+        """Create new dataset associated to a Report
+
+        :param business_id:
+        :param app_id:
+        :param report_id:
+        :param dataset_id:
+        :param dataset_properties: a json with the properties
+        """
+        endpoint: str = (
+            f'business/{business_id}/'
+            f'app/{app_id}/'
+            f'report/{report_id}/'
+            f'reportDataSet'
+        )
+
+        item: Dict = {
+            'reportId': report_id,
+            'dataSetId': dataset_id,
+        }
+
+        if dataset_properties:
+            item['properties'] = dataset_properties
+
+        return self.api_client.query_element(
+            method='POST', endpoint=endpoint, **{'body_params': item},
+        )
+
+    def create_data_points(
+            self, business_id: str, dataset_id: str,
+            items: List[str],
+    ) -> List[Dict]:
+        """Create new row in Data (equivalent to reportEntry)
+
+        :param business_id:
+        :param dataset_id:
+        :param items:
+        """
+        endpoint: str = (
+            f'business/{business_id}/'
+            f'dataSet/{dataset_id}/'
+            f'data'
+        )
+
+        data: List[Dict] = []
+        for item in items:
+            datum: Dict = (
+                self.api_client.query_element(
+                    method='POST', endpoint=endpoint,
+                    **{'body_params': item},
+                )
+            )
+            sleep(.25)
+            data = data + [datum]
+
+        return data
+
     def _create_report_entries(
         self, business_id: str, app_id: str, report_id: str,
         items: List[Dict],
     ) -> List[Dict]:
         """Create new reportEntry associated to a Report
 
         :param business_id:
@@ -811,14 +1013,60 @@
                 )
             )
             sleep(.25)
             report_entries = report_entries + [report_entry]
 
         return report_entries
 
+    def create_file(
+            self, business_id: str, app_id: str,
+            file_metadata: Dict, file_object: bytes,
+    ) -> Dict:
+        """Create new Files associated to an AppId
+
+        Example
+        ------------
+            file_metadata= {
+                name: String
+                fileName: String (It should be normalized)
+                contentType: String (Content type of the file you want to upload. Ex: image/png)
+            }
+
+        :param business_id:
+        :param app_id:
+        :param file_metadata:
+        """
+        endpoint: str = f'business/{business_id}/app/{app_id}/file'
+
+        file_data: str = (
+            self.api_client.query_element(
+                method='POST', endpoint=endpoint,
+                **{'body_params': file_metadata},
+            )
+        )
+        try:
+            url: str = file_data['url']
+        except KeyError:
+            raise KeyError(f'Could not POST file')
+
+        r = self.api_client.raw_request(
+            **dict(
+                method='PUT', url=url, data=file_object,
+                headers={'Content-Type': 'text/csv'},
+            )
+        )
+        try:
+            assert all([r.status_code >= 200,  r.status_code < 300])
+            file_data['Success'] = True
+        except AssertionError:
+            file_data['Success'] = False
+        file_data['Status'] = r.status_code
+        file_data.pop('url')
+        return file_data
+
 
 class UpdateExplorerAPI(CascadeExplorerAPI):
     _find_business_by_name_filter = CascadeExplorerAPI.find_business_by_name_filter
     _find_app_type_by_name_filter = CascadeExplorerAPI.find_app_type_by_name_filter
 
     def __init__(self, api_client):
         self.api_client = api_client
@@ -865,24 +1113,51 @@
         endpoint: str = f'business/{business_id}/app/{app_id}'
         return self.api_client.query_element(
             method='PATCH', endpoint=endpoint,
             **{'body_params': app_metadata},
         )
 
     def update_report(
-        self, business_id: str, app_id: str, report_id: str,
-        report_metadata: Dict,
+            self, business_id: str, app_id: str, report_id: str,
+            report_metadata: Dict,
     ) -> Dict:
         """"""
         endpoint: str = f'business/{business_id}/app/{app_id}/report/{report_id}'
         return self.api_client.query_element(
             method='PATCH', endpoint=endpoint,
             **{'body_params': report_metadata},
         )
 
+    def update_reportdataset(
+            self, business_id: str, app_id: str, report_id: str,
+            reportdataset_id: str, reportdataset_metadata: Dict,
+    ) -> Dict:
+        """"""
+        endpoint: str = (
+            f'business/{business_id}/'
+            f'app/{app_id}/'
+            f'report/{report_id}/'
+            f'{reportdataset_id}'
+        )
+        return self.api_client.query_element(
+            method='PATCH', endpoint=endpoint,
+            **{'body_params': reportdataset_metadata},
+        )
+
+    def update_dataset(
+            self, business_id: str, dataset_id: str,
+            dataset_metadata: Dict,
+    ) -> Dict:
+        """"""
+        endpoint: str = f'business/{business_id}/dataset/{dataset_id}'
+        return self.api_client.query_element(
+            method='PATCH', endpoint=endpoint,
+            **{'body_params': dataset_metadata},
+        )
+
 
 class MultiCascadeExplorerAPI(CascadeExplorerAPI):
 
     def __init__(self, api_client):
         super().__init__(api_client)
 
     # TODO paginate
@@ -925,14 +1200,130 @@
         Having a report_id return the app it belongs to
         """
         app_id: str = self.get_app_id_by_report(report_id=report_id, **kwargs)
         business_id: str = self.get_business_id_by_app(app_id=app_id, **kwargs)
         return business_id
 
 
+class CascadeCreateExplorerAPI(CreateExplorerAPI):
+    update_report = UpdateExplorerAPI.update_report
+
+    def __init__(self, api_client):
+        self.api_client = api_client
+
+    def create_app_from_app_type_normalized_name(self, app_type_name: str) -> Dict:
+        """Create AppType and App if required and return the App component
+        """
+        try:
+            app_type: Dict = self._create_app_type(name=app_type_name)
+        except ValueError:  # It already exists then
+            app_type: Dict = (
+                self._find_app_type_by_name_filter(name=app_type_name)
+            )
+
+        app_type_id: str = app_type['id']
+        apps: Dict = self._get_business_apps(business_id=self.business_id)
+        target_apps = [app for app in apps if app['appType']['id'] == app_type_id]
+
+        if not apps:
+            app: Dict = (
+                self._create_app(
+                    business_id=self.business_id,
+                    app_type_id=app_type_id,
+                )
+            )
+        else:
+            app: Dict = target_apps[0]
+        return app
+
+    def create_report_and_dataset(
+        self, business_id: str, app_id: str,
+        report_metadata: Dict,
+        items: Union[List[str], Dict],
+        report_properties: Dict,
+        report_dataset_properties: Optional[Dict] = None,
+        sort: Optional[Dict] = None,
+        real_time: bool = False,
+    ) -> Dict[str, Union[Dict, List[Dict]]]:
+        """
+        Example
+        -------
+        sort = {
+            'field': 'date'
+            'direction': 'asc',
+        }
+
+        1. Create a report
+        2. Create a dataset
+        3. Create data associated to a dataset
+        4. Associate dataset and report through reportDataSet
+        """
+        report: Dict = self.create_report(
+            business_id=business_id,
+            app_id=app_id,
+            report_metadata=report_metadata,
+            real_time=real_time,
+        )
+
+        dataset: Dict = self.create_dataset(business_id=business_id)
+        dataset_id: str = dataset['id']
+
+        if type(items) == list:  # ECHARTS2
+            items_keys: Optional[List[str]] = list(items[0].keys())
+            report_dataset_properties = {'mapping': items_keys}
+            if sort:
+                report_dataset_properties.update(sort)
+        elif type(items) == dict:  # FORM
+            items_keys: Optional[List[str]] = None
+            items = [items]
+            try:
+                assert report_dataset_properties is not None
+            except AssertionError:
+                raise ValueError(
+                    'report_dataset_properties is required if items is a dict'
+                )
+        else:
+            raise ValueError('items must be a list or dict')
+
+        report_dataset: Dict = self.create_reportdataset(
+            business_id=business_id,
+            app_id=app_id,
+            report_id=report['id'],
+            dataset_id=dataset_id,
+            dataset_properties=json.dumps(report_dataset_properties),
+        )
+
+        data: List[Dict] = self.create_data_points(
+            business_id=business_id,
+            dataset_id=dataset_id,
+            items=items,
+        )
+
+        # Syntax to be accepted by the FrontEnd
+        options_dataset_id: str = '#{' + f'{report_dataset["id"]}' + '}'
+        report_properties['dataset'] = {'source': options_dataset_id}
+        if items_keys is not None:  # ECHARTS2
+            report_properties['dimensions']: items_keys
+        report_properties = {'properties': json.dumps({'option': report_properties})}
+
+        report: Dict = self.update_report(
+            business_id=business_id,
+            app_id=app_id,
+            report_id=report['id'],
+            report_metadata=report_properties,
+        )
+
+        return {
+            'report': report,
+            'dataset': dataset,
+            'report_dataset': report_dataset,
+            'data': data,
+        }
+
+
 class DeleteExplorerApi(MultiCascadeExplorerAPI, UpdateExplorerAPI):
     """Get Businesses, Apps, Paths and Reports in any possible combination
     """
 
     def __init__(self, api_client):
         super().__init__(api_client)
 
@@ -1015,14 +1406,38 @@
 
         endpoint: str = f'business/{business_id}/app/{app_id}/report/{report_id}'
         result: Dict = self.api_client.query_element(
             method='DELETE', endpoint=endpoint
         )
         return result
 
+    def delete_reportdataset(
+            self, business_id: str, app_id: str,
+            report_id: str, reportdataset_id: str,
+    ) -> Dict:
+        """"""
+        endpoint: str = (
+            f'business/{business_id}/'
+            f'app/{app_id}/'
+            f'report/{report_id}/'
+            f'{reportdataset_id}'
+        )
+        result: Dict = self.api_client.query_element(
+            method='DELETE', endpoint=endpoint
+        )
+        return result
+
+    def delete_dataset(self, business_id: str, dataset_id: str) -> Dict:
+        """"""
+        endpoint: str = f'business/{business_id}/dataset/{dataset_id}'
+        result: Dict = self.api_client.query_element(
+            method='DELETE', endpoint=endpoint
+        )
+        return result
+
     def delete_report_entries(
         self, business_id: str, app_id: str, report_id: str,
     ) -> None:
         """Delete a Report, relocating reports underneath to avoid errors
         """
         report_entries: List[Dict] = (
             self.get_report_data(
@@ -1040,26 +1455,42 @@
                 f'report/{report_id}/'
                 f'reportEntry/{report_entry_id}'
             )
             _: Dict = self.api_client.query_element(
                 method='DELETE', endpoint=endpoint
             )
 
+    def delete_file(
+        self, business_id: str, app_id: str, file_id: str,
+    ) -> Dict:
+        """Delete a file
+        """
+        endpoint: str = f'business/{business_id}/app/{app_id}/file/{file_id}'
+        result: Dict = self.api_client.query_element(method='DELETE', endpoint=endpoint)
+        return result
+
 
+# TODO los siguientes puntos:
+#  . Si elimino (delete) un report se eliminan sus reportdataset?
+#  . Tengo función cascade para dado un report coger (GET) todos los reportdataset?
+#  . No puedo crear data sin un dataset asociado, esto es así?
+#  . Si elimino (delete) un dataset se eliminan sus data ?
+#  . Tengo función cascade para dado un dataset coger (GET) todos los data?
 class MultiDeleteApi:
     """Get Businesses, Apps, Paths and Reports in any possible combination
     """
     _get_business = GetExplorerAPI.get_business
     _get_app_type = GetExplorerAPI.get_app_type
     _get_app = GetExplorerAPI.get_app
 
     _delete_business = DeleteExplorerApi.delete_business
     _delete_app = DeleteExplorerApi.delete_app
     _delete_app_type = DeleteExplorerApi.delete_app_type
     _delete_report = DeleteExplorerApi.delete_report
+    _delete_dataset = DeleteExplorerApi.delete_dataset
 
     def __init__(self):
         return
 
     def _delete_business_and_app_type(
         self, business_id: str, app_type_id: str
     ):
@@ -1141,25 +1572,35 @@
             return {}
         except Exception as e_atg:
             raise ValueError(
                 f'{e_atg} | App was not deleted | '
                 f'app_id: {app_id}'
             )
 
+    def delete_report_and_dataset(
+            self, business_id: str, app_id: str, report_id: str, dataset_id: str,
+    ):
+        self._delete_report(
+            business_id=business_id,
+            app_id=app_id,
+            report_id=report_id
+        )
+        self._delete_dataset(business_id=business_id, dataset_id=dataset_id)
+
 
 class MultiCreateApi(MultiDeleteApi):
     """If some upper level elements are not created it does it
     """
     _get_universe_app_types = CascadeExplorerAPI.get_universe_app_types
     _get_app_by_type = CascadeExplorerAPI.get_app_by_type
 
-    _create_business = CreateExplorerAPI.create_business
-    _create_app_type = CreateExplorerAPI.create_app_type
-    _create_app = CreateExplorerAPI.create_app
-    _create_report = CreateExplorerAPI.create_report
+    _create_business = CascadeCreateExplorerAPI.create_business
+    _create_app_type = CascadeCreateExplorerAPI.create_app_type
+    _create_app = CascadeCreateExplorerAPI.create_app
+    _create_report = CascadeCreateExplorerAPI.create_report
 
     def __init__(self):
         super().__init__()
 
     def create_business_and_app(
         self, app_type_id: str, business_name: str, app_metadata: Dict,
     ) -> Dict[str, Dict]:
@@ -1430,15 +1871,15 @@
 
 
 class BusinessExplorerApi:
     """"""
     get_business = GetExplorerAPI.get_business
     get_universe_businesses = CascadeExplorerAPI.get_universe_businesses
     _find_business_by_name_filter = CascadeExplorerAPI.find_business_by_name_filter
-    create_business = CreateExplorerAPI.create_business
+    create_business = CascadeCreateExplorerAPI.create_business
     update_business = UpdateExplorerAPI.update_business
 
     get_business_apps = CascadeExplorerAPI.get_business_apps
     get_business_app_ids = CascadeExplorerAPI.get_business_app_ids
     get_business_all_apps_with_filter = CascadeExplorerAPI.get_business_apps_with_filter
 
     delete_business = DeleteExplorerApi.delete_business
@@ -1448,26 +1889,26 @@
     """"""
     _create_normalized_name = CreateExplorerAPI._create_normalized_name
     _create_key_name = CreateExplorerAPI._create_key_name
 
     get_app_type = GetExplorerAPI.get_app_type
     get_universe_app_types = CascadeExplorerAPI.get_universe_app_types
     _find_app_type_by_name_filter = CascadeExplorerAPI.find_app_type_by_name_filter
-    create_app_type = CreateExplorerAPI.create_app_type
+    create_app_type = CascadeCreateExplorerAPI.create_app_type
     update_app_type = UpdateExplorerAPI.update_app_type
 
     delete_app_type = DeleteExplorerApi.delete_app_type
 
 
 class AppExplorerApi:
     _create_normalized_name = CreateExplorerAPI._create_normalized_name
     _create_key_name = CreateExplorerAPI._create_key_name
 
     get_app = GetExplorerAPI.get_app
-    create_app = CreateExplorerAPI.create_app
+    create_app = CascadeCreateExplorerAPI.create_app
     update_app = UpdateExplorerAPI.update_app
 
     _get_business_apps = CascadeExplorerAPI.get_business_apps
     get_business_apps = CascadeExplorerAPI.get_business_apps
     find_app_by_name_filter = CascadeExplorerAPI.find_app_by_name_filter
     get_app_reports = CascadeExplorerAPI.get_app_reports
     get_app_report_ids = CascadeExplorerAPI.get_app_report_ids
@@ -1485,26 +1926,68 @@
 
     get_report = GetExplorerAPI.get_report
     get_report_data = GetExplorerAPI.get_report_data
     _get_report_with_data = GetExplorerAPI._get_report_with_data
 
     _get_app_reports = CascadeExplorerAPI.get_app_reports
 
-    create_report = CreateExplorerAPI.create_report
+    create_report = CascadeCreateExplorerAPI.create_report
     create_app_and_report = MultiCreateApi.create_app_and_report
 
     update_report = UpdateExplorerAPI.update_report
 
     get_business_id_by_report = MultiCascadeExplorerAPI.get_business_id_by_report
 
     delete_report = DeleteExplorerApi.delete_report
 
 
+class DatasetExplorerApi:
+
+    get_dataset = GetExplorerAPI.get_dataset
+
+    get_dataset_data = CascadeExplorerAPI.get_dataset_data
+
+    create_data_points = CreateExplorerAPI.create_data_points
+    create_dataset = CascadeCreateExplorerAPI.create_dataset
+
+    update_dataset = UpdateExplorerAPI.update_dataset
+
+    delete_dataset = DeleteExplorerApi.delete_dataset
+
+
+class ReportDatasetExplorerApi:
+
+    get_reportdataset = GetExplorerAPI.get_reportdataset
+    get_report_datasets = CascadeExplorerAPI.get_report_datasets
+    get_report_dataset_data = CascadeExplorerAPI.get_report_dataset_data
+
+    create_reportdataset = CascadeCreateExplorerAPI.create_reportdataset
+    create_report_and_dataset = CascadeCreateExplorerAPI.create_report_and_dataset
+
+    update_reportdataset = UpdateExplorerAPI.update_reportdataset
+
+    delete_reportdataset = DeleteExplorerApi.delete_reportdataset
+    delete_report_and_dataset = MultiDeleteApi.delete_report_and_dataset
+
+
+class FileExplorerApi:
+    _get_file = GetExplorerAPI.get_file
+    get_files = GetExplorerAPI.get_files
+
+    _create_file = CreateExplorerAPI.create_file
+
+    _delete_file = DeleteExplorerApi.delete_file
+
+    _get_business_apps = CascadeExplorerAPI.get_business_apps
+    _get_app_by_name = CascadeExplorerAPI.get_app_by_name
+    get_business_apps = CascadeExplorerAPI.get_business_apps
+
+
 class ExplorerApi(
-    CreateExplorerAPI,
+    CascadeCreateExplorerAPI,
     DeleteExplorerApi,
 ):
     """Get Businesses, Apps, Paths and Reports in any possible combination
     """
 
     def __init__(self, api_client):
         super().__init__(api_client)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## shimoku_api_python/api/plot_api.py

```diff
@@ -1,25 +1,30 @@
 """"""
 from sys import stdout
 from typing import List, Dict, Optional, Union, Tuple, Any, Iterable
 import logging
 import json
 from itertools import product
 
+import json5
+import datetime as dt
 import pandas as pd
 from pandas import DataFrame
 
 from shimoku_api_python.exceptions import ApiClientError
 from .data_managing_api import DataValidation
 from .explorer_api import (
     BusinessExplorerApi, CreateExplorerAPI, CascadeExplorerAPI,
-    MultiCreateApi, AppExplorerApi, ReportExplorerApi,
+    CascadeCreateExplorerAPI, MultiCreateApi,
+    AppExplorerApi, ReportExplorerApi,
     DeleteExplorerApi, UniverseExplorerApi,
+    ReportDatasetExplorerApi, DatasetExplorerApi,
 )
 from .data_managing_api import DataManagingApi
+from .app_metadata_api import AppMetadataApi
 from .app_type_metadata_api import AppTypeMetadataApi
 
 
 logger = logging.getLogger(__name__)
 logger.setLevel(logging.INFO)
 logging.basicConfig(
     stream=stdout,
@@ -42,40 +47,48 @@
     update_report = ReportExplorerApi.update_report
     get_report_data = ReportExplorerApi.get_report_data
 
     _find_app_by_name_filter = CascadeExplorerAPI.find_app_by_name_filter
     _find_app_type_by_name_filter = (
         CascadeExplorerAPI.find_app_type_by_name_filter
     )
-    # TODO this shit has to be fixed
+    # TODO this shit (methods with underscore _* and *) has to be fixed
     get_universe_app_types = CascadeExplorerAPI.get_universe_app_types
     _get_universe_app_types = CascadeExplorerAPI.get_universe_app_types
     _get_app_reports = CascadeExplorerAPI.get_app_reports
     _get_app_by_type = CascadeExplorerAPI.get_app_by_type
     _get_app_by_name = CascadeExplorerAPI.get_app_by_name
     _get_app_by_url = CascadeExplorerAPI.get_app_by_url
     _find_business_by_name_filter = CascadeExplorerAPI.find_business_by_name_filter
 
+    create_report = CreateExplorerAPI.create_report
     _create_report = CreateExplorerAPI.create_report
     _create_app_type = CreateExplorerAPI.create_app_type
     _create_normalized_name = CreateExplorerAPI._create_normalized_name
     _create_key_name = CreateExplorerAPI._create_key_name
     _create_app = CreateExplorerAPI.create_app
     _create_business = CreateExplorerAPI.create_business
+    create_dataset = CascadeCreateExplorerAPI.create_dataset
+    create_reportdataset = ReportDatasetExplorerApi.create_reportdataset
+    _create_report_and_dataset = ReportDatasetExplorerApi.create_report_and_dataset
+    create_data_points = DatasetExplorerApi.create_data_points
 
     _get_app_type_by_name = AppTypeMetadataApi.get_app_type_by_name
+    _get_or_create_app_and_apptype = AppMetadataApi.get_or_create_app_and_apptype
 
     _update_report_data = DataManagingApi.update_report_data
     _append_report_data = DataManagingApi.append_report_data
     _transform_report_data_to_chart_data = DataManagingApi._transform_report_data_to_chart_data
+    _convert_input_data_to_db_items = DataManagingApi._convert_input_data_to_db_items
     _is_report_data_empty = DataManagingApi._is_report_data_empty
     _convert_dataframe_to_report_entry = DataManagingApi._convert_dataframe_to_report_entry
     _create_report_entries = DataManagingApi._create_report_entries
 
     _validate_table_data = DataValidation._validate_table_data
+    _validate_input_form_data = DataValidation._validate_input_form_data
     _validate_tree_data = DataValidation._validate_tree_data
     _validate_data_is_pandarable = DataValidation._validate_data_is_pandarable
 
     _create_app_type_and_app = MultiCreateApi.create_app_type_and_app
 
     _delete_report = DeleteExplorerApi.delete_report
     _delete_app = DeleteExplorerApi.delete_app
@@ -83,18 +96,29 @@
 
 
 class BasePlot(PlotAux):
 
     def __init__(self, api_client, **kwargs):
         self.api_client = api_client
 
-        if kwargs.get('business_id'):
-            self.business_id: Optional[str] = kwargs['business_id']
-        else:
-            self.business_id: Optional[str] = None
+    # TODO this method goes somewhere else (scripting tools?)
+    @staticmethod
+    def _convert_to_json(items: List[Dict]) -> str:
+        try:
+            items_str: str = json.dumps(items)
+        except TypeError:
+            # If we have date or datetime values
+            # then we need to convert them to isoformat
+            for datum in items:
+                for k, v in datum.items():
+                    if isinstance(v, dt.date) or isinstance(v, dt.datetime):
+                        datum[k] = v.isoformat()
+
+            items_str: str = json.dumps(items)
+        return items_str
 
     @staticmethod
     def _validate_filters(filters: Dict) -> None:
         # Check the filters is built properly
         try:
             if filters.get('update_filter_type'):
                 cols: List[str] = ['row', 'column', 'filter_cols', 'update_filter_type']
@@ -136,14 +160,82 @@
                 raise ValueError('bentobox_data bentoboxSizeColumns must be a positive integer')
         if bentobox_data.get('bentoboxSizeRows'):
             try:
                 assert bentobox_data['bentoboxSizeRows'] > 0
             except AssertionError:
                 raise ValueError('bentobox_data bentoboxSizeColumns must be a positive integer')
 
+    @staticmethod
+    def _clean_menu_path(menu_path: str) -> Tuple[str, str]:
+        """Break the menu path in the apptype or app normalizedName
+        and the path normalizedName if any"""
+        # remove empty spaces
+        menu_path: str = menu_path.strip()
+        # replace "_" for www protocol it is not good
+        menu_path = menu_path.replace('_', '-')
+
+        try:
+            assert len(menu_path.split('/')) <= 2  # we allow only one level of path
+        except AssertionError:
+            raise ValueError(
+                f'We only allow one subpath in your request | '
+                f'you introduced {menu_path} it should be maximum '
+                f'{"/".join(menu_path.split("/")[:1])}'
+            )
+
+        # Split AppType or App Normalized Name
+        normalized_name: str = menu_path.split('/')[0]
+        name: str = (
+            ' '.join(normalized_name.split('-'))
+        )
+
+        try:
+            path_normalized_name: str = menu_path.split('/')[1]
+            path_name: str = (
+                ' '.join(path_normalized_name.split('-'))
+            )
+        except IndexError:
+            path_name = None
+
+        return name, path_name
+
+    @staticmethod
+    def _fill_report_metadata(
+            path_name: str, report_metadata: Dict,
+            order: Optional[int] = None,
+            rows_size: Optional[int] = None,
+            cols_size: Optional[int] = None,
+            padding: Optional[str] = None,
+    ) -> Dict:
+        if order is not None and rows_size and cols_size:
+            report_metadata['order'] = order
+            report_metadata['sizeRows'] = rows_size
+            report_metadata['sizeColumns'] = cols_size
+
+        if padding:
+            report_metadata['sizePadding'] = padding
+
+        if order is not None:  # elif order fails when order = 0!
+            report_metadata['order'] = order
+        elif report_metadata.get('grid'):
+            report_metadata['order'] = 0
+        else:
+            raise ValueError(
+                'Row and Column or Order must be specified to overwrite a report'
+            )
+
+        report_metadata.update({'path': path_name})
+
+        if report_metadata.get('dataFields'):
+            report_metadata['dataFields'] = (
+                json.dumps(report_metadata['dataFields'])
+            )
+
+        return report_metadata
+
     def _find_target_reports(
             self, menu_path: str,
             grid: Optional[str] = None,
             order: Optional[int] = None,
             component_type: Optional[str] = None,
             by_component_type: bool = True,
     ) -> List[Dict]:
@@ -241,47 +333,14 @@
         ]
 
         if path_order:
             return min(path_order)
         else:
             return order_temp + 1
 
-    def _clean_menu_path(self, menu_path: str) -> Tuple[str, str]:
-        """Break the menu path in the apptype or app normalizedName
-        and the path normalizedName if any"""
-        # remove empty spaces
-        menu_path: str = menu_path.strip()
-        # replace "_" for www protocol it is not good
-        menu_path = menu_path.replace('_', '-')
-
-        try:
-            assert len(menu_path.split('/')) <= 2  # we allow only one level of path
-        except AssertionError:
-            raise ValueError(
-                f'We only allow one subpath in your request | '
-                f'you introduced {menu_path} it should be maximum '
-                f'{"/".join(menu_path.split("/")[:1])}'
-            )
-
-        # Split AppType or App Normalized Name
-        normalized_name: str = menu_path.split('/')[0]
-        name: str = (
-            ' '.join(normalized_name.split('-'))
-        )
-
-        try:
-            path_normalized_name: str = menu_path.split('/')[1]
-            path_name: str = (
-                ' '.join(path_normalized_name.split('-'))
-            )
-        except IndexError:
-            path_name = None
-
-        return name, path_name
-
     def _create_chart(
             self, data: Union[str, DataFrame, List[Dict]],
             menu_path: str, report_metadata: Dict,
             order: Optional[int] = None,
             rows_size: Optional[int] = None,
             cols_size: Optional[int] = None,
             padding: Optional[int] = None,
@@ -294,62 +353,30 @@
         :param report_metadata:
         :param row: Only required for Overwrite
         :param column: Only required for Overwrite
         :param report_type: Only required for Overwrite
         :param overwrite: Whether to Update (delete) any report in
             the same menu_path and grid position or not
         """
-        if order is not None and rows_size and cols_size:
-            report_metadata['order'] = order
-            report_metadata['sizeRows'] = rows_size
-            report_metadata['sizeColumns'] = cols_size
-
-        if padding:
-            report_metadata['sizePadding'] = padding
-
         name, path_name = self._clean_menu_path(menu_path=menu_path)
 
-        try:
-            d: Dict[str, Dict] = self._create_app_type_and_app(
-                business_id=self.business_id,
-                app_type_metadata={'name': name},
-                app_metadata={},
-            )
-            app: Dict = d['app']
-        except ApiClientError:  # Business admin user
-            app: Dict = self._get_app_by_name(business_id=self.business_id, name=name)
-            if not app:
-                app: Dict = self._create_app(
-                    business_id=self.business_id, name=name,
-                )
+        report_metadata: Dict = self._fill_report_metadata(
+            report_metadata=report_metadata, path_name=path_name,
+            order=order, rows_size=rows_size, cols_size=cols_size, padding=padding,
+        )
 
+        app = self._get_or_create_app_and_apptype(business_id=self.business_id, name=name)
         app_id: str = app['id']
 
-        if order is not None:  # elif order fails when order = 0!
-            kwargs = {'order': order}
-        elif report_metadata.get('grid'):
-            kwargs = {'grid': report_metadata.get('grid'), 'order': 0}
-        else:
-            raise ValueError(
-                'Row and Column or Order must be specified to overwrite a report'
-            )
-
-        report_metadata.update({'path': path_name})
-        report_metadata.update(kwargs)
-
-        if report_metadata.get('dataFields'):
-            report_metadata['dataFields'] = (
-                json.dumps(report_metadata['dataFields'])
-            )
-
         if overwrite:
             self.delete(
                 menu_path=menu_path,
                 by_component_type=False,
-                **kwargs
+                order=report_metadata.get('order'),
+                grid=report_metadata.get('grid'),
             )
 
         report: Dict = self._create_report(
             business_id=self.business_id,
             app_id=app_id,
             report_metadata=report_metadata,
             real_time=real_time,
@@ -389,14 +416,15 @@
             title: Optional[str] = None,  # second layer
             subtitle: Optional[str] = None,
             x_axis_name: Optional[str] = None,
             y_axis_name: Optional[str] = None,
             option_modifications: Optional[Dict] = None,  # third layer
             filters: Optional[Dict] = None,
             overwrite: bool = True,
+            report_metadata: Optional[Dict] = None,
             bentobox_data: Optional[Dict] = None,
     ) -> str:
         """For Linechart, Barchart, Stocklinechart, Scatter chart, and alike
 
         Example
         -------------------
         input
@@ -476,19 +504,27 @@
             title='', subtitle=subtitle,
             x_axis_name=x_axis_name,
             y_axis_name=y_axis_name,
             option_modifications=option_modifications,
         )
         data_fields['type'] = echart_type
 
-        report_metadata: Dict = {
-            'reportType': 'ECHARTS',
-            'dataFields': data_fields,
-            'title': title,
-        }
+        if report_metadata:
+            if not report_metadata.get('reportType'):
+                report_metadata['reportType'] = 'ECHARTS'
+            if not report_metadata.get('dataFields'):
+                report_metadata['dataFields'] = data_fields
+            if not report_metadata.get('title'):
+                report_metadata['title'] = 'title'
+        else:
+            report_metadata: Dict = {
+                'reportType': 'ECHARTS',
+                'dataFields': data_fields,
+                'title': title,
+            }
 
         if bentobox_data:
             self._validate_bentobox(bentobox_data)
             report_metadata['bentobox'] = json.dumps(bentobox_data)
 
         if row and column:
             report_metadata['grid'] = f'{row}, {column}'
@@ -779,15 +815,20 @@
         :param apps_order: example {'test': 0, 'more-test': 1}
         """
         apps: List[Dict] = self.get_business_apps(business_id=self.business_id)
 
         for app in apps:
             app_id: str = app['id']
             app_normalized_name_: str = app.get('normalizedName')
+            app_name_: str = app.get('name')
             new_app_order: Union[str, int] = apps_order.get(app_normalized_name_)
+
+            if new_app_order is None:  # try with the non normalized name too
+                new_app_order: Union[str, int] = apps_order.get(app_name_)
+
             if new_app_order:
                 self._update_app(
                     business_id=self.business_id,
                     app_id=app_id,
                     app_metadata={'order': int(new_app_order)},
                 )
 
@@ -816,17 +857,24 @@
                 app_id=app_id,
             )
 
             for report in reports:
                 raw_path_name: str = report.get('path')
                 if not raw_path_name:
                     continue
-                path_name: str = '-'.join(report.get('path').split(' '))
+
+                original_path_name: str = report.get('path')
+                path_name: str = '-'.join(original_path_name.split(' ')).lower()
                 menu_path_: str = f'{app_normalized_name}/{path_name}'
                 new_path_order: Union[str, int] = paths_order.get(menu_path_)
+
+                if new_path_order is None:
+                    menu_path_: str = f'{app_normalized_name}/{original_path_name}'
+                    new_path_order: Union[str, int] = paths_order.get(menu_path_)
+
                 if new_path_order:
                     self.update_report(
                         business_id=self.business_id,
                         app_id=app_id,
                         report_id=report['id'],
                         report_metadata={'pathOrder': int(new_path_order)},
                     )
@@ -879,15 +927,72 @@
             self._append_report_data(
                 business_id=self.business_id,
                 app_id=report['appId'],
                 report_id=report['id'],
                 report_data=df,
             )
 
-    # TODO move part of it to get_reports_by_path_grid_and_type() in report_metadata_api.py
+    def _create_dataset_charts(
+            self, menu_path: str, order: int,
+            rows_size: int, cols_size: int,
+            force_custom_field: bool = False,
+            data: Optional[Union[str, DataFrame, List[Dict]]] = None,
+            padding: Optional[str] = None,
+            report_type: str = 'ECHARTS2',
+            overwrite: bool = True,
+            bentobox_data: Optional[Dict] = None,
+            options: Optional[Dict] = None,
+            report_dataset_properties: Optional[Dict] = None,
+            sort: Optional[Dict] = None,
+            real_time: bool = False,
+    ) -> Dict[str, Union[Dict, List[Dict]]]:
+        # TODO ojo debería no ser solo data tabular!!
+        df: pd.DataFrame = self._validate_data_is_pandarable(data)
+
+        report_metadata: Dict = {'reportType': report_type}
+
+        if bentobox_data:
+            self._validate_bentobox(bentobox_data)
+            report_metadata['bentobox'] = json.dumps(bentobox_data)
+
+        name, path_name = self._clean_menu_path(menu_path=menu_path)
+
+        report_metadata: Dict = self._fill_report_metadata(
+            report_metadata=report_metadata, path_name=path_name,
+            order=order, rows_size=rows_size, cols_size=cols_size, padding=padding,
+        )
+
+        app = self._get_or_create_app_and_apptype(business_id=self.business_id, name=name)
+        app_id: str = app['id']
+
+        if overwrite:
+            self.delete(
+                menu_path=menu_path,
+                by_component_type=False,
+                order=report_metadata.get('order'),
+                grid=report_metadata.get('grid'),
+            )
+
+        items: List[Dict] = self._transform_report_data_to_chart_data(report_data=df)
+
+        if force_custom_field and len(items) == 1:  # 'FORM'
+            items: Dict = self._convert_input_data_to_db_items(items[0])
+        else:  # 'ECHARTS2'
+            items: List[str] = self._convert_input_data_to_db_items(items)
+
+        return self._create_report_and_dataset(
+            business_id=self.business_id, app_id=app_id,
+            report_metadata=report_metadata,
+            items=items,
+            report_properties=options,
+            report_dataset_properties=report_dataset_properties,
+            sort=sort,
+            real_time=real_time,
+        )
+
     def delete(
             self, menu_path: str,
             grid: Optional[str] = None,
             order: Optional[int] = None,
             row: Optional[int] = None,
             column: Optional[int] = None,
             component_type: Optional[str] = None,
@@ -959,14 +1064,304 @@
         else:
             if '/' not in menu_path:
                 self._delete_app(
                     business_id=self.business_id,
                     app_id=app_id,
                 )
 
+    # TODO pending add append_report_data to free Echarts
+    def free_echarts(
+            self, menu_path: str,
+            data: Optional[Union[str, DataFrame, List[Dict]]] = None,
+            options: Optional[Dict] = None,
+            raw_options: Optional[Dict] = None,
+            sort: Optional[Dict] = None,
+            order: Optional[int] = None,
+            rows_size: Optional[int] = None,
+            cols_size: int = 12,
+            padding: Optional[List[int]] = None,
+            overwrite: bool = True,
+            filters: Optional[Dict] = None,
+            bentobox_data: Optional[Dict] = None,
+            real_time: bool = False,
+    ):
+        """
+        Example
+        -------------
+        sort = {
+            'field': 'date'
+            'direction': 'asc',
+        }
+
+        :param data:
+        :param options: eCharts options of the type {'options': options}
+        :param raw_options: eCharts copy paste options of the type {'options': options}
+        :param menu_path:
+        :param sort:
+        :param order:
+        :param rows_size:
+        :param cols_size:
+        :param padding:
+        :param overwrite:
+        :param filters:
+        :param bentobox_data:
+        :param real_time:
+        """
+
+        def transform_dict_js_to_py(options_str: str):
+            """https://discuss.dizzycoding.com/how-to-convert-raw-javascript-object-to-python-dictionary/"""
+            options_str = options_str.replace('\n', '')
+            options_str = options_str.replace(';', '')
+            return json5.loads(options_str)
+
+        def retrieve_data_from_options(options_: Dict) -> Union[Dict, List]:
+            """Retrieve data from eCharts options
+
+            Example
+            -----------
+            input options = {'title': {'text': 'Stacked Area Chart'},
+                 'tooltip': {'trigger': 'axis',
+                  'axisPointer': {'type': 'cross', 'label': {'backgroundColor': '#6a7985'}}},
+                 'legend': {'data': ['Email',
+                   'Union Ads',
+                   'Video Ads',
+                   'Direct',
+                   'Search Engine']},
+                 'toolbox': {'feature': {'saveAsImage': {}}},
+                 'grid': {'left': '3%', 'right': '4%', 'bottom': '3%', 'containLabel': True},
+                 'xAxis': [{'type': 'category',
+                   'boundaryGap': False,
+                   'data': ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']}],
+                 'yAxis': [{'type': 'value'}],
+                 'series': [{'name': 'Email',
+                   'type': 'line',
+                   'stack': 'Total',
+                   'areaStyle': {},
+                   'emphasis': {'focus': 'series'},
+                   'data': [120, 132, 101, 134, 90, 230, 210]},
+                  {'name': 'Union Ads',
+                   'type': 'line',
+                   'stack': 'Total',
+                   'areaStyle': {},
+                   'emphasis': {'focus': 'series'},
+                   'data': [220, 182, 191, 234, 290, 330, 310]},
+                  {'name': 'Video Ads',
+                   'type': 'line',
+                   'stack': 'Total',
+                   'areaStyle': {},
+                   'emphasis': {'focus': 'series'},
+                   'data': [150, 232, 201, 154, 190, 330, 410]},
+                  {'name': 'Direct',
+                   'type': 'line',
+                   'stack': 'Total',
+                   'areaStyle': {},
+                   'emphasis': {'focus': 'series'},
+                   'data': [320, 332, 301, 334, 390, 330, 320]},
+                  {'name': 'Search Engine',
+                   'type': 'line',
+                   'stack': 'Total',
+                   'label': {'show': True, 'position': 'top'},
+                   'areaStyle': {},
+                   'emphasis': {'focus': 'series'},
+                   'data': [820, 932, 901, 934, 1290, 1330, 1320]}]
+                }
+
+            output
+                [{'Mon': 120,
+                  'Tue': 132,
+                  'Wed': 101,
+                  'Thu': 134,
+                  'Fri': 90,
+                  'Sat': 230,
+                  'Sun': 210},
+                 {'Mon': 220,
+                  'Tue': 182,
+                  'Wed': 191,
+                  'Thu': 234,
+                  'Fri': 290,
+                  'Sat': 330,
+                  'Sun': 310},
+                 {'Mon': 150,
+                  'Tue': 232,
+                  'Wed': 201,
+                  'Thu': 154,
+                  'Fri': 190,
+                  'Sat': 330,
+                  'Sun': 410},
+                 {'Mon': 320,
+                  'Tue': 332,
+                  'Wed': 301,
+                  'Thu': 334,
+                  'Fri': 390,
+                  'Sat': 330,
+                  'Sun': 320},
+                 {'Mon': 820,
+                  'Tue': 932,
+                  'Wed': 901,
+                  'Thu': 934,
+                  'Fri': 1290,
+                  'Sat': 1330,
+                  'Sun': 1320
+                }
+            ]
+            """
+            rows = []
+            data = []
+            cols = []
+            if 'xAxis' in options:
+                if type(options['xAxis']) == list:
+                    if len(options['xAxis']) == 1:
+                        if 'data' in options['xAxis'][0]:
+                            rows = options['xAxis'][0]['data']
+                elif type(options['xAxis']) == dict:
+                    if 'data' in options['xAxis']:
+                        rows = options['xAxis']['data']
+                    elif type(options['xAxis']) == dict:
+                        if 'data' in options['yAxis']:
+                            rows = options['yAxis']['data']
+                else:
+                    raise ValueError('xAxis has multiple values only 1 allowed')
+            elif 'radar' in options:
+                if 'indicator' in options['radar']:
+                    rows = [element['name'] for element in options['radar']['indicator']]
+                elif type(options['radar']) == dict:
+                    raise NotImplementedError('Multi-radar not implemented')
+
+            if 'data' in options_:
+                data = options_['data']
+            if 'series' in options:
+                if 'data' in options['series']:
+                    pass
+                else:
+                    for serie in options['series']:
+                        if 'data' in serie:
+                            if serie.get('type') in ['pie', 'gauge']:
+                                for datum in serie['data']:
+                                    data.append(datum)
+                            elif serie.get('type') == 'radar':
+                                for datum in serie['data']:
+                                    data.append(datum['value'])
+                                    cols.append(datum['name'])
+                                break
+                            else:
+                                data.append(serie['data'])
+
+                        if 'name' in serie:
+                            cols.append(serie['name'])
+                        elif 'type' in serie:
+                            cols.append(serie['type'])
+            else:
+                return {}
+
+            df = pd.DataFrame(data)
+            if not rows and not cols:
+                return df.reset_index().to_dict(orient='records')
+            if not rows:
+                return df.to_dict(orient='records')
+
+            if rows:
+                df.columns = rows
+            df_ = df.T
+            df_.columns = cols
+            return df_.reset_index().to_dict(orient='records')
+
+        def _create_free_echarts(
+                data_: Union[str, DataFrame, List[Dict]],
+                sort: Dict,
+        ) -> Dict[str, Union[Dict, List[Dict]]]:
+            if filters:
+                raise NotImplementedError
+
+            return self._create_dataset_charts(
+                options=options,
+                report_type='ECHARTS2',
+                menu_path=menu_path, order=order,
+                rows_size=rows_size, cols_size=cols_size, padding=padding,
+                data=data, bentobox_data=bentobox_data,
+                force_custom_field=False, sort=sort,
+            )
+
+        # TODO many things in common with _create_trend_charts_with_filters() unify!!
+        def _create_free_echarts_with_filters():
+            """"""
+            filter_elements: List[Dict] = []
+            self._validate_filters(filters=filters)
+
+            # We are going to save all the reports one by one
+            for df_temp, filter_element in (
+                    self._create_multifilter_reports(
+                        data=data, filters=filters,
+                    )
+            ):
+                report_id = _create_free_echarts(data_=df_temp)
+                filter_element['reportId'] = [report_id]
+                filter_elements.append(filter_element)
+
+            update_filter_type: Optional[str] = filters.get('update_filter_type')
+            filter_row: Optional[int] = filters.get('row')
+            filter_column: Optional[int] = filters.get('column')
+            filter_order: Optional[int] = filters.get('order')
+
+            if update_filter_type:
+                # concat is to add new filter options
+                # append is to add new reports to existing filter options
+                try:
+                    assert update_filter_type in ['concat', 'append']
+                except AssertionError:
+                    raise ValueError(
+                        f'update_filter_type must be one of both: "concat" or "append" | '
+                        f'Value provided: {update_filter_type}'
+                    )
+                self._update_filter_report(
+                    filter_row=filter_row,
+                    filter_column=filter_column,
+                    filter_order=filter_order,
+                    filter_elements=filter_elements,
+                    menu_path=kwargs['menu_path'],
+                    update_type=update_filter_type,
+                )
+            else:
+                report_metadata: Dict = {
+                    'reportType': 'MULTIFILTER',
+                    'title': '',
+                }
+
+                if filter_row and filter_column:
+                    report_metadata['grid'] = f'{filter_row}, {filter_column}'
+                elif filter_order is not None:
+                    report_metadata['order'] = filter_order
+                else:
+                    raise ValueError('Either row and column or order must be provided')
+
+                self._create_chart(
+                    data=filter_elements,
+                    menu_path=menu_path,
+                    report_metadata=report_metadata,
+                    order=filter_order,
+                    overwrite=True,
+                )
+
+        if options is None:
+            if raw_options is None:
+                raise ValueError('Either "options" or "raw_options" must be provided')
+            else:
+                options = transform_dict_js_to_py(raw_options)
+                data = retrieve_data_from_options(options)
+                sort = {
+                    'field': 'index',
+                    'direction': 'asc',
+                }
+        elif data is None:
+            raise ValueError('If "options" is provided "data" must be provided too')
+
+        if filters:
+            _create_free_echarts_with_filters()
+        else:
+            _create_free_echarts(data, sort=sort)
+
 
 class PlotApi(BasePlot):
     """
     """
 
     def __init__(self, api_client, **kwargs):
         self.api_client = api_client
@@ -1226,14 +1621,15 @@
             'path': path_name,
             'order': order,
             'dataFields': _calculate_table_data_fields(),
         }
 
         if row and column:
             report_metadata['grid']: str = f'{row}, {column}'
+            report_metadata['order']: int = 0
 
         if overwrite:
             if not row and not column and not order:
                 raise ValueError(
                     'Row, Column or Order must be specified to overwrite a report'
                 )
 
@@ -2759,7 +3155,81 @@
             filters=filters,
         )
         """
         raise NotImplementedError
 
     def stacked_barchart(self):
         raise NotImplementedError
+
+    def input_form(
+            self, report_dataset_properties: Dict, menu_path: str,
+            data: Optional[Union[str, DataFrame, List[Dict]]] = None,
+            order: Optional[int] = None,
+            rows_size: Optional[int] = 3, cols_size: int = 12,
+            padding: Optional[List[int]] = None,
+            bentobox_data: Optional[Dict] = None,
+    ):
+        """
+        :param data:
+        :param report_form_dataset_properties:
+        :param menu_path:
+        :param order:
+        :param rows_size:
+        :param cols_size:
+        :param padding:
+        :param bentobox_data:
+        """
+        self._validate_input_form_data(report_dataset_properties)
+
+        if data is None:
+            data = {}
+            for fields in report_dataset_properties['fields']:
+                for field in fields['fields']:
+                    field_name: str = field['fieldName']
+                    input_type: Optional[str] = field.get('inputType')
+                    if input_type == 'text':
+                        data[field_name] = ''
+                    if input_type == 'color':
+                        data[field_name] = '#000000'
+                    elif input_type == 'tel':
+                        data[field_name] = ''  # 633668396
+                    elif input_type == 'email':
+                        data[field_name] = ''  # 'ceo@acme.com'
+                    elif input_type == 'password':
+                        data[field_name] = ''  # '1234'
+                    elif input_type == 'date':
+                        data[field_name] = ''  # '1988-01-30',
+                    elif input_type == 'dateRange':
+                        data[field_name] = ''  # '2018-01-01,2020-01-01'
+                    elif input_type == 'datetimeLocal':
+                        data[field_name] = ''  # '2022-07-08T15:11'
+                    elif input_type == 'month':
+                        data[field_name] = ''  # '2019-11'
+                    elif input_type == 'week':
+                        data[field_name] = ''  # '2022W12'
+                    elif input_type == 'url':
+                        data[field_name] = ''  # 'www.shimoku.com'
+                    elif input_type == 'time':
+                        data[field_name] = ''  # '00:00'
+                    elif input_type == 'range':
+                        data[field_name] = ''  # data[options][0]
+                    elif input_type == 'select':
+                        data[field_name] = ''  # data['options'][0]
+                    elif input_type == 'checkbox':
+                        data[field_name] = ''  # data['options'][0]
+                    elif input_type == 'radio':
+                        data[field_name] = ''  # data['options'][0]
+                    elif input_type == 'number':
+                        data[field_name] = 0
+                    else:
+                        data[field_name] = ''  # default is text
+
+# TODO y esto tambien lo podría usar el método free_echarts!!
+        return self._create_dataset_charts(
+            report_dataset_properties=report_dataset_properties,
+            options={},
+            report_type='FORM',
+            menu_path=menu_path, order=order,
+            rows_size=rows_size, cols_size=cols_size, padding=padding,
+            data=data, bentobox_data=bentobox_data,
+            force_custom_field=True,
+        )
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## shimoku_api_python/api/report_metadata_api.py

```diff
@@ -511,9 +511,9 @@
 
     def unhide_report(self, business_id: str, app_id: str, report_id: str):
         report_metadata: Dict = {'isDisabled': False}
         self.update_report(
             business_id=business_id,
             app_id=app_id,
             report_id=report_id,
-            report_data=report_metadata,
+            report_metadata=report_metadata,
         )
```

## Comparing `shimoku_api_python-0.8.1.dist-info/LICENSE.txt` & `shimoku_api_python-0.9.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `shimoku_api_python-0.8.1.dist-info/METADATA` & `shimoku_api_python-0.9.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: shimoku-api-python
-Version: 0.8.1
+Version: 0.9
 Summary: Add a short description here!
 Home-page: https://github.com/pyscaffold/pyscaffold/
 Author: capitan-ariete
 Author-email: sergi.ortiz.rodriguez@gmail.com
 License: mit
 Project-URL: Documentation, https://pyscaffold.org/
 Platform: any
```

## Comparing `shimoku_api_python-0.8.1.dist-info/RECORD` & `shimoku_api_python-0.9.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,24 +1,26 @@
-shimoku_api_python/__init__.py,sha256=Y02IEcIWaGlFeXhwZmo8DjjDO1R7dL5Q57PxrHZMCXs,1503
-shimoku_api_python/client.py,sha256=00HSEDE5znhYm2KoygaRPKHKMDmCafFXk9VP6SxOayw,13662
+shimoku_api_python/__init__.py,sha256=g0YPS_oP7MeqP--zFrHYz56bxQP9us8fo6rhJ09JoVk,1716
+shimoku_api_python/client.py,sha256=JefAPdr0UbCcz0RChHTX4rDnBhBtm-ziNfMHKmxdmMA,13742
 shimoku_api_python/configuration.py,sha256=qYwJyNMZLLeeVUIRJPokfgNhCkZifnQDX_7lKTHNeGE,7749
 shimoku_api_python/exceptions.py,sha256=Gt_sRikg0PpbjjnYERPbH1XZ5tX8VSibDhKts041XYk,154
-shimoku_api_python/api/app_metadata_api.py,sha256=IDaWv4CQpo6SvJIEL7OnBAMFzvIu3ST-qd5r6xNOCa8,1167
+shimoku_api_python/api/ai_api.py,sha256=tl_xYvwp91ehT3-sbXjWm5EdqM3WpErU2j5-zUXgCdg,10096
+shimoku_api_python/api/app_metadata_api.py,sha256=vtrLJ9bQ_eOr00Y-RKHG7fpmo9CiwJeB3kjICzCwY9Q,2283
 shimoku_api_python/api/app_type_metadata_api.py,sha256=fvi5Ndn0ytPXTo7jnunqpDKUMep3-UK-r8doiiF1tp4,1861
 shimoku_api_python/api/business_metadata_api.py,sha256=BGEKu_D9w04aDdJvJ7-pkY2ExMPddwxvo21OAd7Lg6c,692
-shimoku_api_python/api/data_managing_api.py,sha256=1tY7gGIKxvYO6btCQhzoEFgwRkvjp1_1BDhktrsQjaY,13659
-shimoku_api_python/api/explorer_api.py,sha256=8cQchovWgW_vkcLmLzIDysaDOAiwV0oBbbaLcsGt_Lc,50285
+shimoku_api_python/api/data_managing_api.py,sha256=aJkEr9vIe72PQBPZ7_dcXLvlGsatwPOCphImSF-b1SI,24412
+shimoku_api_python/api/explorer_api.py,sha256=dDZAHC3LmASYfJ-Y1F1kG-zUMEFt0kKjrnSRWVC3arg,66227
+shimoku_api_python/api/file_metadata_api.py,sha256=XdkRTEOwtRqtJLKAA2x7SJ9zDv1bfzUSeJ9pDBm70Es,17143
 shimoku_api_python/api/notifications_api.py,sha256=NzuAXXeENE9LYM2cw4BOvn5zcRIlT4qsD1X3sqIU82Y,523
 shimoku_api_python/api/path_metadata_api.py,sha256=ZnQ6WA5GapObBZqwTGJxKbHic1Lnnaa-M3jPcB3Ot_8,7382
 shimoku_api_python/api/ping_api.py,sha256=SI-qtBrTW_4mRg2qOzDfH9ALrEoT1RcgNTT40I0wUiA,763
-shimoku_api_python/api/plot_api.py,sha256=xlsdVtlsD7S5qbu2btczjcz690y4B0-q3vfsGoaXZTY,98937
-shimoku_api_python/api/report_metadata_api.py,sha256=HhjEwaHCBuTL3DVoCDWhbXfYVc6lfaWSGvqrXwrNVgo,15738
+shimoku_api_python/api/plot_api.py,sha256=vLsOCgVVmrQH_eMCMjS-eAPbYN8J-plD7StgTcFPAxM,118246
+shimoku_api_python/api/report_metadata_api.py,sha256=HrLgHjwfN5GeViCuwGos8agx4JpkNeZ30O_ygM6rjQU,15742
 shimoku_api_python/api/suite_api.py,sha256=M_QYf6Wf4doQioGeCBmCnEmaplvgdQHRMt7ZsgNVB28,10202
 shimoku_api_python/api/universe_metadata_api.py,sha256=fJKOdQEdS3b_YJ_0ka9JIBGPDZ-YZUuUvuoazWulBx4,241
 shimoku_api_python/api/templates/charts_catalog.py,sha256=nuzLKlncXSC0neraur7oGaJe0iS5vOCHVjfebzlff6U,15999
 shimoku_api_python/api/templates/shimoku_backoffice.py,sha256=Jb0EzTxS_4_WQCOpJEp3nJcaiNb2z_ucF0JaPTSiwpk,6577
-shimoku_api_python-0.8.1.dist-info/AUTHORS.rst,sha256=DRiFfm982oLLlfb7mcs4PxLGsYn0Awryv-ZCHvtnG9k,91
-shimoku_api_python-0.8.1.dist-info/LICENSE.txt,sha256=9JPo11fhihig_VWC0QQGQAfTfkZhFjMd96SfPQOjrbY,1081
-shimoku_api_python-0.8.1.dist-info/METADATA,sha256=IzB5fg9juixPsyCQ4f5oLOwyTjZ8Wtnri4Dl3duvzOY,658
-shimoku_api_python-0.8.1.dist-info/WHEEL,sha256=kGT74LWyRUZrL4VgLh6_g12IeVl_9u9ZVhadrgXZUEY,110
-shimoku_api_python-0.8.1.dist-info/top_level.txt,sha256=4eWYvhn_A6tLzIDDCNgRlI33SB1TfKZVhfR0kvorJYs,19
-shimoku_api_python-0.8.1.dist-info/RECORD,,
+shimoku_api_python-0.9.dist-info/AUTHORS.rst,sha256=DRiFfm982oLLlfb7mcs4PxLGsYn0Awryv-ZCHvtnG9k,91
+shimoku_api_python-0.9.dist-info/LICENSE.txt,sha256=9JPo11fhihig_VWC0QQGQAfTfkZhFjMd96SfPQOjrbY,1081
+shimoku_api_python-0.9.dist-info/METADATA,sha256=Srpbwn9jJ49XQ7w4afTS5gQqcFMAbHfwNfzqfFUNxIo,656
+shimoku_api_python-0.9.dist-info/WHEEL,sha256=kGT74LWyRUZrL4VgLh6_g12IeVl_9u9ZVhadrgXZUEY,110
+shimoku_api_python-0.9.dist-info/top_level.txt,sha256=4eWYvhn_A6tLzIDDCNgRlI33SB1TfKZVhfR0kvorJYs,19
+shimoku_api_python-0.9.dist-info/RECORD,,
```

