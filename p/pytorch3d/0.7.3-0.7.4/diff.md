# Comparing `tmp/pytorch3d-0.7.3-cp39-cp39-macosx_10_9_x86_64.whl.zip` & `tmp/pytorch3d-0.7.4-cp39-cp39-macosx_10_9_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,208 +1,212 @@
-Zip file size: 891619 bytes, number of entries: 206
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 projects/__init__.py
--rwxr-xr-x  2.0 unx   816288 b- defN 23-Apr-04 15:09 pytorch3d/_C.cpython-39-darwin.so
--rw-r--r--  2.0 unx      231 b- defN 23-Apr-04 14:52 pytorch3d/__init__.py
--rw-r--r--  2.0 unx      331 b- defN 23-Apr-04 14:52 pytorch3d/common/__init__.py
--rw-r--r--  2.0 unx     1271 b- defN 23-Apr-04 14:52 pytorch3d/common/compat.py
--rw-r--r--  2.0 unx     1733 b- defN 23-Apr-04 14:52 pytorch3d/common/datatypes.py
--rw-r--r--  2.0 unx     2630 b- defN 23-Apr-04 14:52 pytorch3d/common/linear_with_repeat.py
--rw-r--r--  2.0 unx      275 b- defN 23-Apr-04 14:52 pytorch3d/common/workaround/__init__.py
--rw-r--r--  2.0 unx    12151 b- defN 23-Apr-04 14:52 pytorch3d/common/workaround/symeig3x3.py
--rw-r--r--  2.0 unx      872 b- defN 23-Apr-04 14:52 pytorch3d/common/workaround/utils.py
--rw-r--r--  2.0 unx      437 b- defN 23-Apr-04 14:52 pytorch3d/datasets/__init__.py
--rw-r--r--  2.0 unx    11998 b- defN 23-Apr-04 14:52 pytorch3d/datasets/shapenet_base.py
--rw-r--r--  2.0 unx     1539 b- defN 23-Apr-04 14:52 pytorch3d/datasets/utils.py
--rw-r--r--  2.0 unx      378 b- defN 23-Apr-04 14:52 pytorch3d/datasets/r2n2/__init__.py
--rw-r--r--  2.0 unx    19037 b- defN 23-Apr-04 14:52 pytorch3d/datasets/r2n2/r2n2.py
--rw-r--r--  2.0 unx      346 b- defN 23-Apr-04 14:52 pytorch3d/datasets/r2n2/r2n2_synset_dict.json
--rw-r--r--  2.0 unx    18678 b- defN 23-Apr-04 14:52 pytorch3d/datasets/r2n2/utils.py
--rw-r--r--  2.0 unx      316 b- defN 23-Apr-04 14:52 pytorch3d/datasets/shapenet/__init__.py
--rw-r--r--  2.0 unx     7018 b- defN 23-Apr-04 14:52 pytorch3d/datasets/shapenet/shapenet_core.py
--rw-r--r--  2.0 unx     1510 b- defN 23-Apr-04 14:52 pytorch3d/datasets/shapenet/shapenet_synset_dict_v1.json
--rw-r--r--  2.0 unx     1456 b- defN 23-Apr-04 14:52 pytorch3d/datasets/shapenet/shapenet_synset_dict_v2.json
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/__init__.py
--rw-r--r--  2.0 unx     6049 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/eval_demo.py
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/__init__.py
--rw-r--r--  2.0 unx     1899 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/blender_dataset_map_provider.py
--rw-r--r--  2.0 unx    19321 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/data_loader_map_provider.py
--rw-r--r--  2.0 unx     3536 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/data_source.py
--rw-r--r--  2.0 unx     5302 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/dataset_base.py
--rw-r--r--  2.0 unx     4422 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/dataset_map_provider.py
--rw-r--r--  2.0 unx    30400 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/frame_data.py
--rw-r--r--  2.0 unx    27773 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/json_index_dataset.py
--rw-r--r--  2.0 unx    11512 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/json_index_dataset_map_provider.py
--rw-r--r--  2.0 unx    18944 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/json_index_dataset_map_provider_v2.py
--rw-r--r--  2.0 unx     2317 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/llff_dataset_map_provider.py
--rw-r--r--  2.0 unx     3919 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/load_blender.py
--rw-r--r--  2.0 unx     9903 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/load_llff.py
--rw-r--r--  2.0 unx     8824 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/rendered_mesh_dataset_map_provider.py
--rw-r--r--  2.0 unx     8346 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/scene_batch_sampler.py
--rw-r--r--  2.0 unx     7365 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/single_sequence_dataset.py
--rw-r--r--  2.0 unx    11879 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/types.py
--rw-r--r--  2.0 unx    11375 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/utils.py
--rw-r--r--  2.0 unx     3252 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/dataset/visualize.py
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/evaluation/__init__.py
--rw-r--r--  2.0 unx    20128 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/evaluation/evaluate_new_view_synthesis.py
--rw-r--r--  2.0 unx     4882 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/evaluation/evaluator.py
--rw-r--r--  2.0 unx      437 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/__init__.py
--rw-r--r--  2.0 unx     4022 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/base_model.py
--rw-r--r--  2.0 unx    32994 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/generic_model.py
--rw-r--r--  2.0 unx    16616 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/metrics.py
--rw-r--r--  2.0 unx     5593 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/model_dbir.py
--rw-r--r--  2.0 unx    27801 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/overfit_model.py
--rw-r--r--  2.0 unx     7295 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/utils.py
--rw-r--r--  2.0 unx      261 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/feature_extractor/__init__.py
--rw-r--r--  2.0 unx     1238 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/feature_extractor/feature_extractor.py
--rw-r--r--  2.0 unx     8270 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/feature_extractor/resnet_feature_extractor.py
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/global_encoder/__init__.py
--rw-r--r--  2.0 unx     5983 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/global_encoder/autodecoder.py
--rw-r--r--  2.0 unx     3922 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/global_encoder/global_encoder.py
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/implicit_function/__init__.py
--rw-r--r--  2.0 unx     1328 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/implicit_function/base.py
--rw-r--r--  2.0 unx    19282 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/implicit_function/decoding_functions.py
--rw-r--r--  2.0 unx     7795 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/implicit_function/idr_feature_field.py
--rw-r--r--  2.0 unx    10288 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/implicit_function/neural_radiance_field.py
--rw-r--r--  2.0 unx    16948 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/implicit_function/scene_representation_networks.py
--rw-r--r--  2.0 unx     7145 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/implicit_function/utils.py
--rw-r--r--  2.0 unx    49736 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/implicit_function/voxel_grid.py
--rw-r--r--  2.0 unx    28050 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/implicit_function/voxel_grid_implicit_function.py
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/__init__.py
--rw-r--r--  2.0 unx     7936 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/base.py
--rw-r--r--  2.0 unx     7466 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/lstm_renderer.py
--rw-r--r--  2.0 unx     6986 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/multipass_ea.py
--rw-r--r--  2.0 unx     3258 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/ray_point_refiner.py
--rw-r--r--  2.0 unx    11339 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/ray_sampler.py
--rw-r--r--  2.0 unx    22481 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/ray_tracing.py
--rw-r--r--  2.0 unx     8761 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/raymarcher.py
--rw-r--r--  2.0 unx     4690 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/rgb_net.py
--rw-r--r--  2.0 unx    10484 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/renderer/sdf_renderer.py
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/view_pooler/__init__.py
--rw-r--r--  2.0 unx    26105 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/view_pooler/feature_aggregator.py
--rw-r--r--  2.0 unx     5479 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/view_pooler/view_pooler.py
--rw-r--r--  2.0 unx    10712 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/view_pooler/view_sampler.py
--rw-r--r--  2.0 unx      231 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/visualization/__init__.py
--rw-r--r--  2.0 unx    14765 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/models/visualization/render_flyaround.py
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/third_party/__init__.py
--rw-r--r--  2.0 unx     7741 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/third_party/hyperlayers.py
--rw-r--r--  2.0 unx    25540 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/third_party/pytorch_prototyping.py
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/__init__.py
--rw-r--r--  2.0 unx     4823 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/camera_utils.py
--rw-r--r--  2.0 unx     8681 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/circle_fitting.py
--rw-r--r--  2.0 unx    42724 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/config.py
--rw-r--r--  2.0 unx     3823 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/depth_cleanup.py
--rw-r--r--  2.0 unx     9898 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/eval_video_trajectory.py
--rw-r--r--  2.0 unx     1853 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/image_utils.py
--rw-r--r--  2.0 unx     7518 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/metric_utils.py
--rw-r--r--  2.0 unx     4928 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/model_io.py
--rw-r--r--  2.0 unx     6228 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/point_cloud_utils.py
--rw-r--r--  2.0 unx     4974 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/rasterize_mc.py
--rw-r--r--  2.0 unx    16428 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/stats.py
--rw-r--r--  2.0 unx     5104 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/utils.py
--rw-r--r--  2.0 unx     5817 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/video_writer.py
--rw-r--r--  2.0 unx     5753 b- defN 23-Apr-04 14:52 pytorch3d/implicitron/tools/vis_utils.py
--rw-r--r--  2.0 unx      208 b- defN 23-Apr-04 14:52 pytorch3d/implicitron_trainer/__init__.py
--rw-r--r--  2.0 unx     9780 b- defN 23-Apr-04 14:52 pytorch3d/implicitron_trainer/experiment.py
--rw-r--r--  2.0 unx     6143 b- defN 23-Apr-04 14:52 pytorch3d/implicitron_trainer/visualize_reconstruction.py
--rw-r--r--  2.0 unx      402 b- defN 23-Apr-04 14:52 pytorch3d/io/__init__.py
--rw-r--r--  2.0 unx    30192 b- defN 23-Apr-04 14:52 pytorch3d/io/experimental_gltf_io.py
--rw-r--r--  2.0 unx    20604 b- defN 23-Apr-04 14:52 pytorch3d/io/mtl_io.py
--rw-r--r--  2.0 unx    30811 b- defN 23-Apr-04 14:52 pytorch3d/io/obj_io.py
--rw-r--r--  2.0 unx    16019 b- defN 23-Apr-04 14:52 pytorch3d/io/off_io.py
--rw-r--r--  2.0 unx     7588 b- defN 23-Apr-04 14:52 pytorch3d/io/pluggable.py
--rw-r--r--  2.0 unx     4213 b- defN 23-Apr-04 14:52 pytorch3d/io/pluggable_formats.py
--rw-r--r--  2.0 unx    52571 b- defN 23-Apr-04 14:52 pytorch3d/io/ply_io.py
--rw-r--r--  2.0 unx     2689 b- defN 23-Apr-04 14:52 pytorch3d/io/utils.py
--rw-r--r--  2.0 unx      566 b- defN 23-Apr-04 14:52 pytorch3d/loss/__init__.py
--rw-r--r--  2.0 unx     8713 b- defN 23-Apr-04 14:52 pytorch3d/loss/chamfer.py
--rw-r--r--  2.0 unx     1945 b- defN 23-Apr-04 14:52 pytorch3d/loss/mesh_edge_loss.py
--rw-r--r--  2.0 unx     5068 b- defN 23-Apr-04 14:52 pytorch3d/loss/mesh_laplacian_smoothing.py
--rw-r--r--  2.0 unx     5229 b- defN 23-Apr-04 14:52 pytorch3d/loss/mesh_normal_consistency.py
--rw-r--r--  2.0 unx    15957 b- defN 23-Apr-04 14:52 pytorch3d/loss/point_mesh_distance.py
--rw-r--r--  2.0 unx     1510 b- defN 23-Apr-04 14:52 pytorch3d/ops/__init__.py
--rw-r--r--  2.0 unx     5694 b- defN 23-Apr-04 14:52 pytorch3d/ops/ball_query.py
--rw-r--r--  2.0 unx     8898 b- defN 23-Apr-04 14:52 pytorch3d/ops/cameras_alignment.py
--rw-r--r--  2.0 unx     8482 b- defN 23-Apr-04 14:52 pytorch3d/ops/cubify.py
--rw-r--r--  2.0 unx     6375 b- defN 23-Apr-04 14:52 pytorch3d/ops/graph_conv.py
--rw-r--r--  2.0 unx     3811 b- defN 23-Apr-04 14:52 pytorch3d/ops/interp_face_attrs.py
--rw-r--r--  2.0 unx     4875 b- defN 23-Apr-04 14:52 pytorch3d/ops/iou_box3d.py
--rw-r--r--  2.0 unx    10275 b- defN 23-Apr-04 14:52 pytorch3d/ops/knn.py
--rw-r--r--  2.0 unx     6607 b- defN 23-Apr-04 14:52 pytorch3d/ops/laplacian_matrices.py
--rw-r--r--  2.0 unx    12384 b- defN 23-Apr-04 14:52 pytorch3d/ops/marching_cubes.py
--rw-r--r--  2.0 unx    10136 b- defN 23-Apr-04 14:52 pytorch3d/ops/marching_cubes_data.py
--rw-r--r--  2.0 unx     2545 b- defN 23-Apr-04 14:52 pytorch3d/ops/mesh_face_areas_normals.py
--rw-r--r--  2.0 unx     2091 b- defN 23-Apr-04 14:52 pytorch3d/ops/mesh_filtering.py
--rw-r--r--  2.0 unx     8067 b- defN 23-Apr-04 14:52 pytorch3d/ops/packed_to_padded.py
--rw-r--r--  2.0 unx    15916 b- defN 23-Apr-04 14:52 pytorch3d/ops/perspective_n_points.py
--rw-r--r--  2.0 unx    15119 b- defN 23-Apr-04 14:52 pytorch3d/ops/points_alignment.py
--rw-r--r--  2.0 unx     7480 b- defN 23-Apr-04 14:52 pytorch3d/ops/points_normals.py
--rw-r--r--  2.0 unx    31002 b- defN 23-Apr-04 14:52 pytorch3d/ops/points_to_volumes.py
--rw-r--r--  2.0 unx     7827 b- defN 23-Apr-04 14:52 pytorch3d/ops/sample_farthest_points.py
--rw-r--r--  2.0 unx     7091 b- defN 23-Apr-04 14:52 pytorch3d/ops/sample_points_from_meshes.py
--rw-r--r--  2.0 unx    17932 b- defN 23-Apr-04 14:52 pytorch3d/ops/subdivide_meshes.py
--rw-r--r--  2.0 unx     7417 b- defN 23-Apr-04 14:52 pytorch3d/ops/utils.py
--rw-r--r--  2.0 unx     4246 b- defN 23-Apr-04 14:52 pytorch3d/ops/vert_align.py
--rw-r--r--  2.0 unx     2212 b- defN 23-Apr-04 14:52 pytorch3d/renderer/__init__.py
--rw-r--r--  2.0 unx     9879 b- defN 23-Apr-04 14:52 pytorch3d/renderer/blending.py
--rw-r--r--  2.0 unx     6804 b- defN 23-Apr-04 14:52 pytorch3d/renderer/camera_conversions.py
--rw-r--r--  2.0 unx     7930 b- defN 23-Apr-04 14:52 pytorch3d/renderer/camera_utils.py
--rw-r--r--  2.0 unx    70876 b- defN 23-Apr-04 14:52 pytorch3d/renderer/cameras.py
--rw-r--r--  2.0 unx    10686 b- defN 23-Apr-04 14:52 pytorch3d/renderer/compositing.py
--rw-r--r--  2.0 unx    21762 b- defN 23-Apr-04 14:52 pytorch3d/renderer/fisheyecameras.py
--rw-r--r--  2.0 unx    13007 b- defN 23-Apr-04 14:52 pytorch3d/renderer/lighting.py
--rw-r--r--  2.0 unx     2395 b- defN 23-Apr-04 14:52 pytorch3d/renderer/materials.py
--rw-r--r--  2.0 unx    25178 b- defN 23-Apr-04 14:52 pytorch3d/renderer/splatter_blend.py
--rw-r--r--  2.0 unx    16827 b- defN 23-Apr-04 14:52 pytorch3d/renderer/utils.py
--rw-r--r--  2.0 unx      767 b- defN 23-Apr-04 14:52 pytorch3d/renderer/implicit/__init__.py
--rw-r--r--  2.0 unx     4429 b- defN 23-Apr-04 14:52 pytorch3d/renderer/implicit/harmonic_embedding.py
--rw-r--r--  2.0 unx     9323 b- defN 23-Apr-04 14:52 pytorch3d/renderer/implicit/raymarching.py
--rw-r--r--  2.0 unx    31792 b- defN 23-Apr-04 14:52 pytorch3d/renderer/implicit/raysampling.py
--rw-r--r--  2.0 unx    17111 b- defN 23-Apr-04 14:52 pytorch3d/renderer/implicit/renderer.py
--rw-r--r--  2.0 unx     5817 b- defN 23-Apr-04 14:52 pytorch3d/renderer/implicit/sample_pdf.py
--rw-r--r--  2.0 unx     6458 b- defN 23-Apr-04 14:52 pytorch3d/renderer/implicit/utils.py
--rw-r--r--  2.0 unx      995 b- defN 23-Apr-04 14:52 pytorch3d/renderer/mesh/__init__.py
--rw-r--r--  2.0 unx    34991 b- defN 23-Apr-04 14:52 pytorch3d/renderer/mesh/clip.py
--rw-r--r--  2.0 unx    30438 b- defN 23-Apr-04 14:52 pytorch3d/renderer/mesh/rasterize_meshes.py
--rw-r--r--  2.0 unx    12527 b- defN 23-Apr-04 14:52 pytorch3d/renderer/mesh/rasterizer.py
--rw-r--r--  2.0 unx     4013 b- defN 23-Apr-04 14:52 pytorch3d/renderer/mesh/renderer.py
--rw-r--r--  2.0 unx    15488 b- defN 23-Apr-04 14:52 pytorch3d/renderer/mesh/shader.py
--rw-r--r--  2.0 unx     8965 b- defN 23-Apr-04 14:52 pytorch3d/renderer/mesh/shading.py
--rw-r--r--  2.0 unx    65242 b- defN 23-Apr-04 14:52 pytorch3d/renderer/mesh/textures.py
--rw-r--r--  2.0 unx    11238 b- defN 23-Apr-04 14:52 pytorch3d/renderer/mesh/utils.py
--rw-r--r--  2.0 unx     1278 b- defN 23-Apr-04 14:52 pytorch3d/renderer/opengl/__init__.py
--rw-r--r--  2.0 unx    16419 b- defN 23-Apr-04 14:52 pytorch3d/renderer/opengl/opengl_utils.py
--rw-r--r--  2.0 unx    27731 b- defN 23-Apr-04 14:52 pytorch3d/renderer/opengl/rasterizer_opengl.py
--rw-r--r--  2.0 unx      543 b- defN 23-Apr-04 14:52 pytorch3d/renderer/points/__init__.py
--rw-r--r--  2.0 unx     4323 b- defN 23-Apr-04 14:52 pytorch3d/renderer/points/compositor.py
--rw-r--r--  2.0 unx    13071 b- defN 23-Apr-04 14:52 pytorch3d/renderer/points/rasterize_points.py
--rw-r--r--  2.0 unx     5995 b- defN 23-Apr-04 14:52 pytorch3d/renderer/points/rasterizer.py
--rw-r--r--  2.0 unx     2183 b- defN 23-Apr-04 14:52 pytorch3d/renderer/points/renderer.py
--rw-r--r--  2.0 unx      254 b- defN 23-Apr-04 14:52 pytorch3d/renderer/points/pulsar/__init__.py
--rw-r--r--  2.0 unx    27302 b- defN 23-Apr-04 14:52 pytorch3d/renderer/points/pulsar/renderer.py
--rw-r--r--  2.0 unx    24340 b- defN 23-Apr-04 14:52 pytorch3d/renderer/points/pulsar/unified.py
--rw-r--r--  2.0 unx      566 b- defN 23-Apr-04 14:52 pytorch3d/structures/__init__.py
--rw-r--r--  2.0 unx    70129 b- defN 23-Apr-04 14:52 pytorch3d/structures/meshes.py
--rw-r--r--  2.0 unx    50190 b- defN 23-Apr-04 14:52 pytorch3d/structures/pointclouds.py
--rw-r--r--  2.0 unx     8000 b- defN 23-Apr-04 14:52 pytorch3d/structures/utils.py
--rw-r--r--  2.0 unx    45907 b- defN 23-Apr-04 14:52 pytorch3d/structures/volumes.py
--rw-r--r--  2.0 unx     1076 b- defN 23-Apr-04 14:52 pytorch3d/transforms/__init__.py
--rw-r--r--  2.0 unx     2966 b- defN 23-Apr-04 14:52 pytorch3d/transforms/math.py
--rw-r--r--  2.0 unx    20283 b- defN 23-Apr-04 14:52 pytorch3d/transforms/rotation_conversions.py
--rw-r--r--  2.0 unx     7985 b- defN 23-Apr-04 14:52 pytorch3d/transforms/se3.py
--rw-r--r--  2.0 unx    10194 b- defN 23-Apr-04 14:52 pytorch3d/transforms/so3.py
--rw-r--r--  2.0 unx    30693 b- defN 23-Apr-04 14:52 pytorch3d/transforms/transform3d.py
--rw-r--r--  2.0 unx      554 b- defN 23-Apr-04 14:52 pytorch3d/utils/__init__.py
--rw-r--r--  2.0 unx     6366 b- defN 23-Apr-04 14:52 pytorch3d/utils/camera_conversions.py
--rw-r--r--  2.0 unx     2915 b- defN 23-Apr-04 14:52 pytorch3d/utils/checkerboard.py
--rw-r--r--  2.0 unx     2217 b- defN 23-Apr-04 14:52 pytorch3d/utils/ico_sphere.py
--rw-r--r--  2.0 unx     2392 b- defN 23-Apr-04 14:52 pytorch3d/utils/torus.py
--rw-r--r--  2.0 unx      645 b- defN 23-Apr-04 14:52 pytorch3d/vis/__init__.py
--rw-r--r--  2.0 unx    39884 b- defN 23-Apr-04 14:52 pytorch3d/vis/plotly_vis.py
--rw-r--r--  2.0 unx     3814 b- defN 23-Apr-04 14:52 pytorch3d/vis/texture_vis.py
--rw-r--r--  2.0 unx     1534 b- defN 23-Apr-04 15:09 pytorch3d-0.7.3.dist-info/LICENSE
--rw-r--r--  2.0 unx     3351 b- defN 23-Apr-04 15:09 pytorch3d-0.7.3.dist-info/LICENSE-3RD-PARTY
--rw-r--r--  2.0 unx      941 b- defN 23-Apr-04 15:09 pytorch3d-0.7.3.dist-info/METADATA
--rw-r--r--  2.0 unx      109 b- defN 23-Apr-04 15:09 pytorch3d-0.7.3.dist-info/WHEEL
--rw-r--r--  2.0 unx      196 b- defN 23-Apr-04 15:09 pytorch3d-0.7.3.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       19 b- defN 23-Apr-04 15:09 pytorch3d-0.7.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    20067 b- defN 23-Apr-04 15:09 pytorch3d-0.7.3.dist-info/RECORD
-206 files, 3007513 bytes uncompressed, 859251 bytes compressed:  71.4%
+Zip file size: 909861 bytes, number of entries: 210
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 projects/__init__.py
+-rwxr-xr-x  2.0 unx   816288 b- defN 23-May-10 12:03 pytorch3d/_C.cpython-39-darwin.so
+-rw-r--r--  2.0 unx      231 b- defN 23-May-10 11:47 pytorch3d/__init__.py
+-rw-r--r--  2.0 unx      331 b- defN 23-May-10 11:47 pytorch3d/common/__init__.py
+-rw-r--r--  2.0 unx     1271 b- defN 23-May-10 11:47 pytorch3d/common/compat.py
+-rw-r--r--  2.0 unx     1733 b- defN 23-May-10 11:47 pytorch3d/common/datatypes.py
+-rw-r--r--  2.0 unx     2630 b- defN 23-May-10 11:47 pytorch3d/common/linear_with_repeat.py
+-rw-r--r--  2.0 unx      275 b- defN 23-May-10 11:47 pytorch3d/common/workaround/__init__.py
+-rw-r--r--  2.0 unx    12151 b- defN 23-May-10 11:47 pytorch3d/common/workaround/symeig3x3.py
+-rw-r--r--  2.0 unx      872 b- defN 23-May-10 11:47 pytorch3d/common/workaround/utils.py
+-rw-r--r--  2.0 unx      437 b- defN 23-May-10 11:47 pytorch3d/datasets/__init__.py
+-rw-r--r--  2.0 unx    11998 b- defN 23-May-10 11:47 pytorch3d/datasets/shapenet_base.py
+-rw-r--r--  2.0 unx     1539 b- defN 23-May-10 11:47 pytorch3d/datasets/utils.py
+-rw-r--r--  2.0 unx      378 b- defN 23-May-10 11:47 pytorch3d/datasets/r2n2/__init__.py
+-rw-r--r--  2.0 unx    19037 b- defN 23-May-10 11:47 pytorch3d/datasets/r2n2/r2n2.py
+-rw-r--r--  2.0 unx      346 b- defN 23-May-10 11:47 pytorch3d/datasets/r2n2/r2n2_synset_dict.json
+-rw-r--r--  2.0 unx    18678 b- defN 23-May-10 11:47 pytorch3d/datasets/r2n2/utils.py
+-rw-r--r--  2.0 unx      316 b- defN 23-May-10 11:47 pytorch3d/datasets/shapenet/__init__.py
+-rw-r--r--  2.0 unx     7018 b- defN 23-May-10 11:47 pytorch3d/datasets/shapenet/shapenet_core.py
+-rw-r--r--  2.0 unx     1510 b- defN 23-May-10 11:47 pytorch3d/datasets/shapenet/shapenet_synset_dict_v1.json
+-rw-r--r--  2.0 unx     1456 b- defN 23-May-10 11:47 pytorch3d/datasets/shapenet/shapenet_synset_dict_v2.json
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron/__init__.py
+-rw-r--r--  2.0 unx     6049 b- defN 23-May-10 11:47 pytorch3d/implicitron/eval_demo.py
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/__init__.py
+-rw-r--r--  2.0 unx     1899 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/blender_dataset_map_provider.py
+-rw-r--r--  2.0 unx    19321 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/data_loader_map_provider.py
+-rw-r--r--  2.0 unx     3536 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/data_source.py
+-rw-r--r--  2.0 unx     5302 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/dataset_base.py
+-rw-r--r--  2.0 unx     4422 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/dataset_map_provider.py
+-rw-r--r--  2.0 unx    31568 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/frame_data.py
+-rw-r--r--  2.0 unx    27817 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/json_index_dataset.py
+-rw-r--r--  2.0 unx    11512 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/json_index_dataset_map_provider.py
+-rw-r--r--  2.0 unx    18944 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/json_index_dataset_map_provider_v2.py
+-rw-r--r--  2.0 unx     2317 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/llff_dataset_map_provider.py
+-rw-r--r--  2.0 unx     3919 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/load_blender.py
+-rw-r--r--  2.0 unx     9903 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/load_llff.py
+-rw-r--r--  2.0 unx     4830 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/orm_types.py
+-rw-r--r--  2.0 unx     8824 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/rendered_mesh_dataset_map_provider.py
+-rw-r--r--  2.0 unx     8346 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/scene_batch_sampler.py
+-rw-r--r--  2.0 unx     7365 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/single_sequence_dataset.py
+-rw-r--r--  2.0 unx    29663 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/sql_dataset.py
+-rw-r--r--  2.0 unx    16557 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/sql_dataset_provider.py
+-rw-r--r--  2.0 unx     7987 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/train_eval_data_loader_provider.py
+-rw-r--r--  2.0 unx    11885 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/types.py
+-rw-r--r--  2.0 unx    11375 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/utils.py
+-rw-r--r--  2.0 unx     3252 b- defN 23-May-10 11:47 pytorch3d/implicitron/dataset/visualize.py
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron/evaluation/__init__.py
+-rw-r--r--  2.0 unx    20331 b- defN 23-May-10 11:47 pytorch3d/implicitron/evaluation/evaluate_new_view_synthesis.py
+-rw-r--r--  2.0 unx     4970 b- defN 23-May-10 11:47 pytorch3d/implicitron/evaluation/evaluator.py
+-rw-r--r--  2.0 unx      437 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/__init__.py
+-rw-r--r--  2.0 unx     4022 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/base_model.py
+-rw-r--r--  2.0 unx    32994 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/generic_model.py
+-rw-r--r--  2.0 unx    16616 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/metrics.py
+-rw-r--r--  2.0 unx     5699 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/model_dbir.py
+-rw-r--r--  2.0 unx    27801 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/overfit_model.py
+-rw-r--r--  2.0 unx     7295 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/utils.py
+-rw-r--r--  2.0 unx      261 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/feature_extractor/__init__.py
+-rw-r--r--  2.0 unx     1238 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/feature_extractor/feature_extractor.py
+-rw-r--r--  2.0 unx     8270 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/feature_extractor/resnet_feature_extractor.py
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/global_encoder/__init__.py
+-rw-r--r--  2.0 unx     5983 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/global_encoder/autodecoder.py
+-rw-r--r--  2.0 unx     3922 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/global_encoder/global_encoder.py
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/implicit_function/__init__.py
+-rw-r--r--  2.0 unx     1328 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/implicit_function/base.py
+-rw-r--r--  2.0 unx    19282 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/implicit_function/decoding_functions.py
+-rw-r--r--  2.0 unx     7795 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/implicit_function/idr_feature_field.py
+-rw-r--r--  2.0 unx    10288 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/implicit_function/neural_radiance_field.py
+-rw-r--r--  2.0 unx    16948 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/implicit_function/scene_representation_networks.py
+-rw-r--r--  2.0 unx     7145 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/implicit_function/utils.py
+-rw-r--r--  2.0 unx    49736 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/implicit_function/voxel_grid.py
+-rw-r--r--  2.0 unx    28050 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/implicit_function/voxel_grid_implicit_function.py
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/__init__.py
+-rw-r--r--  2.0 unx     7936 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/base.py
+-rw-r--r--  2.0 unx     7466 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/lstm_renderer.py
+-rw-r--r--  2.0 unx     6986 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/multipass_ea.py
+-rw-r--r--  2.0 unx     3258 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/ray_point_refiner.py
+-rw-r--r--  2.0 unx    11339 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/ray_sampler.py
+-rw-r--r--  2.0 unx    22481 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/ray_tracing.py
+-rw-r--r--  2.0 unx     8861 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/raymarcher.py
+-rw-r--r--  2.0 unx     4690 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/rgb_net.py
+-rw-r--r--  2.0 unx    10484 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/renderer/sdf_renderer.py
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/view_pooler/__init__.py
+-rw-r--r--  2.0 unx    26105 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/view_pooler/feature_aggregator.py
+-rw-r--r--  2.0 unx     5479 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/view_pooler/view_pooler.py
+-rw-r--r--  2.0 unx    10712 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/view_pooler/view_sampler.py
+-rw-r--r--  2.0 unx      231 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/visualization/__init__.py
+-rw-r--r--  2.0 unx    14765 b- defN 23-May-10 11:47 pytorch3d/implicitron/models/visualization/render_flyaround.py
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron/third_party/__init__.py
+-rw-r--r--  2.0 unx     7741 b- defN 23-May-10 11:47 pytorch3d/implicitron/third_party/hyperlayers.py
+-rw-r--r--  2.0 unx    25540 b- defN 23-May-10 11:47 pytorch3d/implicitron/third_party/pytorch_prototyping.py
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/__init__.py
+-rw-r--r--  2.0 unx     4823 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/camera_utils.py
+-rw-r--r--  2.0 unx     8681 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/circle_fitting.py
+-rw-r--r--  2.0 unx    42724 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/config.py
+-rw-r--r--  2.0 unx     3823 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/depth_cleanup.py
+-rw-r--r--  2.0 unx     9898 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/eval_video_trajectory.py
+-rw-r--r--  2.0 unx     1853 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/image_utils.py
+-rw-r--r--  2.0 unx     7518 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/metric_utils.py
+-rw-r--r--  2.0 unx     4928 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/model_io.py
+-rw-r--r--  2.0 unx     6228 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/point_cloud_utils.py
+-rw-r--r--  2.0 unx     4974 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/rasterize_mc.py
+-rw-r--r--  2.0 unx    16428 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/stats.py
+-rw-r--r--  2.0 unx     5104 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/utils.py
+-rw-r--r--  2.0 unx     5817 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/video_writer.py
+-rw-r--r--  2.0 unx     5753 b- defN 23-May-10 11:47 pytorch3d/implicitron/tools/vis_utils.py
+-rw-r--r--  2.0 unx      208 b- defN 23-May-10 11:47 pytorch3d/implicitron_trainer/__init__.py
+-rw-r--r--  2.0 unx     9780 b- defN 23-May-10 11:47 pytorch3d/implicitron_trainer/experiment.py
+-rw-r--r--  2.0 unx     6143 b- defN 23-May-10 11:47 pytorch3d/implicitron_trainer/visualize_reconstruction.py
+-rw-r--r--  2.0 unx      402 b- defN 23-May-10 11:47 pytorch3d/io/__init__.py
+-rw-r--r--  2.0 unx    31794 b- defN 23-May-10 11:47 pytorch3d/io/experimental_gltf_io.py
+-rw-r--r--  2.0 unx    20604 b- defN 23-May-10 11:47 pytorch3d/io/mtl_io.py
+-rw-r--r--  2.0 unx    33926 b- defN 23-May-10 11:47 pytorch3d/io/obj_io.py
+-rw-r--r--  2.0 unx    16019 b- defN 23-May-10 11:47 pytorch3d/io/off_io.py
+-rw-r--r--  2.0 unx     7588 b- defN 23-May-10 11:47 pytorch3d/io/pluggable.py
+-rw-r--r--  2.0 unx     4213 b- defN 23-May-10 11:47 pytorch3d/io/pluggable_formats.py
+-rw-r--r--  2.0 unx    52571 b- defN 23-May-10 11:47 pytorch3d/io/ply_io.py
+-rw-r--r--  2.0 unx     2689 b- defN 23-May-10 11:47 pytorch3d/io/utils.py
+-rw-r--r--  2.0 unx      566 b- defN 23-May-10 11:47 pytorch3d/loss/__init__.py
+-rw-r--r--  2.0 unx     8713 b- defN 23-May-10 11:47 pytorch3d/loss/chamfer.py
+-rw-r--r--  2.0 unx     1945 b- defN 23-May-10 11:47 pytorch3d/loss/mesh_edge_loss.py
+-rw-r--r--  2.0 unx     5068 b- defN 23-May-10 11:47 pytorch3d/loss/mesh_laplacian_smoothing.py
+-rw-r--r--  2.0 unx     5229 b- defN 23-May-10 11:47 pytorch3d/loss/mesh_normal_consistency.py
+-rw-r--r--  2.0 unx    15957 b- defN 23-May-10 11:47 pytorch3d/loss/point_mesh_distance.py
+-rw-r--r--  2.0 unx     1510 b- defN 23-May-10 11:47 pytorch3d/ops/__init__.py
+-rw-r--r--  2.0 unx     5694 b- defN 23-May-10 11:47 pytorch3d/ops/ball_query.py
+-rw-r--r--  2.0 unx     8898 b- defN 23-May-10 11:47 pytorch3d/ops/cameras_alignment.py
+-rw-r--r--  2.0 unx     8482 b- defN 23-May-10 11:47 pytorch3d/ops/cubify.py
+-rw-r--r--  2.0 unx     6375 b- defN 23-May-10 11:47 pytorch3d/ops/graph_conv.py
+-rw-r--r--  2.0 unx     3811 b- defN 23-May-10 11:47 pytorch3d/ops/interp_face_attrs.py
+-rw-r--r--  2.0 unx     4875 b- defN 23-May-10 11:47 pytorch3d/ops/iou_box3d.py
+-rw-r--r--  2.0 unx    10275 b- defN 23-May-10 11:47 pytorch3d/ops/knn.py
+-rw-r--r--  2.0 unx     6607 b- defN 23-May-10 11:47 pytorch3d/ops/laplacian_matrices.py
+-rw-r--r--  2.0 unx    12384 b- defN 23-May-10 11:47 pytorch3d/ops/marching_cubes.py
+-rw-r--r--  2.0 unx    10136 b- defN 23-May-10 11:47 pytorch3d/ops/marching_cubes_data.py
+-rw-r--r--  2.0 unx     2545 b- defN 23-May-10 11:47 pytorch3d/ops/mesh_face_areas_normals.py
+-rw-r--r--  2.0 unx     2091 b- defN 23-May-10 11:47 pytorch3d/ops/mesh_filtering.py
+-rw-r--r--  2.0 unx     8067 b- defN 23-May-10 11:47 pytorch3d/ops/packed_to_padded.py
+-rw-r--r--  2.0 unx    15916 b- defN 23-May-10 11:47 pytorch3d/ops/perspective_n_points.py
+-rw-r--r--  2.0 unx    15119 b- defN 23-May-10 11:47 pytorch3d/ops/points_alignment.py
+-rw-r--r--  2.0 unx     7480 b- defN 23-May-10 11:47 pytorch3d/ops/points_normals.py
+-rw-r--r--  2.0 unx    31002 b- defN 23-May-10 11:47 pytorch3d/ops/points_to_volumes.py
+-rw-r--r--  2.0 unx     7827 b- defN 23-May-10 11:47 pytorch3d/ops/sample_farthest_points.py
+-rw-r--r--  2.0 unx     7091 b- defN 23-May-10 11:47 pytorch3d/ops/sample_points_from_meshes.py
+-rw-r--r--  2.0 unx    17932 b- defN 23-May-10 11:47 pytorch3d/ops/subdivide_meshes.py
+-rw-r--r--  2.0 unx     7417 b- defN 23-May-10 11:47 pytorch3d/ops/utils.py
+-rw-r--r--  2.0 unx     4246 b- defN 23-May-10 11:47 pytorch3d/ops/vert_align.py
+-rw-r--r--  2.0 unx     2212 b- defN 23-May-10 11:47 pytorch3d/renderer/__init__.py
+-rw-r--r--  2.0 unx     9879 b- defN 23-May-10 11:47 pytorch3d/renderer/blending.py
+-rw-r--r--  2.0 unx     6804 b- defN 23-May-10 11:47 pytorch3d/renderer/camera_conversions.py
+-rw-r--r--  2.0 unx     7930 b- defN 23-May-10 11:47 pytorch3d/renderer/camera_utils.py
+-rw-r--r--  2.0 unx    70835 b- defN 23-May-10 11:47 pytorch3d/renderer/cameras.py
+-rw-r--r--  2.0 unx    10686 b- defN 23-May-10 11:47 pytorch3d/renderer/compositing.py
+-rw-r--r--  2.0 unx    21762 b- defN 23-May-10 11:47 pytorch3d/renderer/fisheyecameras.py
+-rw-r--r--  2.0 unx    13007 b- defN 23-May-10 11:47 pytorch3d/renderer/lighting.py
+-rw-r--r--  2.0 unx     2395 b- defN 23-May-10 11:47 pytorch3d/renderer/materials.py
+-rw-r--r--  2.0 unx    25178 b- defN 23-May-10 11:47 pytorch3d/renderer/splatter_blend.py
+-rw-r--r--  2.0 unx    16827 b- defN 23-May-10 11:47 pytorch3d/renderer/utils.py
+-rw-r--r--  2.0 unx      767 b- defN 23-May-10 11:47 pytorch3d/renderer/implicit/__init__.py
+-rw-r--r--  2.0 unx     4429 b- defN 23-May-10 11:47 pytorch3d/renderer/implicit/harmonic_embedding.py
+-rw-r--r--  2.0 unx     9323 b- defN 23-May-10 11:47 pytorch3d/renderer/implicit/raymarching.py
+-rw-r--r--  2.0 unx    31658 b- defN 23-May-10 11:47 pytorch3d/renderer/implicit/raysampling.py
+-rw-r--r--  2.0 unx    17111 b- defN 23-May-10 11:47 pytorch3d/renderer/implicit/renderer.py
+-rw-r--r--  2.0 unx     5817 b- defN 23-May-10 11:47 pytorch3d/renderer/implicit/sample_pdf.py
+-rw-r--r--  2.0 unx     6458 b- defN 23-May-10 11:47 pytorch3d/renderer/implicit/utils.py
+-rw-r--r--  2.0 unx      995 b- defN 23-May-10 11:47 pytorch3d/renderer/mesh/__init__.py
+-rw-r--r--  2.0 unx    34991 b- defN 23-May-10 11:47 pytorch3d/renderer/mesh/clip.py
+-rw-r--r--  2.0 unx    30438 b- defN 23-May-10 11:47 pytorch3d/renderer/mesh/rasterize_meshes.py
+-rw-r--r--  2.0 unx    12527 b- defN 23-May-10 11:47 pytorch3d/renderer/mesh/rasterizer.py
+-rw-r--r--  2.0 unx     4013 b- defN 23-May-10 11:47 pytorch3d/renderer/mesh/renderer.py
+-rw-r--r--  2.0 unx    15488 b- defN 23-May-10 11:47 pytorch3d/renderer/mesh/shader.py
+-rw-r--r--  2.0 unx     8965 b- defN 23-May-10 11:47 pytorch3d/renderer/mesh/shading.py
+-rw-r--r--  2.0 unx    65560 b- defN 23-May-10 11:47 pytorch3d/renderer/mesh/textures.py
+-rw-r--r--  2.0 unx    11238 b- defN 23-May-10 11:47 pytorch3d/renderer/mesh/utils.py
+-rw-r--r--  2.0 unx     1278 b- defN 23-May-10 11:47 pytorch3d/renderer/opengl/__init__.py
+-rw-r--r--  2.0 unx    16419 b- defN 23-May-10 11:47 pytorch3d/renderer/opengl/opengl_utils.py
+-rw-r--r--  2.0 unx    27731 b- defN 23-May-10 11:47 pytorch3d/renderer/opengl/rasterizer_opengl.py
+-rw-r--r--  2.0 unx      543 b- defN 23-May-10 11:47 pytorch3d/renderer/points/__init__.py
+-rw-r--r--  2.0 unx     4323 b- defN 23-May-10 11:47 pytorch3d/renderer/points/compositor.py
+-rw-r--r--  2.0 unx    13071 b- defN 23-May-10 11:47 pytorch3d/renderer/points/rasterize_points.py
+-rw-r--r--  2.0 unx     5995 b- defN 23-May-10 11:47 pytorch3d/renderer/points/rasterizer.py
+-rw-r--r--  2.0 unx     2183 b- defN 23-May-10 11:47 pytorch3d/renderer/points/renderer.py
+-rw-r--r--  2.0 unx      254 b- defN 23-May-10 11:47 pytorch3d/renderer/points/pulsar/__init__.py
+-rw-r--r--  2.0 unx    27302 b- defN 23-May-10 11:47 pytorch3d/renderer/points/pulsar/renderer.py
+-rw-r--r--  2.0 unx    24340 b- defN 23-May-10 11:47 pytorch3d/renderer/points/pulsar/unified.py
+-rw-r--r--  2.0 unx      566 b- defN 23-May-10 11:47 pytorch3d/structures/__init__.py
+-rw-r--r--  2.0 unx    70129 b- defN 23-May-10 11:47 pytorch3d/structures/meshes.py
+-rw-r--r--  2.0 unx    50190 b- defN 23-May-10 11:47 pytorch3d/structures/pointclouds.py
+-rw-r--r--  2.0 unx     8000 b- defN 23-May-10 11:47 pytorch3d/structures/utils.py
+-rw-r--r--  2.0 unx    45907 b- defN 23-May-10 11:47 pytorch3d/structures/volumes.py
+-rw-r--r--  2.0 unx     1076 b- defN 23-May-10 11:47 pytorch3d/transforms/__init__.py
+-rw-r--r--  2.0 unx     2966 b- defN 23-May-10 11:47 pytorch3d/transforms/math.py
+-rw-r--r--  2.0 unx    20283 b- defN 23-May-10 11:47 pytorch3d/transforms/rotation_conversions.py
+-rw-r--r--  2.0 unx     7985 b- defN 23-May-10 11:47 pytorch3d/transforms/se3.py
+-rw-r--r--  2.0 unx    10194 b- defN 23-May-10 11:47 pytorch3d/transforms/so3.py
+-rw-r--r--  2.0 unx    30693 b- defN 23-May-10 11:47 pytorch3d/transforms/transform3d.py
+-rw-r--r--  2.0 unx      554 b- defN 23-May-10 11:47 pytorch3d/utils/__init__.py
+-rw-r--r--  2.0 unx     6366 b- defN 23-May-10 11:47 pytorch3d/utils/camera_conversions.py
+-rw-r--r--  2.0 unx     2915 b- defN 23-May-10 11:47 pytorch3d/utils/checkerboard.py
+-rw-r--r--  2.0 unx     2217 b- defN 23-May-10 11:47 pytorch3d/utils/ico_sphere.py
+-rw-r--r--  2.0 unx     2392 b- defN 23-May-10 11:47 pytorch3d/utils/torus.py
+-rw-r--r--  2.0 unx      645 b- defN 23-May-10 11:47 pytorch3d/vis/__init__.py
+-rw-r--r--  2.0 unx    39884 b- defN 23-May-10 11:47 pytorch3d/vis/plotly_vis.py
+-rw-r--r--  2.0 unx     3814 b- defN 23-May-10 11:47 pytorch3d/vis/texture_vis.py
+-rw-r--r--  2.0 unx     1534 b- defN 23-May-10 12:03 pytorch3d-0.7.4.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3351 b- defN 23-May-10 12:03 pytorch3d-0.7.4.dist-info/LICENSE-3RD-PARTY
+-rw-r--r--  2.0 unx     1000 b- defN 23-May-10 12:03 pytorch3d-0.7.4.dist-info/METADATA
+-rw-r--r--  2.0 unx      109 b- defN 23-May-10 12:03 pytorch3d-0.7.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx      196 b- defN 23-May-10 12:03 pytorch3d-0.7.4.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       19 b- defN 23-May-10 12:03 pytorch3d-0.7.4.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    20501 b- defN 23-May-10 12:03 pytorch3d-0.7.4.dist-info/RECORD
+210 files, 3073618 bytes uncompressed, 876783 bytes compressed:  71.5%
```

## zipnote {}

```diff
@@ -102,23 +102,35 @@
 
 Filename: pytorch3d/implicitron/dataset/load_blender.py
 Comment: 
 
 Filename: pytorch3d/implicitron/dataset/load_llff.py
 Comment: 
 
+Filename: pytorch3d/implicitron/dataset/orm_types.py
+Comment: 
+
 Filename: pytorch3d/implicitron/dataset/rendered_mesh_dataset_map_provider.py
 Comment: 
 
 Filename: pytorch3d/implicitron/dataset/scene_batch_sampler.py
 Comment: 
 
 Filename: pytorch3d/implicitron/dataset/single_sequence_dataset.py
 Comment: 
 
+Filename: pytorch3d/implicitron/dataset/sql_dataset.py
+Comment: 
+
+Filename: pytorch3d/implicitron/dataset/sql_dataset_provider.py
+Comment: 
+
+Filename: pytorch3d/implicitron/dataset/train_eval_data_loader_provider.py
+Comment: 
+
 Filename: pytorch3d/implicitron/dataset/types.py
 Comment: 
 
 Filename: pytorch3d/implicitron/dataset/utils.py
 Comment: 
 
 Filename: pytorch3d/implicitron/dataset/visualize.py
@@ -591,29 +603,29 @@
 
 Filename: pytorch3d/vis/plotly_vis.py
 Comment: 
 
 Filename: pytorch3d/vis/texture_vis.py
 Comment: 
 
-Filename: pytorch3d-0.7.3.dist-info/LICENSE
+Filename: pytorch3d-0.7.4.dist-info/LICENSE
 Comment: 
 
-Filename: pytorch3d-0.7.3.dist-info/LICENSE-3RD-PARTY
+Filename: pytorch3d-0.7.4.dist-info/LICENSE-3RD-PARTY
 Comment: 
 
-Filename: pytorch3d-0.7.3.dist-info/METADATA
+Filename: pytorch3d-0.7.4.dist-info/METADATA
 Comment: 
 
-Filename: pytorch3d-0.7.3.dist-info/WHEEL
+Filename: pytorch3d-0.7.4.dist-info/WHEEL
 Comment: 
 
-Filename: pytorch3d-0.7.3.dist-info/entry_points.txt
+Filename: pytorch3d-0.7.4.dist-info/entry_points.txt
 Comment: 
 
-Filename: pytorch3d-0.7.3.dist-info/top_level.txt
+Filename: pytorch3d-0.7.4.dist-info/top_level.txt
 Comment: 
 
-Filename: pytorch3d-0.7.3.dist-info/RECORD
+Filename: pytorch3d-0.7.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pytorch3d/__init__.py

```diff
@@ -1,7 +1,7 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
-__version__ = "0.7.3"
+__version__ = "0.7.4"
```

## pytorch3d/implicitron/dataset/frame_data.py

```diff
@@ -446,14 +446,17 @@
     frame_data_type: ClassVar[Type[FrameDataSubtype]]
 
     @abstractmethod
     def build(
         self,
         frame_annotation: types.FrameAnnotation,
         sequence_annotation: types.SequenceAnnotation,
+        *,
+        load_blobs: bool = True,
+        **kwargs,
     ) -> FrameDataSubtype:
         """An abstract method to build the frame data based on raw frame/sequence
         annotations, load the binary data and adjust them according to the metadata.
         """
         raise NotImplementedError()
 
 
@@ -461,16 +464,17 @@
     """
     A class to build a FrameData object, load and process the binary data (crop and
     resize). This is an abstract class for extending to build FrameData subtypes. Most
     users need to use concrete `FrameDataBuilder` class instead.
     Beware that modifications of frame data are done in-place.
 
     Args:
-        dataset_root: The root folder of the dataset; all the paths in jsons are
-                specified relative to this root (but not json paths themselves).
+        dataset_root: The root folder of the dataset; all paths in frame / sequence
+            annotations are defined w.r.t. this root. Has to be set if any of the
+            load_* flabs below is true.
         load_images: Enable loading the frame RGB data.
         load_depths: Enable loading the frame depth maps.
         load_depth_masks: Enable loading the frame depth map masks denoting the
             depth values used for evaluation (the points consistent across views).
         load_masks: Enable loading frame foreground masks.
         load_point_clouds: Enable loading sequence-level point clouds.
         max_points: Cap on the number of loaded points in the point cloud;
@@ -490,15 +494,15 @@
             and background based on the foreground_probability mask; if no value
             is greater than this threshold, the loader lowers it and repeats.
         box_crop_context: The amount of additional padding added to each
             dimension of the cropping bounding box, relative to box size.
         path_manager: Optionally a PathManager for interpreting paths in a special way.
     """
 
-    dataset_root: str = ""
+    dataset_root: Optional[str] = None
     load_images: bool = True
     load_depths: bool = True
     load_depth_masks: bool = True
     load_masks: bool = True
     load_point_clouds: bool = False
     max_points: int = 0
     mask_images: bool = False
@@ -506,19 +510,45 @@
     image_height: Optional[int] = 800
     image_width: Optional[int] = 800
     box_crop: bool = True
     box_crop_mask_thr: float = 0.4
     box_crop_context: float = 0.3
     path_manager: Any = None
 
+    def __post_init__(self) -> None:
+        load_any_blob = (
+            self.load_images
+            or self.load_depths
+            or self.load_depth_masks
+            or self.load_masks
+            or self.load_point_clouds
+        )
+        if load_any_blob and self.dataset_root is None:
+            raise ValueError(
+                "dataset_root must be set to load any blob data. "
+                "Make sure it is set in either FrameDataBuilder or Dataset params."
+            )
+
+        if self.path_manager is None:
+            dataset_root_exists = os.path.isdir(self.dataset_root)  # pyre-ignore
+        else:
+            dataset_root_exists = self.path_manager.isdir(self.dataset_root)
+
+        if load_any_blob and not dataset_root_exists:
+            raise ValueError(
+                f"dataset_root is passed but {self.dataset_root} does not exist."
+            )
+
     def build(
         self,
         frame_annotation: types.FrameAnnotation,
         sequence_annotation: types.SequenceAnnotation,
+        *,
         load_blobs: bool = True,
+        **kwargs,
     ) -> FrameDataSubtype:
         """Builds the frame data based on raw frame/sequence annotations, loads the
         binary data and adjust them according to the metadata. The processing includes:
             * if box_crop is set, the image/mask/depth are cropped with the bounding
                 box provided or estimated from MaskAnnotation,
             * if image_height/image_width are set, the image/mask/depth are resized to
                 fit that resolution. Note that the aspect ratio is preserved, and the
@@ -551,20 +581,27 @@
             point_cloud_quality_score=safe_as_tensor(
                 point_cloud.quality_score, torch.float
             )
             if point_cloud is not None
             else None,
         )
 
-        if load_blobs and self.load_masks and frame_annotation.mask is not None:
-            (
-                frame_data.fg_probability,
-                frame_data.mask_path,
-                frame_data.bbox_xywh,
-            ) = self._load_fg_probability(frame_annotation)
+        mask_annotation = frame_annotation.mask
+        if mask_annotation is not None:
+            fg_mask_np: Optional[np.ndarray] = None
+            if load_blobs and self.load_masks:
+                fg_mask_np, mask_path = self._load_fg_probability(frame_annotation)
+                frame_data.mask_path = mask_path
+                frame_data.fg_probability = safe_as_tensor(fg_mask_np, torch.float)
+
+            bbox_xywh = mask_annotation.bounding_box_xywh
+            if bbox_xywh is None and fg_mask_np is not None:
+                bbox_xywh = get_bbox_from_mask(fg_mask_np, self.box_crop_mask_thr)
+
+            frame_data.bbox_xywh = safe_as_tensor(bbox_xywh, torch.float)
 
         if frame_annotation.image is not None:
             image_size_hw = safe_as_tensor(frame_annotation.image.size, torch.long)
             frame_data.image_size_hw = image_size_hw  # original image size
             # image size after crop/resize
             frame_data.effective_image_size_hw = image_size_hw
 
@@ -600,33 +637,24 @@
                 new_size_hw=torch.tensor(new_size, dtype=torch.long),  # pyre-ignore
             )
 
         return frame_data
 
     def _load_fg_probability(
         self, entry: types.FrameAnnotation
-    ) -> Tuple[Optional[torch.Tensor], Optional[str], Optional[torch.Tensor]]:
-
-        full_path = os.path.join(self.dataset_root, entry.mask.path)  # pyre-ignore
+    ) -> Tuple[np.ndarray, str]:
+        assert self.dataset_root is not None and entry.mask is not None
+        full_path = os.path.join(self.dataset_root, entry.mask.path)
         fg_probability = load_mask(self._local_path(full_path))
-        # we can use provided bbox_xywh or calculate it based on mask
-        # saves time to skip bbox calculation
-        # pyre-ignore
-        bbox_xywh = entry.mask.bounding_box_xywh or get_bbox_from_mask(
-            fg_probability, self.box_crop_mask_thr
-        )
         if fg_probability.shape[-2:] != entry.image.size:
             raise ValueError(
                 f"bad mask size: {fg_probability.shape[-2:]} vs {entry.image.size}!"
             )
-        return (
-            safe_as_tensor(fg_probability, torch.float),
-            full_path,
-            safe_as_tensor(bbox_xywh, torch.long),
-        )
+
+        return fg_probability, full_path
 
     def _load_images(
         self,
         entry: types.FrameAnnotation,
         fg_probability: Optional[torch.Tensor],
     ) -> Tuple[torch.Tensor, str]:
         assert self.dataset_root is not None and entry.image is not None
@@ -646,24 +674,25 @@
 
     def _load_mask_depth(
         self,
         entry: types.FrameAnnotation,
         fg_probability: Optional[torch.Tensor],
     ) -> Tuple[torch.Tensor, str, torch.Tensor]:
         entry_depth = entry.depth
-        assert entry_depth is not None
+        assert self.dataset_root is not None and entry_depth is not None
         path = os.path.join(self.dataset_root, entry_depth.path)
         depth_map = load_depth(self._local_path(path), entry_depth.scale_adjustment)
 
         if self.mask_depths:
             assert fg_probability is not None
             depth_map *= fg_probability
 
         if self.load_depth_masks:
             assert entry_depth.mask_path is not None
+            # pyre-ignore
             mask_path = os.path.join(self.dataset_root, entry_depth.mask_path)
             depth_mask = load_depth_mask(self._local_path(mask_path))
         else:
             depth_mask = torch.ones_like(depth_map)
 
         return torch.tensor(depth_map), path, torch.tensor(depth_mask)
 
@@ -704,14 +733,15 @@
         Some files in Co3Dv2 have an accidental absolute path stored.
         """
         unwanted_prefix = (
             "/large_experiments/p3/replay/datasets/co3d/co3d45k_220512/export_v23/"
         )
         if path.startswith(unwanted_prefix):
             path = path[len(unwanted_prefix) :]
+        assert self.dataset_root is not None
         return os.path.join(self.dataset_root, path)
 
     def _local_path(self, path: str) -> str:
         if self.path_manager is None:
             return path
         return self.path_manager.get_local_path(path)
```

## pytorch3d/implicitron/dataset/json_index_dataset.py

```diff
@@ -186,14 +186,15 @@
             mask_images=self.mask_images,
             mask_depths=self.mask_depths,
             image_height=self.image_height,
             image_width=self.image_width,
             box_crop=self.box_crop,
             box_crop_mask_thr=self.box_crop_mask_thr,
             box_crop_context=self.box_crop_context,
+            path_manager=self.path_manager,
         )
         logger.info(str(self))
 
     def _extract_and_set_eval_batches(self) -> None:
         """
         Sets eval_batches based on input eval_batch_index.
         """
```

## pytorch3d/implicitron/dataset/types.py

```diff
@@ -200,15 +200,15 @@
     is_optional, contained_type = _resolve_optional(typeannot)
     if is_optional:
         return _dataclass_list_from_dict_list(dlist, contained_type)
 
     # otherwise, we dispatch by the type of the provided annotation to convert to
     if issubclass(cls, tuple) and hasattr(cls, "_fields"):  # namedtuple
         # For namedtuple, call the function recursively on the lists of corresponding keys
-        types = cls._field_types.values()
+        types = cls.__annotations__.values()
         dlist_T = zip(*dlist)
         res_T = [
             _dataclass_list_from_dict_list(key_list, tp)
             for key_list, tp in zip(dlist_T, types)
         ]
         return [cls(*converted_as_tuple) for converted_as_tuple in zip(*res_T)]
     elif issubclass(cls, (list, tuple)):
@@ -266,15 +266,15 @@
     is_optional, contained_type = _resolve_optional(typeannot)
     if is_optional:
         # an Optional not set to None, just use the contents of the Optional.
         return _dataclass_from_dict(d, contained_type)
 
     cls = get_origin(typeannot) or typeannot
     if issubclass(cls, tuple) and hasattr(cls, "_fields"):  # namedtuple
-        types = cls._field_types.values()
+        types = cls.__annotations__.values()
         return cls(*[_dataclass_from_dict(v, tp) for v, tp in zip(d, types)])
     elif issubclass(cls, (list, tuple)):
         types = get_args(typeannot)
         if len(types) == 1:  # probably List; replicate for all items
             types = types * len(d)
         return cls(_dataclass_from_dict(v, tp) for v, tp in zip(d, types))
     elif issubclass(cls, dict):
```

## pytorch3d/implicitron/evaluation/evaluate_new_view_synthesis.py

```diff
@@ -34,16 +34,16 @@
 
 
 @dataclass
 class _Visualizer:
     image_render: torch.Tensor
     image_rgb_masked: torch.Tensor
     depth_render: torch.Tensor
-    depth_map: torch.Tensor
-    depth_mask: torch.Tensor
+    depth_map: Optional[torch.Tensor]
+    depth_mask: Optional[torch.Tensor]
 
     visdom_env: str = "eval_debug"
 
     _viz: Optional["Visdom"] = field(init=False)
 
     def __post_init__(self):
         self._viz = vis_utils.get_visdom_connection()
@@ -71,66 +71,70 @@
         self, depth_loss: float, name_postfix: str, loss_mask_now: torch.Tensor
     ):
         if self._viz is None:
             return
         viz = self._viz
         viz.images(
             torch.cat(
-                (
-                    make_depth_image(self.depth_render, loss_mask_now),
-                    make_depth_image(self.depth_map, loss_mask_now),
+                (make_depth_image(self.depth_render, loss_mask_now),)
+                + (
+                    (make_depth_image(self.depth_map, loss_mask_now),)
+                    if self.depth_map is not None
+                    else ()
                 ),
                 dim=3,
             ),
             env=self.visdom_env,
             win="depth_abs" + name_postfix,
             opts={"title": f"depth_abs_{name_postfix}_{depth_loss:1.2f}"},
         )
         viz.images(
             loss_mask_now,
             env=self.visdom_env,
             win="depth_abs" + name_postfix + "_mask",
             opts={"title": f"depth_abs_{name_postfix}_{depth_loss:1.2f}_mask"},
         )
-        viz.images(
-            self.depth_mask,
-            env=self.visdom_env,
-            win="depth_abs" + name_postfix + "_maskd",
-            opts={"title": f"depth_abs_{name_postfix}_{depth_loss:1.2f}_maskd"},
-        )
+        if self.depth_mask is not None:
+            viz.images(
+                self.depth_mask,
+                env=self.visdom_env,
+                win="depth_abs" + name_postfix + "_maskd",
+                opts={"title": f"depth_abs_{name_postfix}_{depth_loss:1.2f}_maskd"},
+            )
 
         # show the 3D plot
         # pyre-fixme[9]: viewpoint_trivial has type `PerspectiveCameras`; used as
         #  `TensorProperties`.
         viewpoint_trivial: PerspectiveCameras = PerspectiveCameras().to(
             loss_mask_now.device
         )
-        pcl_pred = get_rgbd_point_cloud(
-            viewpoint_trivial,
-            self.image_render,
-            self.depth_render,
-            # mask_crop,
-            torch.ones_like(self.depth_render),
-            # loss_mask_now,
-        )
-        pcl_gt = get_rgbd_point_cloud(
-            viewpoint_trivial,
-            self.image_rgb_masked,
-            self.depth_map,
-            # mask_crop,
-            torch.ones_like(self.depth_map),
-            # loss_mask_now,
-        )
         _pcls = {
-            pn: p
-            for pn, p in zip(("pred_depth", "gt_depth"), (pcl_pred, pcl_gt))
-            if int(p.num_points_per_cloud()) > 0
+            "pred_depth": get_rgbd_point_cloud(
+                viewpoint_trivial,
+                self.image_render,
+                self.depth_render,
+                # mask_crop,
+                torch.ones_like(self.depth_render),
+                # loss_mask_now,
+            )
         }
+        if self.depth_map is not None:
+            _pcls["gt_depth"] = get_rgbd_point_cloud(
+                viewpoint_trivial,
+                self.image_rgb_masked,
+                self.depth_map,
+                # mask_crop,
+                torch.ones_like(self.depth_map),
+                # loss_mask_now,
+            )
+
+        _pcls = {pn: p for pn, p in _pcls.items() if int(p.num_points_per_cloud()) > 0}
+
         plotlyplot = plot_scene(
-            {f"pcl{name_postfix}": _pcls},
+            {f"pcl{name_postfix}": _pcls},  # pyre-ignore
             camera_scale=1.0,
             pointcloud_max_points=10000,
             pointcloud_marker_size=1,
         )
         viz.plotlyplot(
             plotlyplot,
             env=self.visdom_env,
@@ -273,18 +277,18 @@
     image_render = cloned_render["image_render"].clamp(0.0, 1.0)
 
     if visualize:
         visualizer = _Visualizer(
             image_render=image_render,
             image_rgb_masked=image_rgb_masked,
             depth_render=cloned_render["depth_render"],
-            # pyre-fixme[6]: Expected `Tensor` for 4th param but got
-            #  `Optional[torch.Tensor]`.
             depth_map=frame_data.depth_map,
-            depth_mask=frame_data.depth_mask[:1],
+            depth_mask=frame_data.depth_mask[:1]
+            if frame_data.depth_mask is not None
+            else None,
             visdom_env=visualize_visdom_env,
         )
 
     results: Dict[str, Any] = {}
 
     results["iou"] = iou(
         cloned_render["mask_render"],
```

## pytorch3d/implicitron/evaluation/evaluator.py

```diff
@@ -53,14 +53,15 @@
 
     # UNUSED; preserved for compatibility purposes
     camera_difficulty_bin_breaks: Tuple[float, ...] = 0.97, 0.98
 
     def __post_init__(self):
         run_auto_creation(self)
 
+    # pyre-fixme[14]: `run` overrides method defined in `EvaluatorBase` inconsistently.
     def run(
         self,
         model: ImplicitronModelBase,
         dataloader: DataLoader,
         device: torch.device,
         dump_to_json: bool = False,
         exp_dir: Optional[str] = None,
```

## pytorch3d/implicitron/models/generic_model.py

```diff
@@ -356,15 +356,15 @@
                 are used to supervise the renders; the rest corresponding to the source
                 viewpoints from which features will be extracted.
             camera: An instance of CamerasBase containing a batch of `B` cameras corresponding
                 to the viewpoints of target images, from which the rays will be sampled,
                 and source images, which will be used for intersecting with target rays.
             fg_probability: A tensor of shape `(B, 1, H, W)` containing a batch of
                 foreground masks.
-            mask_crop: A binary tensor of shape `(B, 1, H, W)` deonting valid
+            mask_crop: A binary tensor of shape `(B, 1, H, W)` denoting valid
                 regions in the input images (i.e. regions that do not correspond
                 to, e.g., zero-padding). When the `RaySampler`'s sampling mode is set to
                 "mask_sample", rays  will be sampled in the non zero regions.
             depth_map: A tensor of shape `(B, 1, H, W)` containing a batch of depth maps.
             sequence_name: A list of `B` strings corresponding to the sequence names
                 from which images `image_rgb` were extracted. They are used to match
                 target frames with relevant source frames.
```

## pytorch3d/implicitron/models/model_dbir.py

```diff
@@ -37,14 +37,16 @@
     """
 
     render_image_width: int = 256
     render_image_height: int = 256
     bg_color: Tuple[float, float, float] = (0.0, 0.0, 0.0)
     max_points: int = -1
 
+    # pyre-fixme[14]: `forward` overrides method defined in `ImplicitronModelBase`
+    #  inconsistently.
     def forward(
         self,
         *,  # force keyword-only arguments
         image_rgb: Optional[torch.Tensor],
         camera: CamerasBase,
         fg_probability: Optional[torch.Tensor],
         mask_crop: Optional[torch.Tensor],
```

## pytorch3d/implicitron/models/renderer/raymarcher.py

```diff
@@ -107,14 +107,16 @@
         }[self.capping_function_type]
 
         self._weight_function: Callable[[_TTensor, _TTensor], _TTensor] = {
             "product": lambda curr, acc: curr * acc,
             "minimum": lambda curr, acc: torch.minimum(curr, acc),
         }[self.weight_function_type]
 
+    # pyre-fixme[14]: `forward` overrides method defined in `RaymarcherBase`
+    #  inconsistently.
     def forward(
         self,
         rays_densities: torch.Tensor,
         rays_features: torch.Tensor,
         aux: Dict[str, Any],
         ray_lengths: torch.Tensor,
         density_noise_std: float = 0.0,
```

## pytorch3d/io/experimental_gltf_io.py

```diff
@@ -389,15 +389,15 @@
         Args:
             primitive: the mesh primitive being loaded.
             indices: the face indices of the mesh
         """
         attributes = primitive["attributes"]
         vertex_colors = self._get_primitive_attribute(attributes, "COLOR_0", np.float32)
         if vertex_colors is not None:
-            return TexturesVertex(torch.from_numpy(vertex_colors))
+            return TexturesVertex([torch.from_numpy(vertex_colors)])
 
         vertex_texcoords_0 = self._get_primitive_attribute(
             attributes, "TEXCOORD_0", np.float32
         )
         if vertex_texcoords_0 is not None:
             verts_uvs = torch.from_numpy(vertex_texcoords_0)
             verts_uvs[:, 1] = 1 - verts_uvs[:, -1]
@@ -555,20 +555,34 @@
         node = {"name": "Node", "mesh": 0}
         self._json_data["nodes"].append(node)
 
         # mesh primitives
         meshes = defaultdict(list)
         # pyre-fixme[6]: Incompatible parameter type
         meshes["name"] = "Node-Mesh"
-        primitives = {
-            "attributes": {"POSITION": 0, "TEXCOORD_0": 2},
-            "indices": 1,
-            "material": 0,  # default material
-            "mode": _PrimitiveMode.TRIANGLES,
-        }
+        if isinstance(self.mesh.textures, TexturesVertex):
+            primitives = {
+                "attributes": {"POSITION": 0, "COLOR_0": 2},
+                "indices": 1,
+                "mode": _PrimitiveMode.TRIANGLES,
+            }
+        elif isinstance(self.mesh.textures, TexturesUV):
+            primitives = {
+                "attributes": {"POSITION": 0, "TEXCOORD_0": 2},
+                "indices": 1,
+                "mode": _PrimitiveMode.TRIANGLES,
+                "material": 0,
+            }
+        else:
+            primitives = {
+                "attributes": {"POSITION": 0},
+                "indices": 1,
+                "mode": _PrimitiveMode.TRIANGLES,
+            }
+
         meshes["primitives"].append(primitives)
         self._json_data["meshes"].append(meshes)
 
         # default material
         material = {
             "name": "material_1",
             "pbrMetallicRoughness": {
@@ -606,14 +620,22 @@
             data = self.mesh.textures.verts_uvs_list()[0].cpu().numpy()
             data[:, 1] = 1 - data[:, -1]  # flip y tex-coordinate
             element_type = "VEC2"
             buffer_view = 2
             element_min = list(map(float, np.min(data, axis=0)))
             element_max = list(map(float, np.max(data, axis=0)))
             byte_per_element = 2 * _DTYPE_BYTES[_ITEM_TYPES[_ComponentType.FLOAT]]
+        elif key == "texvertices":
+            component_type = _ComponentType.FLOAT
+            data = self.mesh.textures.verts_features_list()[0].cpu().numpy()
+            element_type = "VEC3"
+            buffer_view = 2
+            element_min = list(map(float, np.min(data, axis=0)))
+            element_max = list(map(float, np.max(data, axis=0)))
+            byte_per_element = 3 * _DTYPE_BYTES[_ITEM_TYPES[_ComponentType.FLOAT]]
         elif key == "indices":
             component_type = _ComponentType.UNSIGNED_SHORT
             data = (
                 self.mesh.faces_packed()
                 .cpu()
                 .numpy()
                 .astype(_ITEM_TYPES[component_type])
@@ -642,29 +664,35 @@
             "max": element_max,
             "count": count * 3 if key == "indices" else count,
         }
         self._json_data["accessors"].append(accessor_json)
         return (byte_length, data)
 
     def _write_bufferview(self, key: str, **kwargs):
-        if key not in ["positions", "texcoords", "indices"]:
-            raise ValueError("key must be one of positions, texcoords or indices")
+        if key not in ["positions", "texcoords", "texvertices", "indices"]:
+            raise ValueError(
+                "key must be one of positions, texcoords, texvertices or indices"
+            )
 
         bufferview = {
             "name": "bufferView_%s" % key,
             "buffer": 0,
         }
         target = _TargetType.ARRAY_BUFFER
         if key == "positions":
             byte_per_element = 3 * _DTYPE_BYTES[_ITEM_TYPES[_ComponentType.FLOAT]]
             bufferview["byteStride"] = int(byte_per_element)
         elif key == "texcoords":
             byte_per_element = 2 * _DTYPE_BYTES[_ITEM_TYPES[_ComponentType.FLOAT]]
             target = _TargetType.ARRAY_BUFFER
             bufferview["byteStride"] = int(byte_per_element)
+        elif key == "texvertices":
+            byte_per_element = 3 * _DTYPE_BYTES[_ITEM_TYPES[_ComponentType.FLOAT]]
+            target = _TargetType.ELEMENT_ARRAY_BUFFER
+            bufferview["byteStride"] = int(byte_per_element)
         elif key == "indices":
             byte_per_element = (
                 3 * _DTYPE_BYTES[_ITEM_TYPES[_ComponentType.UNSIGNED_SHORT]]
             )
             target = _TargetType.ELEMENT_ARRAY_BUFFER
 
         bufferview["target"] = target
@@ -697,41 +725,46 @@
         if self.mesh.verts_packed() is None or self.mesh.faces_packed() is None:
             raise ValueError("invalid mesh to save, verts or face indices are empty")
 
         # accessors for positions, texture uvs and face indices
         pos_byte, pos_data = self._write_accessor_json("positions")
         idx_byte, idx_data = self._write_accessor_json("indices")
         include_textures = False
-        if (
-            self.mesh.textures is not None
-            and self.mesh.textures.verts_uvs_list()[0] is not None
-        ):
-            tex_byte, tex_data = self._write_accessor_json("texcoords")
-            include_textures = True
+        if self.mesh.textures is not None:
+            if hasattr(self.mesh.textures, "verts_features_list"):
+                tex_byte, tex_data = self._write_accessor_json("texvertices")
+                include_textures = True
+                texcoords = False
+            elif self.mesh.textures.verts_uvs_list()[0] is not None:
+                tex_byte, tex_data = self._write_accessor_json("texcoords")
+                include_textures = True
+                texcoords = True
 
         # bufferViews for positions, texture coords and indices
         byte_offset = 0
         self._write_bufferview("positions", byte_length=pos_byte, offset=byte_offset)
         byte_offset += pos_byte
 
         self._write_bufferview("indices", byte_length=idx_byte, offset=byte_offset)
         byte_offset += idx_byte
 
         if include_textures:
-            self._write_bufferview(
-                "texcoords", byte_length=tex_byte, offset=byte_offset
-            )
+            if texcoords:
+                self._write_bufferview(
+                    "texcoords", byte_length=tex_byte, offset=byte_offset
+                )
+            else:
+                self._write_bufferview(
+                    "texvertices", byte_length=tex_byte, offset=byte_offset
+                )
             byte_offset += tex_byte
 
         # image bufferView
         include_image = False
-        if (
-            self.mesh.textures is not None
-            and self.mesh.textures.maps_list()[0] is not None
-        ):
+        if self.mesh.textures is not None and hasattr(self.mesh.textures, "maps_list"):
             include_image = True
             image_byte, image_data = self._write_image_buffer(offset=byte_offset)
             byte_offset += image_byte
 
         # buffers
         self._json_data["buffers"].append({"byteLength": int(byte_offset)})
```

## pytorch3d/io/obj_io.py

```diff
@@ -680,28 +680,34 @@
 def save_obj(
     f: PathOrStr,
     verts,
     faces,
     decimal_places: Optional[int] = None,
     path_manager: Optional[PathManager] = None,
     *,
+    normals: Optional[torch.Tensor] = None,
+    faces_normals_idx: Optional[torch.Tensor] = None,
     verts_uvs: Optional[torch.Tensor] = None,
     faces_uvs: Optional[torch.Tensor] = None,
     texture_map: Optional[torch.Tensor] = None,
 ) -> None:
     """
     Save a mesh to an .obj file.
 
     Args:
         f: File (str or path) to which the mesh should be written.
         verts: FloatTensor of shape (V, 3) giving vertex coordinates.
         faces: LongTensor of shape (F, 3) giving faces.
         decimal_places: Number of decimal places for saving.
         path_manager: Optional PathManager for interpreting f if
             it is a str.
+        normals: FloatTensor of shape (V, 3) giving normals for faces_normals_idx
+            to index into.
+        faces_normals_idx: LongTensor of shape (F, 3) giving the index into
+            normals for each vertex in the face.
         verts_uvs: FloatTensor of shape (V, 2) giving the uv coordinate per vertex.
         faces_uvs: LongTensor of shape (F, 3) giving the index into verts_uvs for
             each vertex in the face.
         texture_map: FloatTensor of shape (H, W, 3) representing the texture map
             for the mesh which will be saved as an image. The values are expected
             to be in the range [0, 1],
     """
@@ -709,14 +715,30 @@
         message = "'verts' should either be empty or of shape (num_verts, 3)."
         raise ValueError(message)
 
     if len(faces) and (faces.dim() != 2 or faces.size(1) != 3):
         message = "'faces' should either be empty or of shape (num_faces, 3)."
         raise ValueError(message)
 
+    if (normals is None) != (faces_normals_idx is None):
+        message = "'normals' and 'faces_normals_idx' must both be None or neither."
+        raise ValueError(message)
+
+    if faces_normals_idx is not None and (
+        faces_normals_idx.dim() != 2 or faces_normals_idx.size(1) != 3
+    ):
+        message = (
+            "'faces_normals_idx' should either be empty or of shape (num_faces, 3)."
+        )
+        raise ValueError(message)
+
+    if normals is not None and (normals.dim() != 2 or normals.size(1) != 3):
+        message = "'normals' should either be empty or of shape (num_verts, 3)."
+        raise ValueError(message)
+
     if faces_uvs is not None and (faces_uvs.dim() != 2 or faces_uvs.size(1) != 3):
         message = "'faces_uvs' should either be empty or of shape (num_faces, 3)."
         raise ValueError(message)
 
     if verts_uvs is not None and (verts_uvs.dim() != 2 or verts_uvs.size(1) != 2):
         message = "'verts_uvs' should either be empty or of shape (num_verts, 2)."
         raise ValueError(message)
@@ -738,17 +760,20 @@
             obj_header = "\nmtllib {0}.mtl\nusemtl mesh\n\n".format(output_path.stem)
             f.write(obj_header)
         _save(
             f,
             verts,
             faces,
             decimal_places,
+            normals=normals,
+            faces_normals_idx=faces_normals_idx,
             verts_uvs=verts_uvs,
             faces_uvs=faces_uvs,
             save_texture=save_texture,
+            save_normals=normals is not None,
         )
 
     # Save the .mtl and .png files associated with the texture
     if save_texture:
         image_path = output_path.with_suffix(".png")
         mtl_path = output_path.with_suffix(".mtl")
         if isinstance(f, str):
@@ -773,17 +798,20 @@
 # TODO (nikhilar) Speed up this function.
 def _save(
     f,
     verts,
     faces,
     decimal_places: Optional[int] = None,
     *,
+    normals: Optional[torch.Tensor] = None,
+    faces_normals_idx: Optional[torch.Tensor] = None,
     verts_uvs: Optional[torch.Tensor] = None,
     faces_uvs: Optional[torch.Tensor] = None,
     save_texture: bool = False,
+    save_normals: bool = False,
 ) -> None:
 
     if len(verts) and (verts.dim() != 2 or verts.size(1) != 3):
         message = "'verts' should either be empty or of shape (num_verts, 3)."
         raise ValueError(message)
 
     if len(faces) and (faces.dim() != 2 or faces.size(1) != 3):
@@ -794,59 +822,118 @@
         warnings.warn("Empty 'verts' and 'faces' arguments provided")
         return
 
     verts, faces = verts.cpu(), faces.cpu()
 
     lines = ""
 
-    if len(verts):
-        if decimal_places is None:
-            float_str = "%f"
-        else:
-            float_str = "%" + ".%df" % decimal_places
+    if decimal_places is None:
+        float_str = "%f"
+    else:
+        float_str = "%" + ".%df" % decimal_places
 
+    if len(verts):
         V, D = verts.shape
         for i in range(V):
             vert = [float_str % verts[i, j] for j in range(D)]
             lines += "v %s\n" % " ".join(vert)
 
+    if save_normals:
+        assert normals is not None
+        assert faces_normals_idx is not None
+        lines += _write_normals(normals, faces_normals_idx, float_str)
+
     if save_texture:
+        assert faces_uvs is not None
+        assert verts_uvs is not None
+
         if faces_uvs is not None and (faces_uvs.dim() != 2 or faces_uvs.size(1) != 3):
             message = "'faces_uvs' should either be empty or of shape (num_faces, 3)."
             raise ValueError(message)
 
         if verts_uvs is not None and (verts_uvs.dim() != 2 or verts_uvs.size(1) != 2):
             message = "'verts_uvs' should either be empty or of shape (num_verts, 2)."
             raise ValueError(message)
 
-        # pyre-fixme[16] # undefined attribute cpu
         verts_uvs, faces_uvs = verts_uvs.cpu(), faces_uvs.cpu()
 
         # Save verts uvs after verts
         if len(verts_uvs):
             uV, uD = verts_uvs.shape
             for i in range(uV):
                 uv = [float_str % verts_uvs[i, j] for j in range(uD)]
                 lines += "vt %s\n" % " ".join(uv)
 
+    f.write(lines)
+
     if torch.any(faces >= verts.shape[0]) or torch.any(faces < 0):
         warnings.warn("Faces have invalid indices")
 
     if len(faces):
-        F, P = faces.shape
-        for i in range(F):
-            if save_texture:
-                # Format faces as {verts_idx}/{verts_uvs_idx}
+        _write_faces(
+            f,
+            faces,
+            faces_uvs if save_texture else None,
+            faces_normals_idx if save_normals else None,
+        )
+
+
+def _write_normals(
+    normals: torch.Tensor, faces_normals_idx: torch.Tensor, float_str: str
+) -> str:
+    if faces_normals_idx.dim() != 2 or faces_normals_idx.size(1) != 3:
+        message = (
+            "'faces_normals_idx' should either be empty or of shape (num_faces, 3)."
+        )
+        raise ValueError(message)
+
+    if normals.dim() != 2 or normals.size(1) != 3:
+        message = "'normals' should either be empty or of shape (num_verts, 3)."
+        raise ValueError(message)
+
+    normals, faces_normals_idx = normals.cpu(), faces_normals_idx.cpu()
+
+    lines = []
+    V, D = normals.shape
+    for i in range(V):
+        normal = [float_str % normals[i, j] for j in range(D)]
+        lines.append("vn %s\n" % " ".join(normal))
+    return "".join(lines)
+
+
+def _write_faces(
+    f,
+    faces: torch.Tensor,
+    faces_uvs: Optional[torch.Tensor],
+    faces_normals_idx: Optional[torch.Tensor],
+) -> None:
+    F, P = faces.shape
+    for i in range(F):
+        if faces_normals_idx is not None:
+            if faces_uvs is not None:
+                # Format faces as {verts_idx}/{verts_uvs_idx}/{verts_normals_idx}
                 face = [
-                    "%d/%d" % (faces[i, j] + 1, faces_uvs[i, j] + 1) for j in range(P)
+                    "%d/%d/%d"
+                    % (
+                        faces[i, j] + 1,
+                        faces_uvs[i, j] + 1,
+                        faces_normals_idx[i, j] + 1,
+                    )
+                    for j in range(P)
                 ]
             else:
-                face = ["%d" % (faces[i, j] + 1) for j in range(P)]
-
-            if i + 1 < F:
-                lines += "f %s\n" % " ".join(face)
-
-            elif i + 1 == F:
-                # No newline at the end of the file.
-                lines += "f %s" % " ".join(face)
+                # Format faces as {verts_idx}//{verts_normals_idx}
+                face = [
+                    "%d//%d" % (faces[i, j] + 1, faces_normals_idx[i, j] + 1)
+                    for j in range(P)
+                ]
+        elif faces_uvs is not None:
+            # Format faces as {verts_idx}/{verts_uvs_idx}
+            face = ["%d/%d" % (faces[i, j] + 1, faces_uvs[i, j] + 1) for j in range(P)]
+        else:
+            face = ["%d" % (faces[i, j] + 1) for j in range(P)]
 
-    f.write(lines)
+        if i + 1 < F:
+            f.write("f %s\n" % " ".join(face))
+        else:
+            # No newline at the end of the file.
+            f.write("f %s" % " ".join(face))
```

## pytorch3d/renderer/cameras.py

```diff
@@ -371,22 +371,22 @@
         """
         Specifies whether the camera is defined in NDC space
         or in screen (image) space
         """
         raise NotImplementedError()
 
     def get_znear(self):
-        return self.znear if hasattr(self, "znear") else None
+        return getattr(self, "znear", None)
 
     def get_image_size(self):
         """
         Returns the image size, if provided, expected in the form of (height, width)
         The image size is used for conversion of projected points to screen coordinates.
         """
-        return self.image_size if hasattr(self, "image_size") else None
+        return getattr(self, "image_size", None)
 
     def __getitem__(
         self, index: Union[int, List[int], torch.BoolTensor, torch.LongTensor]
     ) -> "CamerasBase":
         """
         Override for the __getitem__ method in TensorProperties which needs to be
         refactored.
```

## pytorch3d/renderer/implicit/raysampling.py

```diff
@@ -105,25 +105,19 @@
         self._max_depth = max_depth
         self._n_rays_per_image = n_rays_per_image
         self._n_rays_total = n_rays_total
         self._unit_directions = unit_directions
         self._stratified_sampling = stratified_sampling
 
         # get the initial grid of image xy coords
-        _xy_grid = torch.stack(
-            tuple(
-                reversed(
-                    meshgrid_ij(
-                        torch.linspace(min_y, max_y, image_height, dtype=torch.float32),
-                        torch.linspace(min_x, max_x, image_width, dtype=torch.float32),
-                    )
-                )
-            ),
-            dim=-1,
+        y, x = meshgrid_ij(
+            torch.linspace(min_y, max_y, image_height, dtype=torch.float32),
+            torch.linspace(min_x, max_x, image_width, dtype=torch.float32),
         )
+        _xy_grid = torch.stack([x, y], dim=-1)
 
         self.register_buffer("_xy_grid", _xy_grid, persistent=False)
 
     def forward(
         self,
         cameras: CamerasBase,
         *,
```

## pytorch3d/renderer/mesh/textures.py

```diff
@@ -487,14 +487,16 @@
 
     def extend(self, N: int) -> "TexturesAtlas":
         new_props = self._extend(N, ["atlas_padded", "_num_faces_per_mesh"])
         new_tex = self.__class__(atlas=new_props["atlas_padded"])
         new_tex._num_faces_per_mesh = new_props["_num_faces_per_mesh"]
         return new_tex
 
+    # pyre-fixme[14]: `sample_textures` overrides method defined in `TexturesBase`
+    #  inconsistently.
     def sample_textures(self, fragments, **kwargs) -> torch.Tensor:
         """
         This is similar to a nearest neighbor sampling and involves a
         discretization step. The barycentric coordinates from
         rasterization are used to find the nearest grid cell in the texture
         atlas and the RGB is returned as the color.
         This means that this step is differentiable with respect to the RGB
@@ -923,14 +925,16 @@
             align_corners=self.align_corners,
             sampling_mode=self.sampling_mode,
         )
 
         new_tex._num_faces_per_mesh = new_props["_num_faces_per_mesh"]
         return new_tex
 
+    # pyre-fixme[14]: `sample_textures` overrides method defined in `TexturesBase`
+    #  inconsistently.
     def sample_textures(self, fragments, **kwargs) -> torch.Tensor:
         """
         Interpolate a 2D texture map using uv vertex texture coordinates for each
         face in the mesh. First interpolate the vertex uvs using barycentric coordinates
         for each pixel in the rasterized output. Then interpolate the texture map
         using the uv coordinate for each pixel.
 
@@ -1446,14 +1450,16 @@
 
     def extend(self, N: int) -> "TexturesVertex":
         new_props = self._extend(N, ["verts_features_padded", "_num_verts_per_mesh"])
         new_tex = self.__class__(verts_features=new_props["verts_features_padded"])
         new_tex._num_verts_per_mesh = new_props["_num_verts_per_mesh"]
         return new_tex
 
+    # pyre-fixme[14]: `sample_textures` overrides method defined in `TexturesBase`
+    #  inconsistently.
     def sample_textures(self, fragments, faces_packed=None) -> torch.Tensor:
         """
         Determine the color for each rasterized face. Interpolate the colors for
         vertices which form the face using the barycentric coordinates.
         Args:
             fragments:
                 The outputs of rasterization. From this we use
```

## Comparing `pytorch3d-0.7.3.dist-info/LICENSE` & `pytorch3d-0.7.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pytorch3d-0.7.3.dist-info/LICENSE-3RD-PARTY` & `pytorch3d-0.7.4.dist-info/LICENSE-3RD-PARTY`

 * *Files identical despite different names*

## Comparing `pytorch3d-0.7.3.dist-info/METADATA` & `pytorch3d-0.7.4.dist-info/METADATA`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pytorch3d
-Version: 0.7.3
+Version: 0.7.4
 Summary: PyTorch3D is FAIR's library of reusable components for deep Learning with 3D data.
 Home-page: https://github.com/facebookresearch/pytorch3d
 Author: FAIR
 License-File: LICENSE
 License-File: LICENSE-3RD-PARTY
 Requires-Dist: fvcore
 Requires-Dist: iopath
@@ -19,8 +19,9 @@
 Provides-Extra: implicitron
 Requires-Dist: hydra-core (>=1.1) ; extra == 'implicitron'
 Requires-Dist: visdom ; extra == 'implicitron'
 Requires-Dist: lpips ; extra == 'implicitron'
 Requires-Dist: tqdm (>4.29.0) ; extra == 'implicitron'
 Requires-Dist: matplotlib ; extra == 'implicitron'
 Requires-Dist: accelerate ; extra == 'implicitron'
+Requires-Dist: sqlalchemy (>=2.0) ; extra == 'implicitron'
```

## Comparing `pytorch3d-0.7.3.dist-info/RECORD` & `pytorch3d-0.7.4.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 projects/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 pytorch3d/_C.cpython-39-darwin.so,sha256=GP_EjslYsD-B3tiLhFRqsL1lVwVc26yNk1tq2PE2_Xc,816288
-pytorch3d/__init__.py,sha256=ywgJgijk5DopqM_OYPFyc918SLorpVoA3JrbzUvpWPQ,231
+pytorch3d/__init__.py,sha256=TaPVJMSeHbXf3mzDlkq74gATE2T44eHIYZeTDyz34-8,231
 pytorch3d/common/__init__.py,sha256=v6j0ce6z9DTADvbQk_w5gtRyh9xZ3w40SyBlTRiN-Rc,331
 pytorch3d/common/compat.py,sha256=oxcpAUHRi8GYDFhP-GCrFwQrvxWYs5tirjw9bJaEyWI,1271
 pytorch3d/common/datatypes.py,sha256=DeB_EIen2suihFFPpLuG-ndyTKOKNUUW1fUPdpYjEBY,1733
 pytorch3d/common/linear_with_repeat.py,sha256=HK0_c4MdiJMT3RL9_UClTPdw7tfmcUbdtBG4QRAQKH4,2630
 pytorch3d/common/workaround/__init__.py,sha256=tpk7I9fxViu0XECuwL-qqFgxrh_-7MqXlAnCIJDBEzg,275
 pytorch3d/common/workaround/symeig3x3.py,sha256=R6bz_AcdZl0OyYL0UWWI8-apCX-vDIjYUyqMvOOlG5Q,12151
 pytorch3d/common/workaround/utils.py,sha256=J5flmnRRSQJYYL7Q1XJ0ywPFnvqFheoV9CysQSGJl5M,872
@@ -23,35 +23,39 @@
 pytorch3d/implicitron/eval_demo.py,sha256=7dGn1R7Pdb-1O3HIvnILpgC1dll_rcdt36wHWVCLCII,6049
 pytorch3d/implicitron/dataset/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 pytorch3d/implicitron/dataset/blender_dataset_map_provider.py,sha256=51wLJ17QD-z12DNWDV6UeMABgKI6KCDItfR1Q0iBmao,1899
 pytorch3d/implicitron/dataset/data_loader_map_provider.py,sha256=IQAlgtiHjtzlQnWk1jWQHEZOXaeLxQgyZiRfSCbbll0,19321
 pytorch3d/implicitron/dataset/data_source.py,sha256=lMfmJah8W29CookfgzWE1DQ2p9TmQhlDqTMiS2EPJqU,3536
 pytorch3d/implicitron/dataset/dataset_base.py,sha256=eKfQ_zVoyGKyImr4-YxJHFtBDehbzmjMVOpuUtwKjEo,5302
 pytorch3d/implicitron/dataset/dataset_map_provider.py,sha256=5hUwVVA3-UqDKud0L737VlpPlvCz5laNxwCaBubx7OY,4422
-pytorch3d/implicitron/dataset/frame_data.py,sha256=gCD0JZldICUSSeeMD-x0520i_DFJfT6rpNz66rIXJy0,30400
-pytorch3d/implicitron/dataset/json_index_dataset.py,sha256=UF6mHlaDBdkmllIaDqm9kjBrXerVqnOx-Onw43t5cDI,27773
+pytorch3d/implicitron/dataset/frame_data.py,sha256=mAErW4B52dmm3IVv71G4JQ2idKESlBtHOgCuBMvJ3Iw,31568
+pytorch3d/implicitron/dataset/json_index_dataset.py,sha256=oHZWNc9VO5R8jpx84UnhbnGY0O0aS6CvwTiT94plVew,27817
 pytorch3d/implicitron/dataset/json_index_dataset_map_provider.py,sha256=QwIP4IDolOgMaeoZsyquhXEAPOyEmurv0v4CZOHmcdY,11512
 pytorch3d/implicitron/dataset/json_index_dataset_map_provider_v2.py,sha256=Az-cttqxa3_nhoT99ZtFDv8W8CJRl2yP2lHh30d1nMw,18944
 pytorch3d/implicitron/dataset/llff_dataset_map_provider.py,sha256=oVY3KSyw0Gadwwe_1gLBZ1ERe0KfG7SsZkmXfBAjYSE,2317
 pytorch3d/implicitron/dataset/load_blender.py,sha256=oZLliR-d9BTe74I9Cgi-XyvkISokqFuSs-ymwHrhg3E,3919
 pytorch3d/implicitron/dataset/load_llff.py,sha256=y79OrzAtEjQnveACo8SpAGZ4EvcSPEvieS8ymG63ris,9903
+pytorch3d/implicitron/dataset/orm_types.py,sha256=Cew5zN4Ss8kzbJptrfaeivVlgaYb2UZE8Gtr11SFKLw,4830
 pytorch3d/implicitron/dataset/rendered_mesh_dataset_map_provider.py,sha256=d6BZTC3gx7xlEkLIFi0TieC5a3ZZ8rhDpYmhRJzph6w,8824
 pytorch3d/implicitron/dataset/scene_batch_sampler.py,sha256=tZOv_KTqVnjMdyAh-FVqqbxp8C36VwYm5mXdCnD7fHw,8346
 pytorch3d/implicitron/dataset/single_sequence_dataset.py,sha256=BNZ3iN6KrP9EwMnNyPTD8pZY_On3w2AvHxi5GeP5i_Q,7365
-pytorch3d/implicitron/dataset/types.py,sha256=njfPd30fh6Cl0nsRsFd6DudPpWOXnQ7-JNqjksWY6SE,11879
+pytorch3d/implicitron/dataset/sql_dataset.py,sha256=FCFtYpzxcoofjlgrRlhcep4tYF-n0SKvwt7EW7SDw18,29663
+pytorch3d/implicitron/dataset/sql_dataset_provider.py,sha256=fu6SldjHn9IfNhSH8t_he7c3XmHGGa5kqXF-q0sq4vw,16557
+pytorch3d/implicitron/dataset/train_eval_data_loader_provider.py,sha256=_Iq0kZbXb5TolxotPt0MKmhvuiAQbvUfqFx3nz4GsjI,7987
+pytorch3d/implicitron/dataset/types.py,sha256=zpz3eVxdl2BmT8_1v8dXXPAQH_Py0BsXg2mUNch128E,11885
 pytorch3d/implicitron/dataset/utils.py,sha256=wZoz0QQPneePCHYjXgReDkGcohQDswpnMv9vtX0vvyk,11375
 pytorch3d/implicitron/dataset/visualize.py,sha256=VvLVa7AJX5N_00mwE1kZwYiWSvXbF8-wAOkEeEBLMT0,3252
 pytorch3d/implicitron/evaluation/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-pytorch3d/implicitron/evaluation/evaluate_new_view_synthesis.py,sha256=cYI0dgtBzr7XD64V9a3iCg1VD-pHg-npiV-7C2Fnlhg,20128
-pytorch3d/implicitron/evaluation/evaluator.py,sha256=yuzj2PTWkzdgyHCyo4846l6H9AOf21DhLUm0M6pwHmo,4882
+pytorch3d/implicitron/evaluation/evaluate_new_view_synthesis.py,sha256=zby2kZZuJRSI-1qswamvjBb-FkBkdxw7yXGUPKuMdgw,20331
+pytorch3d/implicitron/evaluation/evaluator.py,sha256=nJ5BSagNP6VjSw0wWRgJTYa2n8EEFWfTbi5zuqp4taY,4970
 pytorch3d/implicitron/models/__init__.py,sha256=p-uGTUYOiPkarcGlOoSXVSdByNBKS_9FJ79-wW268YI,437
 pytorch3d/implicitron/models/base_model.py,sha256=01zNUdbLXeeYFtk49cpDXigfARzAvCdzBKXBOU2eibM,4022
-pytorch3d/implicitron/models/generic_model.py,sha256=Y4ve6TdDzsXOu8hI8za0CayRvvG9-Da4h12AiIyS2I8,32994
+pytorch3d/implicitron/models/generic_model.py,sha256=SgOQKQuFoCejQo1qZYGRKLSY_w5iqioxbN0Q3xIE4js,32994
 pytorch3d/implicitron/models/metrics.py,sha256=JVByBlYX242qFKg7Oq3r7X3BnatCST0h-SCAmtxfDTY,16616
-pytorch3d/implicitron/models/model_dbir.py,sha256=vYrmREGBwvlqPihNVA8_E2oBx9biSQUtKjuk_Al9IRQ,5593
+pytorch3d/implicitron/models/model_dbir.py,sha256=nB2x1V5BpWWa8iyUQ7mmGkToQRWXe1I1Y4kPrTt5DCU,5699
 pytorch3d/implicitron/models/overfit_model.py,sha256=LoN3CD-aK9aONMujtd_V4Dlc3SZFA6GJ1OMOP0_6iEQ,27801
 pytorch3d/implicitron/models/utils.py,sha256=KwpWilzDWsY3Sr46CdMTca68-Yg1xKF55rcWdTDsHg0,7295
 pytorch3d/implicitron/models/feature_extractor/__init__.py,sha256=_7p0Db5YdjLyZ2JBBP_JG6it33ll-xr4jtXK1gQiSpo,261
 pytorch3d/implicitron/models/feature_extractor/feature_extractor.py,sha256=CI-rGwtyRds-30vZzmruTryGuEAoTf-d2Ow56QQdto4,1238
 pytorch3d/implicitron/models/feature_extractor/resnet_feature_extractor.py,sha256=VakE7TH9p_LhIockpe0QCkr1aXrypTtGh4FfPLVxO44,8270
 pytorch3d/implicitron/models/global_encoder/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 pytorch3d/implicitron/models/global_encoder/autodecoder.py,sha256=2E2R3mDGTCxbenOnXq_lNa2hrGFvAL9Rb-3EI3FkuDA,5983
@@ -68,15 +72,15 @@
 pytorch3d/implicitron/models/renderer/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 pytorch3d/implicitron/models/renderer/base.py,sha256=bZRZzVFwvx5oPezUJDrcxSGo91gbxY0RldYlaTEL5oo,7936
 pytorch3d/implicitron/models/renderer/lstm_renderer.py,sha256=n0O2le6HFsLC61_GLkJQMZnitw7g5SilHiALZD2T13w,7466
 pytorch3d/implicitron/models/renderer/multipass_ea.py,sha256=f1HGmqmpF6thPkMQT47hrY4JR-wgC5C25Xz6MW9Sh6A,6986
 pytorch3d/implicitron/models/renderer/ray_point_refiner.py,sha256=exTkD-g2b_3GYNaMk7NvTgXTrv12BYyAoqzubw2l9TU,3258
 pytorch3d/implicitron/models/renderer/ray_sampler.py,sha256=y6zvh8iCGzXkvdkgP0A01CGPXRebThnAyVPMdu_wW90,11339
 pytorch3d/implicitron/models/renderer/ray_tracing.py,sha256=jid02hFJ4jjxQZgFFdAtYof9R9p5XpO_yrTvP2aLzzg,22481
-pytorch3d/implicitron/models/renderer/raymarcher.py,sha256=Opghg_VyKtqE3E3tlkayNfJ4zN725d9d-jGJAsII43s,8761
+pytorch3d/implicitron/models/renderer/raymarcher.py,sha256=PNbOOG9xwgaZudLed6_B_ju3rk9yQOBvp6JIKjqhbw4,8861
 pytorch3d/implicitron/models/renderer/rgb_net.py,sha256=6BR0e5O7Rza9eZSmhBEV_QAcZRYQIWzODqbOOxdxALc,4690
 pytorch3d/implicitron/models/renderer/sdf_renderer.py,sha256=PVf6cXfzPDGSDeRNeksh9Co0yE8QKP4wsyKkCir8Dq4,10484
 pytorch3d/implicitron/models/view_pooler/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 pytorch3d/implicitron/models/view_pooler/feature_aggregator.py,sha256=Yed97ofoF3Kzd0TW9OpF4g2cBCzC1h7mUWyxPJYZeDc,26105
 pytorch3d/implicitron/models/view_pooler/view_pooler.py,sha256=ABSwOl48RcxxNmdpT47p8CAJWts7RLOzNA-JfEQKY2s,5479
 pytorch3d/implicitron/models/view_pooler/view_sampler.py,sha256=JQ9Va8HfoQJrluhvh-3BFYYw9F9_U_Xs-EKNeWQRkg0,10712
 pytorch3d/implicitron/models/visualization/__init__.py,sha256=fE0IHi1JJpxsNVBNzWNee2thrNXFFRhY94c80RxNSIE,231
@@ -99,17 +103,17 @@
 pytorch3d/implicitron/tools/utils.py,sha256=I6YmS-Ent-TMEd27Nr_aJ5un_dFWrNbzcM_7-FDthQ8,5104
 pytorch3d/implicitron/tools/video_writer.py,sha256=eVjQpeyeS2siO7NSnh8cvi39kgMkzn7bp3XolPVkg_M,5817
 pytorch3d/implicitron/tools/vis_utils.py,sha256=QLdm7yybpLJqVPwbUkEa0qXoc2phK0MNLNREYDeI9uk,5753
 pytorch3d/implicitron_trainer/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 pytorch3d/implicitron_trainer/experiment.py,sha256=fMr8NLiNP_EdNsSarmbiOysTlmFabkDTXFapqDBxbEM,9780
 pytorch3d/implicitron_trainer/visualize_reconstruction.py,sha256=I2bhV2W61KSjG3i37SI_U8SxGOI0Gnc5HX7E3rt3PCE,6143
 pytorch3d/io/__init__.py,sha256=L_5rDZhnTKibSBAo2Fqky0yrOwsRQHmTNBdLbsOs1O8,402
-pytorch3d/io/experimental_gltf_io.py,sha256=kHx9o2OFhQknTYfX9eMIq3H2TEnVlLSqMn8fDii16G4,30192
+pytorch3d/io/experimental_gltf_io.py,sha256=d-xdFbBEzrySlcnx3K8EHpw2iv6GzHJeBLyG8NM-b_Y,31794
 pytorch3d/io/mtl_io.py,sha256=mM-U3rT0cEvNy0oV7OQ1X4DX6-QYUoZBTq6bErDd6eg,20604
-pytorch3d/io/obj_io.py,sha256=pQArFQrlFJVn6_uH6bX4hJwyaJwuu3aX5sGvB7cM3BI,30811
+pytorch3d/io/obj_io.py,sha256=EIy9fhzJqsMxVcpHWNuJxTqV5MoWkFXD1kMnziaKjQY,33926
 pytorch3d/io/off_io.py,sha256=Bj-3j45vxTlYLBSmbQLYkgyQxsv4pQ7fxs8RJf-NycA,16019
 pytorch3d/io/pluggable.py,sha256=2mSSZnPhJ-gNt1_qSl5cfNA0rYCwBWlvR-QVJ83IusU,7588
 pytorch3d/io/pluggable_formats.py,sha256=xI2QNI4_iU5zSntapkoMon-9qdYCBR_MYde58sWJyeM,4213
 pytorch3d/io/ply_io.py,sha256=FSxVwOWcxFXkrd3BnamU8i_X7la9vNSBChS7Z1LIJ9w,52571
 pytorch3d/io/utils.py,sha256=whQaiGLQZQtrVLOvTvq3-UfhYTtzvH2Bxi0r4zG2AeM,2689
 pytorch3d/loss/__init__.py,sha256=7tyF1GjCvcul32XfftBlLnIMJl13pYMKYaygxBir81Q,566
 pytorch3d/loss/chamfer.py,sha256=T_Gg7gXx6GU1U4UMZfB-ZI5ce10vukd60OD9q0CfQuo,8713
@@ -140,36 +144,36 @@
 pytorch3d/ops/subdivide_meshes.py,sha256=sYgbaIvtIDS9Vta-0lMS5Ip60eNSpnJSu-8EUdcDDzM,17932
 pytorch3d/ops/utils.py,sha256=JzKbOOX_VRjXOB94xMYa2Adp0kSAjjdZv4u772Zjw9Y,7417
 pytorch3d/ops/vert_align.py,sha256=AuMeYJZlGSsBnX8ZtdgO3dgw633UWcfG4IkaDgSXseo,4246
 pytorch3d/renderer/__init__.py,sha256=fipC36ZUsU9fvBRYS5wEJOXF5syWWbQtP9Fw4FwDswI,2212
 pytorch3d/renderer/blending.py,sha256=zQ3vHH6U_Fh-rHQdPLppIqtHuKqLBF246WVwKIbSaj8,9879
 pytorch3d/renderer/camera_conversions.py,sha256=Br2HV6UP4JBuM_Uc3O65_24K0yFrREwGCeLyjVaR9zs,6804
 pytorch3d/renderer/camera_utils.py,sha256=_yzCYaWWxfAf0N0vG-tM-oS9wDkOfYEKQNFpBuJTVbU,7930
-pytorch3d/renderer/cameras.py,sha256=Q77BPYjIgJGS8DqOO8W4G9eTO6wiNUinEDJ8cSgBPKk,70876
+pytorch3d/renderer/cameras.py,sha256=UT1IVU_IWfktL1TggkFOTBWr3cTsMMd8D7bwFSZ5rMY,70835
 pytorch3d/renderer/compositing.py,sha256=iYQ5nP-4CMkIaz3dRLBat8cYID4tK0VQpMj0xG4Gv4Y,10686
 pytorch3d/renderer/fisheyecameras.py,sha256=TUCLniXW1LnEpSouM6NGrBvt5RJN-A3Fwzszokpoydo,21762
 pytorch3d/renderer/lighting.py,sha256=bMWEa1buT2lyVZaMsaaQgQRQSl2OO2NvVNOxXf623GE,13007
 pytorch3d/renderer/materials.py,sha256=-4FjZLKumWUBp0fBZ7I6tyPIEBuyRiL8KlCs6Wi1U_Q,2395
 pytorch3d/renderer/splatter_blend.py,sha256=Vi_riOU0PucssgVg_3GQ5qfBtZBTZoieSyfWbAaVbc0,25178
 pytorch3d/renderer/utils.py,sha256=K5zzSnq9I_elxWNB-li69ggcP3a4VWrhmE2rtWMUvDE,16827
 pytorch3d/renderer/implicit/__init__.py,sha256=bFbAbWuyvhmz-iuIjWCIkr70rugNTKy-LRMTrc621wc,767
 pytorch3d/renderer/implicit/harmonic_embedding.py,sha256=68UNeQH_FqPtH-B67Hfqc_upv8KfEsOdpWSSlNK36c0,4429
 pytorch3d/renderer/implicit/raymarching.py,sha256=TC4IuXYU3dyfnsrCxAduh-wyRn_BKA5V_afvMOWV4G8,9323
-pytorch3d/renderer/implicit/raysampling.py,sha256=IGPNvGIqEjANXyKB-Oh0T90c2OtH37ZTQEYGFf4S0Cg,31792
+pytorch3d/renderer/implicit/raysampling.py,sha256=Pj-tpUMNYAqckhgRFpZoi2p0SSBe4ui3itvWQ4BMhoo,31658
 pytorch3d/renderer/implicit/renderer.py,sha256=MKobx3bjy5MhjURvurn3Wsbgw0kEK_X0MK84OLAqFWI,17111
 pytorch3d/renderer/implicit/sample_pdf.py,sha256=x0EQm7nwhpdWDsAJy63vTzB8DXxvytXNw4xykgShiOw,5817
 pytorch3d/renderer/implicit/utils.py,sha256=q4OWe-ke6RTpY2Ceidqz6YbbrLorIr4orpj6POlLM9A,6458
 pytorch3d/renderer/mesh/__init__.py,sha256=dhh_c6HIVaDpUOJpNlmg0cyDBBUo39aepyV7cWjEHRQ,995
 pytorch3d/renderer/mesh/clip.py,sha256=ooe096351XIiWFJW3yo-L_q2Ra0dZ4V-c_emfItYik4,34991
 pytorch3d/renderer/mesh/rasterize_meshes.py,sha256=Zf4oLv0IAUwBKxUQ4MkQZzhJNF44XEaQ8zeadD-5AJ0,30438
 pytorch3d/renderer/mesh/rasterizer.py,sha256=qvc4Ugufa4J30-vuhJgxD_xjkQRw0xODmAv8AUMh4a8,12527
 pytorch3d/renderer/mesh/renderer.py,sha256=P7svz_xSIXsdgKNlQVfRWgO54vkWbfnTUyo1NYa-YEA,4013
 pytorch3d/renderer/mesh/shader.py,sha256=fTYsVd0p9a370EPSX91U8rpbq3snfLY7bzP0KOvAbgY,15488
 pytorch3d/renderer/mesh/shading.py,sha256=6C8HsN5gIorhr5rHdXYDqz_1_i2NrFuouQqWPVzwmMw,8965
-pytorch3d/renderer/mesh/textures.py,sha256=bgIUD6yfZ_K28beY66NezhrTOYXCwTqfQX8pYyy67Kc,65242
+pytorch3d/renderer/mesh/textures.py,sha256=cMRsdGGDK5v7lGCdH9Gdt0QGv026r8VAiUGxmIBV34s,65560
 pytorch3d/renderer/mesh/utils.py,sha256=UvjDx8GtZzI3HDdXnIq3jgqKnKIxW4IGzoXwyzTz15Q,11238
 pytorch3d/renderer/opengl/__init__.py,sha256=LmNWrtM2R_E3kfE3Mi4EjEmi2D_ruXMr_xolT0p1o9M,1278
 pytorch3d/renderer/opengl/opengl_utils.py,sha256=KPp9X33-u2Gu62-uOTJVVX63uIxaAxVjvUx4F87g0Ws,16419
 pytorch3d/renderer/opengl/rasterizer_opengl.py,sha256=foSL8Nix2GjFbHNpkQR3BYho3yWOFf2y6EyvRuTeFI8,27731
 pytorch3d/renderer/points/__init__.py,sha256=ew4lLQsl4wQgviikES79SjNP2Y6ov7ay1CXugf_5UE0,543
 pytorch3d/renderer/points/compositor.py,sha256=JC0Yv8EUwn3vBFRcz9xc-z6TUSJro9NgSHJ-8YUlUGg,4323
 pytorch3d/renderer/points/rasterize_points.py,sha256=L4aLTvazIG5BHfBV-OzrKd5eO5lv1pCUgl81H_qJlwc,13071
@@ -193,14 +197,14 @@
 pytorch3d/utils/camera_conversions.py,sha256=XSK80fqhHxoAkWg96O0JVAxPvnf2if7moUC72jC_XK4,6366
 pytorch3d/utils/checkerboard.py,sha256=nhZr1p87wY5kdHaT483Trv2QWwtBnti5i0Znz9Gb7Is,2915
 pytorch3d/utils/ico_sphere.py,sha256=rhLBaMPSNFHBQhqOpzexHN0M5gii5azpSRbI0m-Z-HY,2217
 pytorch3d/utils/torus.py,sha256=G4DDb3iZEKN4zWT7IT6dtDeHNPpes-asYcRkWECoZFU,2392
 pytorch3d/vis/__init__.py,sha256=bvB2eQCmuiVi58JdBgEFLW-3I1rdfI_QtrrS5JqUZhA,645
 pytorch3d/vis/plotly_vis.py,sha256=MkgGtxPiB7DpgobAj2HN5wagFVT3qgl-oSSSYElMHdc,39884
 pytorch3d/vis/texture_vis.py,sha256=W8ex6swFC5rVdC7ge8HbIXjMzKkYmeWAG4hPrueTYZE,3814
-pytorch3d-0.7.3.dist-info/LICENSE,sha256=baBWYwSUF6kUCfo0m_rVd47VbJ72gibd4o3_1UFIFXk,1534
-pytorch3d-0.7.3.dist-info/LICENSE-3RD-PARTY,sha256=kA7PiYqkH9VUAlROuvTAbpTjxKDOTb9lPhmTLvigajw,3351
-pytorch3d-0.7.3.dist-info/METADATA,sha256=J85A5RNML5UKw2yPd5od2G8NSeTXIOTPAW4YeuWYM3k,941
-pytorch3d-0.7.3.dist-info/WHEEL,sha256=UlNw_PQRbc8y2O60L5paNCXmEdKCjgP3umWDtB6RxVY,109
-pytorch3d-0.7.3.dist-info/entry_points.txt,sha256=GGwxi0GEhzExLnR865eamhscHXbi0gKReSZpc2_P99A,196
-pytorch3d-0.7.3.dist-info/top_level.txt,sha256=J5TfeDAeYRGCrLjXbOc3m6O6GwVBhwOZl0yT2a_UqDw,19
-pytorch3d-0.7.3.dist-info/RECORD,,
+pytorch3d-0.7.4.dist-info/LICENSE,sha256=baBWYwSUF6kUCfo0m_rVd47VbJ72gibd4o3_1UFIFXk,1534
+pytorch3d-0.7.4.dist-info/LICENSE-3RD-PARTY,sha256=kA7PiYqkH9VUAlROuvTAbpTjxKDOTb9lPhmTLvigajw,3351
+pytorch3d-0.7.4.dist-info/METADATA,sha256=_GaojIXdfh-d4vfGnf5M7skzwqJJgbX32-HIfYnC3UM,1000
+pytorch3d-0.7.4.dist-info/WHEEL,sha256=UlNw_PQRbc8y2O60L5paNCXmEdKCjgP3umWDtB6RxVY,109
+pytorch3d-0.7.4.dist-info/entry_points.txt,sha256=GGwxi0GEhzExLnR865eamhscHXbi0gKReSZpc2_P99A,196
+pytorch3d-0.7.4.dist-info/top_level.txt,sha256=J5TfeDAeYRGCrLjXbOc3m6O6GwVBhwOZl0yT2a_UqDw,19
+pytorch3d-0.7.4.dist-info/RECORD,,
```

