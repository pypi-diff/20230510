# Comparing `tmp/mediapipe_model_maker-0.1.1.1-py3-none-any.whl.zip` & `tmp/mediapipe_model_maker-0.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,70 +1,77 @@
-Zip file size: 106320 bytes, number of entries: 68
--rw-r--r--  2.0 unx      983 b- defN 23-Mar-28 23:24 mediapipe_model_maker/__init__.py
--rw-r--r--  2.0 unx      607 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/__init__.py
--rw-r--r--  2.0 unx      607 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/__init__.py
--rw-r--r--  2.0 unx     2696 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/hyperparameters.py
--rw-r--r--  2.0 unx      607 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/data/__init__.py
--rw-r--r--  2.0 unx     1708 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/data/classification_dataset.py
--rw-r--r--  2.0 unx     1213 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/data/data_util.py
--rw-r--r--  2.0 unx     6339 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/data/dataset.py
--rw-r--r--  2.0 unx      607 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/tasks/__init__.py
--rw-r--r--  2.0 unx     5706 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/tasks/classifier.py
--rw-r--r--  2.0 unx     3034 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/tasks/custom_model.py
--rw-r--r--  2.0 unx      607 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/utils/__init__.py
--rw-r--r--  2.0 unx     3428 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/utils/file_util.py
--rw-r--r--  2.0 unx     4463 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/utils/loss_functions.py
--rw-r--r--  2.0 unx    11159 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/utils/model_util.py
--rw-r--r--  2.0 unx     8664 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/utils/quantization.py
--rw-r--r--  2.0 unx     5247 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/core/utils/test_util.py
--rw-r--r--  2.0 unx      607 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/__init__.py
--rw-r--r--  2.0 unx      607 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/core/__init__.py
--rw-r--r--  2.0 unx     1205 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/core/bert_model_options.py
--rw-r--r--  2.0 unx     2260 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/core/bert_model_spec.py
--rw-r--r--  2.0 unx     1717 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/text_classifier/__init__.py
--rw-r--r--  2.0 unx     2955 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/text_classifier/dataset.py
--rw-r--r--  2.0 unx     1646 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/text_classifier/model_options.py
--rw-r--r--  2.0 unx     2698 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/text_classifier/model_spec.py
--rw-r--r--  2.0 unx     9803 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/text_classifier/preprocessor.py
--rw-r--r--  2.0 unx    18719 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/text_classifier/text_classifier.py
--rw-r--r--  2.0 unx     3846 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/text_classifier/text_classifier_demo.py
--rw-r--r--  2.0 unx     1593 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/text/text_classifier/text_classifier_options.py
--rw-r--r--  2.0 unx      607 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/__init__.py
--rw-r--r--  2.0 unx      607 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/core/__init__.py
--rw-r--r--  2.0 unx     8413 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/core/image_preprocessing.py
--rw-r--r--  2.0 unx     1074 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/core/image_utils.py
--rw-r--r--  2.0 unx     1638 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/core/test_utils.py
--rw-r--r--  2.0 unx      675 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/face_stylizer/__init__.py
--rw-r--r--  2.0 unx     3334 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/face_stylizer/dataset.py
--rw-r--r--  2.0 unx     1673 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/gesture_recognizer/__init__.py
--rw-r--r--  2.0 unx     1737 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/gesture_recognizer/constants.py
--rw-r--r--  2.0 unx     9634 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/gesture_recognizer/dataset.py
--rw-r--r--  2.0 unx     9127 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer.py
--rw-r--r--  2.0 unx     2395 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_demo.py
--rw-r--r--  2.0 unx     1291 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_options.py
--rw-r--r--  2.0 unx     1432 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/gesture_recognizer/hyperparameters.py
--rw-r--r--  2.0 unx     9933 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/gesture_recognizer/metadata_writer.py
--rw-r--r--  2.0 unx     1322 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/gesture_recognizer/model_options.py
--rw-r--r--  2.0 unx     1632 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/image_classifier/__init__.py
--rw-r--r--  2.0 unx     3463 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/image_classifier/dataset.py
--rw-r--r--  2.0 unx     2388 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/image_classifier/hyperparameters.py
--rw-r--r--  2.0 unx     8583 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/image_classifier/image_classifier.py
--rw-r--r--  2.0 unx     3824 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/image_classifier/image_classifier_demo.py
--rw-r--r--  2.0 unx     1459 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/image_classifier/image_classifier_options.py
--rw-r--r--  2.0 unx      940 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/image_classifier/model_options.py
--rw-r--r--  2.0 unx     2700 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/image_classifier/model_spec.py
--rw-r--r--  2.0 unx     1501 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/__init__.py
--rw-r--r--  2.0 unx     6529 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/dataset.py
--rw-r--r--  2.0 unx    16573 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/dataset_util.py
--rw-r--r--  2.0 unx     3855 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/hyperparameters.py
--rw-r--r--  2.0 unx    13477 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/model.py
--rw-r--r--  2.0 unx      987 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/model_options.py
--rw-r--r--  2.0 unx     1886 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/model_spec.py
--rw-r--r--  2.0 unx    13583 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/object_detector.py
--rw-r--r--  2.0 unx     2862 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/object_detector_demo.py
--rw-r--r--  2.0 unx     1452 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/object_detector_options.py
--rw-r--r--  2.0 unx     5635 b- defN 23-Mar-28 23:24 mediapipe_model_maker/python/vision/object_detector/preprocessor.py
--rw-r--r--  2.0 unx     1544 b- defN 23-Mar-28 23:24 mediapipe_model_maker-0.1.1.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-28 23:24 mediapipe_model_maker-0.1.1.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       22 b- defN 23-Mar-28 23:24 mediapipe_model_maker-0.1.1.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     7964 b- defN 23-Mar-28 23:24 mediapipe_model_maker-0.1.1.1.dist-info/RECORD
-68 files, 263174 bytes uncompressed, 92822 bytes compressed:  64.7%
+Zip file size: 117515 bytes, number of entries: 75
+-rw-r--r--  2.0 unx      962 b- defN 23-May-10 06:08 mediapipe_model_maker/__init__.py
+-rw-r--r--  2.0 unx      586 b- defN 23-May-10 06:08 mediapipe_model_maker/python/__init__.py
+-rw-r--r--  2.0 unx      586 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/__init__.py
+-rw-r--r--  2.0 unx     2675 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/hyperparameters.py
+-rw-r--r--  2.0 unx      586 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/data/__init__.py
+-rw-r--r--  2.0 unx     1687 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/data/classification_dataset.py
+-rw-r--r--  2.0 unx     1192 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/data/data_util.py
+-rw-r--r--  2.0 unx     6319 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/data/dataset.py
+-rw-r--r--  2.0 unx      586 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/tasks/__init__.py
+-rw-r--r--  2.0 unx     5685 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/tasks/classifier.py
+-rw-r--r--  2.0 unx     3013 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/tasks/custom_model.py
+-rw-r--r--  2.0 unx      586 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/utils/__init__.py
+-rw-r--r--  2.0 unx     3473 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/utils/file_util.py
+-rw-r--r--  2.0 unx    12358 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/utils/loss_functions.py
+-rw-r--r--  2.0 unx    11138 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/utils/model_util.py
+-rw-r--r--  2.0 unx     8643 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/utils/quantization.py
+-rw-r--r--  2.0 unx     5629 b- defN 23-May-10 06:08 mediapipe_model_maker/python/core/utils/test_util.py
+-rw-r--r--  2.0 unx      586 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/__init__.py
+-rw-r--r--  2.0 unx      586 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/core/__init__.py
+-rw-r--r--  2.0 unx     1184 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/core/bert_model_options.py
+-rw-r--r--  2.0 unx     2239 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/core/bert_model_spec.py
+-rw-r--r--  2.0 unx     1696 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/text_classifier/__init__.py
+-rw-r--r--  2.0 unx     2934 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/text_classifier/dataset.py
+-rw-r--r--  2.0 unx     1625 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/text_classifier/model_options.py
+-rw-r--r--  2.0 unx     2677 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/text_classifier/model_spec.py
+-rw-r--r--  2.0 unx     9782 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/text_classifier/preprocessor.py
+-rw-r--r--  2.0 unx    18698 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/text_classifier/text_classifier.py
+-rw-r--r--  2.0 unx     3825 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/text_classifier/text_classifier_demo.py
+-rw-r--r--  2.0 unx     1572 b- defN 23-May-10 06:08 mediapipe_model_maker/python/text/text_classifier/text_classifier_options.py
+-rw-r--r--  2.0 unx      586 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/__init__.py
+-rw-r--r--  2.0 unx      586 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/core/__init__.py
+-rw-r--r--  2.0 unx     8392 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/core/image_preprocessing.py
+-rw-r--r--  2.0 unx     1053 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/core/image_utils.py
+-rw-r--r--  2.0 unx     1617 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/core/test_utils.py
+-rw-r--r--  2.0 unx     1378 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/face_stylizer/__init__.py
+-rw-r--r--  2.0 unx     1800 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/face_stylizer/constants.py
+-rw-r--r--  2.0 unx     3313 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/face_stylizer/dataset.py
+-rw-r--r--  2.0 unx     8387 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/face_stylizer/face_stylizer.py
+-rw-r--r--  2.0 unx     1386 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/face_stylizer/face_stylizer_options.py
+-rw-r--r--  2.0 unx     1314 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/face_stylizer/hyperparameters.py
+-rw-r--r--  2.0 unx     2062 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/face_stylizer/model_options.py
+-rw-r--r--  2.0 unx     1896 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/face_stylizer/model_spec.py
+-rw-r--r--  2.0 unx     1652 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/gesture_recognizer/__init__.py
+-rw-r--r--  2.0 unx     1716 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/gesture_recognizer/constants.py
+-rw-r--r--  2.0 unx     9613 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/gesture_recognizer/dataset.py
+-rw-r--r--  2.0 unx     9106 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer.py
+-rw-r--r--  2.0 unx     2374 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_demo.py
+-rw-r--r--  2.0 unx     1270 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_options.py
+-rw-r--r--  2.0 unx     1411 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/gesture_recognizer/hyperparameters.py
+-rw-r--r--  2.0 unx     9912 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/gesture_recognizer/metadata_writer.py
+-rw-r--r--  2.0 unx     1301 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/gesture_recognizer/model_options.py
+-rw-r--r--  2.0 unx     1611 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/image_classifier/__init__.py
+-rw-r--r--  2.0 unx     3442 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/image_classifier/dataset.py
+-rw-r--r--  2.0 unx     2367 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/image_classifier/hyperparameters.py
+-rw-r--r--  2.0 unx     8562 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/image_classifier/image_classifier.py
+-rw-r--r--  2.0 unx     3803 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/image_classifier/image_classifier_demo.py
+-rw-r--r--  2.0 unx     1438 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/image_classifier/image_classifier_options.py
+-rw-r--r--  2.0 unx      919 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/image_classifier/model_options.py
+-rw-r--r--  2.0 unx     2679 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/image_classifier/model_spec.py
+-rw-r--r--  2.0 unx     1843 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/__init__.py
+-rw-r--r--  2.0 unx     6531 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/dataset.py
+-rw-r--r--  2.0 unx    16552 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/dataset_util.py
+-rw-r--r--  2.0 unx     1223 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/detection.py
+-rw-r--r--  2.0 unx     2639 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/hyperparameters.py
+-rw-r--r--  2.0 unx    13621 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/model.py
+-rw-r--r--  2.0 unx      966 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/model_options.py
+-rw-r--r--  2.0 unx     2476 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/model_spec.py
+-rw-r--r--  2.0 unx    15973 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/object_detector.py
+-rw-r--r--  2.0 unx     2841 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/object_detector_demo.py
+-rw-r--r--  2.0 unx     1431 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/object_detector_options.py
+-rw-r--r--  2.0 unx     5674 b- defN 23-May-10 06:08 mediapipe_model_maker/python/vision/object_detector/preprocessor.py
+-rw-r--r--  2.0 unx     1541 b- defN 23-May-10 06:08 mediapipe_model_maker-0.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-10 06:08 mediapipe_model_maker-0.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       22 b- defN 23-May-10 06:08 mediapipe_model_maker-0.2.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     8820 b- defN 23-May-10 06:08 mediapipe_model_maker-0.2.0.dist-info/RECORD
+75 files, 292299 bytes uncompressed, 102575 bytes compressed:  64.9%
```

## zipnote {}

```diff
@@ -99,17 +99,35 @@
 
 Filename: mediapipe_model_maker/python/vision/core/test_utils.py
 Comment: 
 
 Filename: mediapipe_model_maker/python/vision/face_stylizer/__init__.py
 Comment: 
 
+Filename: mediapipe_model_maker/python/vision/face_stylizer/constants.py
+Comment: 
+
 Filename: mediapipe_model_maker/python/vision/face_stylizer/dataset.py
 Comment: 
 
+Filename: mediapipe_model_maker/python/vision/face_stylizer/face_stylizer.py
+Comment: 
+
+Filename: mediapipe_model_maker/python/vision/face_stylizer/face_stylizer_options.py
+Comment: 
+
+Filename: mediapipe_model_maker/python/vision/face_stylizer/hyperparameters.py
+Comment: 
+
+Filename: mediapipe_model_maker/python/vision/face_stylizer/model_options.py
+Comment: 
+
+Filename: mediapipe_model_maker/python/vision/face_stylizer/model_spec.py
+Comment: 
+
 Filename: mediapipe_model_maker/python/vision/gesture_recognizer/__init__.py
 Comment: 
 
 Filename: mediapipe_model_maker/python/vision/gesture_recognizer/constants.py
 Comment: 
 
 Filename: mediapipe_model_maker/python/vision/gesture_recognizer/dataset.py
@@ -162,14 +180,17 @@
 
 Filename: mediapipe_model_maker/python/vision/object_detector/dataset.py
 Comment: 
 
 Filename: mediapipe_model_maker/python/vision/object_detector/dataset_util.py
 Comment: 
 
+Filename: mediapipe_model_maker/python/vision/object_detector/detection.py
+Comment: 
+
 Filename: mediapipe_model_maker/python/vision/object_detector/hyperparameters.py
 Comment: 
 
 Filename: mediapipe_model_maker/python/vision/object_detector/model.py
 Comment: 
 
 Filename: mediapipe_model_maker/python/vision/object_detector/model_options.py
@@ -186,20 +207,20 @@
 
 Filename: mediapipe_model_maker/python/vision/object_detector/object_detector_options.py
 Comment: 
 
 Filename: mediapipe_model_maker/python/vision/object_detector/preprocessor.py
 Comment: 
 
-Filename: mediapipe_model_maker-0.1.1.1.dist-info/METADATA
+Filename: mediapipe_model_maker-0.2.0.dist-info/METADATA
 Comment: 
 
-Filename: mediapipe_model_maker-0.1.1.1.dist-info/WHEEL
+Filename: mediapipe_model_maker-0.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: mediapipe_model_maker-0.1.1.1.dist-info/top_level.txt
+Filename: mediapipe_model_maker-0.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: mediapipe_model_maker-0.1.1.1.dist-info/RECORD
+Filename: mediapipe_model_maker-0.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mediapipe_model_maker/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/hyperparameters.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -28,15 +28,15 @@
 
   Attributes:
     learning_rate: The learning rate to use for gradient descent training.
     batch_size: Batch size for training.
     epochs: Number of training iterations over the dataset.
     steps_per_epoch: An optional integer indicate the number of training steps
       per epoch. If not set, the training pipeline calculates the default steps
-      per epoch as the training dataset size devided by batch size.
+      per epoch as the training dataset size divided by batch size.
     shuffle: True if the dataset is shuffled before training.
     export_dir: The location of the model checkpoint files.
     distribution_strategy: A string specifying which Distribution Strategy to
       use. Accepted values are 'off', 'one_device', 'mirrored',
       'parameter_server', 'multi_worker_mirrored', and 'tpu' -- case
       insensitive. 'off' means not to use Distribution Strategy; 'tpu' means to
       use TPUStrategy using `tpu_address`. See the tf.distribute.Strategy
```

## mediapipe_model_maker/python/core/data/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/data/classification_dataset.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/data/data_util.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/data/dataset.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -80,15 +80,15 @@
       batch_size: An integer, the returned dataset will be batched by this size.
       is_training: A boolean, when True, the returned dataset will be optionally
         shuffled and repeated as an endless dataset.
       shuffle: A boolean, when True, the returned dataset will be shuffled to
         create randomness during model training.
       preprocess: A function taking three arguments in order, feature, label and
         boolean is_training.
-      drop_remainder: boolean, whether the finaly batch drops remainder.
+      drop_remainder: boolean, whether the finally batch drops remainder.
 
     Returns:
       A TF dataset ready to be consumed by Keras model.
     """
     dataset = self._dataset
 
     if preprocess:
```

## mediapipe_model_maker/python/core/tasks/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/tasks/classifier.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/tasks/custom_model.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/utils/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/utils/file_util.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -90,8 +90,10 @@
           # top level dir in the .tar.gz file into absolute_path.
           pathlib.Path.mkdir(absolute_path.parent, parents=True, exist_ok=True)
           shutil.copytree(os.path.join(tmpdir, subdirs[0]), absolute_path)
       else:
         pathlib.Path.mkdir(absolute_path.parent, parents=True, exist_ok=True)
         with open(absolute_path, 'wb') as f:
           f.write(r.content)
+    else:
+      print(f'Using existing files at {absolute_path}')
     return str(absolute_path)
```

## mediapipe_model_maker/python/core/utils/loss_functions.py

```diff
@@ -1,26 +1,37 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Loss function utility library."""
 
-from typing import Optional, Sequence
+import abc
+from typing import Mapping, Sequence
+import dataclasses
+from typing import Any, Optional
 
+import numpy as np
 import tensorflow as tf
 
+from mediapipe_model_maker.python.core.utils import file_util
+from mediapipe_model_maker.python.core.utils import model_util
+from official.modeling import tf_utils
+
+
+_VGG_IMAGENET_PERCEPTUAL_MODEL_URL = 'https://storage.googleapis.com/mediapipe-assets/vgg_feature_extractor.tar.gz'
+
 
 class FocalLoss(tf.keras.losses.Loss):
   """Implementation of focal loss (https://arxiv.org/pdf/1708.02002.pdf).
 
   This class computes the focal loss between labels and prediction. Focal loss
   is a weighted loss function that modulates the standard cross-entropy loss
   based on how well the neural network performs on a specific example of a
@@ -41,15 +52,14 @@
   >>> focal_loss(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()
   0.6528
 
   Usage with the `compile()` API:
   ```python
   model.compile(optimizer='sgd', loss=FocalLoss(gamma))
   ```
-
   """
 
   def __init__(self, gamma, class_weight: Optional[Sequence[float]] = None):
     """Constructor.
 
     Args:
       gamma: Focal loss gamma, as described in class docs.
@@ -99,7 +109,256 @@
     ce = -tf.math.log(y_pred)
     modulating_factor = tf.math.pow(1 - y_pred, self._gamma)
     losses = y_true * modulating_factor * ce * sample_weight
     losses = losses * loss_weight[:, tf.newaxis]
     # By default, this function uses "sum_over_batch_size" reduction for the
     # loss per batch.
     return tf.reduce_sum(losses) / batch_size
+
+
+@dataclasses.dataclass
+class PerceptualLossWeight:
+  """The weight for each perceptual loss.
+
+  Attributes:
+    l1: weight for L1 loss.
+    content: weight for content loss.
+    style: weight for style loss.
+  """
+
+  l1: float = 1.0
+  content: float = 1.0
+  style: float = 1.0
+
+
+class ImagePerceptualQualityLoss(tf.keras.losses.Loss):
+  """Image perceptual quality loss.
+
+  It obtains a weighted loss between the VGGPerceptualLoss and L1 loss.
+  """
+
+  def __init__(
+      self,
+      loss_weight: Optional[PerceptualLossWeight] = None,
+      reduction: tf.keras.losses.Reduction = tf.keras.losses.Reduction.NONE,
+  ):
+    """Initializes ImagePerceptualQualityLoss."""
+    self._loss_weight = loss_weight
+    self._losses = {}
+    self._vgg_loss = VGGPerceptualLoss(self._loss_weight)
+    self._reduction = reduction
+
+  def _l1_loss(
+      self,
+      reduction: tf.keras.losses.Reduction = tf.keras.losses.Reduction.NONE,
+  ) -> Any:
+    """Calculates L1 loss."""
+    return tf.keras.losses.MeanAbsoluteError(reduction)
+
+  def __call__(
+      self,
+      img1: tf.Tensor,
+      img2: tf.Tensor,
+  ) -> tf.Tensor:
+    """Computes image perceptual quality loss."""
+    loss_value = []
+    if self._loss_weight is None:
+      self._loss_weight = PerceptualLossWeight()
+
+    if self._loss_weight.content > 0 or self._loss_weight.style > 0:
+      vgg_loss = self._vgg_loss(img1, img2)
+      vgg_loss_value = tf.math.add_n(vgg_loss.values())
+      loss_value.append(vgg_loss_value)
+    if self._loss_weight.l1 > 0:
+      l1_loss = self._l1_loss(reduction=self._reduction)(img1, img2)
+      l1_loss_value = tf_utils.safe_mean(l1_loss * self._loss_weight.l1)
+      loss_value.append(l1_loss_value)
+    total_loss = tf.math.add_n(loss_value)
+    return total_loss
+
+
+class PerceptualLoss(tf.keras.Model, metaclass=abc.ABCMeta):
+  """Base class for perceptual loss model."""
+
+  def __init__(
+      self,
+      feature_weight: Optional[Sequence[float]] = None,
+      loss_weight: Optional[PerceptualLossWeight] = None,
+  ):
+    """Instantiates perceptual loss.
+
+    Args:
+      feature_weight: The weight coeffcients of multiple model extracted
+        features used for calculating the perceptual loss.
+      loss_weight: The weight coefficients between `style_loss` and
+        `content_loss`.
+    """
+    super().__init__()
+    self._loss_op = lambda x, y: tf.math.reduce_mean(tf.abs(x - y))
+    self._loss_style = tf.constant(0.0)
+    self._loss_content = tf.constant(0.0)
+    self._feature_weight = feature_weight
+    self._loss_weight = loss_weight
+
+  def __call__(
+      self,
+      img1: tf.Tensor,
+      img2: tf.Tensor,
+  ) -> Mapping[str, tf.Tensor]:
+    """Computes perceptual loss between two images.
+
+    Args:
+      img1: First batch of images. The pixel values should be normalized to [-1,
+        1].
+      img2: Second batch of images. The pixel values should be normalized to
+        [-1, 1].
+
+    Returns:
+      A mapping between loss name and loss tensors.
+    """
+    x_features = self._compute_features(img1)
+    y_features = self._compute_features(img2)
+
+    if self._loss_weight is None:
+      self._loss_weight = PerceptualLossWeight()
+
+    # If the _feature_weight is not initialized, then initialize it as a list of
+    # all the element equals to 1.0.
+    if self._feature_weight is None:
+      self._feature_weight = [1.0] * len(x_features)
+
+    # If the length of _feature_weight smallert than the length of the feature,
+    # raise a ValueError. Otherwise, only use the first len(x_features) weight
+    # for computing the loss.
+    if len(self._feature_weight) < len(x_features):
+      raise ValueError(
+          f'Input feature weight length {len(self._feature_weight)} is smaller'
+          f' than feature length {len(x_features)}'
+      )
+
+    if self._loss_weight.style > 0.0:
+      self._loss_style = tf_utils.safe_mean(
+          self._loss_weight.style
+          * self._get_style_loss(x_feats=x_features, y_feats=y_features)
+      )
+    if self._loss_weight.content > 0.0:
+      self._loss_content = tf_utils.safe_mean(
+          self._loss_weight.content
+          * self._get_content_loss(x_feats=x_features, y_feats=y_features)
+      )
+
+    return {'style_loss': self._loss_style, 'content_loss': self._loss_content}
+
+  @abc.abstractmethod
+  def _compute_features(self, img: tf.Tensor) -> Sequence[tf.Tensor]:
+    """Computes features from the given image tensor.
+
+    Args:
+      img: Image tensor.
+
+    Returns:
+      A list of multi-scale feature maps.
+    """
+
+  def _get_content_loss(
+      self, x_feats: Sequence[tf.Tensor], y_feats: Sequence[tf.Tensor]
+  ) -> tf.Tensor:
+    """Gets weighted multi-scale content loss.
+
+    Args:
+      x_feats: Reconstructed face image.
+      y_feats: Target face image.
+
+    Returns:
+      A scalar tensor for the content loss.
+    """
+    content_losses = []
+    for coef, x_feat, y_feat in zip(self._feature_weight, x_feats, y_feats):
+      content_loss = self._loss_op(x_feat, y_feat) * coef
+      content_losses.append(content_loss)
+    return tf.math.reduce_sum(content_losses)
+
+  def _get_style_loss(
+      self, x_feats: Sequence[tf.Tensor], y_feats: Sequence[tf.Tensor]
+  ) -> tf.Tensor:
+    """Gets weighted multi-scale style loss.
+
+    Args:
+      x_feats: Reconstructed face image.
+      y_feats: Target face image.
+
+    Returns:
+      A scalar tensor for the style loss.
+    """
+    style_losses = []
+    i = 0
+    for coef, x_feat, y_feat in zip(self._feature_weight, x_feats, y_feats):
+      x_feat_g = _compute_gram_matrix(x_feat)
+      y_feat_g = _compute_gram_matrix(y_feat)
+      style_loss = self._loss_op(x_feat_g, y_feat_g) * coef
+      style_losses.append(style_loss)
+      i = i + 1
+
+    return tf.math.reduce_sum(style_loss)
+
+
+class VGGPerceptualLoss(PerceptualLoss):
+  """Perceptual loss based on VGG19 pretrained on the ImageNet dataset.
+
+  Reference:
+  - [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](
+      https://arxiv.org/abs/1603.08155) (ECCV 2016)
+
+  Perceptual loss measures high-level perceptual and semantic differences
+  between images.
+  """
+
+  def __init__(
+      self,
+      loss_weight: Optional[PerceptualLossWeight] = None,
+  ):
+    """Initializes image quality loss essentials.
+
+    Args:
+      loss_weight: Loss weight coefficients.
+    """
+    super().__init__(
+        feature_weight=np.array([0.1, 0.1, 1.0, 1.0, 1.0]),
+        loss_weight=loss_weight,
+    )
+
+    rgb_mean = tf.constant([0.485, 0.456, 0.406])
+    rgb_std = tf.constant([0.229, 0.224, 0.225])
+
+    self._rgb_mean = tf.reshape(rgb_mean, (1, 1, 1, 3))
+    self._rgb_std = tf.reshape(rgb_std, (1, 1, 1, 3))
+
+    model_path = file_util.DownloadedFiles(
+        'vgg_feature_extractor',
+        _VGG_IMAGENET_PERCEPTUAL_MODEL_URL,
+        is_folder=True,
+    )
+    self._vgg19 = model_util.load_keras_model(model_path.get_path())
+
+  def _compute_features(self, img: tf.Tensor) -> Sequence[tf.Tensor]:
+    """Computes VGG19 features."""
+    img = (img + 1) / 2.0
+    norm_img = (img - self._rgb_mean) / self._rgb_std
+    # no grad, as it only serves as a frozen feature extractor.
+    return self._vgg19(norm_img)
+
+
+def _compute_gram_matrix(feature: tf.Tensor) -> tf.Tensor:
+  """Computes gram matrix for the feature map.
+
+  Args:
+    feature: [B, H, W, C] feature map.
+
+  Returns:
+    [B, C, C] gram matrix.
+  """
+  h, w, c = feature.shape[1:].as_list()
+  feat_reshaped = tf.reshape(feature, shape=(-1, h * w, c))
+  feat_gram = tf.matmul(
+      tf.transpose(feat_reshaped, perm=[0, 2, 1]), feat_reshaped
+  )
+  return feat_gram / (c * h * w)
```

## mediapipe_model_maker/python/core/utils/model_util.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/utils/quantization.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/core/utils/test_util.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,15 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Test utilities for model maker."""
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from typing import List, Union
+from typing import Sequence
+from typing import Dict, List, Union
 
 # Dependency imports
 
 import numpy as np
 import tensorflow as tf
 
 from mediapipe_model_maker.python.core.data import dataset as ds
@@ -90,14 +91,25 @@
 
   # Gets output from keras model.
   keras_output = keras_model.predict_on_batch(input_tensors)
 
   return np.allclose(lite_output, keras_output, atol=atol)
 
 
+def run_tflite(
+    tflite_filename: str,
+    input_tensors: Union[List[tf.Tensor], Dict[str, tf.Tensor]],
+) -> Union[Sequence[tf.Tensor], tf.Tensor]:
+  """Runs TFLite model inference."""
+  with tf.io.gfile.GFile(tflite_filename, "rb") as f:
+    tflite_model = f.read()
+  lite_runner = model_util.get_lite_runner(tflite_model)
+  return lite_runner.run(input_tensors)
+
+
 def test_tflite(keras_model: tf.keras.Model,
                 tflite_model: bytearray,
                 size: Union[int, List[int]],
                 high: float = 1,
                 atol: float = 1e-04) -> bool:
   """Verifies if the output of TFLite model and TF Keras model are identical.
```

## mediapipe_model_maker/python/text/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/core/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/core/bert_model_options.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/core/bert_model_spec.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/text_classifier/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/text_classifier/dataset.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/text_classifier/model_options.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/text_classifier/model_spec.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/text_classifier/preprocessor.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/text_classifier/text_classifier.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/text_classifier/text_classifier_demo.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/text/text_classifier/text_classifier_options.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/core/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/core/image_preprocessing.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/core/image_utils.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/core/test_utils.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/face_stylizer/__init__.py

```diff
@@ -1,14 +1,28 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """MediaPipe Model Maker Python Public API For Face Stylization."""
+
+from mediapipe_model_maker.python.vision.face_stylizer import dataset
+from mediapipe_model_maker.python.vision.face_stylizer import face_stylizer
+from mediapipe_model_maker.python.vision.face_stylizer import face_stylizer_options
+from mediapipe_model_maker.python.vision.face_stylizer import hyperparameters
+from mediapipe_model_maker.python.vision.face_stylizer import model_options
+from mediapipe_model_maker.python.vision.face_stylizer import model_spec
+
+FaceStylizer = face_stylizer.FaceStylizer
+SupportedModels = model_spec.SupportedModels
+ModelOptions = model_options.FaceStylizerModelOptions
+HParams = hyperparameters.HParams
+Dataset = dataset.Dataset
+FaceStylizerOptions = face_stylizer_options.FaceStylizerOptions
```

## mediapipe_model_maker/python/vision/face_stylizer/dataset.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/gesture_recognizer/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/gesture_recognizer/constants.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/gesture_recognizer/dataset.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_demo.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_options.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/gesture_recognizer/hyperparameters.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/gesture_recognizer/metadata_writer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/gesture_recognizer/model_options.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/image_classifier/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/image_classifier/dataset.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/image_classifier/hyperparameters.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/image_classifier/image_classifier.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/image_classifier/image_classifier_demo.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/image_classifier/image_classifier_options.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/image_classifier/model_options.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/image_classifier/model_spec.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2022 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/object_detector/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -24,7 +24,19 @@
 ModelOptions = model_options.ObjectDetectorModelOptions
 ModelSpec = model_spec.ModelSpec
 SupportedModels = model_spec.SupportedModels
 HParams = hyperparameters.HParams
 QATHParams = hyperparameters.QATHParams
 Dataset = dataset.Dataset
 ObjectDetectorOptions = object_detector_options.ObjectDetectorOptions
+
+# Remove duplicated and non-public API
+del dataset
+del dataset_util  # pylint: disable=undefined-variable
+del detection  # pylint: disable=undefined-variable
+del hyperparameters
+del model  # pylint: disable=undefined-variable
+del model_options
+del model_spec
+del object_detector
+del object_detector_options
+del preprocessor  # pylint: disable=undefined-variable
```

## mediapipe_model_maker/python/vision/object_detector/dataset.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -102,22 +102,23 @@
           <file0>.jpg
           ...
         Annotations/
           <file0>.xml
           ...
     Each <file0>.xml annotation file should have the following format:
       <annotation>
-        <filename>file0.jpg<filename>
+        <filename>file0.jpg</filename>
         <object>
           <name>kangaroo</name>
           <bndbox>
             <xmin>233</xmin>
             <ymin>89</ymin>
             <xmax>386</xmax>
             <ymax>262</ymax>
+          </bndbox>
         </object>
         <object>...</object>
       </annotation>
 
     Args:
       data_dir: Name of the directory containing the data files.
       max_num_images: Max number of images to process.
@@ -151,16 +152,16 @@
       cache_prefix: The cache prefix including the cache directory and the cache
         prefix filename, e.g: '/tmp/cache/train'.
 
     Returns:
       ObjectDetectorDataset object.
     """
     # Get TFRecord Files
-    tfrecord_file_patten = cache_prefix + '*.tfrecord'
-    matched_files = tf.io.gfile.glob(tfrecord_file_patten)
+    tfrecord_file_pattern = cache_prefix + '*.tfrecord'
+    matched_files = tf.io.gfile.glob(tfrecord_file_pattern)
     if not matched_files:
       raise ValueError('TFRecord files are empty.')
 
     # Load meta_data.
     meta_data_file = cache_prefix + dataset_util.META_DATA_FILE_SUFFIX
     if not tf.io.gfile.exists(meta_data_file):
       raise ValueError("Metadata file %s doesn't exist." % meta_data_file)
```

## mediapipe_model_maker/python/vision/object_detector/dataset_util.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -341,15 +341,15 @@
     image_width: int,
 ):
   """Converts COCO annotations to feature lists.
 
   Args:
     bbox_annotations: List of dicts with keys ['bbox', 'category_id']
     image_height: Height of image
-    image_width: Width of iamge
+    image_width: Width of image
 
   Returns:
     (data, num_annotations_skipped) tuple where data contains the keys:
     ['xmin', 'xmax', 'ymin', 'ymax', 'is_crowd', 'category_id', 'area'] and
     num_annotations_skipped is the number of skipped annotations because of the
     bbox having 0 area.
   """
```

## mediapipe_model_maker/python/vision/object_detector/hyperparameters.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,73 +10,44 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Hyperparameters for training object detection models."""
 
 import dataclasses
-from typing import List
+from typing import Optional
 
 from mediapipe_model_maker.python.core import hyperparameters as hp
 
 
 @dataclasses.dataclass
 class HParams(hp.BaseHParams):
   """The hyperparameters for training object detectors.
 
   Attributes:
     learning_rate: Learning rate to use for gradient descent training.
     batch_size: Batch size for training.
     epochs: Number of training iterations over the dataset.
-    do_fine_tuning: If true, the base module is trained together with the
-      classification layer on top.
-    learning_rate_boundaries: List of epoch boundaries where
-      learning_rate_boundaries[i] is the epoch where the learning rate will
-      decay to learning_rate * learning_rate_decay_multipliers[i].
-    learning_rate_decay_multipliers: List of learning rate multipliers which
-      calculates the learning rate at the ith boundary as learning_rate *
-      learning_rate_decay_multipliers[i].
+    cosine_decay_epochs: The number of epochs for cosine decay learning rate.
+      See
+      https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay
+        for more info.
+    cosine_decay_alpha: The alpha value for cosine decay learning rate. See
+      https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay
+        for more info.
   """
 
   # Parameters from BaseHParams class.
-  learning_rate: float = 0.003
-  batch_size: int = 32
-  epochs: int = 10
-
-  # Parameters for learning rate decay
-  learning_rate_boundaries: List[int] = dataclasses.field(
-      default_factory=lambda: [5, 8]
-  )
-  learning_rate_decay_multipliers: List[float] = dataclasses.field(
-      default_factory=lambda: [0.1, 0.01]
-  )
-
-  def __post_init__(self):
-    # Validate stepwise learning rate parameters
-    lr_boundary_len = len(self.learning_rate_boundaries)
-    lr_decay_multipliers_len = len(self.learning_rate_decay_multipliers)
-    if lr_boundary_len != lr_decay_multipliers_len:
-      raise ValueError(
-          "Length of learning_rate_boundaries and ",
-          "learning_rate_decay_multipliers do not match: ",
-          f"{lr_boundary_len}!={lr_decay_multipliers_len}",
-      )
-    # Validate learning_rate_boundaries
-    if sorted(self.learning_rate_boundaries) != self.learning_rate_boundaries:
-      raise ValueError(
-          "learning_rate_boundaries is not in ascending order: ",
-          self.learning_rate_boundaries,
-      )
-    if (
-        self.learning_rate_boundaries
-        and self.learning_rate_boundaries[-1] > self.epochs
-    ):
-      raise ValueError(
-          "Values in learning_rate_boundaries cannot be greater ", "than epochs"
-      )
+  learning_rate: float = 0.3
+  batch_size: int = 8
+  epochs: int = 30
+
+  # Parameters for cosine learning rate decay
+  cosine_decay_epochs: Optional[int] = None
+  cosine_decay_alpha: float = 1.0
 
 
 @dataclasses.dataclass
 class QATHParams:
   """The hyperparameters for running quantization aware training (QAT) on object detectors.
 
   For more information on QAT, see:
@@ -90,12 +61,12 @@
       https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay
         for more information.
     decay_rate: Learning rate decay rate for Exponential Decay. See
       https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay
         for more information.
   """
 
-  learning_rate: float = 0.03
-  batch_size: int = 32
-  epochs: int = 10
-  decay_steps: int = 231
+  learning_rate: float = 0.3
+  batch_size: int = 8
+  epochs: int = 15
+  decay_steps: int = 8
   decay_rate: float = 0.96
```

## mediapipe_model_maker/python/vision/object_detector/model.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,26 +14,26 @@
 """Custom Model for Object Detection."""
 
 import os
 from typing import Mapping, Optional, Sequence, Union
 
 import tensorflow as tf
 
+from mediapipe_model_maker.python.vision.object_detector import detection
 from mediapipe_model_maker.python.vision.object_detector import model_options as model_opt
 from mediapipe_model_maker.python.vision.object_detector import model_spec as ms
 from official.core import config_definitions as cfg
 from official.projects.qat.vision.configs import common as qat_common
 from official.projects.qat.vision.modeling import factory as qat_factory
 from official.vision import configs
 from official.vision.losses import focal_loss
 from official.vision.losses import loss_utils
 from official.vision.modeling import factory
 from official.vision.modeling import retinanet_model
 from official.vision.modeling.layers import detection_generator
-from official.vision.serving import detection
 
 
 class ObjectDetectorModel(tf.keras.Model):
   """An object detector model which can be trained using Model Maker's training API.
 
   Attributes:
     loss_trackers: List of tf.keras.metrics.Mean objects used to track the loss
@@ -55,15 +55,17 @@
     """
     super().__init__()
     self._model_spec = model_spec
     self._model_options = model_options
     self._num_classes = num_classes
     self._model = self._build_model()
     checkpoint_folder = self._model_spec.downloaded_files.get_path()
-    checkpoint_file = os.path.join(checkpoint_folder, 'ckpt-277200')
+    checkpoint_file = os.path.join(
+        checkpoint_folder, self._model_spec.checkpoint_name
+    )
     self.load_checkpoint(checkpoint_file)
     self._model.summary()
     self.loss_trackers = [
         tf.keras.metrics.Mean(name=n)
         for n in ['total_loss', 'cls_loss', 'box_loss', 'model_loss']
     ]
 
@@ -76,15 +78,18 @@
         max_level=7,
         num_classes=self._num_classes,
         input_size=self._model_spec.input_image_shape,
         anchor=configs.retinanet.Anchor(
             num_scales=3, aspect_ratios=[0.5, 1.0, 2.0], anchor_size=3
         ),
         backbone=configs.backbones.Backbone(
-            type='mobilenet', mobilenet=configs.backbones.MobileNet()
+            type='mobilenet',
+            mobilenet=configs.backbones.MobileNet(
+                model_id=self._model_spec.model_id
+            ),
         ),
         decoder=configs.decoders.Decoder(
             type='fpn',
             fpn=configs.decoders.FPN(
                 num_filters=128, use_separable_conv=True, use_keras_layer=True
             ),
         ),
@@ -190,14 +195,15 @@
     generator_config = configs.retinanet.DetectionGenerator(
         nms_version='tflite',
         tflite_post_processing=configs.common.TFLitePostProcessingConfig(
             nms_score_threshold=0,
             max_detections=10,
             max_classes_per_detection=1,
             normalize_anchor_coordinates=True,
+            omit_nms=True,
         ),
     )
     tflite_post_processing_config = (
         generator_config.tflite_post_processing.as_dict()
     )
     tflite_post_processing_config['input_image_size'] = (
         self._model_spec.input_image_shape[0],
```

## mediapipe_model_maker/python/vision/object_detector/model_options.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/object_detector/model_spec.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -22,41 +22,60 @@
 
 MOBILENET_V2_FILES = file_util.DownloadedFiles(
     'object_detector/mobilenetv2',
     'https://storage.googleapis.com/tf_model_garden/vision/qat/mobilenetv2_ssd_coco/mobilenetv2_ssd_i256_ckpt.tar.gz',
     is_folder=True,
 )
 
+MOBILENET_MULTI_AVG_FILES = file_util.DownloadedFiles(
+    'object_detector/mobilenetmultiavg',
+    'https://storage.googleapis.com/tf_model_garden/vision/qat/mobilenetv3.5_ssd_coco/mobilenetv3.5_ssd_i256_ckpt.tar.gz',
+    is_folder=True,
+)
+
 
 @dataclasses.dataclass
 class ModelSpec(object):
   """Specification of object detector model."""
 
   # Mean and Stddev image preprocessing normalization values.
   mean_norm = (0.5,)
   stddev_norm = (0.5,)
   mean_rgb = (127.5,)
   stddev_rgb = (127.5,)
 
   downloaded_files: file_util.DownloadedFiles
+  checkpoint_name: str
   input_image_shape: List[int]
+  model_id: str
 
 
 mobilenet_v2_spec = functools.partial(
     ModelSpec,
     downloaded_files=MOBILENET_V2_FILES,
+    checkpoint_name='ckpt-277200',
+    input_image_shape=[256, 256, 3],
+    model_id='MobileNetV2',
+)
+
+mobilenet_multi_avg_spec = functools.partial(
+    ModelSpec,
+    downloaded_files=MOBILENET_MULTI_AVG_FILES,
+    checkpoint_name='ckpt-277200',
     input_image_shape=[256, 256, 3],
+    model_id='MobileNetMultiAVG',
 )
 
 
 @enum.unique
 class SupportedModels(enum.Enum):
   """Predefined object detector model specs supported by Model Maker."""
 
   MOBILENET_V2 = mobilenet_v2_spec
+  MOBILENET_MULTI_AVG = mobilenet_multi_avg_spec
 
   @classmethod
   def get(cls, spec: 'SupportedModels') -> 'ModelSpec':
     """Get model spec from the input enum and initializes it."""
     if spec not in cls:
       raise TypeError(f'Unsupported object detector spec: {spec}')
     return spec.value()
```

## mediapipe_model_maker/python/vision/object_detector/object_detector.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -24,14 +24,15 @@
 from mediapipe_model_maker.python.vision.object_detector import dataset as ds
 from mediapipe_model_maker.python.vision.object_detector import hyperparameters as hp
 from mediapipe_model_maker.python.vision.object_detector import model as model_lib
 from mediapipe_model_maker.python.vision.object_detector import model_options as model_opt
 from mediapipe_model_maker.python.vision.object_detector import model_spec as ms
 from mediapipe_model_maker.python.vision.object_detector import object_detector_options
 from mediapipe_model_maker.python.vision.object_detector import preprocessor
+from mediapipe.tasks.python.metadata.metadata_writers import metadata_info
 from mediapipe.tasks.python.metadata.metadata_writers import metadata_writer
 from mediapipe.tasks.python.metadata.metadata_writers import object_detector as object_detector_writer
 from official.vision.evaluation import coco_evaluator
 
 
 class ObjectDetector(classifier.Classifier):
   """ObjectDetector for building object detection model."""
@@ -53,15 +54,14 @@
     """
     super().__init__(
         model_spec=model_spec, label_names=label_names, shuffle=hparams.shuffle
     )
     self._preprocessor = preprocessor.Preprocessor(model_spec)
     self._hparams = hparams
     self._model_options = model_options
-    self._optimizer = self._create_optimizer()
     self._is_qat = False
 
   @classmethod
   def create(
       cls,
       train_data: ds.Dataset,
       validation_data: ds.Dataset,
@@ -100,14 +100,21 @@
   ):
     """Creates and trains the model.
 
     Args:
       train_data: Training data.
       validation_data: Validation data.
     """
+    self._optimizer = self._create_optimizer(
+        model_util.get_steps_per_epoch(
+            steps_per_epoch=self._hparams.steps_per_epoch,
+            batch_size=self._hparams.batch_size,
+            train_data=train_data,
+        )
+    )
     self._create_model()
     self._train_model(
         train_data, validation_data, preprocessor=self._preprocessor
     )
     self._save_float_ckpt()
 
   def _create_model(self) -> None:
@@ -254,14 +261,35 @@
       groundtruths = y['groundtruths']
       y_pred['image_info'] = groundtruths['image_info']
       y_pred['source_id'] = groundtruths['source_id']
       coco_eval.update_state(groundtruths, y_pred)
     coco_metrics = coco_eval.result()
     return losses, coco_metrics
 
+  def _create_fixed_anchor(
+      self, anchor_box: List[float]
+  ) -> object_detector_writer.FixedAnchor:
+    """Helper function to create FixedAnchor objects from an anchor box array.
+
+    Args:
+      anchor_box: List of anchor box coordinates in the format of [x_min, y_min,
+        x_max, y_max].
+
+    Returns:
+      A FixedAnchor object representing the anchor_box.
+    """
+    image_shape = self._model_spec.input_image_shape[:2]
+    y_center_norm = (anchor_box[0] + anchor_box[2]) / (2 * image_shape[0])
+    x_center_norm = (anchor_box[1] + anchor_box[3]) / (2 * image_shape[1])
+    height_norm = (anchor_box[2] - anchor_box[0]) / image_shape[0]
+    width_norm = (anchor_box[3] - anchor_box[1]) / image_shape[1]
+    return object_detector_writer.FixedAnchor(
+        x_center_norm, y_center_norm, width_norm, height_norm
+    )
+
   def export_model(
       self,
       model_name: str = 'model.tflite',
       quantization_config: Optional[quantization.QuantizationConfig] = None,
   ):
     """Converts and saves the model to a TFLite file with metadata included.
 
@@ -318,36 +346,75 @@
         converter = quantization_config.set_converter_with_quantization(
             converter, preprocess=self._preprocessor
         )
 
       converter.target_spec.supported_ops = (tf.lite.OpsSet.TFLITE_BUILTINS,)
       tflite_model = converter.convert()
 
-    writer = object_detector_writer.MetadataWriter.create(
+    # Build anchors
+    raw_anchor_boxes = self._preprocessor.anchor_boxes
+    anchors = []
+    for _, anchor_boxes in raw_anchor_boxes.items():
+      anchor_boxes_reshaped = anchor_boxes.numpy().reshape((-1, 4))
+      for ab in anchor_boxes_reshaped:
+        anchors.append(self._create_fixed_anchor(ab))
+
+    ssd_anchors_options = object_detector_writer.SsdAnchorsOptions(
+        object_detector_writer.FixedAnchorsSchema(anchors)
+    )
+
+    tensor_decoding_options = object_detector_writer.TensorsDecodingOptions(
+        num_classes=self._num_classes,
+        num_boxes=len(anchors),
+        num_coords=4,
+        keypoint_coord_offset=0,
+        num_keypoints=0,
+        num_values_per_keypoint=2,
+        x_scale=1,
+        y_scale=1,
+        w_scale=1,
+        h_scale=1,
+        apply_exponential_on_box_size=True,
+        sigmoid_score=False,
+    )
+    writer = object_detector_writer.MetadataWriter.create_for_models_without_nms(
         tflite_model,
         self._model_spec.mean_rgb,
         self._model_spec.stddev_rgb,
         labels=metadata_writer.Labels().add(list(self._label_names)),
+        ssd_anchors_options=ssd_anchors_options,
+        tensors_decoding_options=tensor_decoding_options,
+        output_tensors_order=metadata_info.RawDetectionOutputTensorsOrder.LOCATION_SCORE,
     )
     tflite_model_with_metadata, metadata_json = writer.populate()
     model_util.save_tflite(tflite_model_with_metadata, tflite_file)
     with open(metadata_file, 'w') as f:
       f.write(metadata_json)
 
-  def _create_optimizer(self) -> tf.keras.optimizers.Optimizer:
+  def _create_optimizer(
+      self, steps_per_epoch: int
+  ) -> tf.keras.optimizers.Optimizer:
     """Creates an optimizer with learning rate schedule for regular training.
 
     Uses Keras PiecewiseConstantDecay schedule by default.
 
+    Args:
+      steps_per_epoch: Steps per epoch to calculate the step boundaries from the
+        learning_rate_epoch_boundaries
+
     Returns:
       A tf.keras.optimizer.Optimizer for model training.
     """
     init_lr = self._hparams.learning_rate * self._hparams.batch_size / 256
-    lr_values = [init_lr] + [
-        init_lr * m for m in self._hparams.learning_rate_decay_multipliers
-    ]
-    learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
-        self._hparams.learning_rate_boundaries, lr_values
+    decay_epochs = (
+        self._hparams.cosine_decay_epochs
+        if self._hparams.cosine_decay_epochs
+        else self._hparams.epochs
+    )
+    learning_rate = tf.keras.optimizers.schedules.CosineDecay(
+        init_lr,
+        steps_per_epoch * decay_epochs,
+        self._hparams.cosine_decay_alpha,
     )
     return tf.keras.optimizers.experimental.SGD(
-        learning_rate=learning_rate_fn, momentum=0.9
+        learning_rate=learning_rate, momentum=0.9
     )
```

## mediapipe_model_maker/python/vision/object_detector/object_detector_demo.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/object_detector/object_detector_options.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## mediapipe_model_maker/python/vision/object_detector/preprocessor.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The MediaPipe Authors. All Rights Reserved.
+# Copyright 2023 The MediaPipe Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -40,14 +40,34 @@
     self._dtype = tf.float32
     self._match_threshold = 0.5
     self._unmatched_threshold = 0.5
     self._aug_scale_min = 0.5
     self._aug_scale_max = 2.0
     self._max_num_instances = 100
 
+    self._padded_size = preprocess_ops.compute_padded_size(
+        self._output_size, 2**self._max_level
+    )
+
+    input_anchor = anchor.build_anchor_generator(
+        min_level=self._min_level,
+        max_level=self._max_level,
+        num_scales=self._num_scales,
+        aspect_ratios=self._aspect_ratios,
+        anchor_size=self._anchor_size,
+    )
+    self._anchor_boxes = input_anchor(image_size=self._output_size)
+    self._anchor_labeler = anchor.AnchorLabeler(
+        self._match_threshold, self._unmatched_threshold
+    )
+
+  @property
+  def anchor_boxes(self):
+    return self._anchor_boxes
+
   def __call__(
       self, data: Mapping[str, Any], is_training: bool = True
   ) -> Tuple[tf.Tensor, Mapping[str, Any]]:
     """Run the preprocessor on an example.
 
     The data dict should contain the following keys always:
       - image
@@ -86,59 +106,45 @@
     # Convert boxes from normalized coordinates to pixel coordinates.
     boxes = box_ops.denormalize_boxes(boxes, image_shape)
 
     # Resize and crop image.
     image, image_info = preprocess_ops.resize_and_crop_image(
         image,
         self._output_size,
-        padded_size=preprocess_ops.compute_padded_size(
-            self._output_size, 2**self._max_level
-        ),
+        padded_size=self._padded_size,
         aug_scale_min=(self._aug_scale_min if is_training else 1.0),
         aug_scale_max=(self._aug_scale_max if is_training else 1.0),
     )
-    image_height, image_width, _ = image.get_shape().as_list()
 
     # Resize and crop boxes.
     image_scale = image_info[2, :]
     offset = image_info[3, :]
     boxes = preprocess_ops.resize_and_crop_boxes(
         boxes, image_scale, image_info[1, :], offset
     )
     # Filter out ground-truth boxes that are all zeros.
     indices = box_ops.get_non_empty_box_indices(boxes)
     boxes = tf.gather(boxes, indices)
     classes = tf.gather(classes, indices)
 
     # Assign anchors.
-    input_anchor = anchor.build_anchor_generator(
-        min_level=self._min_level,
-        max_level=self._max_level,
-        num_scales=self._num_scales,
-        aspect_ratios=self._aspect_ratios,
-        anchor_size=self._anchor_size,
-    )
-    anchor_boxes = input_anchor(image_size=(image_height, image_width))
-    anchor_labeler = anchor.AnchorLabeler(
-        self._match_threshold, self._unmatched_threshold
-    )
     (cls_targets, box_targets, _, cls_weights, box_weights) = (
-        anchor_labeler.label_anchors(
-            anchor_boxes, boxes, tf.expand_dims(classes, axis=1)
+        self._anchor_labeler.label_anchors(
+            self.anchor_boxes, boxes, tf.expand_dims(classes, axis=1)
         )
     )
 
     # Cast input image to desired data type.
     image = tf.cast(image, dtype=self._dtype)
 
     # Pack labels for model_fn outputs.
     labels = {
         'cls_targets': cls_targets,
         'box_targets': box_targets,
-        'anchor_boxes': anchor_boxes,
+        'anchor_boxes': self.anchor_boxes,
         'cls_weights': cls_weights,
         'box_weights': box_weights,
         'image_info': image_info,
     }
     if not is_training:
       groundtruths = {
           'source_id': data['source_id'],
```

## Comparing `mediapipe_model_maker-0.1.1.1.dist-info/METADATA` & `mediapipe_model_maker-0.2.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mediapipe-model-maker
-Version: 0.1.1.1
+Version: 0.2.0
 Summary: MediaPipe Model Maker is a simple, low-code solution for customizing on-device ML models
 Home-page: https://github.com/google/mediapipe/tree/master/mediapipe/model_maker
 Author: The MediaPipe Authors
 Author-email: mediapipe@google.com
 License: Apache 2.0
 Keywords: mediapipe,model,maker
 Classifier: Development Status :: 3 - Alpha
@@ -22,15 +22,15 @@
 Classifier: Topic :: Scientific/Engineering
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Software Development
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Description-Content-Type: text/markdown
 Requires-Dist: absl-py
-Requires-Dist: mediapipe (==0.9.2.1)
+Requires-Dist: mediapipe (>=0.10.0)
 Requires-Dist: numpy
 Requires-Dist: opencv-python
 Requires-Dist: tensorflow (>=2.10)
 Requires-Dist: tensorflow-datasets
 Requires-Dist: tensorflow-hub
-Requires-Dist: tf-models-official (>=2.11.5)
+Requires-Dist: tf-models-official (==2.11.6)
```

## Comparing `mediapipe_model_maker-0.1.1.1.dist-info/RECORD` & `mediapipe_model_maker-0.2.0.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,68 +1,75 @@
-mediapipe_model_maker/__init__.py,sha256=Ya9BMDZ8ynU9vjzp0ht454Ls_DAl0-rP_8J-el2neaE,983
-mediapipe_model_maker/python/__init__.py,sha256=cwoh8m4nnplDZ6zGrBJtouahx911zYocJ_higEi_BFE,607
-mediapipe_model_maker/python/core/__init__.py,sha256=cwoh8m4nnplDZ6zGrBJtouahx911zYocJ_higEi_BFE,607
-mediapipe_model_maker/python/core/hyperparameters.py,sha256=vJjkXsWwsV-N0Qbqpr6PWN_g-bjFyRop_quRXClpu5o,2696
-mediapipe_model_maker/python/core/data/__init__.py,sha256=cwoh8m4nnplDZ6zGrBJtouahx911zYocJ_higEi_BFE,607
-mediapipe_model_maker/python/core/data/classification_dataset.py,sha256=iT0WsdoCtWNrhYn0rLNbdkMajVR-zZalb4AEV7QVlY4,1708
-mediapipe_model_maker/python/core/data/data_util.py,sha256=uf0Ni8Ke-ctRbsyigeYULyaeOi9cdy66vcjN8vC_-n4,1213
-mediapipe_model_maker/python/core/data/dataset.py,sha256=FS5VKK2BgzOE2Wz3fKbOXv76s26mh9NZ8vVw7mh_V2Q,6339
-mediapipe_model_maker/python/core/tasks/__init__.py,sha256=cwoh8m4nnplDZ6zGrBJtouahx911zYocJ_higEi_BFE,607
-mediapipe_model_maker/python/core/tasks/classifier.py,sha256=xM0Clx9Z33tV04cGNt-PS67dAnaKSYk-RStFwIMTu6M,5706
-mediapipe_model_maker/python/core/tasks/custom_model.py,sha256=YklW0nuloxIrdaRqTnWNaZdRVeLhhZmrzlmNbJ4Q2Xc,3034
-mediapipe_model_maker/python/core/utils/__init__.py,sha256=cwoh8m4nnplDZ6zGrBJtouahx911zYocJ_higEi_BFE,607
-mediapipe_model_maker/python/core/utils/file_util.py,sha256=eAbbPVytisv57jO667-fH9qO7mniGSpmr6DklmNYtKs,3428
-mediapipe_model_maker/python/core/utils/loss_functions.py,sha256=9_TSgcbbMxRmvXvkUS5L5OSkAMisQuuXJLVJ_Hs4m9o,4463
-mediapipe_model_maker/python/core/utils/model_util.py,sha256=ZguDy8qeBD7TTouyEkBe4tmw8AeO08d6xVWRgXn6TL8,11159
-mediapipe_model_maker/python/core/utils/quantization.py,sha256=HM01wxOn6mfWJM04Wkn4DePDyfp9-VsDi81GXUw49Qg,8664
-mediapipe_model_maker/python/core/utils/test_util.py,sha256=GLTpH8VWchEm02CH_XPcE345HTZ3m3btWX5Nxi9fnw0,5247
-mediapipe_model_maker/python/text/__init__.py,sha256=cwoh8m4nnplDZ6zGrBJtouahx911zYocJ_higEi_BFE,607
-mediapipe_model_maker/python/text/core/__init__.py,sha256=cwoh8m4nnplDZ6zGrBJtouahx911zYocJ_higEi_BFE,607
-mediapipe_model_maker/python/text/core/bert_model_options.py,sha256=g_PaqLBjR1LP1_11-I04TnROl1cgNhY48SHZYAHbKLE,1205
-mediapipe_model_maker/python/text/core/bert_model_spec.py,sha256=srIAtu50JbnCy26YVT-lyuQ4EPD3_bRyvawY6RQcbNk,2260
-mediapipe_model_maker/python/text/text_classifier/__init__.py,sha256=jMMeCfpaWnH_2ssYLtaOk3-7eNueBBlmD5PeIzQ5f0U,1717
-mediapipe_model_maker/python/text/text_classifier/dataset.py,sha256=g1MfRhZUmS-s_VSZQr-dWp-eH7uEjx6omloWt4qkyPM,2955
-mediapipe_model_maker/python/text/text_classifier/model_options.py,sha256=XdS_zVMi_hjeIMC_kV8qKa3XlAarwQZhniUWsVTo69A,1646
-mediapipe_model_maker/python/text/text_classifier/model_spec.py,sha256=lRmbX3BcrS7Y9mPFu-7B21fWW2m-f7Q239t4hG_SuJU,2698
-mediapipe_model_maker/python/text/text_classifier/preprocessor.py,sha256=-wZLARwMB0bXr7TxN8y_NZyLXT90i28Yew3a7_C1AjM,9803
-mediapipe_model_maker/python/text/text_classifier/text_classifier.py,sha256=0ht5HcHlGK9qgwud4Zx5CgxUuDUEBpeQNeltqXyvlsM,18719
-mediapipe_model_maker/python/text/text_classifier/text_classifier_demo.py,sha256=Mc6l00_RJvkoRH3pHwLQz_Vv3p9QPmiDEm-jbRE8UMg,3846
-mediapipe_model_maker/python/text/text_classifier/text_classifier_options.py,sha256=5zpQwbfCQ_yvqVg-R-Y5w8XE-A5RAgcKSRVCPElpo00,1593
-mediapipe_model_maker/python/vision/__init__.py,sha256=cwoh8m4nnplDZ6zGrBJtouahx911zYocJ_higEi_BFE,607
-mediapipe_model_maker/python/vision/core/__init__.py,sha256=cwoh8m4nnplDZ6zGrBJtouahx911zYocJ_higEi_BFE,607
-mediapipe_model_maker/python/vision/core/image_preprocessing.py,sha256=Ar_yB_IyrT6mkmMMf4rGRAS_hksmH6TXwk1IOxhl6p8,8413
-mediapipe_model_maker/python/vision/core/image_utils.py,sha256=1sWYticznBFRKdiSI8aj5b71L27Ovs22cvU75Ygc7vQ,1074
-mediapipe_model_maker/python/vision/core/test_utils.py,sha256=q8HqZgfm9hl-jLuQ_E8Py9MduuN2bvYRdYg0lc9BVKA,1638
-mediapipe_model_maker/python/vision/face_stylizer/__init__.py,sha256=WooaUDL1zV2I6v6T3End8XhZNSpJyv0sZhU6uk3UaPc,675
-mediapipe_model_maker/python/vision/face_stylizer/dataset.py,sha256=GZ5_FtqfHVHHjMAbX-ZxcgiHRm0zFgLxoIQgrG4smpk,3334
-mediapipe_model_maker/python/vision/gesture_recognizer/__init__.py,sha256=3dGiti6gd9BuVUtEpslqgZxrM7RglRa4ZD6xE9H1LLw,1673
-mediapipe_model_maker/python/vision/gesture_recognizer/constants.py,sha256=IolE6YrnOipTqtgzM0Fo8pz_cgXVp7-VG8_NZp5oI4c,1737
-mediapipe_model_maker/python/vision/gesture_recognizer/dataset.py,sha256=c1EcZcxCm-X705ydEdMrLsCKcWXgZ6c0zAOKLpfY768,9634
-mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer.py,sha256=y2ri8NHEtta_6pKEKba1TxDuZz6kCeaaumihxMTOFQs,9127
-mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_demo.py,sha256=T1ob5S7w5uHFs2vslPqGB8c6bFz5H_GyKhpdNi4M50U,2395
-mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_options.py,sha256=78yXgMAdHUfyaTOiwHRwccI3YHhIJX5gH-9GSxvvI_E,1291
-mediapipe_model_maker/python/vision/gesture_recognizer/hyperparameters.py,sha256=trSJBtqCh_Lt34LAwDO1cF6x5o2JXga5_X6IiZvKTts,1432
-mediapipe_model_maker/python/vision/gesture_recognizer/metadata_writer.py,sha256=ohDYB31S9OqgtJR1x6DWPr5_FR2TrPxrRLr-cw6fuUc,9933
-mediapipe_model_maker/python/vision/gesture_recognizer/model_options.py,sha256=GOYxafCSG8VBwPLqgkKlatuZtIv5l5j1ZPxZtLBI5Mo,1322
-mediapipe_model_maker/python/vision/image_classifier/__init__.py,sha256=k0dRb6Q-2y6Ga6CQYsqN2VuCgkf8mH-ZVWTGLytsA_I,1632
-mediapipe_model_maker/python/vision/image_classifier/dataset.py,sha256=wuRj2-_EVL8of5rcqbOo9OX1lbx3rqFcjrmk2vrUKmw,3463
-mediapipe_model_maker/python/vision/image_classifier/hyperparameters.py,sha256=rrCRmfOfxUto7_nvzTkEA_kAcqR_WJNoK3ZT5oPW0xc,2388
-mediapipe_model_maker/python/vision/image_classifier/image_classifier.py,sha256=9LL_NljQff_VqUdUNEQU9fW8rW6v5RDxNaPYCL1NKIU,8583
-mediapipe_model_maker/python/vision/image_classifier/image_classifier_demo.py,sha256=4Lzs93wA-8Ty35NkhiqufWsTLggwC2fsBqZEXNJ9Vmw,3824
-mediapipe_model_maker/python/vision/image_classifier/image_classifier_options.py,sha256=z5N2ANCylb_rxHbxJ-dN5R6aobxC9SrHUEjipKDmlrw,1459
-mediapipe_model_maker/python/vision/image_classifier/model_options.py,sha256=geaCl9Knq3EgqocRjhDm-Tl7gT-fPGtE7lBMMcOz0SU,940
-mediapipe_model_maker/python/vision/image_classifier/model_spec.py,sha256=vbyzcEStz1fUaHRUD2AASTDLWJb4WH-hyEPveI7Z_nU,2700
-mediapipe_model_maker/python/vision/object_detector/__init__.py,sha256=6UOqTEp7nrOtW8eWk74niXU1K6tFpaeHsjpTsVyNic8,1501
-mediapipe_model_maker/python/vision/object_detector/dataset.py,sha256=BoowmNFcN1JJaZY75yYZUgwImxYQ1NJh4WTWwoMgJuk,6529
-mediapipe_model_maker/python/vision/object_detector/dataset_util.py,sha256=bY7cJlL57U9h3SLEnOCXPeHp9wSoOpvJAaqNHI6afVM,16573
-mediapipe_model_maker/python/vision/object_detector/hyperparameters.py,sha256=VYymnPsHIcY08vt2UnewZ9VhCmBFr4hDDTabMrhE1yM,3855
-mediapipe_model_maker/python/vision/object_detector/model.py,sha256=eX_FIK9hmc5VsyJcPaVe7KCISJ5-sNF6Cnhv9rH4M4k,13477
-mediapipe_model_maker/python/vision/object_detector/model_options.py,sha256=_yQa_LdA141oLEO8GWWmieuUiEOp4ZVgf_VIEi9lY7Q,987
-mediapipe_model_maker/python/vision/object_detector/model_spec.py,sha256=QYAkJVzir0VoVb8bF0XeD5_mxe4qlD0UjvkBXL7F4No,1886
-mediapipe_model_maker/python/vision/object_detector/object_detector.py,sha256=0mLBf8rXtqj3Tk4nM8BFAALGSbvOjl_A3S5FcIJxVME,13583
-mediapipe_model_maker/python/vision/object_detector/object_detector_demo.py,sha256=AYWBInfo_SOFtPFIBVKZ-QYVb2juHS2dwCl8ytb1riE,2862
-mediapipe_model_maker/python/vision/object_detector/object_detector_options.py,sha256=4UCkuihi7CgpGgwDlppdTuY4sxahnMvK7l9ioCsaCbU,1452
-mediapipe_model_maker/python/vision/object_detector/preprocessor.py,sha256=m8YCMNUltIohysDFKQ5HsShQrSYYs--Fr8H0Nk52M6M,5635
-mediapipe_model_maker-0.1.1.1.dist-info/METADATA,sha256=JvN3H0Quqqj0d5K2drFUZDMh_sGru8-LglPjMNXWIOY,1544
-mediapipe_model_maker-0.1.1.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-mediapipe_model_maker-0.1.1.1.dist-info/top_level.txt,sha256=kPbSZSnbOAk3IdnISh1XxFTw4671ayZuUQ_I91zl5tE,22
-mediapipe_model_maker-0.1.1.1.dist-info/RECORD,,
+mediapipe_model_maker/__init__.py,sha256=s3LQAAu-v8Io7sbwIPFWrWA3aWN6eV2CZuiV4BuM5MI,962
+mediapipe_model_maker/python/__init__.py,sha256=YGHXQMz1ZGPcNgSXggu03b0USZKE8d9Xqvn6NDUl898,586
+mediapipe_model_maker/python/core/__init__.py,sha256=YGHXQMz1ZGPcNgSXggu03b0USZKE8d9Xqvn6NDUl898,586
+mediapipe_model_maker/python/core/hyperparameters.py,sha256=177Hq3Zr8YLdlwNPUgn7MfwHhQSKPx-O2HkWXeQB09w,2675
+mediapipe_model_maker/python/core/data/__init__.py,sha256=YGHXQMz1ZGPcNgSXggu03b0USZKE8d9Xqvn6NDUl898,586
+mediapipe_model_maker/python/core/data/classification_dataset.py,sha256=0HwNlHYp5e71fVK_pXmbquAX5npHTtY4sCRlSu1c-j0,1687
+mediapipe_model_maker/python/core/data/data_util.py,sha256=6JL-42ksQvPMg6MRGanixzxdyXzYw3DxwegPhnnFCUg,1192
+mediapipe_model_maker/python/core/data/dataset.py,sha256=0-etVaevqm9DQ7u48XL5s2w51IiD-E-YevTP43SMh0o,6319
+mediapipe_model_maker/python/core/tasks/__init__.py,sha256=YGHXQMz1ZGPcNgSXggu03b0USZKE8d9Xqvn6NDUl898,586
+mediapipe_model_maker/python/core/tasks/classifier.py,sha256=317b2gDv7FrsrrOlSJX_dU3_oseXsBNKOr5QQ4zTE3w,5685
+mediapipe_model_maker/python/core/tasks/custom_model.py,sha256=MqtDOnPrXSYOXaKLhJDnJO7HbL7JrUJ9qk1ZlXVAVFg,3013
+mediapipe_model_maker/python/core/utils/__init__.py,sha256=YGHXQMz1ZGPcNgSXggu03b0USZKE8d9Xqvn6NDUl898,586
+mediapipe_model_maker/python/core/utils/file_util.py,sha256=ylqtf8jA_Rr0GWhbtHM0yUMV_Qh_rwhvT5wFFY7XTj8,3473
+mediapipe_model_maker/python/core/utils/loss_functions.py,sha256=cjXCi4loQUqXvKfsYw8aQCM6zf2oijoU8WMrVkyvTvU,12358
+mediapipe_model_maker/python/core/utils/model_util.py,sha256=724ODV8ttV0XhjddUGikVNi0_QHrs1XowsopZ02iTqM,11138
+mediapipe_model_maker/python/core/utils/quantization.py,sha256=ZM0Smg37AFr70nSNMRk1n_FWCYh7pUF4mfKFjV-ptDs,8643
+mediapipe_model_maker/python/core/utils/test_util.py,sha256=CeC3PKXIdQb_AkVpygTwpS4OW_fZ29cyqufWnTTgiwE,5629
+mediapipe_model_maker/python/text/__init__.py,sha256=YGHXQMz1ZGPcNgSXggu03b0USZKE8d9Xqvn6NDUl898,586
+mediapipe_model_maker/python/text/core/__init__.py,sha256=YGHXQMz1ZGPcNgSXggu03b0USZKE8d9Xqvn6NDUl898,586
+mediapipe_model_maker/python/text/core/bert_model_options.py,sha256=1ShLq2NAkmN2mU9jm4oLoQ15PwsL2FoeUqTU-kdQefU,1184
+mediapipe_model_maker/python/text/core/bert_model_spec.py,sha256=LSP2EhNrB8GEL3jhS_V9pt4-2D3UslzUO37gnGHvANc,2239
+mediapipe_model_maker/python/text/text_classifier/__init__.py,sha256=U67IqnTWmz8wTjRBHMWLiO-C0YrsSJtl75CgC6PeFQA,1696
+mediapipe_model_maker/python/text/text_classifier/dataset.py,sha256=ym_Ee4anBlV3JqeOt_SZLI7DBF64NGRucffK0DJTDY4,2934
+mediapipe_model_maker/python/text/text_classifier/model_options.py,sha256=v0gl53iF8_zc_QsOsZii3BnRPugSU1asYe07pfgIeQ4,1625
+mediapipe_model_maker/python/text/text_classifier/model_spec.py,sha256=VGe1hnvoAmgut7dYoRhPHGEmBlENZ8vqYYMQGNAebgY,2677
+mediapipe_model_maker/python/text/text_classifier/preprocessor.py,sha256=PDj3vbjk_2CW_oRN0lDjm_MYueHSpGDZulJ6my7fJno,9782
+mediapipe_model_maker/python/text/text_classifier/text_classifier.py,sha256=n3Pkv37y9LLrHCdatr7BUsbq7En4t-7aqXRiQPctXrA,18698
+mediapipe_model_maker/python/text/text_classifier/text_classifier_demo.py,sha256=Oxw9XzhY1QgpSLhj4LM5xbh79Wn86VJ44npwPFTJOD8,3825
+mediapipe_model_maker/python/text/text_classifier/text_classifier_options.py,sha256=47rCoaBFHOUVPsBxKAl7QJ_NbeABVFRRQ7rnz9gRaXk,1572
+mediapipe_model_maker/python/vision/__init__.py,sha256=YGHXQMz1ZGPcNgSXggu03b0USZKE8d9Xqvn6NDUl898,586
+mediapipe_model_maker/python/vision/core/__init__.py,sha256=YGHXQMz1ZGPcNgSXggu03b0USZKE8d9Xqvn6NDUl898,586
+mediapipe_model_maker/python/vision/core/image_preprocessing.py,sha256=I-bm1yxgxD566y3UuiwsxXMNQ__djrE_JRVCYDg26kA,8392
+mediapipe_model_maker/python/vision/core/image_utils.py,sha256=GzMQaFsrEvZ_ji_7bajJiVTzyQeGFPJMbWSLuiZ5nCI,1053
+mediapipe_model_maker/python/vision/core/test_utils.py,sha256=xC2tnq842xBajgwIpvhanFGpEmzQx540jguBw3ahQ3k,1617
+mediapipe_model_maker/python/vision/face_stylizer/__init__.py,sha256=xYN981nQw6rTenjrF5IC12DeWjHRn9scvKCtCfqpN7k,1378
+mediapipe_model_maker/python/vision/face_stylizer/constants.py,sha256=CmmNkWcBQ_vngI4H3FQ7y6_h9hvfsMbzeB2Ca5qja18,1800
+mediapipe_model_maker/python/vision/face_stylizer/dataset.py,sha256=PmrlLGWPxy8PtuVt-Vt9rTtMXyEyO03kHg9XyE9wOBI,3313
+mediapipe_model_maker/python/vision/face_stylizer/face_stylizer.py,sha256=fpCg-Txt4VfQy9qg7yednm-kvxCjNT65Odw3UGVG-Oc,8387
+mediapipe_model_maker/python/vision/face_stylizer/face_stylizer_options.py,sha256=oNglCV7edLEB1EGboPDoFrOCWFTw7AbY2mRmjfO4MDk,1386
+mediapipe_model_maker/python/vision/face_stylizer/hyperparameters.py,sha256=C8yDIBKpDDLAUiFecOrmdeYVR7W2n9OdZ3LnrlNrfnk,1314
+mediapipe_model_maker/python/vision/face_stylizer/model_options.py,sha256=A-I35pK7FErilC2hcuIdkJroG_kjAEyfnPDCkrsmRRo,2062
+mediapipe_model_maker/python/vision/face_stylizer/model_spec.py,sha256=ERiofqYhkCfyDIJD23qTFVn7fQnMNvaXW-BXFC21wbg,1896
+mediapipe_model_maker/python/vision/gesture_recognizer/__init__.py,sha256=ntkp1eROqkRqqJznxFMgza70x57WkheYSFWcMOWfbfM,1652
+mediapipe_model_maker/python/vision/gesture_recognizer/constants.py,sha256=fTk0gYRgGg7RCd-mztiRRStjVSHJFCuDv_7WvLOOfoc,1716
+mediapipe_model_maker/python/vision/gesture_recognizer/dataset.py,sha256=CYOGx2nS8DUHILFndVBLWe8WmqFd0GafaCwEI425nSQ,9613
+mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer.py,sha256=DTJLFg9RydNRYMUWHe9A5DYEbLPICrYZfb1dklGfoKc,9106
+mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_demo.py,sha256=Ceh6Dz7H7_zaAFAgsf8vCuaAKInoquuvxcpWuJgYLTM,2374
+mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer_options.py,sha256=qImXwmkCsjrZjY4usO02le6t4jEClo8gipH0ocCWZdc,1270
+mediapipe_model_maker/python/vision/gesture_recognizer/hyperparameters.py,sha256=ZDTd5xCMRWILP8MkPp-92OtXfLIAcPH9OotC4hAVJL4,1411
+mediapipe_model_maker/python/vision/gesture_recognizer/metadata_writer.py,sha256=qZDGb9TCqqFtUC4-K_Kuw7c-DP6LSfdrnCvF5e9UCWw,9912
+mediapipe_model_maker/python/vision/gesture_recognizer/model_options.py,sha256=vefkuKIMOClGH6bQVSy_kZu3OtcXig7aRCeJs4hOXH0,1301
+mediapipe_model_maker/python/vision/image_classifier/__init__.py,sha256=9Isdkpni49SrxbKGxuk8PZ3npus5sjnK8S1Y87zxJLA,1611
+mediapipe_model_maker/python/vision/image_classifier/dataset.py,sha256=tAPFwAoTenIfK-TJF9lqhdjeBhZFNc4TG-A7LroVOg0,3442
+mediapipe_model_maker/python/vision/image_classifier/hyperparameters.py,sha256=HVxumCAXQNjFWtuyPIv9NnNBpGTrC8dlBn-svVwFru4,2367
+mediapipe_model_maker/python/vision/image_classifier/image_classifier.py,sha256=Ubu_TtFLZjZski_nv71HosHmdjQyZvVH4LT9NTD_118,8562
+mediapipe_model_maker/python/vision/image_classifier/image_classifier_demo.py,sha256=D74ekJ08UVN51zclQv6SRpwzfTs80vpgKozagGbvIHs,3803
+mediapipe_model_maker/python/vision/image_classifier/image_classifier_options.py,sha256=C0-3nJx9pPbEcqV9utu4r-1a5uTAYAjw8nYSo-34Q6E,1438
+mediapipe_model_maker/python/vision/image_classifier/model_options.py,sha256=FgdjDqQQ-kzarLT_zvvnOrpZhcAh-n9ethzMgCZ-qk4,919
+mediapipe_model_maker/python/vision/image_classifier/model_spec.py,sha256=WdfIbDuBDwQJKK04Y9mNCzQYHvVOvfoYQDUwcnGOTYA,2679
+mediapipe_model_maker/python/vision/object_detector/__init__.py,sha256=Ipb7yhecoikl8XDNNhRxR-3IbbvNwdaoD4NP3tGSv7Y,1843
+mediapipe_model_maker/python/vision/object_detector/dataset.py,sha256=BIwhYdF6XZDhO1XQiW47Ax1qwn4LgbBjsqPrKS_-cOU,6531
+mediapipe_model_maker/python/vision/object_detector/dataset_util.py,sha256=1kd-IxpbiUbycOxnsSAV_iZBjj0kopsXRnNu54_XfoA,16552
+mediapipe_model_maker/python/vision/object_detector/detection.py,sha256=DzFUBSuaGH5tI52Fw1_eA84Nm03DOntx9SoMq4Y-RxE,1223
+mediapipe_model_maker/python/vision/object_detector/hyperparameters.py,sha256=LOZLU_IPtGkFzeYUyD2ct7N1QuU4TmnAtoOxiPlDB14,2639
+mediapipe_model_maker/python/vision/object_detector/model.py,sha256=391w8nb4z-Zw0UkgKzElF1MHdj4cXL3ULIyIFYM0vxs,13621
+mediapipe_model_maker/python/vision/object_detector/model_options.py,sha256=nZ-MUBSSDOGFuj7m3OBLLqXuoEQkWxhRY0m92TCVxYQ,966
+mediapipe_model_maker/python/vision/object_detector/model_spec.py,sha256=573J8eI3c6k_SdxwJxfE0w0nFqhRBbfuZZwga9iljSs,2476
+mediapipe_model_maker/python/vision/object_detector/object_detector.py,sha256=C036F8kFK49LZh0uPGc4F5bQL6Ki7jnmUCGgpWBXn2w,15973
+mediapipe_model_maker/python/vision/object_detector/object_detector_demo.py,sha256=NsWwvdvCMpOB6diSUuabiSuexZtxeLHeAqfIjhxzwL8,2841
+mediapipe_model_maker/python/vision/object_detector/object_detector_options.py,sha256=DtcXgZdHKCM3hcJtVWfg-Zh4qcQ-po7QkIZFQoDoZ9w,1431
+mediapipe_model_maker/python/vision/object_detector/preprocessor.py,sha256=SlcolkAbDsk4_JW-CyabqRKEqG6ezVgw3CtByPi9lm8,5674
+mediapipe_model_maker-0.2.0.dist-info/METADATA,sha256=UB5IYTDedgaYmRG_lFGtsNb4WcatYygiooCWx2OMi1Q,1541
+mediapipe_model_maker-0.2.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+mediapipe_model_maker-0.2.0.dist-info/top_level.txt,sha256=kPbSZSnbOAk3IdnISh1XxFTw4671ayZuUQ_I91zl5tE,22
+mediapipe_model_maker-0.2.0.dist-info/RECORD,,
```

