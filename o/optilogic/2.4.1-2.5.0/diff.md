# Comparing `tmp/optilogic-2.4.1-py3-none-any.whl.zip` & `tmp/optilogic-2.5.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,19 +1,19 @@
-Zip file size: 37565 bytes, number of entries: 17
--rw-r--r--  2.0 unx       22 b- defN 23-Apr-15 01:25 optilogic/__init__.py
--rw-r--r--  2.0 unx       48 b- defN 23-Apr-15 01:25 optilogic/pioneer/__init__.py
--rw-r--r--  2.0 unx       20 b- defN 23-Apr-15 01:25 optilogic/pioneer/api/__init__.py
--rw-r--r--  2.0 unx    64397 b- defN 23-Apr-15 01:25 optilogic/pioneer/api/api.py
--rw-r--r--  2.0 unx    95328 b- defN 23-Apr-15 01:25 optilogic/pioneer/api/api_tests.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-15 01:25 optilogic/pioneer/api/quick_tests/__init__.py
--rw-r--r--  2.0 unx     5111 b- defN 23-Apr-15 01:25 optilogic/pioneer/api/quick_tests/airline_hub_location_cbc.py
--rw-r--r--  2.0 unx      317 b- defN 23-Apr-15 01:25 optilogic/pioneer/api/quick_tests/bash.py
--rw-r--r--  2.0 unx      209 b- defN 23-Apr-15 01:25 optilogic/pioneer/api/quick_tests/quick.py
--rw-r--r--  2.0 unx      577 b- defN 23-Apr-15 01:25 optilogic/pioneer/api/quick_tests/sleep.py
--rw-r--r--  2.0 unx      131 b- defN 23-Apr-15 01:25 optilogic/pioneer/job_utils/__init__.py
--rw-r--r--  2.0 unx     3553 b- defN 23-Apr-15 01:25 optilogic/pioneer/job_utils/job_utils.py
--rw-r--r--  2.0 unx     1070 b- defN 23-Apr-15 01:25 optilogic-2.4.1.dist-info/LICENSE
--rw-r--r--  2.0 unx      759 b- defN 23-Apr-15 01:25 optilogic-2.4.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-15 01:25 optilogic-2.4.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Apr-15 01:25 optilogic-2.4.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1523 b- defN 23-Apr-15 01:25 optilogic-2.4.1.dist-info/RECORD
-17 files, 173167 bytes uncompressed, 35003 bytes compressed:  79.8%
+Zip file size: 39706 bytes, number of entries: 17
+-rw-r--r--  2.0 unx       22 b- defN 23-May-10 01:06 optilogic/__init__.py
+-rw-r--r--  2.0 unx       48 b- defN 23-May-10 01:06 optilogic/pioneer/__init__.py
+-rw-r--r--  2.0 unx       20 b- defN 23-May-10 01:06 optilogic/pioneer/api/__init__.py
+-rw-r--r--  2.0 unx    72496 b- defN 23-May-10 01:06 optilogic/pioneer/api/api.py
+-rw-r--r--  2.0 unx    96477 b- defN 23-May-10 01:06 optilogic/pioneer/api/api_tests.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-10 01:06 optilogic/pioneer/api/quick_tests/__init__.py
+-rw-r--r--  2.0 unx     5111 b- defN 23-May-10 01:06 optilogic/pioneer/api/quick_tests/airline_hub_location_cbc.py
+-rw-r--r--  2.0 unx      317 b- defN 23-May-10 01:06 optilogic/pioneer/api/quick_tests/bash.py
+-rw-r--r--  2.0 unx      209 b- defN 23-May-10 01:06 optilogic/pioneer/api/quick_tests/quick.py
+-rw-r--r--  2.0 unx      577 b- defN 23-May-10 01:06 optilogic/pioneer/api/quick_tests/sleep.py
+-rw-r--r--  2.0 unx      131 b- defN 23-May-10 01:06 optilogic/pioneer/job_utils/__init__.py
+-rw-r--r--  2.0 unx     3553 b- defN 23-May-10 01:06 optilogic/pioneer/job_utils/job_utils.py
+-rw-r--r--  2.0 unx     1070 b- defN 23-May-10 01:06 optilogic-2.5.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx      759 b- defN 23-May-10 01:06 optilogic-2.5.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-10 01:06 optilogic-2.5.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 23-May-10 01:06 optilogic-2.5.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1523 b- defN 23-May-10 01:06 optilogic-2.5.0.dist-info/RECORD
+17 files, 182415 bytes uncompressed, 37144 bytes compressed:  79.6%
```

## zipnote {}

```diff
@@ -30,23 +30,23 @@
 
 Filename: optilogic/pioneer/job_utils/__init__.py
 Comment: 
 
 Filename: optilogic/pioneer/job_utils/job_utils.py
 Comment: 
 
-Filename: optilogic-2.4.1.dist-info/LICENSE
+Filename: optilogic-2.5.0.dist-info/LICENSE
 Comment: 
 
-Filename: optilogic-2.4.1.dist-info/METADATA
+Filename: optilogic-2.5.0.dist-info/METADATA
 Comment: 
 
-Filename: optilogic-2.4.1.dist-info/WHEEL
+Filename: optilogic-2.5.0.dist-info/WHEEL
 Comment: 
 
-Filename: optilogic-2.4.1.dist-info/top_level.txt
+Filename: optilogic-2.5.0.dist-info/top_level.txt
 Comment: 
 
-Filename: optilogic-2.4.1.dist-info/RECORD
+Filename: optilogic-2.5.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## optilogic/pioneer/api/api.py

```diff
@@ -38,15 +38,15 @@
 from json import dumps, loads
 from os import getenv
 from re import fullmatch
 from requests import ConnectionError, delete, get, HTTPError, Response, request
 from sys import platform
 from tempfile import gettempdir
 from types import FrameType
-from typing import Any, cast, Dict, List, Literal, Tuple, Optional
+from typing import Any, cast, Dict, List, Literal, Tuple, TypedDict, Optional
 from warnings import warn, warn_explicit
 
 if platform == 'linux':
     from inspect import currentframe
     from signal import alarm, signal, SIGALRM
 
 
@@ -61,22 +61,72 @@
     :param pw: str: password of the user to authenticate and generate api key
     :param auth_legacy: bool: true for conventional username password or false for appkey
     :param cfg: dict: addition config to unpack and setup for api use
     :param apikey: str: username password authentication with one hour time to live
     :param appkey: str: authentication key generated in product without time to live restrictions
     '''
 
+    class DbTemplates(TypedDict):
+        id: Literal[
+            '2.1',
+            '3.1',
+            '5.1',
+            '7.1',
+            '9.2',
+            '9.3',
+            '12.1',
+            '15.1',
+            '18.1',
+            '21.1',
+            '24.1',
+            '27.1',
+            '28.1',
+            '30.1',
+        ]
+        name: Literal[
+            'Empty Database',
+            'Anura New Model v2.4',
+            'Anura New Model v2.5',
+            'Anura New Model v2.6',
+            'Blast Off To Space (BOTS) - v2.4',
+            'Blast Off To Space (BOTS) - v2.5',
+            'Tactical Capacity Optimization',
+            'Detailed Facility Selection',
+            'Greenfield Facility Selection',
+            'China Exit Strategy in Asia',
+            'Out of China',
+            'Multi-Year Capacity Planning',
+            'Fleet Size Optimization - EMEA Geo',
+            'Fleet Size Optimization - US Geo',
+        ]
+
+    DATABASE_TEMPLATES_NEW: Tuple[DbTemplates, ...] = (
+        {'id': '2.1', 'name': 'Empty Database'},
+        {'id': '3.1', 'name': 'Anura New Model v2.4'},
+        {'id': '5.1', 'name': 'Anura New Model v2.5'},
+        {'id': '7.1', 'name': 'Anura New Model v2.6'},
+        {'id': '9.2', 'name': 'Blast Off To Space (BOTS) - v2.4'},
+        {'id': '9.3', 'name': 'Blast Off To Space (BOTS) - v2.5'},
+        {'id': '12.1', 'name': 'Tactical Capacity Optimization'},
+        {'id': '15.1', 'name': 'Detailed Facility Selection'},
+        {'id': '18.1', 'name': 'Greenfield Facility Selection'},
+        {'id': '21.1', 'name': 'China Exit Strategy in Asia'},
+        {'id': '24.1', 'name': 'Out of China'},
+        {'id': '27.1', 'name': 'Multi-Year Capacity Planning'},
+        {'id': '28.1', 'name': 'Fleet Size Optimization - EMEA Geo'},
+        {'id': '30.1', 'name': 'Fleet Size Optimization - US Geo'},
+    )
+
     DATABASE_TEMPLATES: Tuple[str, ...] = (
         'empty',
         'anura_2_4_clean',
         'anura_2_5_clean',
+        'anura_2_6_clean',
         'anura_2_4_blast_off_to_space',
         'anura_2_5_blast_off_to_space',
-        'anura_2_6_clean',
-        '12d0e352-8f06-45aa-b3ba-58d6d93d3186',  # Blast Off To Space (BOTS)
         '37392edb-d582-4ce8-9f75-024651aa8592',  # Tactical Capacity Optimization
         '67a881b3-b6ab-4e95-9452-6dae8e831a6b',  # Detailed Facility Selection
         '71214744-f90a-4dcc-8008-bb9dab9493be',  # Greenfield Facility Selection
         '812d6fae-9fe7-4541-bee5-3cdb78e03eeb',  # China Exit Strategy in Asia
         'b30d9156-d00b-11ed-b72f-9fd8ea75300c',  # Fleet Size Optimization - US Geo
         'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6',  # Out of China
         'd3cba0d2-d00a-11ed-b72f-9fd8ea75300c',  # Fleet Size Optimization - EMEA Geo
@@ -231,75 +281,64 @@
 
         for item in batch['batchItems']:
             if find and item.get('pySearchTerm') is None:
                 raise KeyError('pySearchTerm key is missing')
             if find is False and item.get('pyModulePath') is None:
                 raise KeyError('pyModulePath key is missing')
 
-    def _database_templates_by_name(self, name: str, wildcard=False) -> List[str]:
-        '''find all database template ids by case-insensitive template name
-
-        :param name: str: template name
-        :param wildcard: str: substr matching, default to False
-        '''
-
-        # template[name, id]
-        template_map: Dict[str, str] = {
-            'empty': 'Empty Database',
-            'Anura - Blast off to Space': 'anura_2_5_blast_off_to_space',
-            'Anura - New Model': 'anura_2_5_clean',
-            'Anura - New Model (2.6)': 'anura_2_6_clean',
-            'Blast Off To Space (BOTS)': '12d0e352-8f06-45aa-b3ba-58d6d93d3186',
-            'China Exit Strategy in Asia': '812d6fae-9fe7-4541-bee5-3cdb78e03eeb',
-            'Detailed Facility Selection': '67a881b3-b6ab-4e95-9452-6dae8e831a6b',
-            'Fleet Size Optimization - EMEA Geo': 'd3cba0d2-d00a-11ed-b72f-9fd8ea75300c',
-            'Fleet Size Optimization - US Geo': 'b30d9156-d00b-11ed-b72f-9fd8ea75300c',
-            'Greenfield Facility Selection': '71214744-f90a-4dcc-8008-bb9dab9493be',
-            'Multi-Year Capacity Planning': 'd74fd9cc-e829-4ad3-9f33-f4ef0f3ddae1',
-            'Out of China': 'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6',
-            'Tactical Capacity Optimization': '37392edb-d582-4ce8-9f75-024651aa8592',
-        }
-
-        matches: List[str] = []
-        name = name.lower()
-
-        for t in template_map.items():
-            if wildcard is False:
-                # case-insensitive
-                if name == t[0].lower():
-                    matches.append(t[1])
-            else:
-                # substring case-insensitive
-                if t[0].lower().find(name) > -1:
-                    matches.append(t[1])
-
-        return matches
-
     def __does_storage_exist(
         self,
-        type: Literal['azure_afs', 'azure_workspace', 'onedrive', 'postgres_db'],
+        type: Literal['azure_afs', 'azure_workspace', 'onedrive', 'postgres_db', None],
         name: Optional[str] = None,
     ) -> bool:
-        '''does the account have a specific storage type and optionally by name
+        '''does the account have a specific storage type and or storage name
 
         :param type: str: storage type
         :param name: str: storage name of type, defaults to None
         '''
 
-        assert type in self.STORAGE_DEVICE_TYPES
-        exists: List[bool]
-        devices = self.account_storage_devices()
+        if type is None and name is None:
+            return False
 
-        if name is None:
-            exists = [True for d in devices['storages'] if d['type'] == type]
-        else:
+        exists: List[bool] = []
+        devices: Dict[str, Any] = self.account_storage_devices()
+
+        if type and name:
             exists = [True for d in devices['storages'] if d['type'] == type and d['name'] == name]
+        elif type:
+            assert type in self.STORAGE_DEVICE_TYPES
+            exists = [True for d in devices['storages'] if d['type'] == type]
+        elif name:
+            assert len(name) > 0
+            exists = [True for d in devices['storages'] if d['name'] == name]
 
         return True if len(exists) >= 1 else False
 
+    def __env(
+        self,
+    ) -> Dict[
+        Literal['job_cmd', 'job_dir', 'job_key', 'job_api', 'job_img', 'pip_ver', 'py_ver'], str
+    ]:
+        '''atlas and andromeda common environment variables'''
+
+        cmd: str = getenv('JOB_COMMAND', '')
+        cmd = cmd.strip()
+        d: Dict[
+            Literal['job_cmd', 'job_dir', 'job_key', 'job_api', 'job_img', 'pip_ver', 'py_ver'], str
+        ] = {
+            'job_cmd': cmd,
+            'job_dir': getenv('JOB_FILEPATH', ''),
+            'job_key': getenv('JOB_KEY', ''),
+            'job_api': getenv('OPTILOGIC_API_URL', ''),
+            'job_img': getenv('RELEASE_LIFECYCLE', ''),
+            'pip_ver': getenv('PYTHON_PIP_VERSION', ''),
+            'py_ver': getenv('PYTHON_VERSION', ''),
+        }
+        return d
+
     def __job_key_recent(self) -> str:
         '''most recent job key submitted'''
 
         resp = self._account_jobs(1)  # TODO assert is latest and not stale
         return '' if len(resp.get('jobs', [])) == 0 else resp['jobs'][0]['jobKey']
 
     def __input_timed(self, question: str, max_secs: int = 30, pw: bool = False) -> str:
@@ -318,14 +357,23 @@
             finally:
                 alarm(0)
         else:
             answer = getpass(question) if pw else input(question)
 
         return answer
 
+    @cached(cache=TTLCache(maxsize=1, ttl=60))
+    def __ip_address_is(self) -> str:
+        '''hack - identify external ip4 address if a database pre-exists'''
+
+        # TODO future state granular db per db server ips
+        db: Dict[str, Any] = self.account_storage_device('postgres_db')
+        resp: Dict[str, Any] = self.ip_address_allowed(db['name'])
+        return resp['ip']
+
     def __log_http_request(self, rsp: Response) -> None:
         '''log http requests to disk
 
         :param: resp Response: result from a http request
         '''
 
         if platform != 'linux':
@@ -348,27 +396,27 @@
 
         resp = self.database_schemas()
         versions: List[str] = [s['schemaVersion'] for s in resp['schemas']['anura']]
         return sorted(versions, reverse=True)
 
     def __storage_type(
         self, type: Literal['azure_afs', 'azure_workspace', 'onedrive', 'postgres_db']
-    ) -> list:
+    ) -> List[Dict[str, Any]]:
         '''specific storage type list
 
         :param type: str: storage type
         '''
 
         assert type in self.STORAGE_DEVICE_TYPES
-        resp = self.account_storage_devices()
-        devices = [d for d in resp['storages'] if d['type'] == type]
+        resp: Dict[str, Any] = self.account_storage_devices()
+        devices: List[Dict[str, Any]] = [d for d in resp['storages'] if d['type'] == type]
         return devices
 
     @cached(cache=TTLCache(maxsize=5, ttl=10))
-    def _account_jobs(self, max_jobs: int = 100) -> dict:
+    def _account_jobs(self, max_jobs: int = 100) -> Dict[str, Any]:
         '''any user job from any workspace'''
 
         url: str = f'{self.api_version}jobs-dashboard/jobs?maxResults={max_jobs}'
         resp = self._fetch_json('get', url)
         return resp
 
     @property
@@ -426,39 +474,125 @@
             now_time: float = datetime.now().timestamp() + 120
             mins_left: float = round((cache['expiration_time'] - now_time) / 60, 2)
             self.auth_apikey_mins_left = mins_left
             self.auth_apikey = cache['apikey']
             self.auth_apikey_expiry = cache['expiration_time']
             self.auth_req_header.update({'x-api-key': cache['apikey']})
 
+    def _database_templates_by_name(self, name: str, wildcard=False) -> List[str]:
+        '''find all database template ids by case-insensitive template name
+
+        :param name: str: template name
+        :param wildcard: str: substr matching, default to False
+        '''
+
+        # template[name, id]
+        template_map: Dict[str, str] = {
+            'empty': 'Empty Database',
+            'Anura - Blast off to Space': 'anura_2_5_blast_off_to_space',
+            'Anura - New Model': 'anura_2_5_clean',
+            'Anura - New Model (2.6)': 'anura_2_6_clean',
+            'China Exit Strategy in Asia': '812d6fae-9fe7-4541-bee5-3cdb78e03eeb',
+            'Detailed Facility Selection': '67a881b3-b6ab-4e95-9452-6dae8e831a6b',
+            'Fleet Size Optimization - EMEA Geo': 'd3cba0d2-d00a-11ed-b72f-9fd8ea75300c',
+            'Fleet Size Optimization - US Geo': 'b30d9156-d00b-11ed-b72f-9fd8ea75300c',
+            'Greenfield Facility Selection': '71214744-f90a-4dcc-8008-bb9dab9493be',
+            'Multi-Year Capacity Planning': 'd74fd9cc-e829-4ad3-9f33-f4ef0f3ddae1',
+            'Out of China': 'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6',
+            'Tactical Capacity Optimization': '37392edb-d582-4ce8-9f75-024651aa8592',
+        }
+
+        matches: List[str] = []
+        name = name.lower()
+
+        for t in template_map.items():
+            if wildcard is False:
+                # case-insensitive
+                if name == t[0].lower():
+                    matches.append(t[1])
+            else:
+                # substring case-insensitive
+                if t[0].lower().find(name) > -1:
+                    matches.append(t[1])
+
+        return matches
+
+    def _database_templates_legacy(self) -> Dict[str, Any]:
+        '''GET /v0/storage-templates - legacy available postgres schemas'''
+
+        warn('Deprecated, please use database_templates method', FutureWarning, stacklevel=2)
+        url: str = f'{self.api_version}storage-templates'
+        resp = self._fetch_json('get', url)
+        return resp
+
+    def _database_templates_legacy_by_name(self, name: str, wildcard=False) -> List[str]:
+        '''find all database template ids by case-insensitive template name
+
+        :param name: str: template name
+        :param wildcard: str: substr matching, default to False
+        '''
+
+        # template[name, id]
+        template_map: Dict[str, str] = {
+            'empty': 'Empty Database',
+            'Anura - Blast off to Space': 'anura_2_5_blast_off_to_space',
+            'Anura - New Model': 'anura_2_5_clean',
+            'Anura - New Model (2.6)': 'anura_2_6_clean',
+            'China Exit Strategy in Asia': '812d6fae-9fe7-4541-bee5-3cdb78e03eeb',
+            'Detailed Facility Selection': '67a881b3-b6ab-4e95-9452-6dae8e831a6b',
+            'Fleet Size Optimization - EMEA Geo': 'd3cba0d2-d00a-11ed-b72f-9fd8ea75300c',
+            'Fleet Size Optimization - US Geo': 'b30d9156-d00b-11ed-b72f-9fd8ea75300c',
+            'Greenfield Facility Selection': '71214744-f90a-4dcc-8008-bb9dab9493be',
+            'Multi-Year Capacity Planning': 'd74fd9cc-e829-4ad3-9f33-f4ef0f3ddae1',
+            'Out of China': 'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6',
+            'Tactical Capacity Optimization': '37392edb-d582-4ce8-9f75-024651aa8592',
+        }
+
+        matches: List[str] = []
+        name = name.lower()
+
+        for t in template_map.items():
+            if wildcard is False:
+                # case-insensitive
+                if name == t[0].lower():
+                    matches.append(t[1])
+            else:
+                # substring case-insensitive
+                if t[0].lower().find(name) > -1:
+                    matches.append(t[1])
+
+        return matches
+
     def _fetch_json(
         self,
         method: str,
         url: str,
-        json: Optional[dict] = None,
+        json: Optional[Dict[str, Any]] = None,
         data=None,
-        extra_headers: Optional[dict] = None,
+        extra_headers: Optional[Dict[str, str]] = None,
+        no_header: bool = False,
     ) -> dict:
         '''API calls that return a JSON response
 
         :param method: str: http request methods
         :param url: str: api server endpoint
         :param json: dict: payload in body of post call
         :param data: dictionary or list of tuples or bytes, or file-like object, defaults to None
         :param extra_headers: dict: extend the request headers, defaults to None
         '''
 
         if self.auth_method_legacy is True:
             self._check_apikey_expiration_time()
 
+        headers: Optional[Dict[str, str]] = self.auth_req_header
         if extra_headers:
             extra_headers.update(self.auth_req_header)
             headers = extra_headers
-        else:
-            headers = self.auth_req_header
+        elif no_header:
+            headers = None
 
         resp: Response = Response()
         ret: dict = {}
         try:
             resp = request(method, url, headers=headers, json=json, data=data)
             if self._log_active:
                 self.__log_http_request(resp)
@@ -479,14 +613,22 @@
                 ret = {'crash': True, 'url': url, 'exception': ce}
         except ValueError as ve:
             # JSONDecodeError
             ret = {'crash': True, 'url': url, 'exception': ve, 'text': resp.text}
 
         return ret
 
+    @cached(cache=TTLCache(maxsize=1, ttl=60))
+    def _ip_address_is_quick(self) -> str:
+        '''identify external ip4 address which is not as robust as ip_address_is method'''
+
+        url: str = 'https://api.ipify.org?format=json'
+        resp: Dict[Literal['ip'], str] = self._fetch_json('get', url, no_header=True)
+        return resp['ip']
+
     @property
     def _jobs_active(self) -> int:
         '''total sum of jobs currently in an active state across all workspaces'''
 
         active: int = 0
         states: Dict[str, int] = self._jobs_active_states()
 
@@ -573,29 +715,42 @@
                     matches.append(s)
             elif params == 1:
                 if (n == name) or (t and t == category) or (d and desc and d.find(desc) > -1):
                     matches.append(s)
 
         return matches
 
+    def _storage_attr(
+        self, name: str, attr: Literal['annotation', 'label', 'tag'] = 'tag'
+    ) -> Dict[str, Any]:
+        '''GET /v0/storage/{storageName}/{attribute}
+
+        :param name: str: storage device name
+        :param attr: str: specific storage device attribute, defaults to tag
+        '''
+
+        url: str = f'{self.api_version}storage/{name}/{attr}'
+        resp: Dict[str, Any] = self._fetch_json('get', url)
+        return resp
+
     @cached(cache=TTLCache(maxsize=1, ttl=86400))
     def account_info(self) -> Dict[str, Any]:
         '''GET v0/account - Get wksp and user information about the account'''
 
         url: str = self.api_version + 'account'
         resp = self._fetch_json('get', url)
         if self.auth_username is None:
             self.auth_username = resp['username']
         return resp
 
     def account_storage_device(
         self,
         type: Literal['azure_afs', 'azure_workspace', 'onedrive', 'postgres_db'],
         name: Optional[str] = None,
-    ) -> dict:
+    ) -> Dict[str, Any]:
         '''get the first storage device by type or with optional name
 
         :param type: str: storage device type
         :param name: str: storage device name, defaults to None
         '''
 
         device: dict = {}
@@ -650,14 +805,16 @@
         return resp
 
     @property
     def api_server_online(self) -> bool:
         '''check if API service is up and running'''
 
         response: Response = request('get', f'{self.api_version}ping')
+        if self._log_active:
+            self.__log_http_request(response)
         return True if response.status_code == 200 else False
 
     def authenticate_legacy(self) -> None:
         '''POST ​/refreshApiKey - Legacy auth method that will create an Api Key via username and password
 
         App Key is the preferred auth method for API usage
         https://atlas.optilogic.app/dashboard/#/user-account?tab=appkey
@@ -665,14 +822,16 @@
         '''
 
         # do a fetch, process response, then set
         url: str = self.api_version + 'refreshApiKey'
         headers: dict = {'x-user-id': self.auth_username, 'x-user-password': self.auth_userpass}
 
         response: Response = request('post', url, headers=headers)
+        if self._log_active:
+            self.__log_http_request(response)
         response.raise_for_status()
         resp: dict = response.json()
 
         # set instance members
         self.auth_apikey = resp['apiKey']
         self.auth_apikey_expiry = int(resp['expirationTime'])
         self.auth_apikey_mins_left = round(
@@ -701,29 +860,29 @@
     @property
     def database_count(self) -> int:
         ''' 'quantity of a postgres databases in account'''
         return self.storagetype_count('postgres_db')
 
     def database_create(
         self, name: str, desc: Optional[str] = None, template='anura_2_5_clean'
-    ) -> dict:
+    ) -> Dict[str, Any]:
         '''POST /storage/{database_name} - create a postgres database
 
         :param name: str: name of postgres database to create
-        :param desc: str describe the new database use case
-        :param template: str database template id
-            empty
-            anura_2_5_clean
-            anura_2_6_clean is for Tech Preview users only
+        :param desc: str: describe the purpose
+        :param template: str: database template id
+            empty,
+            anura_2_5_clean,
+            anura_2_6_clean is for Tech Preview users only,
             anura_2_5_blast_off_to_space
         '''
 
         if template not in self.DATABASE_TEMPLATES:
             # try to find id by looking up by name
-            template_ids: List[str] = self._database_templates_by_name(name)
+            template_ids: List[str] = self._database_templates_legacy_by_name(name)
             if len(template_ids) > 0:
                 raise ValueError(
                     f'invalid database template id {template}, possible matches {template_ids}'
                 )
             else:
                 raise ValueError(f'invalid database template id {template}')
 
@@ -737,18 +896,18 @@
         # TODO warn per each subscription level limit
 
         d: dict = {'type': 'sql', 'template': template, 'description': desc}
         if desc is None:
             d.pop('description')
 
         url: str = f'{self.api_version}storage/{name}'
-        resp = self._fetch_json('post', url, json=d)
+        resp: Dict[str, Any] = self._fetch_json('post', url, json=d)
         return resp
 
-    def database_delete(self, name: str) -> dict:
+    def database_delete(self, name: str) -> Dict[str, str]:
         '''wrapper for storage_delete'''
         return self.storage_delete(name)
 
     def database_objects(
         self, name: str, tables: bool = True, views: bool = True
     ) -> Dict[str, Any]:
         '''GET /storage/{database_name}/objects - returns list of schemas with relative tables and views
@@ -766,53 +925,59 @@
             query = query.rstrip(',')
             url += query
 
         resp: Dict[str, Any] = self._fetch_json('get', url)
         return resp
 
     @cached(cache=TTLCache(maxsize=1, ttl=86400))
-    def database_schemas(self) -> dict:
+    def database_schemas(self) -> Dict[str, Any]:
         '''postgres database schemas and versions'''
 
         url: str = f'{self.api_version}storage/db-versions'
-        resp = self._fetch_json('get', url)
+        resp: Dict[str, Any] = self._fetch_json('get', url)
         return resp
 
-    def database_tables(self, name: str) -> dict:
+    def database_tables(self, name: str) -> Dict[str, Any]:
         '''GET /storage/{database_name}/tables - returns list of schemas and tables
 
         :param name: str: postgres database name
         '''
 
         warn('Deprecated, please use database_objects method', FutureWarning, stacklevel=2)
         url: str = f'{self.api_version}storage/{name}/tables'
-        resp = self._fetch_json('get', url)
+        resp: Dict[str, Any] = self._fetch_json('get', url)
         return resp
 
     def database_tables_empty(self, name: str, tables: List[str], dry_run: bool = False) -> dict:
         '''POST /storage/{database_name}/empty-tables - remove all data from specified tables
 
         :param name: str: postgres database name
         :param tables: list[str]: tables to remove all data from
         :param dry_run: bool: preview changes
         '''
 
         d: Dict[str, List[str]] = {'tables': tables}
         url: str = f'{self.api_version}storage/{name}/empty-tables?dryRun={dry_run}'
-        resp = self._fetch_json('post', url, json=d)
+        resp: Dict[str, Any] = self._fetch_json('post', url, json=d)
         return resp
 
-    def database_templates(self) -> dict:
-        '''GET /v0/storage-templates - available postgres schemas'''
+    def database_templates(self) -> Dict[str, Any]:
+        '''GET /v0/storages/db-templates - pre-built db templates available for db creation'''
 
-        url: str = f'{self.api_version}storage-templates'
-        resp = self._fetch_json('get', url)
+        url: str = f'{self.api_version}storages/db-templates'
+        resp: Dict[str, Any] = self._fetch_json('get', url)
         return resp
 
-    def ip_address_allow(self, database_name: str, ip: Optional[str] = None) -> dict:
+    def databases(self) -> List[Dict[str, Any]]:
+        '''list of databases in account'''
+
+        dbs: List[Dict[str, Any]] = self.__storage_type('postgres_db')
+        return dbs
+
+    def ip_address_allow(self, database_name: str, ip: Optional[str] = None) -> Dict[str, str]:
         '''PUT /v0/storage/{database_name}/ip-in-firewall - an ip address to be whitelisted
 
         :param database_name: str: postgres database name
         :param ip: str: ip address to allow through the firewall, defaults to None
 
         omitted ip address will use the client's ip address that is making the request
         '''
@@ -821,18 +986,18 @@
             raise AssertionError(f'postgres database {database_name} does not exist')
 
         url: str = f'{self.api_version}storage/{database_name}/ip-in-firewall'
         if ip:
             url += f'?ipAddress={ip}'
 
         time.sleep(1)
-        resp = self._fetch_json('put', url)
+        resp: Dict[str, str] = self._fetch_json('put', url)
         return resp
 
-    def ip_address_allowed(self, database_name: str, ip: Optional[str] = None) -> dict:
+    def ip_address_allowed(self, database_name: str, ip: Optional[str] = None) -> Dict[str, Any]:
         '''GET /v0/storage/{database_name}/ip-in-firewall - is this ip address whitelisted
 
         :param database_name: str: postgres database name
         :param ip: str: ip address to allow through the firewall, defaults to None
 
         omitted ip address will use the client's ip address that is making the request
         '''
@@ -841,17 +1006,25 @@
             raise AssertionError(f'postgres database {database_name} does not exist')
 
         url: str = f'{self.api_version}storage/{database_name}/ip-in-firewall'
         if ip:
             url += f'?ipAddress={ip}'
 
         time.sleep(1)
-        resp = self._fetch_json('get', url)
+        resp: Dict[str, Any] = self._fetch_json('get', url)
         return resp
 
+    def ip_address_is(self) -> str:
+        '''identify external IP4 address if a database pre-exists'''
+
+        if self.storagetype_database_exists is False:
+            raise AssertionError('a postgres database must pre-exist')
+
+        return self.__ip_address_is()
+
     def onedrive_push(self, path: str) -> dict:
         '''POST /v0​/Studio/onedrive/push - Push Optilogic files to OneDrive
 
         :param path: str: file or subtree path
         '''
 
         raise NotImplementedError
@@ -963,22 +1136,24 @@
         :param name: str: storage device name
         '''
 
         url: str = f'{self.api_version}storage/{name}'
         resp = self._fetch_json('get', url)
         return resp
 
-    def storage_delete(self, name: str) -> dict:
+    def storage_delete(self, name: str) -> Dict[str, str]:
         '''DELETE /v0/storage/{storageName}
 
         :param name: str: storage device name
         '''
 
         url: str = f'{self.api_version}storage/{name}'
         response: Response = delete(url=url, headers=self.auth_req_header)
+        if self._log_active:
+            self.__log_http_request(response)
         response.raise_for_status()
         # returns 204 status code which has no content
         return {'result': 'success'}
 
     def storage_disk_create(self, name: str, type: str = 'hdd', size_gb: int = 10) -> dict:
         '''POST /v0/storage/{storageName} - create a new file storage device
 
@@ -990,14 +1165,18 @@
         raise NotImplementedError
         # TODO check if storagename is already taken
         type = 'transaction-optimized' if type == 'hdd' else 'premium'
         url = f'{self.api_version}storage/{name}?type={type}&size={size_gb}'
         resp = self._fetch_json('post', url)
         return resp
 
+    def storagename_exists(self, name: str) -> bool:
+        '''does the account have any storage by name'''
+        return self.__does_storage_exist(None, name)
+
     def storagename_accountfiles_exists(self, name: str) -> bool:
         '''does the account have account file storage by name'''
         return self.__does_storage_exist('azure_afs', name)
 
     def storagename_database_exists(self, name: str) -> bool:
         '''does the account have postgres database by name'''
         return self.__does_storage_exist('postgres_db', name)
@@ -1060,14 +1239,22 @@
         assert self.storagename_database_exists(database_name)
         d: Dict[str, str] = {'query': query}
 
         url: str = f'{self.api_version}storage/{database_name}/query-sql'
         resp = self._fetch_json('post', url, json=d)
         return resp
 
+    def util_environment(
+        self,
+    ) -> Dict[
+        Literal['job_cmd', 'job_dir', 'job_key', 'job_api', 'job_img', 'pip_ver', 'py_ver'], str
+    ]:
+        '''atlas and andromeda common environment variables'''
+        return self.__env()
+
     def util_job_monitor(
         self,
         wksp: str,
         job_key: str,
         stop_when: Literal[
             'submitted',
             'starting',
@@ -1167,14 +1354,16 @@
         :param wksp: str: workspace to download from
         :param file_path: str: file to download
         '''
 
         dir_path, file_name = os.path.split(file_path)
         url: str = f'{self.api_version}{wksp}/file/{dir_path}/{file_name}?op=download'
         response: Response = get(url=url, headers=self.auth_req_header)
+        if self._log_active:
+            self.__log_http_request(response)
         return response.text
 
     def wksp_file_download_status(self, wksp: str, file_path: str) -> dict:
         '''GET ​/v0​/{workspace}​/file​/{directoryPath}​/{filename} - Get a file metadata
 
         :param wksp: str: workspace to download from
         :param file_path: str: file to download
@@ -1379,15 +1568,15 @@
 
         url: str = f'{self.api_version}{wksp}/job/{jobkey}/ledger'
         if keys:
             url += f'?keys={keys}'
         resp = self._fetch_json('get', url)
         return resp
 
-    def wksp_job_metrics(self, wksp: str, jobkey: str) -> dict:
+    def wksp_job_metrics(self, wksp: str, jobkey: str) -> Dict[str, Any]:
         '''GET /{workspace}/job/{jobKey}/metrics - Get one second cpu and memory sampling of a job
 
         :param wksp: str: workspace scope
         :param jobkey: str: unique id
         '''
 
         url: str = f'{self.api_version}{wksp}/job/{jobkey}/metrics'
@@ -1411,22 +1600,24 @@
         file_path: Optional[str] = None,
         commandArgs: Optional[str] = None,
         tags: Optional[str] = None,
         timeout: Optional[int] = None,
         resourceConfig: Optional[
             Literal['mini', '4xs', '3xs', '2xs', 'xs', 's', 'm', 'l', 'xl', '2xl', '3xl', '4xl']
         ] = None,
-        command: Literal['run', 'presolve', 'run_model'] = 'run',
-    ) -> dict:
+        command: Literal['run', 'run_model', 'tadpole', 'presolve'] = 'run',
+        preview_image: bool = False,
+    ) -> Dict[str, Any]:
         '''POST ​/v0​/{workspace}​/job - Queue a job to run
 
         :param wksp: str: workspace scope
-        :param file_path: str: python module to run
-        :param command: str: run, presolve, run_model, defaults to run
+        :param command: str: run, run_model, tadpole, presolve, defaults to run
         :param commandArgs: str: positional args for py module, defaults to None
+        :param file_path: str: python module to run
+        :param preview_image: bool: latest andromeda executable package candidate, defaults to False
         :param tags: str: earmark the job record, defaults to None
         :param timeout: int: kill job if still running after duration, defaults to None
         :param resourceConfig: str: mini, 4xs, 3xs, 2xs, xs, s, m, l, xl, 2xl, 3xl, 4xl defaults to None
         '''
 
         url: str = f'{self.api_version}{wksp}/job?command={command}'
 
@@ -1442,14 +1633,15 @@
 
             url += f'&directoryPath={dir_path}&filename={file_name}'
 
         url += f'&commandArgs={commandArgs}' if commandArgs else ''
         url += f'&tags={tags}' if tags else ''
         url += f'&timeout={timeout}' if timeout else ''
         url += f'&resourceConfig={resourceConfig}' if resourceConfig else ''
+        url += f'&preview={preview_image}' if preview_image else ''
 
         resp = self._fetch_json('post', url)
         self._job_start_recent_key = resp['jobKey'] if resp.get('jobKey') else ''
 
         return resp
 
     def wksp_job_status(self, wksp: str, jobkey: str) -> dict:
@@ -1552,27 +1744,40 @@
 
         resp = self._fetch_json('post', url, json=batch)
         return resp
 
     def wksp_jobs(
         self,
         wksp: str,
-        command: Optional[str] = None,
+        command: Optional[Literal['run', 'run_model', 'tadpole', 'presolve']] = None,
         history: Optional[str] = None,
-        status: Optional[str] = None,
+        status: Optional[
+            Literal[
+                'submitted',
+                'starting',
+                'started',
+                'running',
+                'canceling',
+                'cancelled',
+                'stopping',
+                'stopped',
+                'done',
+                'error',
+            ]
+        ] = None,
         runSecsMin: Optional[int] = None,
-        runSecsMax: Optional[str] = None,
+        runSecsMax: Optional[int] = None,
         tags: Optional[str] = None,
-    ) -> dict:
+    ) -> Dict[str, Any]:
         '''GET ​/v0​/{workspace}​/jobs - List the jobs for a specific workspace
 
         :param wksp: str: where your files live
 
         QUERYSTRING PARAMETERS
-        :param command: str: run, presolve, run_custom, run_default, supplychain-3echelon, estimate, accelerate, supplychain-2echelon, defaults to None
+        :param command: str: run, run_model, tadpole, presolve, defaults to None
         :param history: str: all, or n days ago, defaults to None
         :param runSecsMax: int: maximum runtime in secs, defaults to None
         :param runSecsMin: int: minimum runtime in secs, defaults to None
         :param status: str: done, error, submitted, starting, running, cancelled, stopped, canceling, stopping, defaults to None
         :param tags: str: filter jobs where csv string matches, defaults to None
         '''
```

## optilogic/pioneer/api/api_tests.py

```diff
@@ -24,17 +24,19 @@
 import unittest
 from contextlib import redirect_stderr, redirect_stdout
 from dateutil.parser import parse
 from datetime import date, datetime, timedelta
 from docopt import docopt
 from io import StringIO
 from json import dumps, loads
+from numbers import Number
 from random import randint
+from re import fullmatch, search
 from sys import platform
-from typing import Any, Dict, List, Tuple, Optional
+from typing import Any, Dict, List, Literal, Tuple, Optional
 from warnings import warn
 from uuid import uuid4
 
 
 class TestApi(unittest.TestCase):
     '''A series of Pioneer REST API unit tests
 
@@ -61,31 +63,32 @@
     @classmethod
     def setUpClass(cls) -> None:
         '''called before test methods are ran
         ensure cache directories and test data inputs are available in target wksp
         '''
 
         cls.API = api.Api(
-            auth_legacy=TestApi.AUTH_LEGACY,
-            appkey=TestApi.APPKEY,
-            un=TestApi.USERNAME,
-            pw=TestApi.USERPASS,
+            auth_legacy=cls.AUTH_LEGACY,
+            appkey=cls.APPKEY,
+            un=cls.USERNAME,
+            pw=cls.USERPASS,
         )
         cls.__jobkey_quick: str = ''
         cls.API._log_active = True
 
         # directory references
         cls.dir_local_current: str = os.path.dirname(__file__)
         cls.dir_testdata_local: str = os.path.join(cls.dir_local_current, 'quick_tests')
         assert os.path.exists(cls.dir_testdata_local)
         assert len(os.listdir(cls.dir_testdata_local)) >= 1
         cls.dir_testdata_remote: str = 'quick_tests'
         cls.files_testdata_local: list[str] = []
         cls.files_testdata_remote: list[str] = []
         cls.py_run_me: str = ''
+        cls.py_run_me_bash: str = ''
         cls.py_run_me_quick: str = ''
 
         # get all directories from wksp
         resp = cls.API.wksp_files(cls.WKSP, '/quick_tests/')
         files_remote: list[str] = [f['filePath'] for f in resp['files']]
 
         # comb over local test data and map to destination file structure
@@ -98,175 +101,164 @@
             dest: str = os.path.join(cls.dir_testdata_remote, f)
             cls.files_testdata_local.append(local)
             cls.files_testdata_remote.append(dest)
             if dest.endswith('sleep.py'):
                 cls.py_run_me = dest
             elif dest.endswith('quick.py'):
                 cls.py_run_me_quick = dest
+            elif dest.endswith('bash.py'):
+                cls.py_run_me_bash = dest
 
             # upload local test data to destination
             for idx, local in enumerate(cls.files_testdata_local):
                 dest = cls.files_testdata_remote[idx]
             res: list[str] = [f for f in files_remote if dest in f]
             if len(res) == 0:
                 print(f'uploading {dest}')
                 resp = cls.API.wksp_file_upload(
                     cls.WKSP, file_path_dest=dest, file_path_local=local
                 )
 
-    def database_ensure_exist(self) -> None:
+    def database_ensure_exist(self, name: str = 'pg_unittest') -> None:
         '''database must exist for db unit tests'''
 
-        db: str = 'pg_unittest'
-        exists: bool = self.API.storagename_database_exists(db)
+        # cache is for 10 seconds
+        exists: bool = self.API.storagename_database_exists(name)
         if exists:
-            resp = self.API.storage(db)
+            resp: Dict[str, Any] = self.API.storage(name)
             assert resp.get('crash') is None
         else:
-            self.API.database_create(name=db, desc='common db for unit tests')
+            self.API.database_create(name, desc='common db for unit tests')
 
     def date_isoformat(self, date_str: str) -> bool:
         '''is date string isoformat'''
 
         d: date
         try:
             d = date.fromisoformat(date_str)
             return isinstance(d, date)
         except ValueError:
             return False
 
     def job_prereq(self) -> None:
         '''for running test methods in isolation'''
 
-        resp = self.API.wksp_job_start(self.WKSP, self.py_run_me_quick, tags='unittest_prereq')
+        resp = self.API.wksp_job_start(
+            self.WKSP, self.py_run_me_quick, tags='unittest_prereq', resourceConfig='mini'
+        )
         self.assertEqual(resp['result'], 'success')
         self.__jobkey_quick = resp['jobKey']
         # BUG ledger and metrics should be immediately available when job is running
         res: bool = self.API.util_job_monitor(self.WKSP, resp['jobKey'], stop_when='done')
         self.assertTrue(res)
 
-    def storage_common(self, d: dict) -> None:
-        ''''''
-        pass
-
-    def storage_azure_afs(self, d: dict) -> None:
-        '''common ssd response for get device and devices'''
+    def storage_common(self, d: Dict[str, Any]) -> None:
+        '''common keys across afs, wksp, onedrive, and postgres storage devices'''
 
-        self.assertIsInstance(d['bytesUsed'], int)
-        self.assertIsInstance(d['capacity'], int)
+        self.assertIsInstance(d['annotations'], dict)
+        # self.assertIsInstance(d['bytesUsed'], int) # BUG OE-7949 OneDrive
+        self.assertTrue(d['bytesUsed'] is None or isinstance(d['bytesUsed'], int))
         self.assertIsInstance(d['created'], int)
         self.assertTrue(d['description'] is None or isinstance(d['description'], str))
         self.assertIsInstance(d['id'], str)
-        self.assertIsInstance(d['internal'], bool)
+        self.assertIsInstance(d['labels'], dict)
         self.assertTrue(d['lockoutReason'] is None or isinstance(d['lockoutReason'], str))
         self.assertIsInstance(d['name'], str)
-        self.assertIsInstance(d['tier'], str)
+        self.assertIsInstance(d['notes'], str)
+        self.assertIsInstance(d['tags'], str)
         self.assertIsInstance(d['type'], str)
         self.assertIsInstance(d['updated'], int)
 
-    def storage_azure_workspace(self, d: dict) -> None:
+    def storage_azure_afs(self, d: Dict[str, Any]) -> None:
+        '''common ssd response for get device and devices'''
+
+        self.storage_common(d)
+        self.assertIsInstance(d['capacity'], int)
+        self.assertIsInstance(d['internal'], bool)
+        self.assertIsInstance(d['tier'], str)
+
+    def storage_azure_workspace(self, d: Dict[str, Any]) -> None:
         '''common wksp response for get device and devices'''
 
-        self.assertIsInstance(d['bytesUsed'], int)
+        self.storage_common(d)
         self.assertIsInstance(d['capacity'], int)
-        self.assertIsInstance(d['created'], int)
-        self.assertTrue(d['description'] is None or isinstance(d['description'], str))
-        self.assertIsInstance(d['id'], str)
         self.assertIsInstance(d['internal'], bool)
-        self.assertTrue(d['lockoutReason'] is None or isinstance(d['lockoutReason'], str))
-        self.assertIsInstance(d['name'], str)
         self.assertIsInstance(d['tier'], str)
-        self.assertIsInstance(d['type'], str)
-        self.assertIsInstance(d['updated'], int)
         self.assertIsInstance(d['workspaceKey'], str)
 
-    def storage_database(self, d: dict) -> None:
+    def storage_database(self, d: Dict[str, Any]) -> None:
         '''common db response for get device and devices'''
 
-        self.assertTrue(d['bytesUsed'] is None or isinstance(d['bytesUsed'], int))
-        self.assertTrue(
-            d['bytesUsedLastUpdated'] is None or isinstance(d['bytesUsedLastUpdated'], int)
-        )
-        self.assertIsInstance(d['created'], int)
+        self.storage_common(d)
+        self.assertIsInstance(d['bytesUsedLastUpdated'], int)
         self.assertIsInstance(d['dbname'], str)
         self.assertIsInstance(d['defaultSchema'], str)
-        self.assertTrue(d['description'] is None or isinstance(d['description'], str))
         self.assertIsInstance(d['host'], str)
-        self.assertIsInstance(d['id'], str)
-        self.assertTrue(d['lockoutReason'] is None or isinstance(d['lockoutReason'], str))
-        self.assertIsInstance(d['name'], str)
         self.assertIsInstance(d['port'], int)
         self.assertIsInstance(d['schemaStatus'], str)
         self.assertTrue(d['schemaStatus'] in ('error', 'invalid', 'valid'))
-        self.assertIsInstance(d['schemaStatusLastUpdated'], float)
+        # self.assertIsInstance(d['schemaStatusLastUpdated'], float)  # BUG OE-7561
         self.assertIsInstance(d['schemaVersion'], str)
-        self.assertIsInstance(d['type'], str)
-        self.assertIsInstance(d['updated'], int)
         self.assertIsInstance(d['user'], str)
         # empty pg datase vs anura schema
         if d['defaultSchema'].startswith('anura_2_'):
             self.assertRegex(d['schemaVersion'], r'2\.[4-9]\.\d+')
-            self.assertIsInstance(d['schemaStatusLastValidated'], float)
+            # self.assertIsInstance(d['schemaStatusLastValidated'], float)  # BUG OE-7561
         else:
-            self.assertEqual(d['defaultSchema'], '"$user"')
+            # self.assertTrue(len(d['defaultSchema']) == 0)  # TODO OE-7561
             self.assertTrue(len(d['schemaVersion']) == 0)
 
     def storage_onedrive(self, d: dict) -> None:
         '''common onedrive storage response for get device and devices'''
 
+        self.storage_common(d)
         self.assertIsInstance(d['accountName'], str)
         self.assertIsInstance(d['authenticated'], int)
-        self.assertTrue(d['bytesUsed'] is None or isinstance(d['bytesUsed'], int))
         self.assertIsInstance(d['capacity'], int)
         self.assertIsInstance(d['created'], int)
-        self.assertTrue(d['description'] is None or isinstance(d['description'], str))
         self.assertIsInstance(d['endpointSuffix'], str)
         self.assertIsInstance(d['homeAccountId'], str)
-        self.assertIsInstance(d['id'], str)
         self.assertIsInstance(d['internal'], bool)
-        self.assertTrue(d['lockoutReason'] is None or isinstance(d['lockoutReason'], str))
-        self.assertIsInstance(d['name'], str)
         self.assertIsInstance(d['protocol'], str)
-        self.assertIsInstance(d['tier'], str)
-        self.assertIsInstance(d['type'], str)
-        self.assertIsInstance(d['updated'], int)
         self.assertIsInstance(d['userId'], str)
         self.assertIsInstance(d['username'], str)
 
     def test_000_init_api_version_bad(self) -> None:
         '''recover from a bad api version provided'''
 
         bad_version: int = 99
 
         with redirect_stderr(StringIO()) as err:
-            a = api.Api(auth_legacy=TestApi.AUTH_LEGACY, version=bad_version, ut=True)
+            a = api.Api(auth_legacy=self.AUTH_LEGACY, version=bad_version, ut=True)
             output: str = err.getvalue().strip()
 
         self.assertGreater(output.find(f'API version {bad_version} not supported'), -1)
         self.assertRegex(a.api_version, r'app/v0/')
 
     def test_000_init_password_missing(self) -> None:
         '''get password uses a secret stream and input will not echo'''
 
         if platform != 'linux':
             self.skipTest('only linux has timed inputs')
 
         with redirect_stdout(StringIO()) as out:
             try:
-                a = api.Api(auth_legacy=True, un=TestApi.USERNAME, ut=True)
+                a = api.Api(auth_legacy=True, un=self.USERNAME, ut=True)
             except (EOFError, TimeoutError):
                 pass
             output: str = out.getvalue().strip()
 
         self.assertEqual(output.find('REQUIRED API User Password'), -1)
 
     def test_000_prereqs(self) -> None:
         '''ensure job data is available to test against'''
 
-        resp = self.API.wksp_job_start(self.WKSP, self.py_run_me_quick, tags='unittest_preseed')
+        resp = self.API.wksp_job_start(
+            self.WKSP, self.py_run_me_quick, tags='unittest_preseed', resourceConfig='mini'
+        )
         self.assertEqual(resp['result'], 'success')
         self.__jobkey_quick = resp['jobKey']
         stime: float = time.time()
         print('Pre-seeding by running a new job')
         res: bool = self.API.util_job_monitor(
             self.WKSP, resp['jobKey'], stop_when='done', secs_max=300
         )
@@ -317,16 +309,18 @@
         self.assertIsInstance(resp['limits']['databaseCount'], int)
         self.assertIsInstance(resp['limits']['fileStorageGb'], int)
         self.assertEqual(resp['limits']['concurrentJobs'], 50)
         self.assertEqual(resp['limits']['databaseCount'], 100)
         self.assertEqual(resp['limits']['fileStorageGb'], 500)  # max possible
 
         self.assertIsInstance(resp['usage'], dict)
-        self.assertEqual(len(resp['usage']), 4)
+        self.assertEqual(len(resp['usage']), 5)
         self.assertIsInstance(resp['usage']['databaseCount'], int)
+        self.assertIsInstance(resp['usage']['databaseStorageBytes'], int)
+        dbs_total_bytes: int = resp['usage']['databaseStorageBytes']
         self.assertIsInstance(resp['usage']['fileStorageCount'], int)
         self.assertIsInstance(resp['usage']['fileStorageGb'], int)
         self.assertIsInstance(resp['usage']['workspaceCount'], int)
         self.assertGreaterEqual(resp['usage']['databaseCount'], 0)
         self.assertLessEqual(resp['usage']['databaseCount'], resp['limits']['databaseCount'])
         self.assertTrue(0 < resp['usage']['fileStorageCount'] < 10)
         self.assertTrue(0 < resp['usage']['workspaceCount'] < 10)
@@ -335,14 +329,21 @@
             resp['limits']['fileStorageGb'] * resp['usage']['fileStorageCount'],
         )
 
         self.assertIsInstance(resp['username'], str)
         if self.API.auth_username:
             self.assertEqual(resp['username'], self.API.auth_username)
 
+        dbs: List[Dict[str, Any]] = self.API.databases()
+        total: int = 0
+        for db in dbs:
+            total += db['bytesUsed']
+
+        self.assertEqual(dbs_total_bytes, total)
+
     def test_account_jobs(self) -> None:
         ''' 'any user job from any workspace'''
 
         job_count: int = 50
         resp = self.API._account_jobs(max_jobs=job_count)
         self.assertIsInstance(resp['jobs'], list)
         self.assertIsInstance(resp['result'], str)
@@ -388,15 +389,17 @@
                 pass
 
     def test_account_jobs_active(self) -> None:
         '''compare active account jobs count to all active wksp jobs'''
 
         start_new_job: bool = bool(randint(0, 1))
         if start_new_job:
-            self.API.wksp_job_start(self.WKSP, self.py_run_me, tags='unittest_jobs_active')
+            self.API.wksp_job_start(
+                self.WKSP, self.py_run_me, tags='unittest_jobs_active', resourceConfig='mini'
+            )
 
         active_account: int = 0
         resp = self.API._account_jobs(
             max_jobs=200
         )  # BUG submitted counts as active therefore must account for excessive submitted jobs
         for job in resp['jobs']:
             if job['status'] in self.API.JOBSTATES_ACTIVE:
@@ -415,24 +418,24 @@
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
         self.assertGreaterEqual(resp['count'], 1)
         self.assertIsInstance(resp['storages'], list)
         with self.subTest():
             for d in resp['storages']:
                 if d['type'] == 'azure_afs':
-                    self.assertEqual(len(d), 11)
+                    self.assertEqual(len(d), 15)
                     self.storage_azure_afs(d)
                 if d['type'] == 'azure_workspace':
-                    self.assertEqual(len(d), 12)
+                    self.assertEqual(len(d), 16)
                     self.storage_azure_workspace(d)
                 elif d['type'] == 'onedrive':
-                    self.assertEqual(len(d), 18)
+                    self.assertEqual(len(d), 22)
                     self.storage_onedrive(d)
                 elif d['type'] == 'postgres_db':
-                    self.assertEqual(len(d), 18)
+                    self.assertEqual(len(d), 22)
                     self.storage_database(d)
 
     def test_account_usage(self) -> None:
         '''atlas and andromeda information'''
 
         resp = self.API._account_usage()
         self.assertEqual(resp['result'], 'success')
@@ -548,15 +551,15 @@
         '''only version zero is supported'''
 
         self.assertTrue(self.API.api_version.endswith('v0/'))
 
     def test_database_create_delete(self) -> None:
         '''create a postgres database then delete'''
 
-        bots: List[str] = self.API._database_templates_by_name('blast', wildcard=True)
+        bots: List[str] = self.API._database_templates_legacy_by_name('blast', wildcard=True)
         self.assertGreaterEqual(len(bots), 1)
         dbname: str = f'pg_unittest_{time.perf_counter_ns()}'
 
         # create database
         resp: dict = self.API.database_create(dbname, desc=f'unittest {dbname}', template=bots[0])
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['result'], 'success')
@@ -592,15 +595,15 @@
 
         tname: str = 'Out of China'
         self.assertFalse(tname in self.API.DATABASE_TEMPLATES)
         with self.assertRaises(ValueError):
             self.API.database_create(name='cant', template=tname)
 
         # get template id
-        tids: List[str] = self.API._database_templates_by_name(tname)
+        tids: List[str] = self.API._database_templates_legacy_by_name(tname)
         self.assertEqual(len(tids), 1)
         self.assertEqual(tids[0], 'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6')
         dbname: str = f'pg_unittest_{time.perf_counter_ns()}'
 
         # create database with matching template id
         resp = self.API.database_create(dbname, desc=f'unittest {dbname} {tname}', template=tids[0])
         self.assertIsInstance(resp['result'], str)
@@ -783,215 +786,137 @@
         resp = self.API.sql_query(db, QUERY_ROWS)
         rows_cleared: int = int(resp['queryResults'][0]['count'])
         self.assertEqual(rows_cleared, 0)
 
     def test_database_templates(self) -> None:
         '''empty db or anura schemas'''
 
-        resp = self.API.database_templates()
+        resp: Dict[str, Any] = self.API.database_templates()
         self.assertEqual(resp['result'], 'success')
+        self.assertEqual(len(resp.keys()), 3)
         self.assertIsInstance(resp['count'], int)
-        self.assertEqual(resp['count'], 15)
+        self.assertEqual(resp['count'], 14)
         self.assertIsInstance(resp['templates'], list)
         tcount: int = len(resp['templates'])
-        self.assertEqual(tcount, 15)
+        self.assertEqual(tcount, 14)
         self.assertEqual(tcount, resp['count'])
-        self.assertEqual(tcount, len(self.API.DATABASE_TEMPLATES))
-
-        TEMPLATE_KEYS: Tuple[str, ...] = ('id', 'name', 'schema', 'serverName')
         for t in resp['templates']:
             self.assertIsInstance(t, dict)
-            for k in TEMPLATE_KEYS:
-                self.assertIsInstance(t[k], str)
+            for k in t.keys():
+                self.assertIsInstance(k, str)
 
-        TEMPLATES: list[dict] = [
-            {
-                'id': 'empty',
-                'name': 'Empty Database',
-                'role': '',
-                'schema': '',
-                'serverName': 'default',
-            },
-            {
-                'id': 'anura_2_4_blast_off_to_space',
-                'name': 'Anura - Blast off to Space (2.4)',
-                'role': 'pioneer-team',
-                'schema': 'anura_2_4',
-                'serverName': 'default',
-            },
-            {
-                'id': 'anura_2_4_clean',
-                'name': 'Anura - New Model (2.4)',
-                'role': 'pioneer-team',
-                'schema': 'anura_2_4',
-                'serverName': 'default',
-            },
-            {
-                'id': 'anura_2_5_blast_off_to_space',
-                'name': 'Anura - Blast off to Space',
-                'role': '',
-                'schema': 'anura_2_5',
-                'serverName': 'default',
-            },
-            {
-                'id': 'anura_2_5_clean',
-                'name': 'Anura - New Model',
-                'role': '',
-                'schema': 'anura_2_5',
-                'serverName': 'default',
-            },
-            {
-                'id': '12d0e352-8f06-45aa-b3ba-58d6d93d3186',
-                'name': 'Blast Off To Space (BOTS)',
-                'role': 'hidden',
-                'schema': 'anura_2_5',
-                'serverName': 'default',
-            },
-            {
-                'id': '812d6fae-9fe7-4541-bee5-3cdb78e03eeb',
-                'name': 'China Exit Strategy in Asia',
-                'role': 'hidden',
-                'schema': 'anura_2_5',
-                'serverName': 'default',
-            },
-            {
-                'id': '67a881b3-b6ab-4e95-9452-6dae8e831a6b',
-                'name': 'Detailed Facility Selection',
-                'role': 'hidden',
-                'schema': 'anura_2_5',
-                'serverName': 'default',
-            },
-            {
-                "id": "d3cba0d2-d00a-11ed-b72f-9fd8ea75300c",
-                "name": "Fleet Size Optimization - EMEA Geo",
-                "role": "hidden",
-                "schema": "anura_2_5",
-                "serverName": "default",
-            },
-            {
-                "id": "b30d9156-d00b-11ed-b72f-9fd8ea75300c",
-                "name": "Fleet Size Optimization - US Geo",
-                "role": "hidden",
-                "schema": "anura_2_5",
-                "serverName": "default",
-            },
-            {
-                'id': '71214744-f90a-4dcc-8008-bb9dab9493be',
-                'name': 'Greenfield Facility Selection',
-                'role': 'hidden',
-                'schema': 'anura_2_5',
-                'serverName': 'default',
-            },
-            {
-                'id': 'd74fd9cc-e829-4ad3-9f33-f4ef0f3ddae1',
-                'name': 'Multi-Year Capacity Planning',
-                'role': 'hidden',
-                'schema': 'anura_2_5',
-                'serverName': 'default',
-            },
-            {
-                'id': 'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6',
-                'name': 'Out of China',
-                'role': 'hidden',
-                'schema': 'anura_2_5',
-                'serverName': 'default',
-            },
-            {
-                'id': '37392edb-d582-4ce8-9f75-024651aa8592',
-                'name': 'Tactical Capacity Optimization',
-                'role': 'hidden',
-                'schema': 'anura_2_5',
-                'serverName': 'default',
-            },
-            {
-                'id': 'anura_2_6_clean',
-                'name': 'Anura - New Model (2.6)',
-                'role': 'preview-schema',
-                'schema': 'anura_2_6',
-                'serverName': 'default',
-            },
-        ]
+    def test_database_templates_legacy(self) -> None:
+        '''legacy empty db or anura schemas'''
 
-        self.assertEqual(resp['templates'], TEMPLATES)
+        resp: Dict[str, Any] = self.API._database_templates_legacy()
+        self.assertEqual(resp['result'], 'success')
+        self.assertIsInstance(resp['count'], int)
+        self.assertEqual(resp['count'], 15)  # TODO OE-7816
+        self.assertIsInstance(resp['templates'], list)
+        tcount: int = len(resp['templates'])
+        self.assertEqual(tcount, 15)
+        self.assertEqual(tcount, resp['count'])
+        self.assertEqual(tcount, len(self.API.DATABASE_TEMPLATES) + 1)  # TODO OE-7816
 
-    def test_database_templates_by_name(self) -> None:
+        TEMPLATE_KEYS: Tuple[str, ...] = ('id', 'is_default', 'name', 'role', 'schema')
+        for t in resp['templates']:
+            self.assertIsInstance(t, dict)
+            for k in t.keys():
+                self.assertIsInstance(k, str)
+            keys = tuple(sorted(t.keys()))
+            # self.assertEqual(keys, TEMPLATE_KEYS) # TODO OE-7816
+            # if t['name'] in ('Empty Database', 'Blast Off To Space (BOTS)', 'Global Risk Analysis'):
+            #    continue  # OE-7918 legacy call changed
+            # self.assertTrue(t['id'] in self.API.DATABASE_TEMPLATES)
+
+    def test_database_templates_legacy_by_name(self) -> None:
         '''look up the database template id by case-insensitive template name'''
 
         template_names: List[str] = [
             'empty',
             'Anura - Blast off to Space',
             'Anura - New Model',
             'Anura - New Model (2.6)',
-            'Blast Off To Space (BOTS)',
             'China Exit Strategy in Asia',
             'Detailed Facility Selection',
             'Fleet Size Optimization - EMEA Geo',
             'Fleet Size Optimization - US Geo',
             'Greenfield Facility Selection',
             'Multi-Year Capacity Planning',
             'Out of China',
             'Tactical Capacity Optimization',
         ]
 
         for name in template_names:
-            tids = self.API._database_templates_by_name(name)
+            tids: List[str] = self.API._database_templates_legacy_by_name(name)
             self.assertEqual(len(tids), 1)
 
     def test_ip_address_allow(self) -> None:
         '''whitelist ip address'''
 
         self.database_ensure_exist()
-        db = self.API.account_storage_device(type='postgres_db')
-        resp = self.API.ip_address_allow(database_name=db['name'], ip='127.0.0.0')
+        db: Dict[str, Any] = self.API.account_storage_device(type='postgres_db')
+        resp: Dict[str, str] = self.API.ip_address_allow(database_name=db['name'], ip='127.0.0.0')
 
         self.assertIsInstance(resp['ip'], str)
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['ip'], '127.0.0.0')
         self.assertEqual(resp['result'], 'accepted')
         self.assertIn('five-minute delay', resp['message'])
 
     def test_ip_address_allow_invalid(self) -> None:
         '''unable to whitelist, ip address is invalid'''
 
         self.database_ensure_exist()
-        db = self.API.account_storage_device(type='postgres_db')
-        r = self.API.ip_address_allow(database_name=db['name'], ip='alpha.0.0.0')
+        db: Dict[str, Any] = self.API.account_storage_device(type='postgres_db')
+        r: Dict[str, Any] = self.API.ip_address_allow(database_name=db['name'], ip='alpha.0.0.0')
         resp: dict = r['resp'].json()
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['message'], 'ipAddress is missing or invalid')
+        self.assertEqual(resp['result'], 'error')
 
     def test_ip_address_allowed(self) -> None:
         '''ip address is whitelisted'''
 
         self.database_ensure_exist()
-        db = self.API.account_storage_device(type='postgres_db')
-        resp = self.API.ip_address_allowed(database_name=db['name'], ip='127.0.0.0')
+        db: Dict[str, Any] = self.API.account_storage_device(type='postgres_db')
+        resp: Dict[str, Any] = self.API.ip_address_allowed(database_name=db['name'], ip='127.0.0.0')
         self.assertIsInstance(resp['allowed'], bool)
         self.assertIsInstance(resp['ip'], str)
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['allowed'], True)
         self.assertEqual(resp['ip'], '127.0.0.0')
         self.assertEqual(resp['result'], 'success')
         self.assertIn('is in the firewall', resp['message'])
 
     def test_ip_address_allowed_invalid(self) -> None:
         '''ip address is invalid'''
 
         self.database_ensure_exist()
-        db = self.API.account_storage_device(type='postgres_db')
-        r = self.API.ip_address_allowed(database_name=db['name'], ip='alpha.0.0.0')
+        db: Dict[str, Any] = self.API.account_storage_device(type='postgres_db')
+        r: Dict[str, Any] = self.API.ip_address_allowed(database_name=db['name'], ip='alpha.0.0.0')
         resp: dict = r['resp'].json()
 
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['message'], 'ipAddress is missing or invalid')
 
+    def test_ip_address_is(self) -> None:
+        '''identify external IP4 address'''
+
+        ip: str = self.API.ip_address_is()
+        pat = r'^((25[0-5]|(2[0-4]|1\d|[1-9]|)\d)\.?\b){4}$'
+        valid_ip4 = bool(fullmatch(pat, ip))
+        self.assertTrue(valid_ip4)
+        ipq: str = self.API._ip_address_is_quick()
+        self.assertEqual(ip, ipq)
+
     def test_onedrive_push(self) -> None:
         '''push optilogic files to onedrive'''
 
         with self.assertRaises(NotImplementedError):
             self.API.onedrive_push('fakeFilePath')
 
         return  # OE-7039 API: OneDrive Push Broke
@@ -1214,26 +1139,49 @@
             for device in devices['storages']:
                 d: Dict[str, Any] = self.API.storage(device['name'])
                 self.assertEqual(d['result'], 'success')
                 if d['type'] == 'azure_afs':
                     self.assertEqual(len(d), 12)
                     self.storage_azure_afs(d)
                 if d['type'] == 'azure_workspace':
-                    self.assertEqual(len(d), 13)
+                    self.assertEqual(len(d), 17)
                     self.storage_azure_workspace(d)
                 elif d['type'] == 'onedrive':
-                    self.assertEqual(len(d), 20)
+                    self.assertEqual(len(d), 24)
                     self.storage_onedrive(d)
                     # connect is not in get devices call due to real time performance
                     self.assertIsInstance(d['connected'], bool)
                 elif d['type'] == 'postgres_db':
                     # storage item contains an additional result key
-                    self.assertEqual(len(d), 19)
+                    self.assertEqual(len(d), 23)
                     self.storage_database(d)
 
+    def test_storage_attr(self) -> None:
+        '''device attributes: annotations, label, and tag'''
+
+        attrs: Tuple[Literal['annotation'], Literal['label'], Literal['tag']] = (
+            'annotation',
+            'label',
+            'tag',
+        )
+        devices: Dict[str, Any] = self.API.account_storage_devices()
+        self.assertEqual(devices['result'], 'success')
+        with self.subTest():
+            for d in devices['storages']:
+                for a in attrs:
+                    resp: Dict[str, Any] = self.API._storage_attr(d['name'], a)
+                    self.assertEqual(resp['result'], 'success')
+                    self.assertEqual(len(resp.keys()), 2)
+                    a += 's'  #  BUG PR-924 route vs attribute is not the same
+                    self.assertTrue(a in resp.keys())
+                    if a == 'tags':
+                        self.assertIsInstance(resp[a], str)
+                    else:
+                        self.assertIsInstance(resp[a], dict)
+
     @unittest.skip('api has not implemented')
     def test_storage_disk_create(self):
         '''create a new file storage device'''
 
         raise NotImplementedError
 
     @unittest.skip('api is incomplete')
@@ -1252,14 +1200,31 @@
         )
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['rowCount'], int)
         self.assertGreaterEqual(resp['rowCount'], 1)
         self.assertIsInstance(resp['queryResults'], list)
         self.assertGreaterEqual(len(resp['queryResults']), 1)
 
+    def test_util_env(self) -> None:
+        '''atlas and andromeda common environment variables'''
+
+        keys: Tuple[str, ...] = (
+            'job_cmd',
+            'job_dir',
+            'job_key',
+            'job_api',
+            'job_img',
+            'pip_ver',
+            'py_ver',
+        )
+        d = self.API.util_environment()
+        for k in d.keys():
+            self.assertTrue(k in keys)
+            self.assertIsInstance(d[k], str)
+
     def test_util_job_monitor_bad(self) -> None:
         '''job monitor check invalid job key or job state'''
 
         # invalid job state
         with self.assertRaises(ValueError):
             self.API.util_job_monitor(self.WKSP, '5633e372-337a-454c-aae4-10084ea5bac6', 'invalid')  # type: ignore
         # invalid job key
@@ -1600,33 +1565,33 @@
         resp = self.API.wksp_job_metrics(self.WKSP, self.__jobkey_quick)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
         self.assertGreaterEqual(resp['count'], 1)
         self.assertIsInstance(resp['max'], dict)
         self.assertEqual(len(resp['max']), 7)
         self.assertIsInstance(resp['max']['memoryPercent'], float)
-        self.assertIsInstance(resp['max']['memoryResident'], float)
+        self.assertIsInstance(resp['max']['memoryResident'], Number)
         self.assertIsInstance(resp['max']['memoryAvailable'], int)
         self.assertIsInstance(resp['max']['cpuPercent'], float)
         self.assertIsInstance(resp['max']['cpuUsed'], float)
-        self.assertIsInstance(resp['max']['cpuAvailable'], int)
+        self.assertIsInstance(resp['max']['cpuAvailable'], Number)
         self.assertIsInstance(resp['max']['processCount'], int)
         self.assertIsInstance(resp['records'], list)
         self.assertGreaterEqual(len(resp['records']), 1)
         self.assertIsInstance(resp['records'][0]['timestamp'], int)
         self.assertIsInstance(resp['records'][0]['datetime'], str)
         self.assertTrue(resp['records'][0]['datetime'].endswith('Z'))
         dt: datetime = parse(resp['records'][0]['datetime'])
         self.assertTrue(dt.tzname(), 'UTC')
         now: datetime = datetime.utcnow()
         self.assertEqual(dt.year, now.year)
         self.assertEqual(dt.month, now.month)
         self.assertEqual(dt.day, now.day)
         self.assertIsInstance(resp['records'][0], dict)
-        self.assertIsInstance(resp['records'][0]['cpuAvailable'], int)
+        self.assertIsInstance(resp['records'][0]['cpuAvailable'], Number)
         self.assertIsInstance(resp['records'][0]['cpuPercent'], float)
         self.assertIsInstance(resp['records'][0]['cpuUsed'], float)
         self.assertIsInstance(resp['records'][0]['memoryAvailable'], int)
         self.assertIsInstance(resp['records'][0]['memoryPercent'], float)
         self.assertIsInstance(resp['records'][0]['memoryResident'], float)
         self.assertIsInstance(resp['records'][0]['processCount'], int)
 
@@ -1639,115 +1604,165 @@
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['max'], dict)
         self.assertEqual(len(resp['max']), 7)
         self.assertIsInstance(resp['max']['memoryPercent'], float)
         self.assertIsInstance(resp['max']['memoryResident'], float)
         self.assertIsInstance(resp['max']['memoryAvailable'], int)
         self.assertIsInstance(resp['max']['cpuPercent'], float)
-        self.assertIsInstance(resp['max']['cpuUsed'], float)
-        self.assertIsInstance(resp['max']['cpuAvailable'], int)
+        self.assertIsInstance(resp['max']['cpuUsed'], Number)
+        self.assertIsInstance(resp['max']['cpuAvailable'], Number)
         self.assertIsInstance(resp['max']['processCount'], int)
 
     def test_wksp_job_metrics_mia(self) -> None:
         '''
         OE-5276 Job Metrics are Missing Sometimes
         OE-5369 Job Metrics Missing v3
         '''
 
         # spin up a few jobs to create load and evaluate all
         jobs_max: int = 9
         tag_time: float = time.time()
         tag: str = f'metrics_{tag_time}'
-        secs: int = 15
+        secs: int = 20
+        d: dict = {}  # store job key and elapsed time without metrics
         for job in range(jobs_max):
-            self.API.wksp_job_start(self.WKSP, self.py_run_me, tags=tag, timeout=secs)
+            resp = self.API.wksp_job_start(
+                self.WKSP, self.py_run_me, tags=tag, timeout=secs, resourceConfig='mini'
+            )
+            if resp.get('crash'):
+                print(resp)
+                jobs_max -= 1
+                continue  
+            d[resp['jobKey']] = {'seen_first': None, 'missing_last': None}
 
         # check the jobs that are about to run
-        d: dict = {}
+        metrics_missing = False
         check: bool = True
         while check:
-            jobs = self.API.wksp_jobs(self.WKSP, tags=tag)
-
-            # jobs all finished?
+            # stop if all jobs finished
+            jobs: Dict[str, Any] = self.API.wksp_jobs(self.WKSP, tags=tag)
             terminal: int = 0
             for t in self.API.JOBSTATES_TERMINAL:
                 terminal += jobs['statusCounts'].get(t)
-
             if terminal == jobs_max:
                 check = False
                 break
 
             # check running jobs for metrics
-            active = self.API.wksp_jobs(self.WKSP, status='running', tags=tag)
+            active: Dict[str, Any] = self.API.wksp_jobs(self.WKSP, status='running', tags=tag)
+            for job in active['jobs']:
+                # elapsed run time
+                st: str = str(job['startDatetime'])
+                st = st.replace('T', ' ')
+                st = st.replace('Z', '')
+                job_start: datetime = datetime.fromisoformat(st)
+                now: datetime = datetime.utcnow()
+                delta: timedelta = now - job_start
+
+                # first time observed the job was running
+                if d[job['jobKey']].get('seen_first') is None:
+                    d[job['jobKey']]['seen_first'] = str(delta)
 
-            if active['statusCounts']['running'] >= 1:
-                for job in active['jobs']:
-                    resp = self.API.wksp_job_metrics(self.WKSP, job['jobKey'])
-                    self.assertEqual(resp['result'], 'success')
-                    self.assertIsInstance(resp['count'], int)
-                    # self.assertGreaterEqual(resp['count'], 1)
-
-                    # missing metrics!
-                    if resp['count'] == 0:
-                        st: str = str(job['startDatetime'])
-                        st = st.replace('T', ' ')
-                        st = st.replace('Z', '')
-                        job_start: datetime = datetime.fromisoformat(st)
-                        now: datetime = datetime.utcnow()
-                        delta: timedelta = now - job_start
-                        d[job['jobKey']] = str(
-                            delta
-                        )  # store job key and elapsed time without metrics
-
-                        print(
-                            f"{str(delta)} secs elapsed and metrics missing for job {job['jobKey']}"
-                        )
-                        with self.subTest():
-                            self.assertLess(delta.total_seconds(), 5)
+                # check for metrics
+                resp: Dict[str, Any] = self.API.wksp_job_metrics(self.WKSP, job['jobKey'])
+                self.assertEqual(resp['result'], 'success')
+                self.assertIsInstance(resp['count'], int)
 
-            time.sleep(1)
+                # missing metrics?
+                if resp['count'] == 0:
+                    metrics_missing = True
+                    d[job['jobKey']]['missing_last'] = str(delta)
+                    print(f"{str(delta)} secs elapsed, metrics missing for job {job['jobKey']}")
+                    with self.subTest():
+                        self.assertLess(delta.total_seconds(), 10)
 
-        # were there any jobs that failed metric check?
-        if len(d) >= 1:
+        if metrics_missing:
+            print('\n\nTEST_WKSP_JOB_METRICS_MIA')
             print(f"\n\nJob Submitted: {jobs_max}, Job Duration: {secs}, Job Tag: {tag}")
-            print('\n JobKey, LastSeenWithMissingMetricCount_RunDuration')
-            for k in d.items():
-                print(k[1], k[0])
+            for item in d.items():
+                print(item)
 
     def test_wksp_job_start(self) -> None:
         '''creating a job'''
 
-        resp = self.API.wksp_job_start(self.WKSP, file_path=self.py_run_me, tags='unittest_start')
+        resp = self.API.wksp_job_start(
+            self.WKSP, file_path=self.py_run_me, tags='unittest_start', resourceConfig='mini'
+        )
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp['jobKey']), 36)
         job_info_keys: Tuple[str, ...] = (
             'workspace',
             'directoryPath',
             'filename',
             'command',
             'resourceConfig',
             'tags',
             'timeout',
         )
         for key in resp['jobInfo'].keys():
             self.assertIn(key, job_info_keys)
 
+    def test_wksp_job_start_preview(self) -> None:
+        '''create a job using andromeda preview image and compare to stable'''
+
+        job_beta: Dict[str, Any] = self.API.wksp_job_start(
+            self.WKSP,
+            file_path=self.py_run_me_bash,
+            commandArgs="'pip list -v'",
+            resourceConfig='mini',
+            tags='unittest',
+            preview_image=True,
+        )
+        self.assertEqual(job_beta['result'], 'success')
+
+        job: Dict[str, Any] = self.API.wksp_job_start(
+            self.WKSP,
+            file_path=self.py_run_me_bash,
+            commandArgs="'pip list -v'",
+            resourceConfig='mini',
+            tags='unittest',
+            preview_image=False,
+        )
+        self.assertEqual(job['result'], 'success')
+
+        done = self.API.util_job_monitor(self.WKSP, job_key=job_beta['jobKey'], stop_when='done')
+        self.assertTrue(done)
+        std_out: str = self.API.wksp_job_file_result(self.WKSP, job_beta['jobKey'])
+        self.assertIsInstance(std_out, str)
+        self.assertGreater(len(std_out), 100)
+        m = search(r'(?P<pkg>neo)\s+(?P<ver>[\w\.]+)', std_out)
+        version_preview: str = m.groupdict()['ver'] if m else ''
+        self.assertRegex(version_preview, r'2\.[6-9]\.\d+')
+
+        done = self.API.util_job_monitor(self.WKSP, job_key=job['jobKey'], stop_when='done')
+        self.assertTrue(done)
+        std_out: str = self.API.wksp_job_file_result(self.WKSP, job['jobKey'])
+        self.assertIsInstance(std_out, str)
+        self.assertGreater(len(std_out), 100)
+        m = search(r'(?P<pkg>neo)\s+(?P<ver>[\w\.]+)', std_out)
+        version: str = m.groupdict()['ver'] if m else ''
+        self.assertRegex(version, r'2\.5\.\d+')
+
+        self.assertNotEqual(version, version_preview)
+
     def test_wksp_job_start_sample(self) -> None:
         '''create job api call reponse time is the slowest and fails often withg 504s'''
 
         max: int = int(self.API.account_info()['limits']['concurrentJobs'] * 0.5)
         min: int = 10
         job_count: int = max if max > min else min
         jobs: List[str] = []
         tag: str = 'unittest_job_speed'
 
         # start jobs
         for j in range(job_count):
             with self.subTest():
-                resp = self.API.wksp_job_start(self.WKSP, self.py_run_me_quick, tags=tag)
+                resp = self.API.wksp_job_start(
+                    self.WKSP, self.py_run_me_quick, tags=tag, resourceConfig='mini'
+                )
                 self.assertEqual(resp['result'], 'success')
             # d = {}
             # d['key'] = resp['jobKey']
             # jobs.append(d)
             # spin up a few jobs to create load and evaluate all
 
         return
@@ -1853,15 +1868,15 @@
 
     def test_wksp_job_stop(self) -> None:
         '''stop a most recently created job'''
 
         # guarantee a job is currently running
         resp = self.API.wksp_job_status(self.WKSP, self.API._job_start_recent_key)
         if resp['status'] in self.API.JOBSTATES_TERMINAL:
-            resp = self.API.wksp_job_start(self.WKSP, self.py_run_me)
+            resp = self.API.wksp_job_start(self.WKSP, self.py_run_me, resourceConfig='mini')
             success: bool = self.API.util_job_monitor(self.WKSP, resp['jobKey'])
             if success is False:
                 self.skipTest('failed to start job within two minutes')
 
         # stop running job
         resp = self.API.wksp_job_stop(self.WKSP, self.API._job_start_recent_key)
         self.assertEqual(resp['result'], 'success')
```

## Comparing `optilogic-2.4.1.dist-info/LICENSE` & `optilogic-2.5.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `optilogic-2.4.1.dist-info/METADATA` & `optilogic-2.5.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: optilogic
-Version: 2.4.1
+Version: 2.5.0
 Summary: Tools for interfacing with Optilogic Jobs and APIs
 Home-page: https://optilogic.com
 Author: Optilogic
 Author-email: support@optilogic.com
 License: MIT
 Project-URL: Documentation, https://api-docs.optilogic.app/documentation
 Keywords: mip,mixed integer programming,network optimization,optimization,risk,simulation,supply chain design
```

## Comparing `optilogic-2.4.1.dist-info/RECORD` & `optilogic-2.5.0.dist-info/RECORD`

 * *Files 19% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 optilogic/__init__.py,sha256=t3hXtM_MyajzEyk-V1xxM1ZzFZCZtADNjJvvShxdl4A,22
 optilogic/pioneer/__init__.py,sha256=ELVJEdWM5Ia28-LjwVes2qvd7nGkORgVOw0g21vsGPU,48
 optilogic/pioneer/api/__init__.py,sha256=kaCK0y1h6dN5NhQnehgpJCEwyihVDPuuQ7xgtexVric,20
-optilogic/pioneer/api/api.py,sha256=FywVf9D587XpUefm0iN5h7pXBAdi0a7fgZs2eYgxBho,64397
-optilogic/pioneer/api/api_tests.py,sha256=achleDhh_nM2Iffed1C9FZ1OyDwzIxT5kcQW-XHPKsM,95328
+optilogic/pioneer/api/api.py,sha256=LWCVnQP_lsPR_i5MiIPtYiqDrIcy-Ez3BJ9H8BwIlEA,72496
+optilogic/pioneer/api/api_tests.py,sha256=cl_GE9gAgv8Yg2UgWMgxZEHfNCGiZHuHYiMA_lDIfxI,96477
 optilogic/pioneer/api/quick_tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 optilogic/pioneer/api/quick_tests/airline_hub_location_cbc.py,sha256=S6nxwIvWFqBo1tsFRi0D9VBD8ohBDhb42aBzXkKNgw4,5111
 optilogic/pioneer/api/quick_tests/bash.py,sha256=WPnbdoPVGxOwdMlsk-IyxVWeqXae5DbdN2bpMzNgVCQ,317
 optilogic/pioneer/api/quick_tests/quick.py,sha256=Yf_jhZbx5Nx5W3Dc_iJz8t70KvMDBn7xVTMmNvBn5gM,209
 optilogic/pioneer/api/quick_tests/sleep.py,sha256=CSb8T0inq6NAlDqW321vFCBmldPIiiGWfAU98OwUmV4,577
 optilogic/pioneer/job_utils/__init__.py,sha256=ewS513f69vgJB7L8QZIYtwl6umBFDtOqM52Ms0pAzy4,131
 optilogic/pioneer/job_utils/job_utils.py,sha256=nBeFlkmqXjiNQub-i962qk64c-M2QCMjQvLLv5fuv6U,3553
-optilogic-2.4.1.dist-info/LICENSE,sha256=BfGwbupGKO-1uV11X7aBMgE3LCjo5N7OxqZpMFhP7ls,1070
-optilogic-2.4.1.dist-info/METADATA,sha256=nmN21pq0lFwxpNSKq9tUbPqHCrl3COCqXgTqTqlgL3g,759
-optilogic-2.4.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-optilogic-2.4.1.dist-info/top_level.txt,sha256=A59vwR2Gu9fxXluO4gG-COTlfSKCJW910611UXOwnuo,10
-optilogic-2.4.1.dist-info/RECORD,,
+optilogic-2.5.0.dist-info/LICENSE,sha256=BfGwbupGKO-1uV11X7aBMgE3LCjo5N7OxqZpMFhP7ls,1070
+optilogic-2.5.0.dist-info/METADATA,sha256=0-vMNZbFP8ryF_r9x7fDKMtO5tlhB0mT9lAXjBs7m7U,759
+optilogic-2.5.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+optilogic-2.5.0.dist-info/top_level.txt,sha256=A59vwR2Gu9fxXluO4gG-COTlfSKCJW910611UXOwnuo,10
+optilogic-2.5.0.dist-info/RECORD,,
```

