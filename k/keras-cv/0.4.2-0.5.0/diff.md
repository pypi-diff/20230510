# Comparing `tmp/keras_cv-0.4.2-py3-none-any.whl.zip` & `tmp/keras_cv-0.5.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,354 +1,402 @@
-Zip file size: 634861 bytes, number of entries: 352
--rw-r--r--  2.0 unx     1113 b- defN 23-Feb-04 00:29 keras_cv/__init__.py
--rw-r--r--  2.0 unx     1144 b- defN 23-Feb-04 00:29 keras_cv/version_check.py
--rw-r--r--  2.0 unx     1353 b- defN 23-Feb-04 00:29 keras_cv/version_check_test.py
--rw-r--r--  2.0 unx     1540 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/__init__.py
--rw-r--r--  2.0 unx    18119 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/converters.py
--rw-r--r--  2.0 unx     6148 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/converters_test.py
--rw-r--r--  2.0 unx     4078 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/formats.py
--rw-r--r--  2.0 unx     6124 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/iou.py
--rw-r--r--  2.0 unx     5814 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/iou_test.py
--rw-r--r--  2.0 unx     3490 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/mask_invalid_detections.py
--rw-r--r--  2.0 unx     2906 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/mask_invalid_detections_test.py
--rw-r--r--  2.0 unx     2866 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/to_dense.py
--rw-r--r--  2.0 unx     1147 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/to_dense_test.py
--rw-r--r--  2.0 unx     2586 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/to_ragged.py
--rw-r--r--  2.0 unx     2081 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/to_ragged_test.py
--rw-r--r--  2.0 unx     7061 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/utils.py
--rw-r--r--  2.0 unx     5509 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/utils_test.py
--rw-r--r--  2.0 unx     3411 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/validate_format.py
--rw-r--r--  2.0 unx     1568 b- defN 23-Feb-04 00:29 keras_cv/bounding_box/validate_format_test.py
--rw-r--r--  2.0 unx      652 b- defN 23-Feb-04 00:29 keras_cv/bounding_box_3d/__init__.py
--rw-r--r--  2.0 unx     1605 b- defN 23-Feb-04 00:29 keras_cv/bounding_box_3d/formats.py
--rw-r--r--  2.0 unx     1016 b- defN 23-Feb-04 00:29 keras_cv/callbacks/__init__.py
--rw-r--r--  2.0 unx     4571 b- defN 23-Feb-04 00:29 keras_cv/callbacks/pycoco_callback.py
--rw-r--r--  2.0 unx     3186 b- defN 23-Feb-04 00:29 keras_cv/callbacks/pycoco_callback_test.py
--rw-r--r--  2.0 unx     4831 b- defN 23-Feb-04 00:29 keras_cv/callbacks/waymo_evaluation_callback.py
--rw-r--r--  2.0 unx     2645 b- defN 23-Feb-04 00:29 keras_cv/callbacks/waymo_evaluation_callback_test.py
--rw-r--r--  2.0 unx      909 b- defN 23-Feb-04 00:29 keras_cv/core/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/core/factor_sampler/__init__.py
--rw-r--r--  2.0 unx     1555 b- defN 23-Feb-04 00:29 keras_cv/core/factor_sampler/constant_factor_sampler.py
--rw-r--r--  2.0 unx      964 b- defN 23-Feb-04 00:29 keras_cv/core/factor_sampler/constant_factor_sampler_test.py
--rw-r--r--  2.0 unx     1335 b- defN 23-Feb-04 00:29 keras_cv/core/factor_sampler/factor_sampler.py
--rw-r--r--  2.0 unx     2393 b- defN 23-Feb-04 00:29 keras_cv/core/factor_sampler/normal_factor_sampler.py
--rw-r--r--  2.0 unx     1076 b- defN 23-Feb-04 00:29 keras_cv/core/factor_sampler/normal_factor_sampler_test_.py
--rw-r--r--  2.0 unx     2014 b- defN 23-Feb-04 00:29 keras_cv/core/factor_sampler/uniform_factor_sampler.py
--rw-r--r--  2.0 unx     1026 b- defN 23-Feb-04 00:29 keras_cv/core/factor_sampler/uniform_factor_sampler_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/custom_ops/__init__.py
--rw-r--r--  2.0 unx      625 b- defN 23-Feb-04 00:29 keras_cv/datasets/__init__.py
--rw-r--r--  2.0 unx      633 b- defN 23-Feb-04 00:29 keras_cv/datasets/imagenet/__init__.py
--rw-r--r--  2.0 unx     4391 b- defN 23-Feb-04 00:29 keras_cv/datasets/imagenet/load.py
--rw-r--r--  2.0 unx      635 b- defN 23-Feb-04 00:29 keras_cv/datasets/pascal_voc/__init__.py
--rw-r--r--  2.0 unx     3702 b- defN 23-Feb-04 00:29 keras_cv/datasets/pascal_voc/load.py
--rw-r--r--  2.0 unx    18489 b- defN 23-Feb-04 00:29 keras_cv/datasets/pascal_voc/segmentation.py
--rw-r--r--  2.0 unx    11789 b- defN 23-Feb-04 00:29 keras_cv/datasets/pascal_voc/segmentation_test.py
--rw-r--r--  2.0 unx      926 b- defN 23-Feb-04 00:29 keras_cv/datasets/waymo/__init__.py
--rw-r--r--  2.0 unx     2767 b- defN 23-Feb-04 00:29 keras_cv/datasets/waymo/load.py
--rw-r--r--  2.0 unx     2098 b- defN 23-Feb-04 00:29 keras_cv/datasets/waymo/load_test.py
--rw-r--r--  2.0 unx     1980 b- defN 23-Feb-04 00:29 keras_cv/datasets/waymo/struct.py
--rw-r--r--  2.0 unx    25514 b- defN 23-Feb-04 00:29 keras_cv/datasets/waymo/transformer.py
--rw-r--r--  2.0 unx     5163 b- defN 23-Feb-04 00:29 keras_cv/datasets/waymo/transformer_test.py
--rw-r--r--  2.0 unx      783 b- defN 23-Feb-04 00:29 keras_cv/keypoint/__init__.py
--rw-r--r--  2.0 unx     6945 b- defN 23-Feb-04 00:29 keras_cv/keypoint/converters.py
--rw-r--r--  2.0 unx     5121 b- defN 23-Feb-04 00:29 keras_cv/keypoint/converters_test.py
--rw-r--r--  2.0 unx     1726 b- defN 23-Feb-04 00:29 keras_cv/keypoint/formats.py
--rw-r--r--  2.0 unx     1597 b- defN 23-Feb-04 00:29 keras_cv/keypoint/utils.py
--rw-r--r--  2.0 unx     1940 b- defN 23-Feb-04 00:29 keras_cv/keypoint/utils_test.py
--rw-r--r--  2.0 unx     5897 b- defN 23-Feb-04 00:29 keras_cv/layers/__init__.py
--rw-r--r--  2.0 unx     8720 b- defN 23-Feb-04 00:29 keras_cv/layers/feature_pyramid.py
--rw-r--r--  2.0 unx     4880 b- defN 23-Feb-04 00:29 keras_cv/layers/feature_pyramid_test.py
--rw-r--r--  2.0 unx     7953 b- defN 23-Feb-04 00:29 keras_cv/layers/fusedmbconv.py
--rw-r--r--  2.0 unx     2214 b- defN 23-Feb-04 00:29 keras_cv/layers/fusedmbconv_test.py
--rw-r--r--  2.0 unx     8165 b- defN 23-Feb-04 00:29 keras_cv/layers/mbconv.py
--rw-r--r--  2.0 unx     2179 b- defN 23-Feb-04 00:29 keras_cv/layers/mbconv_test.py
--rw-r--r--  2.0 unx    12304 b- defN 23-Feb-04 00:29 keras_cv/layers/serialization_test.py
--rw-r--r--  2.0 unx     6304 b- defN 23-Feb-04 00:29 keras_cv/layers/spatial_pyramid.py
--rw-r--r--  2.0 unx     1264 b- defN 23-Feb-04 00:29 keras_cv/layers/spatial_pyramid_test.py
--rw-r--r--  2.0 unx     4977 b- defN 23-Feb-04 00:29 keras_cv/layers/transformer_encoder.py
--rw-r--r--  2.0 unx     2105 b- defN 23-Feb-04 00:29 keras_cv/layers/transformer_encoder_test.py
--rw-r--r--  2.0 unx     7570 b- defN 23-Feb-04 00:29 keras_cv/layers/vit_layers.py
--rw-r--r--  2.0 unx     2793 b- defN 23-Feb-04 00:29 keras_cv/layers/vit_layers_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/__init__.py
--rw-r--r--  2.0 unx    11151 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/anchor_generator.py
--rw-r--r--  2.0 unx     6318 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/anchor_generator_test.py
--rw-r--r--  2.0 unx    11167 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/box_matcher.py
--rw-r--r--  2.0 unx     4873 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/box_matcher_test.py
--rw-r--r--  2.0 unx     4839 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/multi_class_non_max_suppression.py
--rw-r--r--  2.0 unx     2636 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py
--rw-r--r--  2.0 unx     9481 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/retina_net_label_encoder.py
--rw-r--r--  2.0 unx     4500 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/retina_net_label_encoder_test.py
--rw-r--r--  2.0 unx    15263 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/roi_align.py
--rw-r--r--  2.0 unx     9674 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/roi_generator.py
--rw-r--r--  2.0 unx     8183 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/roi_generator_test.py
--rw-r--r--  2.0 unx     6170 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/roi_pool.py
--rw-r--r--  2.0 unx     9223 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/roi_pool_test.py
--rw-r--r--  2.0 unx     8870 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/roi_sampler.py
--rw-r--r--  2.0 unx    11080 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/roi_sampler_test.py
--rw-r--r--  2.0 unx     8992 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/rpn_label_encoder.py
--rw-r--r--  2.0 unx     5473 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/rpn_label_encoder_test.py
--rw-r--r--  2.0 unx     3391 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/sampling.py
--rw-r--r--  2.0 unx     5894 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection/sampling_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection_3d/__init__.py
--rw-r--r--  2.0 unx    15985 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection_3d/center_net_label_encoder.py
--rw-r--r--  2.0 unx     4237 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection_3d/center_net_label_encoder_test.py
--rw-r--r--  2.0 unx     7983 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection_3d/heatmap_decoder.py
--rw-r--r--  2.0 unx     9897 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection_3d/voxel_utils.py
--rw-r--r--  2.0 unx     2788 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection_3d/voxel_utils_test.py
--rw-r--r--  2.0 unx     8972 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection_3d/voxelization.py
--rw-r--r--  2.0 unx     4017 b- defN 23-Feb-04 00:29 keras_cv/layers/object_detection_3d/voxelization_test.py
--rw-r--r--  2.0 unx     3724 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/__init__.py
--rw-r--r--  2.0 unx    12250 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/aug_mix.py
--rw-r--r--  2.0 unx     2205 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/aug_mix_test.py
--rw-r--r--  2.0 unx     1286 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/augmenter.py
--rw-r--r--  2.0 unx     1903 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/augmenter_test.py
--rw-r--r--  2.0 unx     3070 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/auto_contrast.py
--rw-r--r--  2.0 unx     3318 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/auto_contrast_test.py
--rw-r--r--  2.0 unx    20102 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/base_image_augmentation_layer.py
--rw-r--r--  2.0 unx    10713 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py
--rw-r--r--  2.0 unx     3507 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/channel_shuffle.py
--rw-r--r--  2.0 unx     2995 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/channel_shuffle_test.py
--rw-r--r--  2.0 unx     5935 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/cut_mix.py
--rw-r--r--  2.0 unx     4980 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/cut_mix_test.py
--rw-r--r--  2.0 unx     4884 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/equalization.py
--rw-r--r--  2.0 unx     2422 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/equalization_test.py
--rw-r--r--  2.0 unx     7759 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/fourier_mix.py
--rw-r--r--  2.0 unx     3387 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/fourier_mix_test.py
--rw-r--r--  2.0 unx     3874 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/grayscale.py
--rw-r--r--  2.0 unx     2852 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/grayscale_test.py
--rw-r--r--  2.0 unx     9660 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/grid_mask.py
--rw-r--r--  2.0 unx     3771 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/grid_mask_test.py
--rw-r--r--  2.0 unx     9419 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/jittered_resize.py
--rw-r--r--  2.0 unx     6511 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/jittered_resize_test.py
--rw-r--r--  2.0 unx     4970 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/maybe_apply.py
--rw-r--r--  2.0 unx     4607 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/maybe_apply_test.py
--rw-r--r--  2.0 unx     5974 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/mix_up.py
--rw-r--r--  2.0 unx     4436 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/mix_up_test.py
--rw-r--r--  2.0 unx    12955 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/mosaic.py
--rw-r--r--  2.0 unx     3581 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/mosaic_test.py
--rw-r--r--  2.0 unx     4171 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/posterization.py
--rw-r--r--  2.0 unx     3746 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/posterization_test.py
--rw-r--r--  2.0 unx     4709 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/ragged_image_test.py
--rw-r--r--  2.0 unx    10563 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/rand_augment.py
--rw-r--r--  2.0 unx     3544 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/rand_augment_test.py
--rw-r--r--  2.0 unx     4567 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_aspect_ratio.py
--rw-r--r--  2.0 unx     2620 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_aspect_ratio_test.py
--rw-r--r--  2.0 unx     4391 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_augmentation_pipeline.py
--rw-r--r--  2.0 unx     3224 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py
--rw-r--r--  2.0 unx     4516 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_brightness.py
--rw-r--r--  2.0 unx     3205 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_brightness_test.py
--rw-r--r--  2.0 unx     4353 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_channel_shift.py
--rw-r--r--  2.0 unx     4133 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_channel_shift_test.py
--rw-r--r--  2.0 unx     3852 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_choice.py
--rw-r--r--  2.0 unx     2489 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_choice_test.py
--rw-r--r--  2.0 unx     3110 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_color_degeneration.py
--rw-r--r--  2.0 unx     2638 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_color_degeneration_test.py
--rw-r--r--  2.0 unx     6568 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_color_jitter.py
--rw-r--r--  2.0 unx     4302 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_color_jitter_test.py
--rw-r--r--  2.0 unx     3737 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_contrast.py
--rw-r--r--  2.0 unx     1957 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_contrast_test.py
--rw-r--r--  2.0 unx     7260 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_crop.py
--rw-r--r--  2.0 unx    11232 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_crop_and_resize.py
--rw-r--r--  2.0 unx    10527 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_crop_and_resize_test.py
--rw-r--r--  2.0 unx     6382 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_crop_test.py
--rw-r--r--  2.0 unx     7007 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_cutout.py
--rw-r--r--  2.0 unx     4904 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_cutout_test.py
--rw-r--r--  2.0 unx     7325 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_flip.py
--rw-r--r--  2.0 unx     9249 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_flip_test.py
--rw-r--r--  2.0 unx     4490 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_gaussian_blur.py
--rw-r--r--  2.0 unx     3157 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_gaussian_blur_test.py
--rw-r--r--  2.0 unx     4142 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_hue.py
--rw-r--r--  2.0 unx     4023 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_hue_test.py
--rw-r--r--  2.0 unx     2922 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_jpeg_quality.py
--rw-r--r--  2.0 unx     1984 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_jpeg_quality_test.py
--rw-r--r--  2.0 unx    11761 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_rotation.py
--rw-r--r--  2.0 unx     7136 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_rotation_test.py
--rw-r--r--  2.0 unx     3867 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_saturation.py
--rw-r--r--  2.0 unx     3704 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_saturation_test.py
--rw-r--r--  2.0 unx     5286 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_sharpness.py
--rw-r--r--  2.0 unx     2348 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_sharpness_test.py
--rw-r--r--  2.0 unx    12056 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_shear.py
--rw-r--r--  2.0 unx     7941 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_shear_test.py
--rw-r--r--  2.0 unx    10215 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_translation.py
--rw-r--r--  2.0 unx     8662 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_translation_test.py
--rw-r--r--  2.0 unx     9600 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_zoom.py
--rw-r--r--  2.0 unx     5766 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/random_zoom_test.py
--rw-r--r--  2.0 unx     9113 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/randomly_zoomed_crop.py
--rw-r--r--  2.0 unx     5191 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/randomly_zoomed_crop_test.py
--rw-r--r--  2.0 unx     4287 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/repeated_augmentation.py
--rw-r--r--  2.0 unx     1753 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/repeated_augmentation_test.py
--rw-r--r--  2.0 unx     2777 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/rescaling.py
--rw-r--r--  2.0 unx     2123 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/rescaling_test.py
--rw-r--r--  2.0 unx    12849 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/resizing.py
--rw-r--r--  2.0 unx    11721 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/resizing_test.py
--rw-r--r--  2.0 unx     5189 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/solarization.py
--rw-r--r--  2.0 unx     2718 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/solarization_test.py
--rw-r--r--  2.0 unx     4703 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/with_labels_test.py
--rw-r--r--  2.0 unx     5229 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/with_mixed_precision_test.py
--rw-r--r--  2.0 unx     4521 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing/with_segmentation_masks_test.py
--rw-r--r--  2.0 unx     1737 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/__init__.py
--rw-r--r--  2.0 unx     8050 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py
--rw-r--r--  2.0 unx     4761 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py
--rw-r--r--  2.0 unx     5403 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/frustum_random_dropping_points.py
--rw-r--r--  2.0 unx     3597 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/frustum_random_dropping_points_test.py
--rw-r--r--  2.0 unx     5852 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise.py
--rw-r--r--  2.0 unx     6926 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise_test.py
--rw-r--r--  2.0 unx     3080 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_dropping_points.py
--rw-r--r--  2.0 unx     3494 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_dropping_points_test.py
--rw-r--r--  2.0 unx     4541 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_flip.py
--rw-r--r--  2.0 unx     3187 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_flip_test.py
--rw-r--r--  2.0 unx     5514 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_rotation.py
--rw-r--r--  2.0 unx     3049 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_rotation_test.py
--rw-r--r--  2.0 unx     6902 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_scaling.py
--rw-r--r--  2.0 unx     4579 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_scaling_test.py
--rw-r--r--  2.0 unx     4610 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_translation.py
--rw-r--r--  2.0 unx     2847 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/global_random_translation_test.py
--rw-r--r--  2.0 unx    11522 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes.py
--rw-r--r--  2.0 unx     8013 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes_test.py
--rw-r--r--  2.0 unx    12574 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/random_copy_paste.py
--rw-r--r--  2.0 unx     8288 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/random_copy_paste_test.py
--rw-r--r--  2.0 unx     5476 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/random_drop_box.py
--rw-r--r--  2.0 unx    12278 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/random_drop_box_test.py
--rw-r--r--  2.0 unx     7282 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/swap_background.py
--rw-r--r--  2.0 unx    10580 b- defN 23-Feb-04 00:29 keras_cv/layers/preprocessing_3d/swap_background_test.py
--rw-r--r--  2.0 unx      868 b- defN 23-Feb-04 00:29 keras_cv/layers/regularization/__init__.py
--rw-r--r--  2.0 unx     2545 b- defN 23-Feb-04 00:29 keras_cv/layers/regularization/drop_path.py
--rw-r--r--  2.0 unx     2460 b- defN 23-Feb-04 00:29 keras_cv/layers/regularization/drop_path_test.py
--rw-r--r--  2.0 unx     8594 b- defN 23-Feb-04 00:29 keras_cv/layers/regularization/dropblock_2d.py
--rw-r--r--  2.0 unx     3653 b- defN 23-Feb-04 00:29 keras_cv/layers/regularization/dropblock_2d_test.py
--rw-r--r--  2.0 unx     4227 b- defN 23-Feb-04 00:29 keras_cv/layers/regularization/squeeze_excite.py
--rw-r--r--  2.0 unx     1897 b- defN 23-Feb-04 00:29 keras_cv/layers/regularization/squeeze_excite_test.py
--rw-r--r--  2.0 unx     2730 b- defN 23-Feb-04 00:29 keras_cv/layers/regularization/stochastic_depth.py
--rw-r--r--  2.0 unx     1758 b- defN 23-Feb-04 00:29 keras_cv/layers/regularization/stochastic_depth_test.py
--rw-r--r--  2.0 unx      925 b- defN 23-Feb-04 00:29 keras_cv/losses/__init__.py
--rw-r--r--  2.0 unx     4094 b- defN 23-Feb-04 00:29 keras_cv/losses/focal.py
--rw-r--r--  2.0 unx     2442 b- defN 23-Feb-04 00:29 keras_cv/losses/focal_test.py
--rw-r--r--  2.0 unx     6947 b- defN 23-Feb-04 00:29 keras_cv/losses/giou_loss.py
--rw-r--r--  2.0 unx     2453 b- defN 23-Feb-04 00:29 keras_cv/losses/giou_loss_test.py
--rw-r--r--  2.0 unx     4480 b- defN 23-Feb-04 00:29 keras_cv/losses/iou_loss.py
--rw-r--r--  2.0 unx     2433 b- defN 23-Feb-04 00:29 keras_cv/losses/iou_loss_test.py
--rw-r--r--  2.0 unx     4195 b- defN 23-Feb-04 00:29 keras_cv/losses/penalty_reduced_focal_loss.py
--rw-r--r--  2.0 unx     3634 b- defN 23-Feb-04 00:29 keras_cv/losses/penalty_reduced_focal_loss_test.py
--rw-r--r--  2.0 unx     2159 b- defN 23-Feb-04 00:29 keras_cv/losses/serialization_test.py
--rw-r--r--  2.0 unx     3363 b- defN 23-Feb-04 00:29 keras_cv/losses/simclr_loss.py
--rw-r--r--  2.0 unx     2123 b- defN 23-Feb-04 00:29 keras_cv/losses/simclr_loss_test.py
--rw-r--r--  2.0 unx     1858 b- defN 23-Feb-04 00:29 keras_cv/losses/smooth_l1.py
--rw-r--r--  2.0 unx     1225 b- defN 23-Feb-04 00:29 keras_cv/losses/smooth_l1_test.py
--rw-r--r--  2.0 unx      721 b- defN 23-Feb-04 00:29 keras_cv/metrics/__init__.py
--rw-r--r--  2.0 unx     1416 b- defN 23-Feb-04 00:29 keras_cv/metrics/serialization_test.py
--rw-r--r--  2.0 unx     1006 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/__init__.py
--rw-r--r--  2.0 unx    15979 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/mean_average_precision.py
--rw-r--r--  2.0 unx     8967 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/mean_average_precision_test.py
--rw-r--r--  2.0 unx     8287 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/pycoco_wrapper.py
--rw-r--r--  2.0 unx    10202 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/recall.py
--rw-r--r--  2.0 unx    11276 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/recall_test.py
--rw-r--r--  2.0 unx     5806 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/utils.py
--rw-r--r--  2.0 unx     4769 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/utils_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/numerical_tests/__init__.py
--rw-r--r--  2.0 unx     5341 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/numerical_tests/mean_average_precision_test.py
--rw-r--r--  2.0 unx     4848 b- defN 23-Feb-04 00:29 keras_cv/metrics/coco/numerical_tests/recall_correctness_test.py
--rw-r--r--  2.0 unx     5484 b- defN 23-Feb-04 00:29 keras_cv/models/__init__.py
--rw-r--r--  2.0 unx    12800 b- defN 23-Feb-04 00:29 keras_cv/models/convmixer.py
--rw-r--r--  2.0 unx     1828 b- defN 23-Feb-04 00:29 keras_cv/models/convmixer_test.py
--rw-r--r--  2.0 unx    17867 b- defN 23-Feb-04 00:29 keras_cv/models/convnext.py
--rw-r--r--  2.0 unx     2449 b- defN 23-Feb-04 00:29 keras_cv/models/convnext_test.py
--rw-r--r--  2.0 unx    14649 b- defN 23-Feb-04 00:29 keras_cv/models/csp_darknet.py
--rw-r--r--  2.0 unx     1878 b- defN 23-Feb-04 00:29 keras_cv/models/csp_darknet_test.py
--rw-r--r--  2.0 unx     9335 b- defN 23-Feb-04 00:29 keras_cv/models/darknet.py
--rw-r--r--  2.0 unx     1816 b- defN 23-Feb-04 00:29 keras_cv/models/darknet_test.py
--rw-r--r--  2.0 unx    11232 b- defN 23-Feb-04 00:29 keras_cv/models/densenet.py
--rw-r--r--  2.0 unx     1994 b- defN 23-Feb-04 00:29 keras_cv/models/densenet_test.py
--rw-r--r--  2.0 unx    20354 b- defN 23-Feb-04 00:29 keras_cv/models/efficientnet_lite.py
--rw-r--r--  2.0 unx     2160 b- defN 23-Feb-04 00:29 keras_cv/models/efficientnet_lite_test.py
--rw-r--r--  2.0 unx    23344 b- defN 23-Feb-04 00:29 keras_cv/models/efficientnet_v1.py
--rw-r--r--  2.0 unx     2258 b- defN 23-Feb-04 00:29 keras_cv/models/efficientnet_v1_test.py
--rw-r--r--  2.0 unx    30652 b- defN 23-Feb-04 00:29 keras_cv/models/efficientnet_v2.py
--rw-r--r--  2.0 unx     2225 b- defN 23-Feb-04 00:29 keras_cv/models/efficientnet_v2_test.py
--rw-r--r--  2.0 unx    12561 b- defN 23-Feb-04 00:29 keras_cv/models/mlp_mixer.py
--rw-r--r--  2.0 unx     2043 b- defN 23-Feb-04 00:29 keras_cv/models/mlp_mixer_test.py
--rw-r--r--  2.0 unx    19887 b- defN 23-Feb-04 00:29 keras_cv/models/mobilenet_v3.py
--rw-r--r--  2.0 unx     1849 b- defN 23-Feb-04 00:29 keras_cv/models/mobilenet_v3_test.py
--rw-r--r--  2.0 unx     5908 b- defN 23-Feb-04 00:29 keras_cv/models/models_test.py
--rw-r--r--  2.0 unx    45343 b- defN 23-Feb-04 00:29 keras_cv/models/regnet.py
--rw-r--r--  2.0 unx     2258 b- defN 23-Feb-04 00:29 keras_cv/models/regnetx_test.py
--rw-r--r--  2.0 unx     2258 b- defN 23-Feb-04 00:29 keras_cv/models/regnety_test.py
--rw-r--r--  2.0 unx    18529 b- defN 23-Feb-04 00:29 keras_cv/models/resnet_v1.py
--rw-r--r--  2.0 unx     2054 b- defN 23-Feb-04 00:29 keras_cv/models/resnet_v1_test.py
--rw-r--r--  2.0 unx    19679 b- defN 23-Feb-04 00:29 keras_cv/models/resnet_v2.py
--rw-r--r--  2.0 unx     4156 b- defN 23-Feb-04 00:29 keras_cv/models/resnet_v2_test.py
--rw-r--r--  2.0 unx     3681 b- defN 23-Feb-04 00:29 keras_cv/models/utils.py
--rw-r--r--  2.0 unx     2268 b- defN 23-Feb-04 00:29 keras_cv/models/utils_test.py
--rw-r--r--  2.0 unx     6172 b- defN 23-Feb-04 00:29 keras_cv/models/vgg16.py
--rw-r--r--  2.0 unx     1772 b- defN 23-Feb-04 00:29 keras_cv/models/vgg16_test.py
--rw-r--r--  2.0 unx     6501 b- defN 23-Feb-04 00:29 keras_cv/models/vgg19.py
--rw-r--r--  2.0 unx     1772 b- defN 23-Feb-04 00:29 keras_cv/models/vgg19_test.py
--rw-r--r--  2.0 unx    24036 b- defN 23-Feb-04 00:29 keras_cv/models/vit.py
--rw-r--r--  2.0 unx     2403 b- defN 23-Feb-04 00:29 keras_cv/models/vit_test.py
--rw-r--r--  2.0 unx     9819 b- defN 23-Feb-04 00:29 keras_cv/models/weights.py
--rw-r--r--  2.0 unx     1029 b- defN 23-Feb-04 00:29 keras_cv/models/__internal__/__init__.py
--rw-r--r--  2.0 unx    12046 b- defN 23-Feb-04 00:29 keras_cv/models/__internal__/darknet_utils.py
--rw-r--r--  2.0 unx     5966 b- defN 23-Feb-04 00:29 keras_cv/models/__internal__/unet.py
--rw-r--r--  2.0 unx     1693 b- defN 23-Feb-04 00:29 keras_cv/models/__internal__/unet_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/__init__.py
--rw-r--r--  2.0 unx     4304 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/__internal__.py
--rw-r--r--  2.0 unx     1574 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/__test_utils__.py
--rw-r--r--  2.0 unx    23583 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/faster_rcnn.py
--rw-r--r--  2.0 unx     3787 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/faster_rcnn_test.py
--rw-r--r--  2.0 unx     3460 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/predict_utils.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/retina_net/__init__.py
--rw-r--r--  2.0 unx    18911 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/retina_net/retina_net.py
--rw-r--r--  2.0 unx    10432 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/retina_net/retina_net_inference_test.py
--rw-r--r--  2.0 unx     8561 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/retina_net/retina_net_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/retina_net/__internal__/__init__.py
--rw-r--r--  2.0 unx      817 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/retina_net/__internal__/layers/__init__.py
--rw-r--r--  2.0 unx     2504 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/retina_net/__internal__/layers/feature_pyramid.py
--rw-r--r--  2.0 unx     2482 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection/retina_net/__internal__/layers/prediction_head.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection_3d/__init__.py
--rw-r--r--  2.0 unx     8569 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection_3d/center_pillar.py
--rw-r--r--  2.0 unx     5337 b- defN 23-Feb-04 00:29 keras_cv/models/object_detection_3d/center_pillar_test.py
--rw-r--r--  2.0 unx      644 b- defN 23-Feb-04 00:29 keras_cv/models/segmentation/__init__.py
--rw-r--r--  2.0 unx    12653 b- defN 23-Feb-04 00:29 keras_cv/models/segmentation/deeplab.py
--rw-r--r--  2.0 unx     6215 b- defN 23-Feb-04 00:29 keras_cv/models/segmentation/deeplab_test.py
--rw-r--r--  2.0 unx     1324 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/__init__.py
--rw-r--r--  2.0 unx     6863 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/clip_tokenizer.py
--rw-r--r--  2.0 unx    17410 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/constants.py
--rw-r--r--  2.0 unx     2648 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/decoder.py
--rw-r--r--  2.0 unx    13001 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/diffusion_model.py
--rw-r--r--  2.0 unx     2691 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/image_encoder.py
--rw-r--r--  2.0 unx     7420 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/noise_scheduler.py
--rw-r--r--  2.0 unx    26051 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/stable_diffusion.py
--rw-r--r--  2.0 unx     2043 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/stable_diffusion_test.py
--rw-r--r--  2.0 unx     6456 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/text_encoder.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/__internal__/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/__internal__/layers/__init__.py
--rw-r--r--  2.0 unx     1868 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py
--rw-r--r--  2.0 unx     1005 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/__internal__/layers/padded_conv2d.py
--rw-r--r--  2.0 unx     1560 b- defN 23-Feb-04 00:29 keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py
--rw-r--r--  2.0 unx     1313 b- defN 23-Feb-04 00:29 keras_cv/ops/__init__.py
--rw-r--r--  2.0 unx     1589 b- defN 23-Feb-04 00:29 keras_cv/ops/iou_3d.py
--rw-r--r--  2.0 unx     2199 b- defN 23-Feb-04 00:29 keras_cv/ops/iou_3d_test.py
--rw-r--r--  2.0 unx    17543 b- defN 23-Feb-04 00:29 keras_cv/ops/point_cloud.py
--rw-r--r--  2.0 unx    13690 b- defN 23-Feb-04 00:29 keras_cv/ops/point_cloud_test.py
--rw-r--r--  2.0 unx     7275 b- defN 23-Feb-04 00:29 keras_cv/ops/within_box_3d_test.py
--rw-r--r--  2.0 unx      810 b- defN 23-Feb-04 00:29 keras_cv/training/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-Feb-04 00:29 keras_cv/training/contrastive/__init__.py
--rw-r--r--  2.0 unx     9017 b- defN 23-Feb-04 00:29 keras_cv/training/contrastive/contrastive_trainer.py
--rw-r--r--  2.0 unx     6011 b- defN 23-Feb-04 00:29 keras_cv/training/contrastive/contrastive_trainer_test.py
--rw-r--r--  2.0 unx     3183 b- defN 23-Feb-04 00:29 keras_cv/training/contrastive/simclr_trainer.py
--rw-r--r--  2.0 unx     1425 b- defN 23-Feb-04 00:29 keras_cv/training/contrastive/simclr_trainer_test.py
--rw-r--r--  2.0 unx     1128 b- defN 23-Feb-04 00:29 keras_cv/utils/__init__.py
--rw-r--r--  2.0 unx     2446 b- defN 23-Feb-04 00:29 keras_cv/utils/conv_utils.py
--rw-r--r--  2.0 unx     3073 b- defN 23-Feb-04 00:29 keras_cv/utils/fill_utils.py
--rw-r--r--  2.0 unx    11243 b- defN 23-Feb-04 00:29 keras_cv/utils/fill_utils_test.py
--rw-r--r--  2.0 unx    13664 b- defN 23-Feb-04 00:29 keras_cv/utils/preprocessing.py
--rw-r--r--  2.0 unx     2303 b- defN 23-Feb-04 00:29 keras_cv/utils/preprocessing_test.py
--rw-r--r--  2.0 unx     2797 b- defN 23-Feb-04 00:29 keras_cv/utils/resource_loader.py
--rw-r--r--  2.0 unx     4690 b- defN 23-Feb-04 00:29 keras_cv/utils/target_gather.py
--rw-r--r--  2.0 unx     5346 b- defN 23-Feb-04 00:29 keras_cv/utils/target_gather_test.py
--rw-r--r--  2.0 unx     3639 b- defN 23-Feb-04 00:29 keras_cv/utils/test_utils.py
--rw-r--r--  2.0 unx     1771 b- defN 23-Feb-04 00:29 keras_cv/utils/train.py
--rw-r--r--  2.0 unx    11412 b- defN 23-Feb-04 00:30 keras_cv-0.4.2.dist-info/LICENSE
--rw-r--r--  2.0 unx     9740 b- defN 23-Feb-04 00:30 keras_cv-0.4.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Feb-04 00:30 keras_cv-0.4.2.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-Feb-04 00:30 keras_cv-0.4.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    35734 b- defN 23-Feb-04 00:30 keras_cv-0.4.2.dist-info/RECORD
-352 files, 2047761 bytes uncompressed, 576687 bytes compressed:  71.9%
+Zip file size: 721593 bytes, number of entries: 400
+-rw-r--r--  2.0 unx     1182 b- defN 23-May-10 00:10 keras_cv/__init__.py
+-rw-r--r--  2.0 unx     2271 b- defN 23-May-10 00:10 keras_cv/conftest.py
+-rw-r--r--  2.0 unx     1159 b- defN 23-May-10 00:10 keras_cv/version_check.py
+-rw-r--r--  2.0 unx     1362 b- defN 23-May-10 00:10 keras_cv/version_check_test.py
+-rw-r--r--  2.0 unx     1611 b- defN 23-May-10 00:10 keras_cv/bounding_box/__init__.py
+-rw-r--r--  2.0 unx    18483 b- defN 23-May-10 00:10 keras_cv/bounding_box/converters.py
+-rw-r--r--  2.0 unx     7134 b- defN 23-May-10 00:10 keras_cv/bounding_box/converters_test.py
+-rw-r--r--  2.0 unx      909 b- defN 23-May-10 00:10 keras_cv/bounding_box/ensure_tensor.py
+-rw-r--r--  2.0 unx     1443 b- defN 23-May-10 00:10 keras_cv/bounding_box/ensure_tensor_test.py
+-rw-r--r--  2.0 unx     4035 b- defN 23-May-10 00:10 keras_cv/bounding_box/formats.py
+-rw-r--r--  2.0 unx     6412 b- defN 23-May-10 00:10 keras_cv/bounding_box/iou.py
+-rw-r--r--  2.0 unx     6130 b- defN 23-May-10 00:10 keras_cv/bounding_box/iou_test.py
+-rw-r--r--  2.0 unx     3751 b- defN 23-May-10 00:10 keras_cv/bounding_box/mask_invalid_detections.py
+-rw-r--r--  2.0 unx     3901 b- defN 23-May-10 00:10 keras_cv/bounding_box/mask_invalid_detections_test.py
+-rw-r--r--  2.0 unx     3204 b- defN 23-May-10 00:10 keras_cv/bounding_box/to_dense.py
+-rw-r--r--  2.0 unx     1147 b- defN 23-May-10 00:10 keras_cv/bounding_box/to_dense_test.py
+-rw-r--r--  2.0 unx     3014 b- defN 23-May-10 00:10 keras_cv/bounding_box/to_ragged.py
+-rw-r--r--  2.0 unx     2713 b- defN 23-May-10 00:10 keras_cv/bounding_box/to_ragged_test.py
+-rw-r--r--  2.0 unx     7178 b- defN 23-May-10 00:10 keras_cv/bounding_box/utils.py
+-rw-r--r--  2.0 unx     5569 b- defN 23-May-10 00:10 keras_cv/bounding_box/utils_test.py
+-rw-r--r--  2.0 unx     3479 b- defN 23-May-10 00:10 keras_cv/bounding_box/validate_format.py
+-rw-r--r--  2.0 unx     1581 b- defN 23-May-10 00:10 keras_cv/bounding_box/validate_format_test.py
+-rw-r--r--  2.0 unx      652 b- defN 23-May-10 00:10 keras_cv/bounding_box_3d/__init__.py
+-rw-r--r--  2.0 unx     1609 b- defN 23-May-10 00:10 keras_cv/bounding_box_3d/formats.py
+-rw-r--r--  2.0 unx      727 b- defN 23-May-10 00:10 keras_cv/callbacks/__init__.py
+-rw-r--r--  2.0 unx     4693 b- defN 23-May-10 00:10 keras_cv/callbacks/pycoco_callback.py
+-rw-r--r--  2.0 unx     3263 b- defN 23-May-10 00:10 keras_cv/callbacks/pycoco_callback_test.py
+-rw-r--r--  2.0 unx     6912 b- defN 23-May-10 00:10 keras_cv/callbacks/waymo_evaluation_callback.py
+-rw-r--r--  2.0 unx     3413 b- defN 23-May-10 00:10 keras_cv/callbacks/waymo_evaluation_callback_test.py
+-rw-r--r--  2.0 unx      936 b- defN 23-May-10 00:10 keras_cv/core/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/__init__.py
+-rw-r--r--  2.0 unx     1667 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/constant_factor_sampler.py
+-rw-r--r--  2.0 unx      964 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/constant_factor_sampler_test.py
+-rw-r--r--  2.0 unx     1343 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/factor_sampler.py
+-rw-r--r--  2.0 unx     2501 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/normal_factor_sampler.py
+-rw-r--r--  2.0 unx     1120 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/normal_factor_sampler_test_.py
+-rw-r--r--  2.0 unx     2183 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/uniform_factor_sampler.py
+-rw-r--r--  2.0 unx     1026 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/uniform_factor_sampler_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/custom_ops/__init__.py
+-rw-r--r--  2.0 unx      625 b- defN 23-May-10 00:10 keras_cv/datasets/__init__.py
+-rw-r--r--  2.0 unx      633 b- defN 23-May-10 00:10 keras_cv/datasets/imagenet/__init__.py
+-rw-r--r--  2.0 unx     4439 b- defN 23-May-10 00:10 keras_cv/datasets/imagenet/load.py
+-rw-r--r--  2.0 unx      635 b- defN 23-May-10 00:10 keras_cv/datasets/pascal_voc/__init__.py
+-rw-r--r--  2.0 unx     3642 b- defN 23-May-10 00:10 keras_cv/datasets/pascal_voc/load.py
+-rw-r--r--  2.0 unx    18789 b- defN 23-May-10 00:10 keras_cv/datasets/pascal_voc/segmentation.py
+-rw-r--r--  2.0 unx    12179 b- defN 23-May-10 00:10 keras_cv/datasets/pascal_voc/segmentation_test.py
+-rw-r--r--  2.0 unx     1103 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/__init__.py
+-rw-r--r--  2.0 unx     2935 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/load.py
+-rw-r--r--  2.0 unx     2114 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/load_test.py
+-rw-r--r--  2.0 unx     1980 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/struct.py
+-rw-r--r--  2.0 unx    27253 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/transformer.py
+-rw-r--r--  2.0 unx     6947 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/transformer_test.py
+-rw-r--r--  2.0 unx      783 b- defN 23-May-10 00:10 keras_cv/keypoint/__init__.py
+-rw-r--r--  2.0 unx     6955 b- defN 23-May-10 00:10 keras_cv/keypoint/converters.py
+-rw-r--r--  2.0 unx     5181 b- defN 23-May-10 00:10 keras_cv/keypoint/converters_test.py
+-rw-r--r--  2.0 unx     1725 b- defN 23-May-10 00:10 keras_cv/keypoint/formats.py
+-rw-r--r--  2.0 unx     1597 b- defN 23-May-10 00:10 keras_cv/keypoint/utils.py
+-rw-r--r--  2.0 unx     2000 b- defN 23-May-10 00:10 keras_cv/keypoint/utils_test.py
+-rw-r--r--  2.0 unx     6057 b- defN 23-May-10 00:10 keras_cv/layers/__init__.py
+-rw-r--r--  2.0 unx     8828 b- defN 23-May-10 00:10 keras_cv/layers/feature_pyramid.py
+-rw-r--r--  2.0 unx     5125 b- defN 23-May-10 00:10 keras_cv/layers/feature_pyramid_test.py
+-rw-r--r--  2.0 unx     8076 b- defN 23-May-10 00:10 keras_cv/layers/fusedmbconv.py
+-rw-r--r--  2.0 unx     2273 b- defN 23-May-10 00:10 keras_cv/layers/fusedmbconv_test.py
+-rw-r--r--  2.0 unx     8349 b- defN 23-May-10 00:10 keras_cv/layers/mbconv.py
+-rw-r--r--  2.0 unx     2216 b- defN 23-May-10 00:10 keras_cv/layers/mbconv_test.py
+-rw-r--r--  2.0 unx    12344 b- defN 23-May-10 00:10 keras_cv/layers/serialization_test.py
+-rw-r--r--  2.0 unx     6281 b- defN 23-May-10 00:10 keras_cv/layers/spatial_pyramid.py
+-rw-r--r--  2.0 unx     1290 b- defN 23-May-10 00:10 keras_cv/layers/spatial_pyramid_test.py
+-rw-r--r--  2.0 unx     5249 b- defN 23-May-10 00:10 keras_cv/layers/transformer_encoder.py
+-rw-r--r--  2.0 unx     2135 b- defN 23-May-10 00:10 keras_cv/layers/transformer_encoder_test.py
+-rw-r--r--  2.0 unx     7723 b- defN 23-May-10 00:10 keras_cv/layers/vit_layers.py
+-rw-r--r--  2.0 unx     2886 b- defN 23-May-10 00:10 keras_cv/layers/vit_layers_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/__init__.py
+-rw-r--r--  2.0 unx    11340 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/anchor_generator.py
+-rw-r--r--  2.0 unx     6318 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/anchor_generator_test.py
+-rw-r--r--  2.0 unx    11483 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/box_matcher.py
+-rw-r--r--  2.0 unx     4939 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/box_matcher_test.py
+-rw-r--r--  2.0 unx     5140 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/multi_class_non_max_suppression.py
+-rw-r--r--  2.0 unx     1610 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py
+-rw-r--r--  2.0 unx    15804 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_align.py
+-rw-r--r--  2.0 unx     9833 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_generator.py
+-rw-r--r--  2.0 unx     9256 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_generator_test.py
+-rw-r--r--  2.0 unx     6427 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_pool.py
+-rw-r--r--  2.0 unx     9712 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_pool_test.py
+-rw-r--r--  2.0 unx     9064 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_sampler.py
+-rw-r--r--  2.0 unx    11644 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_sampler_test.py
+-rw-r--r--  2.0 unx     9272 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/rpn_label_encoder.py
+-rw-r--r--  2.0 unx     5631 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/rpn_label_encoder_test.py
+-rw-r--r--  2.0 unx     3426 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/sampling.py
+-rw-r--r--  2.0 unx     7277 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/sampling_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/__init__.py
+-rw-r--r--  2.0 unx    16374 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/centernet_label_encoder.py
+-rw-r--r--  2.0 unx     4863 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py
+-rw-r--r--  2.0 unx     8107 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/heatmap_decoder.py
+-rw-r--r--  2.0 unx    10029 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/voxel_utils.py
+-rw-r--r--  2.0 unx     2841 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/voxel_utils_test.py
+-rw-r--r--  2.0 unx     9159 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/voxelization.py
+-rw-r--r--  2.0 unx     4114 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/voxelization_test.py
+-rw-r--r--  2.0 unx     4003 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/__init__.py
+-rw-r--r--  2.0 unx    12754 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/aug_mix.py
+-rw-r--r--  2.0 unx     2205 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/aug_mix_test.py
+-rw-r--r--  2.0 unx     3509 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/auto_contrast.py
+-rw-r--r--  2.0 unx     3331 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/auto_contrast_test.py
+-rw-r--r--  2.0 unx    20504 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/base_image_augmentation_layer.py
+-rw-r--r--  2.0 unx    10953 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py
+-rw-r--r--  2.0 unx     4559 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/channel_shuffle.py
+-rw-r--r--  2.0 unx     4065 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/channel_shuffle_test.py
+-rw-r--r--  2.0 unx     5992 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/cut_mix.py
+-rw-r--r--  2.0 unx     5040 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/cut_mix_test.py
+-rw-r--r--  2.0 unx     4929 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/equalization.py
+-rw-r--r--  2.0 unx     2446 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/equalization_test.py
+-rw-r--r--  2.0 unx     8025 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/fourier_mix.py
+-rw-r--r--  2.0 unx     3447 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/fourier_mix_test.py
+-rw-r--r--  2.0 unx     3841 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/grayscale.py
+-rw-r--r--  2.0 unx     2820 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/grayscale_test.py
+-rw-r--r--  2.0 unx     9733 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/grid_mask.py
+-rw-r--r--  2.0 unx     3823 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/grid_mask_test.py
+-rw-r--r--  2.0 unx    11349 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/jittered_resize.py
+-rw-r--r--  2.0 unx     7360 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/jittered_resize_test.py
+-rw-r--r--  2.0 unx     5019 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/maybe_apply.py
+-rw-r--r--  2.0 unx     4634 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/maybe_apply_test.py
+-rw-r--r--  2.0 unx     6100 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/mix_up.py
+-rw-r--r--  2.0 unx     4581 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/mix_up_test.py
+-rw-r--r--  2.0 unx    13424 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/mosaic.py
+-rw-r--r--  2.0 unx     3726 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/mosaic_test.py
+-rw-r--r--  2.0 unx     4237 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/posterization.py
+-rw-r--r--  2.0 unx     3792 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/posterization_test.py
+-rw-r--r--  2.0 unx     5343 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/ragged_image_test.py
+-rw-r--r--  2.0 unx    10805 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/rand_augment.py
+-rw-r--r--  2.0 unx     3758 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/rand_augment_test.py
+-rw-r--r--  2.0 unx     4683 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_aspect_ratio.py
+-rw-r--r--  2.0 unx     2277 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_aspect_ratio_test.py
+-rw-r--r--  2.0 unx     4754 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_augmentation_pipeline.py
+-rw-r--r--  2.0 unx     3340 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py
+-rw-r--r--  2.0 unx     5241 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_brightness.py
+-rw-r--r--  2.0 unx     3337 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_brightness_test.py
+-rw-r--r--  2.0 unx     4396 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_channel_shift.py
+-rw-r--r--  2.0 unx     3964 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_channel_shift_test.py
+-rw-r--r--  2.0 unx     4503 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_choice.py
+-rw-r--r--  2.0 unx     3316 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_choice_test.py
+-rw-r--r--  2.0 unx     3392 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_color_degeneration.py
+-rw-r--r--  2.0 unx     2648 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_color_degeneration_test.py
+-rw-r--r--  2.0 unx     6946 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_color_jitter.py
+-rw-r--r--  2.0 unx     3902 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_color_jitter_test.py
+-rw-r--r--  2.0 unx     4864 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_contrast.py
+-rw-r--r--  2.0 unx     2213 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_contrast_test.py
+-rw-r--r--  2.0 unx    10847 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_crop.py
+-rw-r--r--  2.0 unx    11178 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_crop_and_resize.py
+-rw-r--r--  2.0 unx    10241 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_crop_and_resize_test.py
+-rw-r--r--  2.0 unx     9856 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_crop_test.py
+-rw-r--r--  2.0 unx     7024 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_cutout.py
+-rw-r--r--  2.0 unx     4978 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_cutout_test.py
+-rw-r--r--  2.0 unx     9003 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_flip.py
+-rw-r--r--  2.0 unx    10994 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_flip_test.py
+-rw-r--r--  2.0 unx     4576 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_gaussian_blur.py
+-rw-r--r--  2.0 unx     3245 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_gaussian_blur_test.py
+-rw-r--r--  2.0 unx     5433 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_hue.py
+-rw-r--r--  2.0 unx     4230 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_hue_test.py
+-rw-r--r--  2.0 unx     3021 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_jpeg_quality.py
+-rw-r--r--  2.0 unx     1984 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_jpeg_quality_test.py
+-rw-r--r--  2.0 unx    12266 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_rotation.py
+-rw-r--r--  2.0 unx     7395 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_rotation_test.py
+-rw-r--r--  2.0 unx     4951 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_saturation.py
+-rw-r--r--  2.0 unx     8248 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_saturation_test.py
+-rw-r--r--  2.0 unx     5813 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_sharpness.py
+-rw-r--r--  2.0 unx     2724 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_sharpness_test.py
+-rw-r--r--  2.0 unx    12947 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_shear.py
+-rw-r--r--  2.0 unx     9331 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_shear_test.py
+-rw-r--r--  2.0 unx    11148 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_translation.py
+-rw-r--r--  2.0 unx     8917 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_translation_test.py
+-rw-r--r--  2.0 unx    10340 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_zoom.py
+-rw-r--r--  2.0 unx     5859 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_zoom_test.py
+-rw-r--r--  2.0 unx    11418 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/randomly_zoomed_crop.py
+-rw-r--r--  2.0 unx     7266 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/randomly_zoomed_crop_test.py
+-rw-r--r--  2.0 unx     4677 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/repeated_augmentation.py
+-rw-r--r--  2.0 unx     1753 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/repeated_augmentation_test.py
+-rw-r--r--  2.0 unx     2766 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/rescaling.py
+-rw-r--r--  2.0 unx     2123 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/rescaling_test.py
+-rw-r--r--  2.0 unx    12328 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/resizing.py
+-rw-r--r--  2.0 unx    11545 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/resizing_test.py
+-rw-r--r--  2.0 unx     6315 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/solarization.py
+-rw-r--r--  2.0 unx     3152 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/solarization_test.py
+-rw-r--r--  2.0 unx    19868 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py
+-rw-r--r--  2.0 unx    20975 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py
+-rw-r--r--  2.0 unx     5015 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/with_labels_test.py
+-rw-r--r--  2.0 unx     5473 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/with_mixed_precision_test.py
+-rw-r--r--  2.0 unx     4812 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/with_segmentation_masks_test.py
+-rw-r--r--  2.0 unx     1769 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/__init__.py
+-rw-r--r--  2.0 unx     7987 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py
+-rw-r--r--  2.0 unx     4789 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py
+-rw-r--r--  2.0 unx     6272 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/frustum_random_dropping_points.py
+-rw-r--r--  2.0 unx     5454 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/frustum_random_dropping_points_test.py
+-rw-r--r--  2.0 unx     6969 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise.py
+-rw-r--r--  2.0 unx     8880 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise_test.py
+-rw-r--r--  2.0 unx     3815 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_dropping_points.py
+-rw-r--r--  2.0 unx     5100 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_dropping_points_test.py
+-rw-r--r--  2.0 unx     4458 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_flip.py
+-rw-r--r--  2.0 unx     3275 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_flip_test.py
+-rw-r--r--  2.0 unx     6016 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_rotation.py
+-rw-r--r--  2.0 unx     3158 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_rotation_test.py
+-rw-r--r--  2.0 unx     7090 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_scaling.py
+-rw-r--r--  2.0 unx     4610 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_scaling_test.py
+-rw-r--r--  2.0 unx     4733 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_translation.py
+-rw-r--r--  2.0 unx     2935 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_translation_test.py
+-rw-r--r--  2.0 unx    11464 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes.py
+-rw-r--r--  2.0 unx     7460 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes_test.py
+-rw-r--r--  2.0 unx    12606 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/random_copy_paste.py
+-rw-r--r--  2.0 unx     8348 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/random_copy_paste_test.py
+-rw-r--r--  2.0 unx     5418 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/random_drop_box.py
+-rw-r--r--  2.0 unx    12338 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/random_drop_box_test.py
+-rw-r--r--  2.0 unx     7322 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/swap_background.py
+-rw-r--r--  2.0 unx    11087 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/swap_background_test.py
+-rw-r--r--  2.0 unx      868 b- defN 23-May-10 00:10 keras_cv/layers/regularization/__init__.py
+-rw-r--r--  2.0 unx     2545 b- defN 23-May-10 00:10 keras_cv/layers/regularization/drop_path.py
+-rw-r--r--  2.0 unx     2460 b- defN 23-May-10 00:10 keras_cv/layers/regularization/drop_path_test.py
+-rw-r--r--  2.0 unx     8831 b- defN 23-May-10 00:10 keras_cv/layers/regularization/dropblock_2d.py
+-rw-r--r--  2.0 unx     3653 b- defN 23-May-10 00:10 keras_cv/layers/regularization/dropblock_2d_test.py
+-rw-r--r--  2.0 unx     4906 b- defN 23-May-10 00:10 keras_cv/layers/regularization/squeeze_excite.py
+-rw-r--r--  2.0 unx     1882 b- defN 23-May-10 00:10 keras_cv/layers/regularization/squeeze_excite_test.py
+-rw-r--r--  2.0 unx     2738 b- defN 23-May-10 00:10 keras_cv/layers/regularization/stochastic_depth.py
+-rw-r--r--  2.0 unx     1771 b- defN 23-May-10 00:10 keras_cv/layers/regularization/stochastic_depth_test.py
+-rw-r--r--  2.0 unx      989 b- defN 23-May-10 00:10 keras_cv/losses/__init__.py
+-rw-r--r--  2.0 unx     4854 b- defN 23-May-10 00:10 keras_cv/losses/centernet_box_loss.py
+-rw-r--r--  2.0 unx     1459 b- defN 23-May-10 00:10 keras_cv/losses/centernet_box_loss_test.py
+-rw-r--r--  2.0 unx     4140 b- defN 23-May-10 00:10 keras_cv/losses/focal.py
+-rw-r--r--  2.0 unx     2486 b- defN 23-May-10 00:10 keras_cv/losses/focal_test.py
+-rw-r--r--  2.0 unx     7410 b- defN 23-May-10 00:10 keras_cv/losses/giou_loss.py
+-rw-r--r--  2.0 unx     2541 b- defN 23-May-10 00:10 keras_cv/losses/giou_loss_test.py
+-rw-r--r--  2.0 unx     4921 b- defN 23-May-10 00:10 keras_cv/losses/iou_loss.py
+-rw-r--r--  2.0 unx     2521 b- defN 23-May-10 00:10 keras_cv/losses/iou_loss_test.py
+-rw-r--r--  2.0 unx     4283 b- defN 23-May-10 00:10 keras_cv/losses/penalty_reduced_focal_loss.py
+-rw-r--r--  2.0 unx     3744 b- defN 23-May-10 00:10 keras_cv/losses/penalty_reduced_focal_loss_test.py
+-rw-r--r--  2.0 unx     2211 b- defN 23-May-10 00:10 keras_cv/losses/serialization_test.py
+-rw-r--r--  2.0 unx     3468 b- defN 23-May-10 00:10 keras_cv/losses/simclr_loss.py
+-rw-r--r--  2.0 unx     2145 b- defN 23-May-10 00:10 keras_cv/losses/simclr_loss_test.py
+-rw-r--r--  2.0 unx     1885 b- defN 23-May-10 00:10 keras_cv/losses/smooth_l1.py
+-rw-r--r--  2.0 unx     1225 b- defN 23-May-10 00:10 keras_cv/losses/smooth_l1_test.py
+-rw-r--r--  2.0 unx      663 b- defN 23-May-10 00:10 keras_cv/metrics/__init__.py
+-rw-r--r--  2.0 unx      719 b- defN 23-May-10 00:10 keras_cv/metrics/coco/__init__.py
+-rw-r--r--  2.0 unx     8388 b- defN 23-May-10 00:10 keras_cv/metrics/coco/pycoco_wrapper.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/metrics/object_detection/__init__.py
+-rw-r--r--  2.0 unx     9843 b- defN 23-May-10 00:10 keras_cv/metrics/object_detection/box_coco_metrics.py
+-rw-r--r--  2.0 unx     4779 b- defN 23-May-10 00:10 keras_cv/metrics/object_detection/box_coco_metrics_test.py
+-rw-r--r--  2.0 unx     4079 b- defN 23-May-10 00:10 keras_cv/models/__init__.py
+-rw-r--r--  2.0 unx     6387 b- defN 23-May-10 00:10 keras_cv/models/task.py
+-rw-r--r--  2.0 unx     1093 b- defN 23-May-10 00:10 keras_cv/models/utils.py
+-rw-r--r--  2.0 unx     1133 b- defN 23-May-10 00:10 keras_cv/models/utils_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/__internal__/__init__.py
+-rw-r--r--  2.0 unx     5966 b- defN 23-May-10 00:10 keras_cv/models/__internal__/unet.py
+-rw-r--r--  2.0 unx     1693 b- defN 23-May-10 00:10 keras_cv/models/__internal__/unet_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/__init__.py
+-rw-r--r--  2.0 unx     6264 b- defN 23-May-10 00:10 keras_cv/models/backbones/backbone.py
+-rw-r--r--  2.0 unx     1839 b- defN 23-May-10 00:10 keras_cv/models/backbones/backbone_presets.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/__init__.py
+-rw-r--r--  2.0 unx    12000 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py
+-rw-r--r--  2.0 unx     6518 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py
+-rw-r--r--  2.0 unx     4028 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py
+-rw-r--r--  2.0 unx     5552 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py
+-rw-r--r--  2.0 unx    12186 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/__init__.py
+-rw-r--r--  2.0 unx     9316 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py
+-rw-r--r--  2.0 unx    12981 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py
+-rw-r--r--  2.0 unx    19514 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets.py
+-rw-r--r--  2.0 unx     2287 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py
+-rw-r--r--  2.0 unx     8546 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/__init__.py
+-rw-r--r--  2.0 unx    15819 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py
+-rw-r--r--  2.0 unx     6080 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py
+-rw-r--r--  2.0 unx     1418 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py
+-rw-r--r--  2.0 unx     3266 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/__init__.py
+-rw-r--r--  2.0 unx    17276 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py
+-rw-r--r--  2.0 unx     5551 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py
+-rw-r--r--  2.0 unx     3546 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py
+-rw-r--r--  2.0 unx     6283 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/__init__.py
+-rw-r--r--  2.0 unx    18820 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py
+-rw-r--r--  2.0 unx     5617 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py
+-rw-r--r--  2.0 unx     3724 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py
+-rw-r--r--  2.0 unx     5693 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/classification/__init__.py
+-rw-r--r--  2.0 unx     4645 b- defN 23-May-10 00:10 keras_cv/models/classification/image_classifier.py
+-rw-r--r--  2.0 unx     8454 b- defN 23-May-10 00:10 keras_cv/models/classification/image_classifier_presets.py
+-rw-r--r--  2.0 unx     7098 b- defN 23-May-10 00:10 keras_cv/models/classification/image_classifier_test.py
+-rw-r--r--  2.0 unx     4514 b- defN 23-May-10 00:10 keras_cv/models/legacy/__init__.py
+-rw-r--r--  2.0 unx    14440 b- defN 23-May-10 00:10 keras_cv/models/legacy/convmixer.py
+-rw-r--r--  2.0 unx     1835 b- defN 23-May-10 00:10 keras_cv/models/legacy/convmixer_test.py
+-rw-r--r--  2.0 unx    20264 b- defN 23-May-10 00:10 keras_cv/models/legacy/convnext.py
+-rw-r--r--  2.0 unx     2456 b- defN 23-May-10 00:10 keras_cv/models/legacy/convnext_test.py
+-rw-r--r--  2.0 unx    11050 b- defN 23-May-10 00:10 keras_cv/models/legacy/darknet.py
+-rw-r--r--  2.0 unx     1823 b- defN 23-May-10 00:10 keras_cv/models/legacy/darknet_test.py
+-rw-r--r--  2.0 unx    13432 b- defN 23-May-10 00:10 keras_cv/models/legacy/densenet.py
+-rw-r--r--  2.0 unx     2001 b- defN 23-May-10 00:10 keras_cv/models/legacy/densenet_test.py
+-rw-r--r--  2.0 unx    22966 b- defN 23-May-10 00:10 keras_cv/models/legacy/efficientnet_lite.py
+-rw-r--r--  2.0 unx     2173 b- defN 23-May-10 00:10 keras_cv/models/legacy/efficientnet_lite_test.py
+-rw-r--r--  2.0 unx    29352 b- defN 23-May-10 00:10 keras_cv/models/legacy/efficientnet_v1.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-May-10 00:10 keras_cv/models/legacy/efficientnet_v1_test.py
+-rw-r--r--  2.0 unx    14405 b- defN 23-May-10 00:10 keras_cv/models/legacy/mlp_mixer.py
+-rw-r--r--  2.0 unx     2050 b- defN 23-May-10 00:10 keras_cv/models/legacy/mlp_mixer_test.py
+-rw-r--r--  2.0 unx     6541 b- defN 23-May-10 00:10 keras_cv/models/legacy/models_test.py
+-rw-r--r--  2.0 unx    45767 b- defN 23-May-10 00:10 keras_cv/models/legacy/regnet.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-May-10 00:10 keras_cv/models/legacy/regnetx_test.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-May-10 00:10 keras_cv/models/legacy/regnety_test.py
+-rw-r--r--  2.0 unx     3662 b- defN 23-May-10 00:10 keras_cv/models/legacy/utils.py
+-rw-r--r--  2.0 unx     2320 b- defN 23-May-10 00:10 keras_cv/models/legacy/utils_test.py
+-rw-r--r--  2.0 unx     8144 b- defN 23-May-10 00:10 keras_cv/models/legacy/vgg16.py
+-rw-r--r--  2.0 unx     1779 b- defN 23-May-10 00:10 keras_cv/models/legacy/vgg16_test.py
+-rw-r--r--  2.0 unx     7103 b- defN 23-May-10 00:10 keras_cv/models/legacy/vgg19.py
+-rw-r--r--  2.0 unx     1779 b- defN 23-May-10 00:10 keras_cv/models/legacy/vgg19_test.py
+-rw-r--r--  2.0 unx    26159 b- defN 23-May-10 00:10 keras_cv/models/legacy/vit.py
+-rw-r--r--  2.0 unx     2410 b- defN 23-May-10 00:10 keras_cv/models/legacy/vit_test.py
+-rw-r--r--  2.0 unx     8729 b- defN 23-May-10 00:10 keras_cv/models/legacy/weights.py
+-rw-r--r--  2.0 unx      651 b- defN 23-May-10 00:10 keras_cv/models/legacy/segmentation/__init__.py
+-rw-r--r--  2.0 unx    12412 b- defN 23-May-10 00:10 keras_cv/models/legacy/segmentation/deeplab.py
+-rw-r--r--  2.0 unx     5955 b- defN 23-May-10 00:10 keras_cv/models/legacy/segmentation/deeplab_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/object_detection/__init__.py
+-rw-r--r--  2.0 unx     4329 b- defN 23-May-10 00:10 keras_cv/models/object_detection/__internal__.py
+-rw-r--r--  2.0 unx     1904 b- defN 23-May-10 00:10 keras_cv/models/object_detection/__test_utils__.py
+-rw-r--r--  2.0 unx     3490 b- defN 23-May-10 00:10 keras_cv/models/object_detection/predict_utils.py
+-rw-r--r--  2.0 unx      898 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/__init__.py
+-rw-r--r--  2.0 unx     2501 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/feature_pyramid.py
+-rw-r--r--  2.0 unx     2841 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/prediction_head.py
+-rw-r--r--  2.0 unx    23051 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet.py
+-rw-r--r--  2.0 unx     9759 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py
+-rw-r--r--  2.0 unx     4594 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py
+-rw-r--r--  2.0 unx     1634 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet_presets.py
+-rw-r--r--  2.0 unx     7822 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet_test.py
+-rw-r--r--  2.0 unx      687 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/__init__.py
+-rw-r--r--  2.0 unx     7018 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py
+-rw-r--r--  2.0 unx     6577 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone_presets.py
+-rw-r--r--  2.0 unx    22502 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py
+-rw-r--r--  2.0 unx     1595 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py
+-rw-r--r--  2.0 unx     4708 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py
+-rw-r--r--  2.0 unx     2090 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_iou_loss.py
+-rw-r--r--  2.0 unx    14891 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py
+-rw-r--r--  2.0 unx     2884 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/__init__.py
+-rw-r--r--  2.0 unx     3419 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/binary_crossentropy.py
+-rw-r--r--  2.0 unx      954 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/__init__.py
+-rw-r--r--  2.0 unx     6313 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_decoder.py
+-rw-r--r--  2.0 unx     5423 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_head.py
+-rw-r--r--  2.0 unx     1859 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_head_test.py
+-rw-r--r--  2.0 unx     1923 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder.py
+-rw-r--r--  2.0 unx     2986 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder_test.py
+-rw-r--r--  2.0 unx     5198 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_pafpn.py
+-rw-r--r--  2.0 unx     1806 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_pafpn_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/object_detection_3d/__init__.py
+-rw-r--r--  2.0 unx    10038 b- defN 23-May-10 00:10 keras_cv/models/object_detection_3d/center_pillar.py
+-rw-r--r--  2.0 unx     5577 b- defN 23-May-10 00:10 keras_cv/models/object_detection_3d/center_pillar_test.py
+-rw-r--r--  2.0 unx     1324 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__init__.py
+-rw-r--r--  2.0 unx     7025 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/clip_tokenizer.py
+-rw-r--r--  2.0 unx    17410 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/constants.py
+-rw-r--r--  2.0 unx     2690 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/decoder.py
+-rw-r--r--  2.0 unx    13271 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/diffusion_model.py
+-rw-r--r--  2.0 unx     2757 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/image_encoder.py
+-rw-r--r--  2.0 unx     7770 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/noise_scheduler.py
+-rw-r--r--  2.0 unx    19421 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/stable_diffusion.py
+-rw-r--r--  2.0 unx     2414 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/stable_diffusion_test.py
+-rw-r--r--  2.0 unx     6680 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/text_encoder.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/layers/__init__.py
+-rw-r--r--  2.0 unx     1948 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py
+-rw-r--r--  2.0 unx     1005 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/layers/padded_conv2d.py
+-rw-r--r--  2.0 unx     1560 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py
+-rw-r--r--  2.0 unx      624 b- defN 23-May-10 00:10 keras_cv/ops/__init__.py
+-rw-r--r--  2.0 unx     1589 b- defN 23-May-10 00:10 keras_cv/ops/iou_3d.py
+-rw-r--r--  2.0 unx     2242 b- defN 23-May-10 00:10 keras_cv/ops/iou_3d_test.py
+-rw-r--r--  2.0 unx     1522 b- defN 23-May-10 00:10 keras_cv/point_cloud/__init__.py
+-rw-r--r--  2.0 unx    18327 b- defN 23-May-10 00:10 keras_cv/point_cloud/point_cloud.py
+-rw-r--r--  2.0 unx    14038 b- defN 23-May-10 00:10 keras_cv/point_cloud/point_cloud_test.py
+-rw-r--r--  2.0 unx     7774 b- defN 23-May-10 00:10 keras_cv/point_cloud/within_box_3d_test.py
+-rw-r--r--  2.0 unx      810 b- defN 23-May-10 00:10 keras_cv/training/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/training/contrastive/__init__.py
+-rw-r--r--  2.0 unx     9414 b- defN 23-May-10 00:10 keras_cv/training/contrastive/contrastive_trainer.py
+-rw-r--r--  2.0 unx     6082 b- defN 23-May-10 00:10 keras_cv/training/contrastive/contrastive_trainer_test.py
+-rw-r--r--  2.0 unx     3197 b- defN 23-May-10 00:10 keras_cv/training/contrastive/simclr_trainer.py
+-rw-r--r--  2.0 unx     1609 b- defN 23-May-10 00:10 keras_cv/training/contrastive/simclr_trainer_test.py
+-rw-r--r--  2.0 unx     1408 b- defN 23-May-10 00:10 keras_cv/utils/__init__.py
+-rw-r--r--  2.0 unx     2080 b- defN 23-May-10 00:10 keras_cv/utils/conditional_imports.py
+-rw-r--r--  2.0 unx     2474 b- defN 23-May-10 00:10 keras_cv/utils/conv_utils.py
+-rw-r--r--  2.0 unx     3105 b- defN 23-May-10 00:10 keras_cv/utils/fill_utils.py
+-rw-r--r--  2.0 unx    11265 b- defN 23-May-10 00:10 keras_cv/utils/fill_utils_test.py
+-rw-r--r--  2.0 unx    14465 b- defN 23-May-10 00:10 keras_cv/utils/preprocessing.py
+-rw-r--r--  2.0 unx     2303 b- defN 23-May-10 00:10 keras_cv/utils/preprocessing_test.py
+-rw-r--r--  2.0 unx     1802 b- defN 23-May-10 00:10 keras_cv/utils/python_utils.py
+-rw-r--r--  2.0 unx     2843 b- defN 23-May-10 00:10 keras_cv/utils/resource_loader.py
+-rw-r--r--  2.0 unx     4731 b- defN 23-May-10 00:10 keras_cv/utils/target_gather.py
+-rw-r--r--  2.0 unx     5346 b- defN 23-May-10 00:10 keras_cv/utils/target_gather_test.py
+-rw-r--r--  2.0 unx     3628 b- defN 23-May-10 00:10 keras_cv/utils/test_utils.py
+-rw-r--r--  2.0 unx      991 b- defN 23-May-10 00:10 keras_cv/utils/to_numpy.py
+-rw-r--r--  2.0 unx     2813 b- defN 23-May-10 00:10 keras_cv/utils/train.py
+-rw-r--r--  2.0 unx      754 b- defN 23-May-10 00:10 keras_cv/visualization/__init__.py
+-rw-r--r--  2.0 unx     5497 b- defN 23-May-10 00:10 keras_cv/visualization/draw_bounding_boxes.py
+-rw-r--r--  2.0 unx     6120 b- defN 23-May-10 00:10 keras_cv/visualization/plot_bounding_box_gallery.py
+-rw-r--r--  2.0 unx     3945 b- defN 23-May-10 00:10 keras_cv/visualization/plot_image_gallery.py
+-rw-r--r--  2.0 unx    11412 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    10792 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    41631 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/RECORD
+400 files, 2334301 bytes uncompressed, 653439 bytes compressed:  72.0%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: keras_cv/__init__.py
 Comment: 
 
+Filename: keras_cv/conftest.py
+Comment: 
+
 Filename: keras_cv/version_check.py
 Comment: 
 
 Filename: keras_cv/version_check_test.py
 Comment: 
 
 Filename: keras_cv/bounding_box/__init__.py
@@ -12,14 +15,20 @@
 
 Filename: keras_cv/bounding_box/converters.py
 Comment: 
 
 Filename: keras_cv/bounding_box/converters_test.py
 Comment: 
 
+Filename: keras_cv/bounding_box/ensure_tensor.py
+Comment: 
+
+Filename: keras_cv/bounding_box/ensure_tensor_test.py
+Comment: 
+
 Filename: keras_cv/bounding_box/formats.py
 Comment: 
 
 Filename: keras_cv/bounding_box/iou.py
 Comment: 
 
 Filename: keras_cv/bounding_box/iou_test.py
@@ -222,20 +231,14 @@
 
 Filename: keras_cv/layers/object_detection/multi_class_non_max_suppression.py
 Comment: 
 
 Filename: keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py
 Comment: 
 
-Filename: keras_cv/layers/object_detection/retina_net_label_encoder.py
-Comment: 
-
-Filename: keras_cv/layers/object_detection/retina_net_label_encoder_test.py
-Comment: 
-
 Filename: keras_cv/layers/object_detection/roi_align.py
 Comment: 
 
 Filename: keras_cv/layers/object_detection/roi_generator.py
 Comment: 
 
 Filename: keras_cv/layers/object_detection/roi_generator_test.py
@@ -264,18 +267,18 @@
 
 Filename: keras_cv/layers/object_detection/sampling_test.py
 Comment: 
 
 Filename: keras_cv/layers/object_detection_3d/__init__.py
 Comment: 
 
-Filename: keras_cv/layers/object_detection_3d/center_net_label_encoder.py
+Filename: keras_cv/layers/object_detection_3d/centernet_label_encoder.py
 Comment: 
 
-Filename: keras_cv/layers/object_detection_3d/center_net_label_encoder_test.py
+Filename: keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py
 Comment: 
 
 Filename: keras_cv/layers/object_detection_3d/heatmap_decoder.py
 Comment: 
 
 Filename: keras_cv/layers/object_detection_3d/voxel_utils.py
 Comment: 
@@ -294,20 +297,14 @@
 
 Filename: keras_cv/layers/preprocessing/aug_mix.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/aug_mix_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing/augmenter.py
-Comment: 
-
-Filename: keras_cv/layers/preprocessing/augmenter_test.py
-Comment: 
-
 Filename: keras_cv/layers/preprocessing/auto_contrast.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/auto_contrast_test.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/base_image_augmentation_layer.py
@@ -543,14 +540,20 @@
 
 Filename: keras_cv/layers/preprocessing/solarization.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/solarization_test.py
 Comment: 
 
+Filename: keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py
+Comment: 
+
+Filename: keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py
+Comment: 
+
 Filename: keras_cv/layers/preprocessing/with_labels_test.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/with_mixed_precision_test.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/with_segmentation_masks_test.py
@@ -657,14 +660,20 @@
 
 Filename: keras_cv/layers/regularization/stochastic_depth_test.py
 Comment: 
 
 Filename: keras_cv/losses/__init__.py
 Comment: 
 
+Filename: keras_cv/losses/centernet_box_loss.py
+Comment: 
+
+Filename: keras_cv/losses/centernet_box_loss_test.py
+Comment: 
+
 Filename: keras_cv/losses/focal.py
 Comment: 
 
 Filename: keras_cv/losses/focal_test.py
 Comment: 
 
 Filename: keras_cv/losses/giou_loss.py
@@ -699,234 +708,345 @@
 
 Filename: keras_cv/losses/smooth_l1_test.py
 Comment: 
 
 Filename: keras_cv/metrics/__init__.py
 Comment: 
 
-Filename: keras_cv/metrics/serialization_test.py
+Filename: keras_cv/metrics/coco/__init__.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/__init__.py
+Filename: keras_cv/metrics/coco/pycoco_wrapper.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/mean_average_precision.py
+Filename: keras_cv/metrics/object_detection/__init__.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/mean_average_precision_test.py
+Filename: keras_cv/metrics/object_detection/box_coco_metrics.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/pycoco_wrapper.py
+Filename: keras_cv/metrics/object_detection/box_coco_metrics_test.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/recall.py
+Filename: keras_cv/models/__init__.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/recall_test.py
+Filename: keras_cv/models/task.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/utils.py
+Filename: keras_cv/models/utils.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/utils_test.py
+Filename: keras_cv/models/utils_test.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/numerical_tests/__init__.py
+Filename: keras_cv/models/__internal__/__init__.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/numerical_tests/mean_average_precision_test.py
+Filename: keras_cv/models/__internal__/unet.py
 Comment: 
 
-Filename: keras_cv/metrics/coco/numerical_tests/recall_correctness_test.py
+Filename: keras_cv/models/__internal__/unet_test.py
 Comment: 
 
-Filename: keras_cv/models/__init__.py
+Filename: keras_cv/models/backbones/__init__.py
 Comment: 
 
-Filename: keras_cv/models/convmixer.py
+Filename: keras_cv/models/backbones/backbone.py
 Comment: 
 
-Filename: keras_cv/models/convmixer_test.py
+Filename: keras_cv/models/backbones/backbone_presets.py
 Comment: 
 
-Filename: keras_cv/models/convnext.py
+Filename: keras_cv/models/backbones/csp_darknet/__init__.py
 Comment: 
 
-Filename: keras_cv/models/convnext_test.py
+Filename: keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py
 Comment: 
 
-Filename: keras_cv/models/csp_darknet.py
+Filename: keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py
 Comment: 
 
-Filename: keras_cv/models/csp_darknet_test.py
+Filename: keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py
 Comment: 
 
-Filename: keras_cv/models/darknet.py
+Filename: keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py
 Comment: 
 
-Filename: keras_cv/models/darknet_test.py
+Filename: keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py
 Comment: 
 
-Filename: keras_cv/models/densenet.py
+Filename: keras_cv/models/backbones/efficientnet_v2/__init__.py
 Comment: 
 
-Filename: keras_cv/models/densenet_test.py
+Filename: keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py
 Comment: 
 
-Filename: keras_cv/models/efficientnet_lite.py
+Filename: keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py
 Comment: 
 
-Filename: keras_cv/models/efficientnet_lite_test.py
+Filename: keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets.py
 Comment: 
 
-Filename: keras_cv/models/efficientnet_v1.py
+Filename: keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py
 Comment: 
 
-Filename: keras_cv/models/efficientnet_v1_test.py
+Filename: keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py
 Comment: 
 
-Filename: keras_cv/models/efficientnet_v2.py
+Filename: keras_cv/models/backbones/mobilenet_v3/__init__.py
 Comment: 
 
-Filename: keras_cv/models/efficientnet_v2_test.py
+Filename: keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py
 Comment: 
 
-Filename: keras_cv/models/mlp_mixer.py
+Filename: keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py
 Comment: 
 
-Filename: keras_cv/models/mlp_mixer_test.py
+Filename: keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py
 Comment: 
 
-Filename: keras_cv/models/mobilenet_v3.py
+Filename: keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py
 Comment: 
 
-Filename: keras_cv/models/mobilenet_v3_test.py
+Filename: keras_cv/models/backbones/resnet_v1/__init__.py
 Comment: 
 
-Filename: keras_cv/models/models_test.py
+Filename: keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py
 Comment: 
 
-Filename: keras_cv/models/regnet.py
+Filename: keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py
 Comment: 
 
-Filename: keras_cv/models/regnetx_test.py
+Filename: keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py
 Comment: 
 
-Filename: keras_cv/models/regnety_test.py
+Filename: keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py
 Comment: 
 
-Filename: keras_cv/models/resnet_v1.py
+Filename: keras_cv/models/backbones/resnet_v2/__init__.py
 Comment: 
 
-Filename: keras_cv/models/resnet_v1_test.py
+Filename: keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py
 Comment: 
 
-Filename: keras_cv/models/resnet_v2.py
+Filename: keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py
 Comment: 
 
-Filename: keras_cv/models/resnet_v2_test.py
+Filename: keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py
 Comment: 
 
-Filename: keras_cv/models/utils.py
+Filename: keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py
 Comment: 
 
-Filename: keras_cv/models/utils_test.py
+Filename: keras_cv/models/classification/__init__.py
 Comment: 
 
-Filename: keras_cv/models/vgg16.py
+Filename: keras_cv/models/classification/image_classifier.py
 Comment: 
 
-Filename: keras_cv/models/vgg16_test.py
+Filename: keras_cv/models/classification/image_classifier_presets.py
 Comment: 
 
-Filename: keras_cv/models/vgg19.py
+Filename: keras_cv/models/classification/image_classifier_test.py
 Comment: 
 
-Filename: keras_cv/models/vgg19_test.py
+Filename: keras_cv/models/legacy/__init__.py
 Comment: 
 
-Filename: keras_cv/models/vit.py
+Filename: keras_cv/models/legacy/convmixer.py
 Comment: 
 
-Filename: keras_cv/models/vit_test.py
+Filename: keras_cv/models/legacy/convmixer_test.py
 Comment: 
 
-Filename: keras_cv/models/weights.py
+Filename: keras_cv/models/legacy/convnext.py
 Comment: 
 
-Filename: keras_cv/models/__internal__/__init__.py
+Filename: keras_cv/models/legacy/convnext_test.py
 Comment: 
 
-Filename: keras_cv/models/__internal__/darknet_utils.py
+Filename: keras_cv/models/legacy/darknet.py
 Comment: 
 
-Filename: keras_cv/models/__internal__/unet.py
+Filename: keras_cv/models/legacy/darknet_test.py
 Comment: 
 
-Filename: keras_cv/models/__internal__/unet_test.py
+Filename: keras_cv/models/legacy/densenet.py
+Comment: 
+
+Filename: keras_cv/models/legacy/densenet_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/efficientnet_lite.py
+Comment: 
+
+Filename: keras_cv/models/legacy/efficientnet_lite_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/efficientnet_v1.py
+Comment: 
+
+Filename: keras_cv/models/legacy/efficientnet_v1_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/mlp_mixer.py
+Comment: 
+
+Filename: keras_cv/models/legacy/mlp_mixer_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/models_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/regnet.py
+Comment: 
+
+Filename: keras_cv/models/legacy/regnetx_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/regnety_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/utils.py
+Comment: 
+
+Filename: keras_cv/models/legacy/utils_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/vgg16.py
+Comment: 
+
+Filename: keras_cv/models/legacy/vgg16_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/vgg19.py
+Comment: 
+
+Filename: keras_cv/models/legacy/vgg19_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/vit.py
+Comment: 
+
+Filename: keras_cv/models/legacy/vit_test.py
+Comment: 
+
+Filename: keras_cv/models/legacy/weights.py
+Comment: 
+
+Filename: keras_cv/models/legacy/segmentation/__init__.py
+Comment: 
+
+Filename: keras_cv/models/legacy/segmentation/deeplab.py
+Comment: 
+
+Filename: keras_cv/models/legacy/segmentation/deeplab_test.py
 Comment: 
 
 Filename: keras_cv/models/object_detection/__init__.py
 Comment: 
 
 Filename: keras_cv/models/object_detection/__internal__.py
 Comment: 
 
 Filename: keras_cv/models/object_detection/__test_utils__.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/faster_rcnn.py
+Filename: keras_cv/models/object_detection/predict_utils.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/retinanet/__init__.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/faster_rcnn_test.py
+Filename: keras_cv/models/object_detection/retinanet/feature_pyramid.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/predict_utils.py
+Filename: keras_cv/models/object_detection/retinanet/prediction_head.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/retina_net/__init__.py
+Filename: keras_cv/models/object_detection/retinanet/retinanet.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/retina_net/retina_net.py
+Filename: keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/retina_net/retina_net_inference_test.py
+Filename: keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/retina_net/retina_net_test.py
+Filename: keras_cv/models/object_detection/retinanet/retinanet_presets.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/retina_net/__internal__/__init__.py
+Filename: keras_cv/models/object_detection/retinanet/retinanet_test.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/retina_net/__internal__/layers/__init__.py
+Filename: keras_cv/models/object_detection/yolo_v8/__init__.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/retina_net/__internal__/layers/feature_pyramid.py
+Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/retina_net/__internal__/layers/prediction_head.py
+Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone_presets.py
 Comment: 
 
-Filename: keras_cv/models/object_detection_3d/__init__.py
+Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py
 Comment: 
 
-Filename: keras_cv/models/object_detection_3d/center_pillar.py
+Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py
 Comment: 
 
-Filename: keras_cv/models/object_detection_3d/center_pillar_test.py
+Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_iou_loss.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolox/__init__.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolox/binary_crossentropy.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolox/layers/__init__.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolox/layers/yolox_decoder.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolox/layers/yolox_head.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolox/layers/yolox_head_test.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolox/layers/yolox_label_encoder.py
 Comment: 
 
-Filename: keras_cv/models/segmentation/__init__.py
+Filename: keras_cv/models/object_detection/yolox/layers/yolox_label_encoder_test.py
 Comment: 
 
-Filename: keras_cv/models/segmentation/deeplab.py
+Filename: keras_cv/models/object_detection/yolox/layers/yolox_pafpn.py
+Comment: 
+
+Filename: keras_cv/models/object_detection/yolox/layers/yolox_pafpn_test.py
+Comment: 
+
+Filename: keras_cv/models/object_detection_3d/__init__.py
+Comment: 
+
+Filename: keras_cv/models/object_detection_3d/center_pillar.py
 Comment: 
 
-Filename: keras_cv/models/segmentation/deeplab_test.py
+Filename: keras_cv/models/object_detection_3d/center_pillar_test.py
 Comment: 
 
 Filename: keras_cv/models/stable_diffusion/__init__.py
 Comment: 
 
 Filename: keras_cv/models/stable_diffusion/clip_tokenizer.py
 Comment: 
@@ -975,21 +1095,24 @@
 
 Filename: keras_cv/ops/iou_3d.py
 Comment: 
 
 Filename: keras_cv/ops/iou_3d_test.py
 Comment: 
 
-Filename: keras_cv/ops/point_cloud.py
+Filename: keras_cv/point_cloud/__init__.py
+Comment: 
+
+Filename: keras_cv/point_cloud/point_cloud.py
 Comment: 
 
-Filename: keras_cv/ops/point_cloud_test.py
+Filename: keras_cv/point_cloud/point_cloud_test.py
 Comment: 
 
-Filename: keras_cv/ops/within_box_3d_test.py
+Filename: keras_cv/point_cloud/within_box_3d_test.py
 Comment: 
 
 Filename: keras_cv/training/__init__.py
 Comment: 
 
 Filename: keras_cv/training/contrastive/__init__.py
 Comment: 
@@ -1005,14 +1128,17 @@
 
 Filename: keras_cv/training/contrastive/simclr_trainer_test.py
 Comment: 
 
 Filename: keras_cv/utils/__init__.py
 Comment: 
 
+Filename: keras_cv/utils/conditional_imports.py
+Comment: 
+
 Filename: keras_cv/utils/conv_utils.py
 Comment: 
 
 Filename: keras_cv/utils/fill_utils.py
 Comment: 
 
 Filename: keras_cv/utils/fill_utils_test.py
@@ -1020,38 +1146,56 @@
 
 Filename: keras_cv/utils/preprocessing.py
 Comment: 
 
 Filename: keras_cv/utils/preprocessing_test.py
 Comment: 
 
+Filename: keras_cv/utils/python_utils.py
+Comment: 
+
 Filename: keras_cv/utils/resource_loader.py
 Comment: 
 
 Filename: keras_cv/utils/target_gather.py
 Comment: 
 
 Filename: keras_cv/utils/target_gather_test.py
 Comment: 
 
 Filename: keras_cv/utils/test_utils.py
 Comment: 
 
+Filename: keras_cv/utils/to_numpy.py
+Comment: 
+
 Filename: keras_cv/utils/train.py
 Comment: 
 
-Filename: keras_cv-0.4.2.dist-info/LICENSE
+Filename: keras_cv/visualization/__init__.py
+Comment: 
+
+Filename: keras_cv/visualization/draw_bounding_boxes.py
+Comment: 
+
+Filename: keras_cv/visualization/plot_bounding_box_gallery.py
+Comment: 
+
+Filename: keras_cv/visualization/plot_image_gallery.py
+Comment: 
+
+Filename: keras_cv-0.5.0.dist-info/LICENSE
 Comment: 
 
-Filename: keras_cv-0.4.2.dist-info/METADATA
+Filename: keras_cv-0.5.0.dist-info/METADATA
 Comment: 
 
-Filename: keras_cv-0.4.2.dist-info/WHEEL
+Filename: keras_cv-0.5.0.dist-info/WHEEL
 Comment: 
 
-Filename: keras_cv-0.4.2.dist-info/top_level.txt
+Filename: keras_cv-0.5.0.dist-info/top_level.txt
 Comment: 
 
-Filename: keras_cv-0.4.2.dist-info/RECORD
+Filename: keras_cv-0.5.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## keras_cv/__init__.py

```diff
@@ -14,21 +14,23 @@
 
 # isort:off
 from keras_cv import version_check
 
 version_check.check_tf_version()
 # isort:on
 
+from keras_cv import bounding_box
 from keras_cv import callbacks
 from keras_cv import datasets
 from keras_cv import layers
 from keras_cv import losses
 from keras_cv import metrics
 from keras_cv import models
 from keras_cv import training
 from keras_cv import utils
+from keras_cv import visualization
 from keras_cv.core import ConstantFactorSampler
 from keras_cv.core import FactorSampler
 from keras_cv.core import NormalFactorSampler
 from keras_cv.core import UniformFactorSampler
 
-__version__ = "0.4.2"
+__version__ = "0.5.0"
```

## keras_cv/version_check.py

```diff
@@ -20,12 +20,13 @@
 
 MIN_VERSION = "2.11.0"
 
 
 def check_tf_version():
     if parse(tf.__version__) < parse(MIN_VERSION):
         raise RuntimeError(
-            f"The Tensorflow package version needs to be at least {MIN_VERSION} "
-            "for KerasCV to run. Currently, your TensorFlow version is "
-            f"{tf.__version__}. Please upgrade with `$ pip install --upgrade tensorflow`. "
-            "You can use `pip freeze` to check afterwards that everything is ok."
+            "The Tensorflow package version needs to be at least "
+            f"{MIN_VERSION} for KerasCV to run. Currently, your TensorFlow "
+            f"version is {tf.__version__}. Please upgrade with `$ pip install "
+            "--upgrade tensorflow`. You can use `pip freeze` to check "
+            "afterwards that everything is ok."
         )
```

## keras_cv/version_check_test.py

```diff
@@ -28,15 +28,16 @@
     tf.__version__ = actual_tf_version
 
 
 def test_check_tf_version_error():
     tf.__version__ = "2.8.0"
 
     with pytest.raises(
-        RuntimeError, match="Tensorflow package version needs to be at least 2.11.0"
+        RuntimeError,
+        match="Tensorflow package version needs to be at least 2.11.0",
     ):
         version_check.check_tf_version()
 
 
 def test_check_tf_version_passes_rc2():
     # should pass
     tf.__version__ = "2.11.1rc2"
```

## keras_cv/bounding_box/__init__.py

```diff
@@ -11,21 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from keras_cv.bounding_box.converters import _decode_deltas_to_boxes
 from keras_cv.bounding_box.converters import _encode_box_to_deltas
 from keras_cv.bounding_box.converters import convert_format
+from keras_cv.bounding_box.ensure_tensor import ensure_tensor
 from keras_cv.bounding_box.formats import CENTER_XYWH
 from keras_cv.bounding_box.formats import REL_XYXY
 from keras_cv.bounding_box.formats import REL_YXYX
 from keras_cv.bounding_box.formats import XYWH
 from keras_cv.bounding_box.formats import XYXY
 from keras_cv.bounding_box.formats import YXYX
 from keras_cv.bounding_box.iou import compute_iou
-from keras_cv.bounding_box.mask_invalid_detections import mask_invalid_detections
+from keras_cv.bounding_box.mask_invalid_detections import (
+    mask_invalid_detections,
+)
 from keras_cv.bounding_box.to_dense import to_dense
 from keras_cv.bounding_box.to_ragged import to_ragged
 from keras_cv.bounding_box.utils import as_relative
 from keras_cv.bounding_box.utils import clip_to_image
 from keras_cv.bounding_box.utils import is_relative
 from keras_cv.bounding_box.validate_format import validate_format
```

## keras_cv/bounding_box/converters.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,52 +14,55 @@
 """Converter functions for working with bounding box formats."""
 
 from typing import List
 from typing import Optional
 from typing import Union
 
 import tensorflow as tf
+from tensorflow import keras
 
 
-# Internal exception to propagate the fact images was not passed to a converter that
-# needs it
+# Internal exception to propagate the fact images was not passed to a converter
+# that needs it.
 class RequiresImagesException(Exception):
     pass
 
 
 ALL_AXES = [1, 1, 1, 1]
 
 
 def _encode_box_to_deltas(
     anchors: tf.Tensor,
     boxes: tf.Tensor,
     anchor_format: str,
     box_format: str,
     variance: Optional[Union[List[float], tf.Tensor]] = None,
+    image_shape=None,
 ):
     """Converts bounding_boxes from `center_yxhw` to delta format."""
     if variance is not None:
         if tf.is_tensor(variance):
             var_len = variance.get_shape().as_list()[-1]
         else:
             var_len = len(variance)
         if var_len != 4:
             raise ValueError(f"`variance` must be length 4, got {variance}")
     encoded_anchors = convert_format(
         anchors,
         source=anchor_format,
         target="center_yxhw",
+        image_shape=image_shape,
     )
     boxes = convert_format(
-        boxes,
-        source=box_format,
-        target="center_yxhw",
+        boxes, source=box_format, target="center_yxhw", image_shape=image_shape
+    )
+    anchor_dimensions = tf.maximum(
+        encoded_anchors[..., 2:], keras.backend.epsilon()
     )
-    anchor_dimensions = tf.maximum(encoded_anchors[..., 2:], tf.keras.backend.epsilon())
-    box_dimensions = tf.maximum(boxes[..., 2:], tf.keras.backend.epsilon())
+    box_dimensions = tf.maximum(boxes[..., 2:], keras.backend.epsilon())
     # anchors be unbatched, boxes can either be batched or unbatched.
     boxes_delta = tf.concat(
         [
             (boxes[..., :2] - encoded_anchors[..., :2]) / anchor_dimensions,
             tf.math.log(box_dimensions / anchor_dimensions),
         ],
         axis=-1,
@@ -71,14 +74,15 @@
 
 def _decode_deltas_to_boxes(
     anchors: tf.Tensor,
     boxes_delta: tf.Tensor,
     anchor_format: str,
     box_format: str,
     variance: Optional[Union[List[float], tf.Tensor]] = None,
+    image_shape=None,
 ):
     """Converts bounding_boxes from delta format to `center_yxhw`."""
     if variance is not None:
         if tf.is_tensor(variance):
             var_len = variance.get_shape().as_list()[-1]
         else:
             var_len = len(variance)
@@ -87,26 +91,33 @@
     tf.nest.assert_same_structure(anchors, boxes_delta)
 
     def decode_single_level(anchor, box_delta):
         encoded_anchor = convert_format(
             anchor,
             source=anchor_format,
             target="center_yxhw",
+            image_shape=image_shape,
         )
         if variance is not None:
             box_delta = box_delta * variance
         # anchors be unbatched, boxes can either be batched or unbatched.
         box = tf.concat(
             [
-                box_delta[..., :2] * encoded_anchor[..., 2:] + encoded_anchor[..., :2],
+                box_delta[..., :2] * encoded_anchor[..., 2:]
+                + encoded_anchor[..., :2],
                 tf.math.exp(box_delta[..., 2:]) * encoded_anchor[..., 2:],
             ],
             axis=-1,
         )
-        box = convert_format(box, source="center_yxhw", target=box_format)
+        box = convert_format(
+            box,
+            source="center_yxhw",
+            target=box_format,
+            image_shape=image_shape,
+        )
         return box
 
     if isinstance(anchors, dict) and isinstance(boxes_delta, dict):
         boxes = {}
         for lvl, anchor in anchors.items():
             boxes[lvl] = decode_single_level(anchor, boxes_delta[lvl])
         return boxes
@@ -134,15 +145,20 @@
     x, y, width, height = tf.split(boxes, ALL_AXES, axis=-1)
     return tf.concat([x, y, x + width, y + height], axis=-1)
 
 
 def _xyxy_to_center_yxhw(boxes, images=None, image_shape=None):
     left, top, right, bottom = tf.split(boxes, ALL_AXES, axis=-1)
     return tf.concat(
-        [(top + bottom) / 2.0, (left + right) / 2.0, bottom - top, right - left],
+        [
+            (top + bottom) / 2.0,
+            (left + right) / 2.0,
+            bottom - top,
+            right - left,
+        ],
         axis=-1,
     )
 
 
 def _rel_xywh_to_xyxy(boxes, images=None, image_shape=None):
     image_height, image_width = _image_shape(images, image_shape, boxes)
     x, y, width, height = tf.split(boxes, ALL_AXES, axis=-1)
@@ -182,15 +198,20 @@
         axis=-1,
     )
 
 
 def _xyxy_to_center_xywh(boxes, images=None, image_shape=None):
     left, top, right, bottom = tf.split(boxes, ALL_AXES, axis=-1)
     return tf.concat(
-        [(left + right) / 2.0, (top + bottom) / 2.0, right - left, bottom - top],
+        [
+            (left + right) / 2.0,
+            (top + bottom) / 2.0,
+            right - left,
+            bottom - top,
+        ],
         axis=-1,
     )
 
 
 def _rel_xyxy_to_xyxy(boxes, images=None, image_shape=None):
     image_height, image_width = _image_shape(images, image_shape, boxes)
     left, top, right, bottom = tf.split(
@@ -282,72 +303,74 @@
 
 def convert_format(
     boxes, source, target, images=None, image_shape=None, dtype="float32"
 ):
     f"""Converts bounding_boxes from one format to another.
 
     Supported formats are:
-    - `"xyxy"`, also known as `corners` format.  In this format the first four axes
-        represent `[left, top, right, bottom]` in that order.
-    - `"rel_xyxy"`.  In this format, the axes are the same as `"xyxy"` but the x
-        coordinates are normalized using the image width, and the y axes the image
-        height.  All values in `rel_xyxy` are in the range `(0, 1)`.
-    - `"xywh"`.  In this format the first four axes represent
+    - `"xyxy"`, also known as `corners` format. In this format the first four
+        axes represent `[left, top, right, bottom]` in that order.
+    - `"rel_xyxy"`. In this format, the axes are the same as `"xyxy"` but the x
+        coordinates are normalized using the image width, and the y axes the
+        image height. All values in `rel_xyxy` are in the range `(0, 1)`.
+    - `"xywh"`. In this format the first four axes represent
         `[left, top, width, height]`.
-    - `"rel_xywh".  In this format the first four axes represent
-        [left, top, width, height], just like `"xywh"`.  Unlike `"xywh"`, the values
-        are in the range (0, 1) instead of absolute pixel values.
-    - `"center_xyWH"`.  In this format the first two coordinates represent the x and y
-        coordinates of the center of the bounding box, while the last two represent
-        the width and height of the bounding box.
-    - `"center_yxHW"`.  In this format the first two coordinates represent the y and x
-        coordinates of the center of the bounding box, while the last two represent
-        the height and width of the bounding box.
-    - `"yxyx"`.  In this format the first four axes represent [top, left, bottom, right]
-        in that order.
-    - `"rel_yxyx"`.  In this format, the axes are the same as `"yxyx"` but the x
-        coordinates are normalized using the image width, and the y axes the image
-        height.  All values in `rel_yxyx` are in the range (0, 1).
-    Formats are case insensitive.  It is recommended that you capitalize width and
-    height to maximize the visual difference between `"xyWH"` and `"xyxy"`.
-
-    Relative formats, abbreviated `rel`, make use of the shapes of the `images` passed.
-    In these formats, the coordinates, widths, and heights are all specified as
-    percentages of the host image.  `images` may be a ragged Tensor.  Note that using a
-    ragged Tensor for images may cause a substantial performance loss, as each image
-    will need to be processed separately due to the mismatching image shapes.
+    - `"rel_xywh". In this format the first four axes represent
+        [left, top, width, height], just like `"xywh"`. Unlike `"xywh"`, the
+        values are in the range (0, 1) instead of absolute pixel values.
+    - `"center_xyWH"`. In this format the first two coordinates represent the x
+        and y coordinates of the center of the bounding box, while the last two
+        represent the width and height of the bounding box.
+    - `"center_yxHW"`. In this format the first two coordinates represent the y
+        and x coordinates of the center of the bounding box, while the last two
+        represent the height and width of the bounding box.
+    - `"yxyx"`. In this format the first four axes represent
+        [top, left, bottom, right] in that order.
+    - `"rel_yxyx"`. In this format, the axes are the same as `"yxyx"` but the x
+        coordinates are normalized using the image width, and the y axes the
+        image height. All values in `rel_yxyx` are in the range (0, 1).
+    Formats are case insensitive. It is recommended that you capitalize width
+    and height to maximize the visual difference between `"xyWH"` and `"xyxy"`.
+
+    Relative formats, abbreviated `rel`, make use of the shapes of the `images`
+    passed. In these formats, the coordinates, widths, and heights are all
+    specified as percentages of the host image. `images` may be a ragged
+    Tensor. Note that using a ragged Tensor for images may cause a substantial
+    performance loss, as each image will need to be processed separately due to
+    the mismatching image shapes.
 
     Usage:
 
     ```python
     boxes = load_coco_dataset()
     boxes_in_xywh = keras_cv.bounding_box.convert_format(
         boxes,
         source='xyxy',
         target='xyWH'
     )
     ```
 
     Args:
-        boxes: tf.Tensor representing bounding boxes in the format specified in the
-            `source` parameter.  `boxes` can optionally have extra dimensions stacked on
-             the final axis to store metadata.  boxes should be a 3D Tensor, with the
-             shape `[batch_size, num_boxes, 4]`.  Alternatively, boxes can be a
-             dictionary with key 'boxes' containing a Tensor matching the aforementioned
-             spec.
-        source: One of {" ".join([f'"{f}"' for f in TO_XYXY_CONVERTERS.keys()])}.  Used
-            to specify the original format of the `boxes` parameter.
-        target: One of {" ".join([f'"{f}"' for f in TO_XYXY_CONVERTERS.keys()])}.  Used
-            to specify the destination format of the `boxes` parameter.
-        images: (Optional) a batch of images aligned with `boxes` on the first axis.
-            Should be at least 3 dimensions, with the first 3 dimensions representing:
-            `[batch_size, height, width]`.  Used in some converters to compute relative
-            pixel values of the bounding box dimensions.  Required when transforming
-            from a rel format to a non-rel format.
-        dtype: the data type to use when transforming the boxes.  Defaults to
+        boxes: tf.Tensor representing bounding boxes in the format specified in
+            the `source` parameter. `boxes` can optionally have extra
+            dimensions stacked on the final axis to store metadata. boxes
+            should be a 3D Tensor, with the shape `[batch_size, num_boxes, 4]`.
+            Alternatively, boxes can be a dictionary with key 'boxes' containing
+            a Tensor matching the aforementioned spec.
+        source:One of {" ".join([f'"{f}"' for f in TO_XYXY_CONVERTERS.keys()])}.
+            Used to specify the original format of the `boxes` parameter.
+        target:One of {" ".join([f'"{f}"' for f in TO_XYXY_CONVERTERS.keys()])}.
+            Used to specify the destination format of the `boxes` parameter.
+        images: (Optional) a batch of images aligned with `boxes` on the first
+            axis. Should be at least 3 dimensions, with the first 3 dimensions
+            representing: `[batch_size, height, width]`. Used in some
+            converters to compute relative pixel values of the bounding box
+            dimensions. Required when transforming from a rel format to a
+            non-rel format.
+        dtype: the data type to use when transforming the boxes, defaults to
             `tf.float32`.
     """
     if isinstance(boxes, dict):
         boxes["boxes"] = convert_format(
             boxes["boxes"],
             source=source,
             target=target,
@@ -355,38 +378,38 @@
             image_shape=image_shape,
             dtype=dtype,
         )
         return boxes
 
     if boxes.shape[-1] != 4:
         raise ValueError(
-            "Expected `boxes` to be a Tensor with a "
-            f"final dimension of `4`. Instead, got `boxes.shape={boxes.shape}`."
+            "Expected `boxes` to be a Tensor with a final dimension of "
+            f"`4`. Instead, got `boxes.shape={boxes.shape}`."
         )
     if images is not None and image_shape is not None:
         raise ValueError(
-            "convert_format() expects either `images` or `image_shape`, "
-            f"but not both.  Received images={images} image_shape={image_shape}"
+            "convert_format() expects either `images` or `image_shape`, but "
+            f"not both. Received images={images} image_shape={image_shape}"
         )
 
     _validate_image_shape(image_shape)
 
     source = source.lower()
     target = target.lower()
     if source not in TO_XYXY_CONVERTERS:
         raise ValueError(
-            f"`convert_format()` received an unsupported format for the argument "
-            f"`source`.  `source` should be one of {TO_XYXY_CONVERTERS.keys()}. "
-            f"Got source={source}"
+            "`convert_format()` received an unsupported format for the "
+            "argument `source`. `source` should be one of "
+            f"{TO_XYXY_CONVERTERS.keys()}. Got source={source}"
         )
     if target not in FROM_XYXY_CONVERTERS:
         raise ValueError(
-            f"`convert_format()` received an unsupported format for the argument "
-            f"`target`.  `target` should be one of {FROM_XYXY_CONVERTERS.keys()}. "
-            f"Got target={target}"
+            "`convert_format()` received an unsupported format for the "
+            "argument `target`. `target` should be one of "
+            f"{FROM_XYXY_CONVERTERS.keys()}. Got target={target}"
         )
 
     boxes = tf.cast(boxes, dtype)
     if source == target:
         return boxes
 
     # rel->rel conversions should not require images
@@ -399,16 +422,16 @@
     from_xyxy_fn = FROM_XYXY_CONVERTERS[target]
 
     try:
         in_xyxy = to_xyxy_fn(boxes, images=images, image_shape=image_shape)
         result = from_xyxy_fn(in_xyxy, images=images, image_shape=image_shape)
     except RequiresImagesException:
         raise ValueError(
-            "convert_format() must receive `images` or `image_shape` when transforming "
-            f"between relative and absolute formats."
+            "convert_format() must receive `images` or `image_shape` when "
+            "transforming between relative and absolute formats."
             f"convert_format() received source=`{format}`, target=`{format}, "
             f"but images={images} and image_shape={image_shape}."
         )
 
     return _format_outputs(result, squeeze)
 
 
@@ -427,18 +450,20 @@
             raise ValueError(
                 "Expected len(images.shape)=2, or len(images.shape)=3, got "
                 f"len(images.shape)={images_rank}"
             )
         images_include_batch = images_rank == 4
         if boxes_includes_batch != images_include_batch:
             raise ValueError(
-                "convert_format() expects both boxes and images to be batched, or both "
-                f"boxes and images to be unbatched.  Received len(boxes.shape)={boxes_rank}, "
-                f"len(images.shape)={images_rank}.  Expected either len(boxes.shape)=2 AND "
-                "len(images.shape)=3, or len(boxes.shape)=3 AND len(images.shape)=4."
+                "convert_format() expects both boxes and images to be batched, "
+                "or both boxes and images to be unbatched. Received "
+                f"len(boxes.shape)={boxes_rank}, "
+                f"len(images.shape)={images_rank}. Expected either "
+                "len(boxes.shape)=2 AND len(images.shape)=3, or "
+                "len(boxes.shape)=3 AND len(images.shape)=4."
             )
         if not images_include_batch:
             images = tf.expand_dims(images, axis=0)
 
     if not boxes_includes_batch:
         return tf.expand_dims(boxes, axis=0), images, True
     return boxes, images, False
@@ -469,15 +494,15 @@
                 "image_shape.shape should be (3), but got "
                 f"image_shape.shape={image_shape.shape}"
             )
         return
 
     # Warn about failure cases
     raise ValueError(
-        "Expected image_shape to be either a tuple, list, Tensor.  "
+        "Expected image_shape to be either a tuple, list, Tensor. "
         f"Received image_shape={image_shape}"
     )
 
 
 def _format_outputs(boxes, squeeze):
     if squeeze:
         return tf.squeeze(boxes, axis=0)
@@ -490,14 +515,15 @@
 
     if image_shape is None:
         if not isinstance(images, tf.RaggedTensor):
             image_shape = tf.shape(images)
             height, width = image_shape[1], image_shape[2]
         else:
             height = tf.reshape(images.row_lengths(), (-1, 1))
-            width = tf.reshape(tf.reduce_max(images.row_lengths(axis=2), 1), (-1, 1))
-            if isinstance(boxes, tf.RaggedTensor):
-                height = tf.expand_dims(height, axis=-1)
-                width = tf.expand_dims(width, axis=-1)
+            width = tf.reshape(
+                tf.reduce_max(images.row_lengths(axis=2), 1), (-1, 1)
+            )
+            height = tf.expand_dims(height, axis=-1)
+            width = tf.expand_dims(width, axis=-1)
     else:
         height, width = image_shape[0], image_shape[1]
     return tf.cast(height, boxes.dtype), tf.cast(width, boxes.dtype)
```

## keras_cv/bounding_box/converters_test.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,16 +16,20 @@
 
 import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_cv import bounding_box
 
-xyxy_box = tf.constant([[[10, 20, 110, 120], [20, 30, 120, 130]]], dtype=tf.float32)
-yxyx_box = tf.constant([[[20, 10, 120, 110], [30, 20, 130, 120]]], dtype=tf.float32)
+xyxy_box = tf.constant(
+    [[[10, 20, 110, 120], [20, 30, 120, 130]]], dtype=tf.float32
+)
+yxyx_box = tf.constant(
+    [[[20, 10, 120, 110], [30, 20, 130, 120]]], dtype=tf.float32
+)
 rel_xyxy_box = tf.constant(
     [[[0.01, 0.02, 0.11, 0.12], [0.02, 0.03, 0.12, 0.13]]], dtype=tf.float32
 )
 rel_xyxy_box_ragged_images = tf.constant(
     [[[0.10, 0.20, 1.1, 1.20], [0.40, 0.6, 2.40, 2.6]]], dtype=tf.float32
 )
 rel_yxyx_box = tf.constant(
@@ -33,29 +37,33 @@
 )
 rel_yxyx_box_ragged_images = tf.constant(
     [[[0.2, 0.1, 1.2, 1.1], [0.6, 0.4, 2.6, 2.4]]], dtype=tf.float32
 )
 center_xywh_box = tf.constant(
     [[[60, 70, 100, 100], [70, 80, 100, 100]]], dtype=tf.float32
 )
-xywh_box = tf.constant([[[10, 20, 100, 100], [20, 30, 100, 100]]], dtype=tf.float32)
+xywh_box = tf.constant(
+    [[[10, 20, 100, 100], [20, 30, 100, 100]]], dtype=tf.float32
+)
 rel_xywh_box = tf.constant(
     [[[0.01, 0.02, 0.1, 0.1], [0.02, 0.03, 0.1, 0.1]]], dtype=tf.float32
 )
 rel_xywh_box_ragged_images = tf.constant(
     [[[0.1, 0.2, 1, 1], [0.4, 0.6, 2, 2]]], dtype=tf.float32
 )
 
 ragged_images = tf.ragged.constant(
     [np.ones(shape=[100, 100, 3]), np.ones(shape=[50, 50, 3])],  # 2 images
     ragged_rank=2,
 )
 
 images = tf.ones([2, 1000, 1000, 3])
 
+ragged_classes = tf.ragged.constant([[0], [0]], dtype=tf.float32)
+
 boxes = {
     "xyxy": xyxy_box,
     "center_xywh": center_xywh_box,
     "rel_xywh": rel_xywh_box,
     "xywh": xywh_box,
     "rel_xyxy": rel_xyxy_box,
     "yxyx": yxyx_box,
@@ -75,15 +83,17 @@
 test_cases = [
     (f"{source}_{target}", source, target)
     for (source, target) in itertools.permutations(boxes.keys(), 2)
 ] + [("xyxy_xyxy", "xyxy", "xyxy")]
 
 test_image_ragged = [
     (f"{source}_{target}", source, target)
-    for (source, target) in itertools.permutations(boxes_ragged_images.keys(), 2)
+    for (source, target) in itertools.permutations(
+        boxes_ragged_images.keys(), 2
+    )
 ] + [("xyxy_xyxy", "xyxy", "xyxy")]
 
 
 class ConvertersTestCase(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(*test_cases)
     def test_converters(self, source, target):
         source_box = boxes[source]
@@ -94,16 +104,16 @@
                 source_box, source=source, target=target, images=images
             ),
             target_box,
         )
 
     @parameterized.named_parameters(*test_image_ragged)
     def test_converters_ragged_images(self, source, target):
-        source_box = boxes_ragged_images[source]
-        target_box = boxes_ragged_images[target]
+        source_box = _raggify(boxes_ragged_images[source])
+        target_box = _raggify(boxes_ragged_images[target])
         self.assertAllClose(
             bounding_box.convert_format(
                 source_box, source=source, target=target, images=ragged_images
             ),
             target_box,
         )
 
@@ -126,15 +136,17 @@
                 source_box, source="xyxy", target="xywh", images=images
             )
 
     def test_without_images(self):
         source_box = boxes["xyxy"]
         target_box = boxes["xywh"]
         self.assertAllClose(
-            bounding_box.convert_format(source_box, source="xyxy", target="xywh"),
+            bounding_box.convert_format(
+                source_box, source="xyxy", target="xywh"
+            ),
             target_box,
         )
 
     def test_rel_to_rel_without_images(self):
         source_box = boxes["rel_xyxy"]
         target_box = boxes["rel_yxyx"]
         self.assertAllClose(
@@ -168,17 +180,40 @@
 
     @parameterized.named_parameters(*test_cases)
     def test_ragged_bounding_box_with_image_shape(self, source, target):
         source_box = _raggify(boxes[source])
         target_box = _raggify(boxes[target])
         self.assertAllClose(
             bounding_box.convert_format(
-                source_box, source=source, target=target, image_shape=(1000, 1000, 3)
+                source_box,
+                source=source,
+                target=target,
+                image_shape=(1000, 1000, 3),
             ),
             target_box,
         )
 
+    @parameterized.named_parameters(*test_image_ragged)
+    def test_dense_bounding_box_with_ragged_images(self, source, target):
+        source_box = _raggify(boxes_ragged_images[source])
+        target_box = _raggify(boxes_ragged_images[target])
+        source_bounding_boxes = {"boxes": source_box, "classes": ragged_classes}
+        source_bounding_boxes = bounding_box.to_dense(source_bounding_boxes)
+
+        result_bounding_boxes = bounding_box.convert_format(
+            source_bounding_boxes,
+            source=source,
+            target=target,
+            images=ragged_images,
+        )
+        result_bounding_boxes = bounding_box.to_ragged(result_bounding_boxes)
+
+        self.assertAllClose(
+            result_bounding_boxes["boxes"],
+            target_box,
+        )
+
 
 def _raggify(tensor):
     tensor = tf.squeeze(tensor, axis=0)
     tensor = tf.RaggedTensor.from_row_lengths(tensor, [1, 1])
     return tensor
```

## keras_cv/bounding_box/formats.py

```diff
@@ -19,36 +19,36 @@
 class XYXY:
     """XYXY contains axis indices for the XYXY format.
 
     All values in the XYXY format should be absolute pixel values.
 
     The XYXY format consists of the following required indices:
 
-    - LEFT: left hand side of the bounding box
+    - LEFT: left of the bounding box
     - TOP: top of the bounding box
     - RIGHT: right of the bounding box
     - BOTTOM: bottom of the bounding box
     """
 
     LEFT = 0
     TOP = 1
     RIGHT = 2
     BOTTOM = 3
 
 
 class REL_XYXY:
     """REL_XYXY contains axis indices for the REL_XYXY format.
 
-    REL_XYXY is like XYXY, but each value is relative to the width and height of the
-    origin image.  Values are percentages of the origin images' width and height
-    respectively.
+    REL_XYXY is like XYXY, but each value is relative to the width and height of
+    the origin image. Values are percentages of the origin images' width and
+    height respectively.
 
     The REL_XYXY format consists of the following required indices:
 
-    - LEFT: left hand side of the bounding box
+    - LEFT: left of the bounding box
     - TOP: top of the bounding box
     - RIGHT: right of the bounding box
     - BOTTOM: bottom of the bounding box
     """
 
     LEFT = 0
     TOP = 1
@@ -93,17 +93,17 @@
     WIDTH = 2
     HEIGHT = 3
 
 
 class REL_XYWH:
     """REL_XYWH contains axis indices for the XYWH format.
 
-    REL_XYXY is like XYWH, but each value is relative to the width and height of the
-    origin image.  Values are percentages of the origin images' width and height
-    respectively.
+    REL_XYXY is like XYWH, but each value is relative to the width and height of
+    the origin image. Values are percentages of the origin images' width and
+    height respectively.
 
     - X: X coordinate of the left of the bounding box
     - Y: Y coordinate of the top of the bounding box
     - WIDTH: width of the bounding box
     - HEIGHT: height of the bounding box
     """
 
@@ -117,36 +117,36 @@
     """YXYX contains axis indices for the YXYX format.
 
     All values in the YXYX format should be absolute pixel values.
 
     The YXYX format consists of the following required indices:
 
     - TOP: top of the bounding box
-    - LEFT: left hand side of the bounding box
+    - LEFT: left of the bounding box
     - BOTTOM: bottom of the bounding box
     - RIGHT: right of the bounding box
     """
 
     TOP = 0
     LEFT = 1
     BOTTOM = 2
     RIGHT = 3
 
 
 class REL_YXYX:
     """REL_YXYX contains axis indices for the REL_YXYX format.
 
-    REL_YXYX is like YXYX, but each value is relative to the width and height of the
-    origin image.  Values are percentages of the origin images' width and height
-    respectively.
+    REL_YXYX is like YXYX, but each value is relative to the width and height of
+    the origin image. Values are percentages of the origin images' width and
+    height respectively.
 
     The REL_YXYX format consists of the following required indices:
 
     - TOP: top of the bounding box
-    - LEFT: left hand side of the bounding box
+    - LEFT: left of the bounding box
     - BOTTOM: bottom of the bounding box
     - RIGHT: right of the bounding box
     """
 
     TOP = 0
     LEFT = 1
     BOTTOM = 2
```

## keras_cv/bounding_box/iou.py

```diff
@@ -22,15 +22,17 @@
 
     Args:
       box: [N, 4] or [batch_size, N, 4] float Tensor, either batched
         or unbatched boxes.
     Returns:
       a float Tensor of [N] or [batch_size, N]
     """
-    y_min, x_min, y_max, x_max = tf.split(box[..., :4], num_or_size_splits=4, axis=-1)
+    y_min, x_min, y_max, x_max = tf.split(
+        box[..., :4], num_or_size_splits=4, axis=-1
+    )
     return tf.squeeze((y_max - y_min) * (x_max - x_min), axis=-1)
 
 
 def _compute_intersection(boxes1, boxes2):
     """Computes intersection area between two sets of boxes.
 
     Args:
@@ -64,71 +66,88 @@
 
 def compute_iou(
     boxes1,
     boxes2,
     bounding_box_format,
     use_masking=False,
     mask_val=-1,
+    images=None,
+    image_shape=None,
 ):
     """Computes a lookup table vector containing the ious for a given set boxes.
 
-    The lookup vector is to be indexed by [`boxes1_index`,`boxes2_index`] if boxes
-    are unbatched and by [`batch`, `boxes1_index`,`boxes2_index`] if the boxes are
-    batched.
+    The lookup vector is to be indexed by [`boxes1_index`,`boxes2_index`] if
+    boxes are unbatched and by [`batch`, `boxes1_index`,`boxes2_index`] if the
+    boxes are batched.
 
     The users can pass `boxes1` and `boxes2` to be different ranks. For example:
-    1) `boxes1`: [batch_size, M, 4], `boxes2`: [batch_size, N, 4] -> return [batch_size, M, N].
-    2) `boxes1`: [batch_size, M, 4], `boxes2`: [N, 4] -> return [batch_size, M, N]
-    3) `boxes1`: [M, 4], `boxes2`: [batch_size, N, 4] -> return [batch_size, M, N]
+    1) `boxes1`: [batch_size, M, 4], `boxes2`: [batch_size, N, 4] -> return
+        [batch_size, M, N].
+    2) `boxes1`: [batch_size, M, 4], `boxes2`: [N, 4] -> return
+        [batch_size, M, N]
+    3) `boxes1`: [M, 4], `boxes2`: [batch_size, N, 4] -> return
+        [batch_size, M, N]
     4) `boxes1`: [M, 4], `boxes2`: [N, 4] -> return [M, N]
 
     Args:
-      boxes1: a list of bounding boxes in 'corners' format. Can be batched or unbatched.
-      boxes2: a list of bounding boxes in 'corners' format. Can be batched or unbatched.
+      boxes1: a list of bounding boxes in 'corners' format. Can be batched or
+        unbatched.
+      boxes2: a list of bounding boxes in 'corners' format. Can be batched or
+        unbatched.
       bounding_box_format: a case-insensitive string which is one of `"xyxy"`,
         `"rel_xyxy"`, `"xyWH"`, `"center_xyWH"`, `"yxyx"`, `"rel_yxyx"`.
         For detailed information on the supported format, see the
         [KerasCV bounding box documentation](https://keras.io/api/keras_cv/bounding_box/formats/).
-    use_masking: whether masking will be applied. This will mask all `boxes1` or `boxes2` that
-        have values less then 0 in all its 4 dimensions. Default to `False`.
-    mask_val: int to mask those returned IOUs if the masking is True. Default to -1.
+      use_masking: whether masking will be applied. This will mask all `boxes1`
+        or `boxes2` that have values less than 0 in all its 4 dimensions.
+        Default to `False`.
+      mask_val: int to mask those returned IOUs if the masking is True, defaults
+        to -1.
 
     Returns:
       iou_lookup_table: a vector containing the pairwise ious of boxes1 and
         boxes2.
-    """
+    """  # noqa: E501
 
     boxes1_rank = len(boxes1.shape)
     boxes2_rank = len(boxes2.shape)
 
     if boxes1_rank not in [2, 3]:
         raise ValueError(
-            "compute_iou() expects boxes1 to be batched, or "
-            f"to be unbatched. Received len(boxes1.shape)={boxes1_rank}, "
-            f"len(boxes2.shape)={boxes2_rank}. Expected either len(boxes1.shape)=2 AND "
-            "or len(boxes1.shape)=3."
+            "compute_iou() expects boxes1 to be batched, or to be unbatched. "
+            f"Received len(boxes1.shape)={boxes1_rank}, "
+            f"len(boxes2.shape)={boxes2_rank}. Expected either "
+            "len(boxes1.shape)=2 AND or len(boxes1.shape)=3."
         )
     if boxes2_rank not in [2, 3]:
         raise ValueError(
-            "compute_iou() expects boxes2 to be batched, or "
-            f"to be unbatched. Received len(boxes1.shape)={boxes1_rank}, "
-            f"len(boxes2.shape)={boxes2_rank}. Expected either len(boxes2.shape)=2 AND "
-            "or len(boxes2.shape)=3."
+            "compute_iou() expects boxes2 to be batched, or to be unbatched. "
+            f"Received len(boxes1.shape)={boxes1_rank}, "
+            f"len(boxes2.shape)={boxes2_rank}. Expected either "
+            "len(boxes2.shape)=2 AND or len(boxes2.shape)=3."
         )
 
     target_format = "yxyx"
     if bounding_box.is_relative(bounding_box_format):
         target_format = bounding_box.as_relative(target_format)
 
     boxes1 = bounding_box.convert_format(
-        boxes1, source=bounding_box_format, target=target_format
+        boxes1,
+        source=bounding_box_format,
+        target=target_format,
+        images=images,
+        image_shape=image_shape,
     )
 
     boxes2 = bounding_box.convert_format(
-        boxes2, source=bounding_box_format, target=target_format
+        boxes2,
+        source=bounding_box_format,
+        target=target_format,
+        images=images,
+        image_shape=image_shape,
     )
 
     intersect_area = _compute_intersection(boxes1, boxes2)
     boxes1_area = _compute_area(boxes1)
     boxes2_area = _compute_area(boxes2)
     boxes2_area_rank = len(boxes2_area.shape)
     boxes2_axis = 1 if (boxes2_area_rank == 2) else 0
@@ -144,10 +163,12 @@
 
     if not use_masking:
         return res
 
     mask_val_t = tf.cast(mask_val, res.dtype) * tf.ones_like(res)
     boxes1_mask = tf.less(tf.reduce_max(boxes1, axis=-1, keepdims=True), 0.0)
     boxes2_mask = tf.less(tf.reduce_max(boxes2, axis=-1, keepdims=True), 0.0)
-    background_mask = tf.logical_or(boxes1_mask, tf.transpose(boxes2_mask, perm))
+    background_mask = tf.logical_or(
+        boxes1_mask, tf.transpose(boxes2_mask, perm)
+    )
     iou_lookup_table = tf.where(background_mask, mask_val_t, res)
     return iou_lookup_table
```

## keras_cv/bounding_box/iou_test.py

```diff
@@ -77,16 +77,24 @@
                 [bb1, top_left_bounding_box, far_away_box],
                 [bb1, top_left_bounding_box, far_away_box],
             ],
             dtype=tf.float32,
         )
         sample_y_pred = tf.constant(
             [
-                [bb1_off_by_1_pred, top_left_bounding_box, another_far_away_pred],
-                [bb1_off_by_1_pred, top_left_bounding_box, another_far_away_pred],
+                [
+                    bb1_off_by_1_pred,
+                    top_left_bounding_box,
+                    another_far_away_pred,
+                ],
+                [
+                    bb1_off_by_1_pred,
+                    top_left_bounding_box,
+                    another_far_away_pred,
+                ],
             ],
             dtype=tf.float32,
         )
 
         result = iou_lib.compute_iou(sample_y_true, sample_y_pred, "yxyx")
         self.assertAllClose(expected_result, result.numpy())
 
@@ -143,15 +151,23 @@
             [
                 [bb1, top_left_bounding_box, far_away_box],
             ],
             dtype=tf.float32,
         )
         sample_y_pred = tf.constant(
             [
-                [bb1_off_by_1_pred, top_left_bounding_box, another_far_away_pred],
-                [bb1_off_by_1_pred, top_left_bounding_box, another_far_away_pred],
+                [
+                    bb1_off_by_1_pred,
+                    top_left_bounding_box,
+                    another_far_away_pred,
+                ],
+                [
+                    bb1_off_by_1_pred,
+                    top_left_bounding_box,
+                    another_far_away_pred,
+                ],
             ],
             dtype=tf.float32,
         )
 
         result = iou_lib.compute_iou(sample_y_true, sample_y_pred, "yxyx")
         self.assertAllClose(expected_result, result.numpy())
```

## keras_cv/bounding_box/mask_invalid_detections.py

```diff
@@ -16,69 +16,80 @@
 from keras_cv.bounding_box.to_ragged import to_ragged
 from keras_cv.bounding_box.validate_format import validate_format
 
 
 def mask_invalid_detections(bounding_boxes, output_ragged=False):
     """masks out invalid detections with -1s.
 
-    This utility is mainly used on the output of `tf.image.combined_non_max_suppression`
-    operations.  The output of `tf.image.combined_non_max_suppression` contains all of
-    the detections, even invalid ones.  Users are expected to use `num_detections` to
-    determine how many boxes are in each image.
+    This utility is mainly used on the output of
+    `tf.image.combined_non_max_suppression` operations. The output of
+    `tf.image.combined_non_max_suppression` contains all the detections, even
+    invalid ones. Users are expected to use `num_detections` to determine how
+    many boxes are in each image.
 
     In contrast, KerasCV expects all bounding boxes to be padded with -1s.
     This function uses the value of `num_detections` to mask out
     invalid boxes with -1s.
 
     Args:
-        bounding_boxes: a dictionary complying with KerasCV bounding box format.  In
-            addition to the normal required keys, these boxes are also expected to have
-            a `num_detections` key.
-        output_ragged: whether or not to output RaggedTensor based bounding boxes.
+        bounding_boxes: a dictionary complying with KerasCV bounding box format.
+            In addition to the normal required keys, these boxes are also
+            expected to have a `num_detections` key.
+        output_ragged: whether to output RaggedTensor based bounding
+            boxes.
     Returns:
-        bounding boxes with proper masking of the boxes according to `num_detections`.
-        This allows proper interop with `tf.image.combined_non_max_suppression`.
-        Returned boxes match the specification fed to the function, so if the bounding
-        box tensor uses `tf.RaggedTensor` to represent boxes the returned value will
-        also return `tf.RaggedTensor` representations.
+        bounding boxes with proper masking of the boxes according to
+        `num_detections`. This allows proper interop with
+        `tf.image.combined_non_max_suppression`. Returned boxes match the
+        specification fed to the function, so if the bounding box tensor uses
+        `tf.RaggedTensor` to represent boxes the returned value will also return
+        `tf.RaggedTensor` representations.
     """
     # ensure we are complying with KerasCV bounding box format.
     info = validate_format(bounding_boxes)
     if info["ragged"]:
         raise ValueError(
-            "`bounding_box.mask_invalid_detections()` requires inputs to be Dense "
-            "tensors.  Please call `bounding_box.to_dense(bounding_boxes)` before "
-            "passing your boxes to `bounding_box.mask_invalid_detections()`."
+            "`bounding_box.mask_invalid_detections()` requires inputs to be "
+            "Dense tensors. Please call "
+            "`bounding_box.to_dense(bounding_boxes)` before passing your boxes "
+            "to `bounding_box.mask_invalid_detections()`."
         )
     if "num_detections" not in bounding_boxes:
         raise ValueError(
             "`bounding_boxes` must have key 'num_detections' "
             "to be used with `bounding_box.mask_invalid_detections()`."
         )
 
     boxes = bounding_boxes.get("boxes")
     classes = bounding_boxes.get("classes")
+    confidence = bounding_boxes.get("confidence", None)
     num_detections = bounding_boxes.get("num_detections")
 
     # Create a mask to select only the first N boxes from each batch
     mask = tf.repeat(
         tf.expand_dims(tf.range(tf.shape(boxes)[1]), axis=0),
         repeats=tf.shape(boxes)[0],
         axis=0,
     )
     mask = mask < num_detections[:, None]
 
     classes = tf.where(mask, classes, -tf.ones_like(classes))
 
-    # resuse mask for boxes
+    if confidence is not None:
+        confidence = tf.where(mask, confidence, -tf.ones_like(confidence))
+
+    # reuse mask for boxes
     mask = tf.expand_dims(mask, axis=-1)
     mask = tf.repeat(mask, repeats=boxes.shape[-1], axis=-1)
     boxes = tf.where(mask, boxes, -tf.ones_like(boxes))
 
     result = bounding_boxes.copy()
+
     result["boxes"] = boxes
     result["classes"] = classes
+    if confidence is not None:
+        result["confidence"] = confidence
 
     if output_ragged:
         return to_ragged(result)
 
     return result
```

## keras_cv/bounding_box/mask_invalid_detections_test.py

```diff
@@ -24,21 +24,25 @@
             "num_detections": tf.constant([2, 3, 4, 2]),
             "classes": tf.random.uniform((4, 100)),
         }
 
         result = bounding_box.mask_invalid_detections(bounding_boxes)
 
         negative_one_boxes = result["boxes"][:, 5:, :]
-        self.assertAllClose(negative_one_boxes, -tf.ones_like(negative_one_boxes))
+        self.assertAllClose(
+            negative_one_boxes, -tf.ones_like(negative_one_boxes)
+        )
 
         preserved_boxes = result["boxes"][:, :2, :]
         self.assertAllClose(preserved_boxes, bounding_boxes["boxes"][:, :2, :])
 
         boxes_from_image_3 = result["boxes"][2, :4, :]
-        self.assertAllClose(boxes_from_image_3, bounding_boxes["boxes"][2, :4, :])
+        self.assertAllClose(
+            boxes_from_image_3, bounding_boxes["boxes"][2, :4, :]
+        )
 
     def test_correctly_masks_based_on_max_dets_in_graph(self):
         bounding_boxes = {
             "boxes": tf.random.uniform((4, 100, 4)),
             "num_detections": tf.constant([2, 3, 4, 2]),
             "classes": tf.random.uniform((4, 100)),
         }
@@ -46,28 +50,57 @@
         @tf.function()
         def apply_mask_detections(bounding_boxes):
             return bounding_box.mask_invalid_detections(bounding_boxes)
 
         result = apply_mask_detections(bounding_boxes)
 
         negative_one_boxes = result["boxes"][:, 5:, :]
-        self.assertAllClose(negative_one_boxes, -tf.ones_like(negative_one_boxes))
+        self.assertAllClose(
+            negative_one_boxes, -tf.ones_like(negative_one_boxes)
+        )
 
         preserved_boxes = result["boxes"][:, :2, :]
         self.assertAllClose(preserved_boxes, bounding_boxes["boxes"][:, :2, :])
 
         boxes_from_image_3 = result["boxes"][2, :4, :]
-        self.assertAllClose(boxes_from_image_3, bounding_boxes["boxes"][2, :4, :])
+        self.assertAllClose(
+            boxes_from_image_3, bounding_boxes["boxes"][2, :4, :]
+        )
 
     def test_ragged_outputs(self):
         bounding_boxes = {
-            "boxes": tf.stack([tf.random.uniform((10, 4)), tf.random.uniform((10, 4))]),
+            "boxes": tf.stack(
+                [tf.random.uniform((10, 4)), tf.random.uniform((10, 4))]
+            ),
+            "num_detections": tf.constant([2, 3]),
+            "classes": tf.stack(
+                [tf.random.uniform((10,)), tf.random.uniform((10,))]
+            ),
+        }
+
+        result = bounding_box.mask_invalid_detections(
+            bounding_boxes, output_ragged=True
+        )
+        self.assertTrue(isinstance(result["boxes"], tf.RaggedTensor))
+        self.assertEqual(result["boxes"][0].shape[0], 2)
+        self.assertEqual(result["boxes"][1].shape[0], 3)
+
+    def test_correctly_masks_confidence(self):
+        bounding_boxes = {
+            "boxes": tf.stack(
+                [tf.random.uniform((10, 4)), tf.random.uniform((10, 4))]
+            ),
+            "confidence": tf.random.uniform((2, 10)),
             "num_detections": tf.constant([2, 3]),
-            "classes": tf.stack([tf.random.uniform((10,)), tf.random.uniform((10,))]),
+            "classes": tf.stack(
+                [tf.random.uniform((10,)), tf.random.uniform((10,))]
+            ),
         }
 
         result = bounding_box.mask_invalid_detections(
             bounding_boxes, output_ragged=True
         )
         self.assertTrue(isinstance(result["boxes"], tf.RaggedTensor))
         self.assertEqual(result["boxes"][0].shape[0], 2)
         self.assertEqual(result["boxes"][1].shape[0], 3)
+        self.assertEqual(result["confidence"][0].shape[0], 2)
+        self.assertEqual(result["confidence"][1].shape[0], 3)
```

## keras_cv/bounding_box/to_dense.py

```diff
@@ -37,23 +37,28 @@
 
 def to_dense(bounding_boxes, max_boxes=None, default_value=-1):
     """to_dense converts bounding boxes to Dense tensors
 
     Args:
         bounding_boxes: bounding boxes in KerasCV dictionary format.
         max_boxes: the maximum number of boxes, used to pad tensors to a given
-            shape.  This can be used to make object detection pipelines TPU
+            shape. This can be used to make object detection pipelines TPU
             compatible.
-        default_value: the default value to pad bounding boxes with.  defaults
+        default_value: the default value to pad bounding boxes with. defaults
             to -1.
     """
     info = validate_format.validate_format(bounding_boxes)
 
+    # guards against errors in metrics regarding modification of inputs.
+    # also guards against unexpected behavior when modifying downstream
+    bounding_boxes = bounding_boxes.copy()
+
     # Already running in masked mode
     if not info["ragged"]:
+        # even if already ragged, still copy the dictionary for API consistency
         return bounding_boxes
 
     if isinstance(bounding_boxes["classes"], tf.RaggedTensor):
         bounding_boxes["classes"] = bounding_boxes["classes"].to_tensor(
             default_value=default_value,
             shape=_classes_shape(
                 info["is_batched"], bounding_boxes["classes"].shape, max_boxes
@@ -66,15 +71,19 @@
             shape=_box_shape(
                 info["is_batched"], bounding_boxes["boxes"].shape, max_boxes
             ),
         )
 
     if "confidence" in bounding_boxes:
         if isinstance(bounding_boxes["confidence"], tf.RaggedTensor):
-            bounding_boxes["confidence"] = bounding_boxes["confidence"].to_tensor(
+            bounding_boxes["confidence"] = bounding_boxes[
+                "confidence"
+            ].to_tensor(
                 default_value=default_value,
                 shape=_classes_shape(
-                    info["is_batched"], bounding_boxes["confidence"].shape, max_boxes
+                    info["is_batched"],
+                    bounding_boxes["confidence"].shape,
+                    max_boxes,
                 ),
             )
 
     return bounding_boxes
```

## keras_cv/bounding_box/to_ragged.py

```diff
@@ -15,18 +15,19 @@
 
 import keras_cv.bounding_box.validate_format as validate_format
 
 
 def to_ragged(bounding_boxes, sentinel=-1, dtype=tf.float32):
     """converts a Dense padded bounding box `tf.Tensor` to a `tf.RaggedTensor`.
 
-    Bounding boxes are ragged tensors in most use cases. Converting them to a dense
-    tensor makes it easier to work with Tensorflow ecosystem.
+    Bounding boxes are ragged tensors in most use cases. Converting them to a
+    dense tensor makes it easier to work with Tensorflow ecosystem.
     This function can be used to filter out the masked out bounding boxes by
-    checking for padded sentinel value of the class_id axis of the bounding_boxes.
+    checking for padded sentinel value of the class_id axis of the
+    bounding_boxes.
 
     Usage:
     ```python
     bounding_boxes = {
         "boxes": tf.constant([[2, 3, 4, 5], [0, 1, 2, 3]]),
         "classes": tf.constant([[-1, 1]]),
     }
@@ -35,37 +36,50 @@
     # {
     #     "boxes": [[0, 1, 2, 3]],
     #     "classes": [[1]]
     # }
     ```
 
     Args:
-        bounding_boxes: a Tensor of bounding boxes.  May be batched, or unbatched.
-        sentinel: The value indicating that a bounding box does not exist at the current
-            index, and the corresponding box is padding.  Defaults to -1.
+        bounding_boxes: a Tensor of bounding boxes. May be batched, or
+            unbatched.
+        sentinel: The value indicating that a bounding box does not exist at the
+            current index, and the corresponding box is padding, defaults to -1.
         dtype: the data type to use for the underlying Tensors.
     Returns:
-        dictionary of `tf.RaggedTensor` or 'tf.Tensor' containing the filtered bounding
-        boxes.
+        dictionary of `tf.RaggedTensor` or 'tf.Tensor' containing the filtered
+        bounding boxes.
     """
     info = validate_format.validate_format(bounding_boxes)
 
     if info["ragged"]:
         return bounding_boxes
 
     boxes = bounding_boxes.get("boxes")
     classes = bounding_boxes.get("classes")
+    confidence = bounding_boxes.get("confidence", None)
+
     mask = classes != sentinel
 
     boxes = tf.ragged.boolean_mask(boxes, mask)
     classes = tf.ragged.boolean_mask(classes, mask)
+    if confidence is not None:
+        confidence = tf.ragged.boolean_mask(confidence, mask)
 
     if isinstance(boxes, tf.Tensor):
         boxes = tf.RaggedTensor.from_tensor(boxes)
 
     if isinstance(classes, tf.Tensor) and len(classes.shape) > 1:
         classes = tf.RaggedTensor.from_tensor(classes)
 
+    if confidence is not None:
+        if isinstance(confidence, tf.Tensor) and len(confidence.shape) > 1:
+            confidence = tf.RaggedTensor.from_tensor(confidence)
+
     result = bounding_boxes.copy()
     result["boxes"] = tf.cast(boxes, dtype)
     result["classes"] = tf.cast(classes, dtype)
+
+    if confidence is not None:
+        result["confidence"] = tf.cast(confidence, dtype)
+
     return result
```

## keras_cv/bounding_box/to_ragged_test.py

```diff
@@ -19,32 +19,54 @@
 class ToRaggedTest(tf.test.TestCase):
     def test_converts_to_ragged(self):
         bounding_boxes = {
             "boxes": tf.constant(
                 [[[0, 0, 0, 0], [0, 0, 0, 0]], [[2, 3, 4, 5], [0, 1, 2, 3]]]
             ),
             "classes": tf.constant([[-1, -1], [-1, 1]]),
+            "confidence": tf.constant([[0.5, 0.7], [0.23, 0.12]]),
         }
         bounding_boxes = bounding_box.to_ragged(bounding_boxes)
 
         self.assertEqual(bounding_boxes["boxes"][1].shape, [1, 4])
         self.assertEqual(bounding_boxes["classes"][1].shape, [1])
+        self.assertEqual(
+            bounding_boxes["confidence"][1].shape,
+            [
+                1,
+            ],
+        )
+
         self.assertEqual(bounding_boxes["classes"][0].shape, [0])
         self.assertEqual(bounding_boxes["boxes"][0].shape, [0, 4])
+        self.assertEqual(
+            bounding_boxes["confidence"][0].shape,
+            [
+                0,
+            ],
+        )
 
     def test_round_trip(self):
         original = {
             "boxes": tf.constant(
-                [[[0, 0, 0, 0], [-1, -1, -1, -1]], [[-1, -1, -1, -1], [-1, -1, -1, -1]]]
+                [
+                    [[0, 0, 0, 0], [-1, -1, -1, -1]],
+                    [[-1, -1, -1, -1], [-1, -1, -1, -1]],
+                ]
             ),
             "classes": tf.constant([[1, -1], [-1, -1]]),
+            "confidence": tf.constant([[0.5, -1], [-1, -1]]),
         }
         bounding_boxes = bounding_box.to_ragged(original)
         bounding_boxes = bounding_box.to_dense(bounding_boxes, max_boxes=2)
 
         self.assertEqual(bounding_boxes["boxes"][1].shape, [2, 4])
         self.assertEqual(bounding_boxes["classes"][1].shape, [2])
         self.assertEqual(bounding_boxes["classes"][0].shape, [2])
         self.assertEqual(bounding_boxes["boxes"][0].shape, [2, 4])
+        self.assertEqual(bounding_boxes["confidence"][0].shape, [2])
 
         self.assertAllEqual(bounding_boxes["boxes"], original["boxes"])
         self.assertAllEqual(bounding_boxes["classes"], original["classes"])
+        self.assertAllEqual(
+            bounding_boxes["confidence"], original["confidence"]
+        )
```

## keras_cv/bounding_box/utils.py

```diff
@@ -17,18 +17,21 @@
 
 from keras_cv import bounding_box
 from keras_cv.bounding_box.formats import XYWH
 
 
 def is_relative(bounding_box_format):
     """A util to check if a bounding box format uses relative coordinates"""
-    if bounding_box_format.lower() not in bounding_box.converters.TO_XYXY_CONVERTERS:
+    if (
+        bounding_box_format.lower()
+        not in bounding_box.converters.TO_XYXY_CONVERTERS
+    ):
         raise ValueError(
             "`is_relative()` received an unsupported format for the argument "
-            f"`bounding_box_format`.  `bounding_box_format` should be one of "
+            f"`bounding_box_format`. `bounding_box_format` should be one of "
             f"{bounding_box.converters.TO_XYXY_CONVERTERS.keys()}. "
             f"Got bounding_box_format={bounding_box_format}"
         )
 
     return bounding_box_format.startswith("rel")
 
 
@@ -50,31 +53,36 @@
         boxes,
         source=bounding_box_format,
         target="rel_xywh",
     )
     widths = boxes[..., XYWH.WIDTH]
     heights = boxes[..., XYWH.HEIGHT]
     # handle corner case where shear performs a full inversion.
-    return tf.where(tf.math.logical_and(widths > 0, heights > 0), widths * heights, 0.0)
+    return tf.where(
+        tf.math.logical_and(widths > 0, heights > 0), widths * heights, 0.0
+    )
 
 
 # bounding_boxes is a dictionary with shape:
 # {"boxes": [None, None, 4], "mask": [None, None]}
-def clip_to_image(bounding_boxes, bounding_box_format, images=None, image_shape=None):
+def clip_to_image(
+    bounding_boxes, bounding_box_format, images=None, image_shape=None
+):
     """clips bounding boxes to image boundaries.
 
-    `clip_to_image()` clips bounding boxes that have coordinates out of bounds of an
-    image down to the boundaries of the image.  This is done by converting the bounding
-    box to relative formats, then clipping them to the `[0, 1]` range.  Additionally,
-    bounding boxes that end up with a zero area have their class ID set to -1,
-    indicating that there is no object present in them.
+    `clip_to_image()` clips bounding boxes that have coordinates out of bounds
+    of an image down to the boundaries of the image. This is done by converting
+    the bounding box to relative formats, then clipping them to the `[0, 1]`
+    range. Additionally, bounding boxes that end up with a zero area have their
+    class ID set to -1, indicating that there is no object present in them.
 
     Args:
         bounding_boxes: bounding box tensor to clip.
-        bounding_box_format: the KerasCV bounding box format the bounding boxes are in.
+        bounding_box_format: the KerasCV bounding box format the bounding boxes
+            are in.
         images: list of images to clip the bounding boxes to.
         image_shape: the shape of the images to clip the bounding boxes to.
     """
     boxes, classes = bounding_boxes["boxes"], bounding_boxes["classes"]
 
     boxes = bounding_box.convert_format(
         boxes,
@@ -90,27 +98,31 @@
             tf.clip_by_value(x1, clip_value_min=0, clip_value_max=1),
             tf.clip_by_value(y1, clip_value_min=0, clip_value_max=1),
             tf.clip_by_value(x2, clip_value_min=0, clip_value_max=1),
             tf.clip_by_value(y2, clip_value_min=0, clip_value_max=1),
         ],
         axis=-1,
     )
-    areas = _relative_area(clipped_bounding_boxes, bounding_box_format="rel_xyxy")
+    areas = _relative_area(
+        clipped_bounding_boxes, bounding_box_format="rel_xyxy"
+    )
     clipped_bounding_boxes = bounding_box.convert_format(
         clipped_bounding_boxes,
         source="rel_xyxy",
         target=bounding_box_format,
         images=images,
         image_shape=image_shape,
     )
     clipped_bounding_boxes = tf.where(
         tf.expand_dims(areas > 0.0, axis=-1), clipped_bounding_boxes, -1.0
     )
     classes = tf.where(areas > 0.0, classes, tf.constant(-1, classes.dtype))
-    nan_indices = tf.math.reduce_any(tf.math.is_nan(clipped_bounding_boxes), axis=-1)
+    nan_indices = tf.math.reduce_any(
+        tf.math.is_nan(clipped_bounding_boxes), axis=-1
+    )
     classes = tf.where(nan_indices, tf.constant(-1, classes.dtype), classes)
 
     # TODO update dict and return
     clipped_bounding_boxes, classes = _format_outputs(
         clipped_bounding_boxes, classes, squeeze
     )
 
@@ -155,18 +167,20 @@
             raise ValueError(
                 "Expected len(images.shape)=2, or len(images.shape)=3, got "
                 f"len(images.shape)={images_rank}"
             )
         images_include_batch = images_rank == 4
         if boxes_includes_batch != images_include_batch:
             raise ValueError(
-                "clip_to_image() expects both boxes and images to be batched, or both "
-                f"boxes and images to be unbatched.  Received len(boxes.shape)={boxes_rank}, "
-                f"len(images.shape)={images_rank}.  Expected either len(boxes.shape)=2 AND "
-                "len(images.shape)=3, or len(boxes.shape)=3 AND len(images.shape)=4."
+                "clip_to_image() expects both boxes and images to be batched, "
+                "or both boxes and images to be unbatched. Received "
+                f"len(boxes.shape)={boxes_rank}, "
+                f"len(images.shape)={images_rank}. Expected either "
+                "len(boxes.shape)=2 AND len(images.shape)=3, or "
+                "len(boxes.shape)=3 AND len(images.shape)=4."
             )
         if not images_include_batch:
             images = tf.expand_dims(images, axis=0)
 
     if not boxes_includes_batch:
         return (
             tf.expand_dims(boxes, axis=0),
```

## keras_cv/bounding_box/utils_test.py

```diff
@@ -56,15 +56,17 @@
         self.assertAllLessEqual(bounding_boxes["boxes"], 1)
 
     def test_clip_to_image_filters_fully_out_bounding_boxes(self):
         # Test xyxy format unbatched
         height = 256
         width = 256
         bounding_boxes = {
-            "boxes": tf.convert_to_tensor([[257, 257, 400, 400], [100, 100, 300, 300]]),
+            "boxes": tf.convert_to_tensor(
+                [[257, 257, 400, 400], [100, 100, 300, 300]]
+            ),
             "classes": tf.convert_to_tensor([0, 0]),
         }
         image = tf.ones(shape=(height, width, 3))
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes, bounding_box_format="xyxy", images=image
         )
 
@@ -78,15 +80,17 @@
         )
 
     def test_clip_to_image_filters_fully_out_bounding_boxes_negative_area(self):
         # Test xyxy format unbatched
         height = 256
         width = 256
         bounding_boxes = {
-            "boxes": tf.convert_to_tensor([[110, 120, 100, 100], [100, 100, 300, 300]]),
+            "boxes": tf.convert_to_tensor(
+                [[110, 120, 100, 100], [100, 100, 300, 300]]
+            ),
             "classes": [0, 0],
         }
         image = tf.ones(shape=(height, width, 3))
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes, bounding_box_format="xyxy", images=image
         )
         self.assertAllEqual(
```

## keras_cv/bounding_box/validate_format.py

```diff
@@ -10,70 +10,75 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 
 
-def validate_format(bounding_boxes):
-    """validates that a given set of bounding boxes complies with KerasCV format.
+def validate_format(bounding_boxes, variable_name="bounding_boxes"):
+    """validates that a given set of bounding boxes complies with KerasCV
+    format.
 
-    For a set of bounding boxes to be valid it must satisfy the following conditions:
+    For a set of bounding boxes to be valid it must satisfy the following
+    conditions:
     - `bounding_boxes` must be a dictionary
     - contains keys `"boxes"` and `"classes"`
-    - each entry must have matching first two dimensions; representing the batch axis
-        and the number of boxes per image axis.
-    - either both `"boxes"` and `"classes"` are batched, or both are unbatched
+    - each entry must have matching first two dimensions; representing the batch
+        axis and the number of boxes per image axis.
+    - either both `"boxes"` and `"classes"` are batched, or both are unbatched.
 
     Additionally, one of the following must be satisfied:
     - `"boxes"` and `"classes"` are both Ragged
     - `"boxes"` and `"classes"` are both Dense
     - `"boxes"` and `"classes"` are unbatched
 
     Args:
-        bounding_boxes: dictionary of bounding boxes according to KerasCV format.
+        bounding_boxes: dictionary of bounding boxes according to KerasCV
+        format.
 
     Raises:
         ValueError if any of the above conditions are not met
     """
     if not isinstance(bounding_boxes, dict):
         raise ValueError(
-            "Expected `bounding_boxes` to be a dictionary, got "
-            f"`bounding_boxes={bounding_boxes}`."
+            f"Expected `{variable_name}` to be a dictionary, got "
+            f"`{variable_name}={bounding_boxes}`."
         )
     if not all([x in bounding_boxes for x in ["boxes", "classes"]]):
         raise ValueError(
-            "Expected `bounding_boxes` to be a dictionary containing keys "
-            "`'classes'` and `'boxes'`.  Got "
-            f"`bounding_boxes.keys()={bounding_boxes.keys()}`."
+            f"Expected `{variable_name}` to be a dictionary containing keys "
+            "`'classes'` and `'boxes'`. Got "
+            f"`{variable_name}.keys()={bounding_boxes.keys()}`."
         )
 
     boxes = bounding_boxes.get("boxes")
     classes = bounding_boxes.get("classes")
     info = {}
 
     is_batched = len(boxes.shape) == 3
     info["is_batched"] = is_batched
     info["ragged"] = isinstance(boxes, tf.RaggedTensor)
 
     if not is_batched:
         if boxes.shape[:1] != classes.shape[:1]:
             raise ValueError(
                 "Expected `boxes` and `classes` to have matching dimensions "
-                "on the first axis when operating in unbatched mode. "
-                f"Got `boxes.shape={boxes.shape}`, `classes.shape={classes.shape}`."
+                "on the first axis when operating in unbatched mode. Got "
+                f"`boxes.shape={boxes.shape}`, `classes.shape={classes.shape}`."
             )
 
         info["classes_one_hot"] = len(classes.shape) == 2
         # No Ragged checks needed in unbatched mode.
         return info
 
     info["classes_one_hot"] = len(classes.shape) == 3
 
-    if isinstance(boxes, tf.RaggedTensor) != isinstance(classes, tf.RaggedTensor):
+    if isinstance(boxes, tf.RaggedTensor) != isinstance(
+        classes, tf.RaggedTensor
+    ):
         raise ValueError(
             "Either both `boxes` and `classes` "
             "should be Ragged, or neither should be ragged."
             f" Got `type(boxes)={type(boxes)}`, type(classes)={type(classes)}."
         )
 
     # Batched mode checks
```

## keras_cv/bounding_box/validate_format_test.py

```diff
@@ -21,15 +21,16 @@
         with self.assertRaisesRegex(
             ValueError, "Expected `bounding_boxes` to be a dictionary, got "
         ):
             bounding_box.validate_format(tf.ones((4, 3, 6)))
 
     def test_mismatch_dimensions(self):
         with self.assertRaisesRegex(
-            ValueError, "Expected `boxes` and `classes` to have matching dimensions"
+            ValueError,
+            "Expected `boxes` and `classes` to have matching dimensions",
         ):
             bounding_box.validate_format(
                 {"boxes": tf.ones((4, 3, 6)), "classes": tf.ones((4, 6))}
             )
 
     def test_bad_keys(self):
         with self.assertRaisesRegex(ValueError, "containing keys"):
```

## keras_cv/bounding_box_3d/formats.py

```diff
@@ -13,15 +13,16 @@
 # limitations under the License.
 """
 formats.py contains axis information for each supported format.
 """
 
 
 class CENTER_XYZ_DXDYDZ_PHI:
-    """CENTER_XYZ_DXDYDZ_PHI contains axis indices for the CENTER_XYZ_DXDYDZ_PHI format.
+    """CENTER_XYZ_DXDYDZ_PHI contains axis indices for the CENTER_XYZ_DXDYDZ_PHI
+    format.
 
     CENTER_XYZ_DXDYDZ_PHI is a 3D box format that supports vertical boxes with a
     heading rotated around the Z axis.
 
     The CENTER_XYZ_DXDYDZ_PHI format consists of the following required indices:
 
     - X: X coordinate of the center of the bounding box
```

## keras_cv/callbacks/__init__.py

```diff
@@ -7,20 +7,9 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-try:
-    from keras_cv.callbacks.pycoco_callback import PyCOCOCallback
-except ImportError:
-    print(
-        "You do not have pyococotools installed, so the `PyCOCOCallback` API is not available."
-    )
-
-try:
-    from keras_cv.callbacks.waymo_evaluation_callback import WaymoEvaluationCallback
-except ImportError:
-    print(
-        "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available."
-    )
+from keras_cv.callbacks.pycoco_callback import PyCOCOCallback
+from keras_cv.callbacks.waymo_evaluation_callback import WaymoEvaluationCallback
```

## keras_cv/callbacks/pycoco_callback.py

```diff
@@ -12,64 +12,74 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 from keras.callbacks import Callback
 
 from keras_cv import bounding_box
 from keras_cv.metrics.coco import compute_pycoco_metrics
+from keras_cv.models.object_detection.__internal__ import unpack_input
+from keras_cv.utils.conditional_imports import assert_pycocotools_installed
 
 
 class PyCOCOCallback(Callback):
-    def __init__(self, validation_data, bounding_box_format, cache=True, **kwargs):
-        """Creates a callback to evaluate PyCOCO metrics on a validation dataset.
+    def __init__(
+        self, validation_data, bounding_box_format, cache=True, **kwargs
+    ):
+        """Creates a callback to evaluate PyCOCO metrics on a validation
+        dataset.
 
         Args:
-            validation_data: a tf.data.Dataset containing validation data. Entries
-                should have the form ```(images, {"boxes": boxes,
+            validation_data: a tf.data.Dataset containing validation data.
+                Entries should have the form ```(images, {"boxes": boxes,
                 "classes": classes})```.
             bounding_box_format: the KerasCV bounding box format used in the
                 validation dataset (e.g. "xywh")
-            cache: whether the callback should cache the dataset between iterations.
-                Note that if the validation dataset has shuffling of any kind
-                (e.g from `shuffle_files=True` in a call to TFDS.load or a call
-                to tf.data.Dataset.shuffle() with `reshuffle_each_iteration=True`),
-                you **must** cache the dataset to preserve iteration order. This
-                will store your entire dataset in main memory, so for large datasets
-                consider avoiding shuffle operations and passing `cache=False`.
+            cache: whether the callback should cache the dataset between
+                iterations. Note that if the validation dataset has shuffling of
+                any kind (e.g. from `shuffle_files=True` in a call to TFDS).
+                Load or a call to tf.data.Dataset.shuffle() with
+                `reshuffle_each_iteration=True`), you **must** cache the dataset
+                to preserve iteration order. This will store your entire dataset
+                in main memory, so for large datasets consider avoiding shuffle
+                operations and passing `cache=False`.
         """
+        assert_pycocotools_installed("PyCOCOCallback")
         self.model = None
         self.val_data = validation_data
         if cache:
             # We cache the dataset to preserve a consistent iteration order.
             self.val_data = self.val_data.cache()
         self.bounding_box_format = bounding_box_format
         super().__init__(**kwargs)
 
     def on_epoch_end(self, epoch, logs=None):
         logs = logs or {}
 
-        def images_only(images, boxes):
+        def images_only(data):
+            images, boxes = unpack_input(data)
             return images
 
-        def boxes_only(images, boxes):
-            return boxes
+        def boxes_only(data):
+            images, boxes = unpack_input(data)
+            return bounding_box.to_ragged(boxes)
 
         images_only_ds = self.val_data.map(images_only)
         y_pred = self.model.predict(images_only_ds)
-        box_pred = tf.convert_to_tensor(y_pred["boxes"])
-        cls_pred = tf.convert_to_tensor(y_pred["classes"])
-        confidence_pred = tf.convert_to_tensor(y_pred["confidence"])
-        valid_det = tf.convert_to_tensor(y_pred["num_detections"])
+        box_pred = y_pred["boxes"]
+        cls_pred = y_pred["classes"]
+        confidence_pred = y_pred["confidence"]
+        valid_det = y_pred["num_detections"]
 
         gt = [boxes for boxes in self.val_data.map(boxes_only)]
         gt_boxes = tf.concat(
-            [tf.RaggedTensor.from_tensor(boxes["boxes"]) for boxes in gt], axis=0
+            [boxes["boxes"] for boxes in gt],
+            axis=0,
         )
         gt_classes = tf.concat(
-            [tf.RaggedTensor.from_tensor(boxes["classes"]) for boxes in gt],
+            [boxes["classes"] for boxes in gt],
             axis=0,
         )
 
         first_image_batch = next(iter(images_only_ds))
         height = first_image_batch.shape[1]
         width = first_image_batch.shape[2]
         total_images = gt_boxes.shape[0]
@@ -78,32 +88,33 @@
             gt_boxes, source=self.bounding_box_format, target="yxyx"
         )
 
         source_ids = tf.strings.as_string(
             tf.linspace(1, total_images, total_images), precision=0
         )
 
-        ground_truth = {}
-        ground_truth["source_id"] = [source_ids]
-        ground_truth["height"] = [tf.tile(tf.constant([height]), [total_images])]
-        ground_truth["width"] = [tf.tile(tf.constant([width]), [total_images])]
-
-        ground_truth["num_detections"] = [gt_boxes.row_lengths(axis=1)]
-        ground_truth["boxes"] = [gt_boxes.to_tensor(-1)]
-        ground_truth["classes"] = [gt_classes.to_tensor(-1)]
+        ground_truth = {
+            "source_id": [source_ids],
+            "height": [tf.tile(tf.constant([height]), [total_images])],
+            "width": [tf.tile(tf.constant([width]), [total_images])],
+            "num_detections": [gt_boxes.row_lengths(axis=1)],
+            "boxes": [gt_boxes.to_tensor(-1)],
+            "classes": [gt_classes.to_tensor(-1)],
+        }
+
         box_pred = bounding_box.convert_format(
             box_pred, source=self.bounding_box_format, target="yxyx"
         )
 
-        predictions = {}
-
-        predictions["source_id"] = [source_ids]
-        predictions["detection_boxes"] = [box_pred]
-        predictions["detection_classes"] = [cls_pred]
-        predictions["detection_scores"] = [confidence_pred]
-        predictions["num_detections"] = [valid_det]
+        predictions = {
+            "source_id": [source_ids],
+            "detection_boxes": [box_pred],
+            "detection_classes": [cls_pred],
+            "detection_scores": [confidence_pred],
+            "num_detections": [valid_det],
+        }
 
         metrics = compute_pycoco_metrics(ground_truth, predictions)
         # Mark these as validation metrics by prepending a val_ prefix
         metrics = {"val_" + name: val for name, val in metrics.items()}
 
         logs.update(metrics)
```

## keras_cv/callbacks/pycoco_callback_test.py

```diff
@@ -10,35 +10,36 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import pytest
 import tensorflow as tf
+from tensorflow import keras
 
 import keras_cv
 from keras_cv.callbacks import PyCOCOCallback
 from keras_cv.metrics.coco.pycoco_wrapper import METRIC_NAMES
-from keras_cv.models.object_detection.__test_utils__ import _create_bounding_box_dataset
+from keras_cv.models.object_detection.__test_utils__ import (
+    _create_bounding_box_dataset,
+)
 
 
 class PyCOCOCallbackTest(tf.test.TestCase):
     @pytest.fixture(autouse=True)
     def cleanup_global_session(self):
         # Code before yield runs before the test
         yield
-        tf.keras.backend.clear_session()
+        keras.backend.clear_session()
 
     def test_model_fit_retinanet(self):
         model = keras_cv.models.RetinaNet(
-            classes=10,
+            num_classes=10,
             bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet50V2(
-                include_top=False, include_rescaling=True, weights=None
-            ).as_backbone(),
+            backbone=keras_cv.models.ResNet50V2Backbone(),
         )
         # all metric formats must match
         model.compile(
             optimizer="adam",
             box_loss="smoothl1",
             classification_loss="focal",
         )
@@ -46,28 +47,32 @@
         train_ds = _create_bounding_box_dataset(
             bounding_box_format="xyxy", use_dictionary_box_format=True
         )
         val_ds = _create_bounding_box_dataset(
             bounding_box_format="xyxy", use_dictionary_box_format=True
         )
 
-        callback = PyCOCOCallback(validation_data=val_ds, bounding_box_format="xyxy")
+        callback = PyCOCOCallback(
+            validation_data=val_ds, bounding_box_format="xyxy"
+        )
         history = model.fit(train_ds, callbacks=[callback])
 
         self.assertAllInSet(
             [f"val_{metric}" for metric in METRIC_NAMES], history.history.keys()
         )
 
+    @pytest.mark.skip(
+        reason="Causing OOMs on GitHub actions. This is not a user facing API "
+        "and will be replaced in a matter of weeks, so we shouldn't "
+        "invest engineering resources into working around the OOMs here."
+    )
     def test_model_fit_rcnn(self):
         model = keras_cv.models.FasterRCNN(
-            classes=10,
+            num_classes=10,
             bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet50V2(
-                include_top=False, include_rescaling=True, weights=None
-            ).as_backbone(),
         )
         model.compile(
             optimizer="adam",
             box_loss="Huber",
             classification_loss="SparseCategoricalCrossentropy",
             rpn_box_loss="Huber",
             rpn_classification_loss="BinaryCrossentropy",
```

## keras_cv/callbacks/waymo_evaluation_callback.py

```diff
@@ -9,108 +9,177 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 from keras.callbacks import Callback
-from waymo_open_dataset.metrics.python.wod_detection_evaluator import (
-    WODDetectionEvaluator,
-)
+
+from keras_cv.utils import assert_waymo_open_dataset_installed
+
+try:
+    from waymo_open_dataset import label_pb2
+    from waymo_open_dataset.metrics.python.wod_detection_evaluator import (
+        WODDetectionEvaluator,
+    )
+    from waymo_open_dataset.protos import breakdown_pb2
+    from waymo_open_dataset.protos import metrics_pb2
+except ImportError:
+    WODDetectionEvaluator = None
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 
 
 class WaymoEvaluationCallback(Callback):
     def __init__(self, validation_data, config=None, **kwargs):
         """Creates a callback to evaluate Waymo Open Dataset (WOD) metrics on a
         validation dataset.
 
         Args:
-            validation_data: a tf.data.Dataset containing validation data. Entries
-                should have the form `(point_clouds, {"bounding_boxes": bounding_boxes}`.
-                Padded bounding box should have a class of -1 to be correctly filtered out.
+            validation_data: a tf.data.Dataset containing validation data.
+                Entries should have the form `(point_clouds, {"bounding_boxes":
+                bounding_boxes}`. Padded bounding box should have a class of -1
+                to be correctly filtered out.
             config: an optional `metrics_pb2.Config` object from WOD to specify
                 what metrics should be evaluated.
         """
+        assert_waymo_open_dataset_installed(
+            "keras_cv.callbacks.WaymoEvaluationCallback()"
+        )
         self.model = None
         self.val_data = validation_data
-        self.evaluator = WODDetectionEvaluator(config=config)
+        self.evaluator = WODDetectionEvaluator(
+            config=config or self._get_default_config()
+        )
         super().__init__(**kwargs)
 
+    def _get_default_config(self):
+        """Returns the default Config proto for detection."""
+        config = metrics_pb2.Config()
+
+        config.breakdown_generator_ids.append(
+            breakdown_pb2.Breakdown.OBJECT_TYPE
+        )
+        difficulty = config.difficulties.add()
+        difficulty.levels.append(label_pb2.Label.LEVEL_1)
+        difficulty.levels.append(label_pb2.Label.LEVEL_2)
+
+        config.matcher_type = metrics_pb2.MatcherProto.TYPE_HUNGARIAN
+        config.iou_thresholds.append(0.0)  # Unknown
+        config.iou_thresholds.append(0.7)  # Vehicle
+        config.iou_thresholds.append(0.5)  # Pedestrian
+        config.iou_thresholds.append(0.5)  # Sign
+        config.iou_thresholds.append(0.5)  # Cyclist
+        config.box_type = label_pb2.Label.Box.TYPE_3D
+
+        for i in range(100):
+            config.score_cutoffs.append(i * 0.01)
+        config.score_cutoffs.append(1.0)
+
+        return config
+
     def on_epoch_end(self, epoch, logs=None):
         logs = logs or {}
 
         gt, preds = self._eval_dataset(self.val_data)
         self.evaluator.update_state(gt, preds)
 
         metrics = self.evaluator.evaluate()
 
         metrics_dict = {
-            "average_precision": metrics.average_precision,
-            "average_precision_ha_weighted": metrics.average_precision_ha_weighted,
-            "precision_recall": metrics.precision_recall,
-            "precision_recall_ha_weighted": metrics.precision_recall_ha_weighted,
-            "breakdown": metrics.breakdown,
+            "average_precision_vehicle_l1": metrics.average_precision[0],
+            "average_precision_vehicle_l2": metrics.average_precision[1],
+            "average_precision_ped_l1": metrics.average_precision[2],
+            "average_precision_ped_l2": metrics.average_precision[3],
         }
 
         logs.update(metrics_dict)
 
     def _eval_dataset(self, dataset):
         def point_clouds_only(point_clouds, target):
             return point_clouds
 
         def boxes_only(point_clouds, target):
-            return target["boxes"]
+            return target["3d_boxes"]
+
+        model_outputs = self.model.predict(dataset.map(point_clouds_only))[
+            "3d_boxes"
+        ]
+
+        def flatten_target(boxes):
+            return tf.concat(
+                [
+                    boxes["boxes"],
+                    tf.expand_dims(
+                        tf.cast(boxes["classes"], tf.float32), axis=-1
+                    ),
+                    tf.expand_dims(
+                        tf.cast(boxes["difficulty"], tf.float32), axis=-1
+                    ),
+                ],
+                axis=-1,
+            )
 
-        predicted_boxes, predicted_classes = self.model.predict(
-            dataset.map(point_clouds_only)
+        gt_boxes = tf.concat(
+            [flatten_target(x) for x in iter(dataset.map(boxes_only))], axis=0
         )
-        gt_boxes = tf.concat([x for x in iter(dataset.map(boxes_only))], axis=0)
 
         boxes_per_gt_frame = gt_boxes.shape[1]
         num_frames = gt_boxes.shape[0]
 
         gt_boxes = tf.reshape(gt_boxes, (num_frames * boxes_per_gt_frame, 9))
 
-        # Remove boxes with class of -1 (these are non-boxes that come from padding)
-        gt_real_boxes = tf.not_equal(gt_boxes[:, CENTER_XYZ_DXDYDZ_PHI.CLASS], -1)
+        # Remove padded boxes
+        gt_real_boxes = tf.concat(
+            [x["mask"] for x in iter(dataset.map(boxes_only))], axis=0
+        )
+        gt_real_boxes = tf.reshape(
+            gt_real_boxes, (num_frames * boxes_per_gt_frame)
+        )
         gt_boxes = tf.boolean_mask(gt_boxes, gt_real_boxes)
 
         frame_ids = tf.cast(tf.linspace(1, num_frames, num_frames), tf.int64)
 
-        ground_truth = {}
-        ground_truth["ground_truth_frame_id"] = tf.boolean_mask(
-            tf.repeat(frame_ids, boxes_per_gt_frame), gt_real_boxes
+        ground_truth = {
+            "ground_truth_frame_id": tf.boolean_mask(
+                tf.repeat(frame_ids, boxes_per_gt_frame), gt_real_boxes
+            ),
+            "ground_truth_bbox": gt_boxes[:, : CENTER_XYZ_DXDYDZ_PHI.PHI + 1],
+            "ground_truth_type": tf.cast(
+                gt_boxes[:, CENTER_XYZ_DXDYDZ_PHI.CLASS], tf.uint8
+            ),
+            "ground_truth_difficulty": tf.cast(
+                gt_boxes[:, CENTER_XYZ_DXDYDZ_PHI.CLASS + 1], tf.uint8
+            ),
+        }
+
+        boxes_per_pred_frame = model_outputs["boxes"].shape[1]
+        total_predicted_boxes = boxes_per_pred_frame * num_frames
+        predicted_boxes = tf.reshape(
+            model_outputs["boxes"], (total_predicted_boxes, 7)
         )
-        ground_truth["ground_truth_bbox"] = gt_boxes[:, : CENTER_XYZ_DXDYDZ_PHI.PHI + 1]
-        ground_truth["ground_truth_type"] = tf.cast(
-            gt_boxes[:, CENTER_XYZ_DXDYDZ_PHI.CLASS], tf.uint8
+        predicted_classes = tf.cast(
+            tf.reshape(model_outputs["classes"], (total_predicted_boxes, 1)),
+            tf.uint8,
         )
-        ground_truth["ground_truth_difficulty"] = tf.cast(
-            gt_boxes[:, CENTER_XYZ_DXDYDZ_PHI.CLASS + 1], tf.uint8
+        prediction_scores = tf.reshape(
+            model_outputs["confidence"], (total_predicted_boxes, 1)
         )
-
-        boxes_per_pred_frame = predicted_boxes.shape[1]
-        total_predicted_boxes = boxes_per_pred_frame * num_frames
-        predicted_boxes = tf.reshape(predicted_boxes, (total_predicted_boxes, 7))
-        predicted_classes = tf.reshape(predicted_classes, (total_predicted_boxes, 2))
-        # Remove boxes with class of -1 (these are non-boxes that come from padding)
-        pred_real_boxes = tf.reduce_all(predicted_classes != -1, axis=[-1])
+        # Remove boxes that come from padding
+        pred_real_boxes = tf.squeeze(prediction_scores > 0)
         predicted_boxes = tf.boolean_mask(predicted_boxes, pred_real_boxes)
         predicted_classes = tf.boolean_mask(predicted_classes, pred_real_boxes)
+        prediction_scores = tf.boolean_mask(prediction_scores, pred_real_boxes)
 
-        predictions = {}
-
-        predictions["prediction_frame_id"] = tf.boolean_mask(
-            tf.repeat(frame_ids, boxes_per_pred_frame), pred_real_boxes
-        )
-        predictions["prediction_bbox"] = predicted_boxes
-        predictions["prediction_type"] = tf.cast(
-            tf.argmax(predicted_classes, axis=-1), tf.uint8
-        )
-        predictions["prediction_score"] = tf.reduce_max(predicted_classes, axis=-1)
-        predictions["prediction_overlap_nlz"] = tf.cast(
-            tf.zeros(predicted_boxes.shape[0]), tf.bool
-        )
+        predictions = {
+            "prediction_frame_id": tf.boolean_mask(
+                tf.repeat(frame_ids, boxes_per_pred_frame), pred_real_boxes
+            ),
+            "prediction_bbox": predicted_boxes,
+            "prediction_type": tf.squeeze(predicted_classes),
+            "prediction_score": tf.squeeze(prediction_scores),
+            "prediction_overlap_nlz": tf.cast(
+                tf.zeros(predicted_boxes.shape[0]), tf.bool
+            ),
+        }
 
         return ground_truth, predictions
```

## keras_cv/callbacks/waymo_evaluation_callback_test.py

```diff
@@ -7,35 +7,32 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import keras
+
 import pytest
 import tensorflow as tf
+from tensorflow import keras
 
-try:
-    from keras_cv.callbacks import WaymoEvaluationCallback
-except ImportError:
-    pass
+from keras_cv.callbacks import WaymoEvaluationCallback
 
 NUM_RECORDS = 10
 POINT_FEATURES = 3
 NUM_POINTS = 20
 NUM_BOXES = 2
-BOX_FEATURES = 9
+BOX_FEATURES = 7
 
 METRIC_KEYS = [
-    "average_precision",
-    "average_precision_ha_weighted",
-    "precision_recall",
-    "precision_recall_ha_weighted",
-    "breakdown",
+    "average_precision_vehicle_l1",
+    "average_precision_vehicle_l2",
+    "average_precision_ped_l1",
+    "average_precision_ped_l2",
 ]
 
 
 class WaymoEvaluationCallbackTest(tf.test.TestCase):
     @pytest.mark.skipif(True, reason="Requires Waymo Open Dataset")
     def test_model_fit(self):
         # Silly hypothetical model
@@ -43,41 +40,62 @@
 
         points = tf.random.normal((NUM_RECORDS, POINT_FEATURES, NUM_POINTS))
         # Some random boxes, and some -1 boxes (to mimic padding ragged boxes)
         boxes = tf.concat(
             [
                 tf.random.uniform((NUM_RECORDS // 2, NUM_BOXES, BOX_FEATURES)),
                 tf.cast(
-                    tf.fill((NUM_RECORDS // 2, NUM_BOXES, BOX_FEATURES), -1), tf.float32
+                    tf.fill((NUM_RECORDS // 2, NUM_BOXES, BOX_FEATURES), -1),
+                    tf.float32,
                 ),
             ],
             axis=0,
         )
         dataset = tf.data.Dataset.from_tensor_slices(
             (
                 points,
                 {
-                    "boxes": boxes,
+                    "3d_boxes": {
+                        "boxes": boxes,
+                        "classes": tf.ones((NUM_RECORDS, NUM_BOXES)),
+                        "difficulty": tf.ones((NUM_RECORDS, NUM_BOXES)),
+                        "mask": tf.concat(
+                            [
+                                tf.ones((NUM_RECORDS // 2, NUM_BOXES)),
+                                tf.zeros((NUM_RECORDS // 2, NUM_BOXES)),
+                            ],
+                            axis=0,
+                        ),
+                    }
                 },
             )
         ).batch(5)
 
         callback = WaymoEvaluationCallback(validation_data=dataset)
         history = model.fit(points, boxes, callbacks=[callback])
 
         self.assertAllInSet(METRIC_KEYS, history.history.keys())
 
     def build_model(self):
-        inputs = tf.keras.Input(shape=(POINT_FEATURES, NUM_POINTS))
+        inputs = keras.Input(shape=(POINT_FEATURES, NUM_POINTS))
         x = keras.layers.Flatten()(inputs)
-        x = keras.layers.Dense(BOX_FEATURES * NUM_BOXES)(x)
-        x = keras.layers.Reshape((NUM_BOXES, BOX_FEATURES))(x)
-        x = keras.layers.Lambda(lambda x: (x[:, :, :7], x[:, :, 7:]))(x)
+        # Add extra features for class and confidence
+        x = keras.layers.Dense(NUM_BOXES * (BOX_FEATURES + 2))(x)
+        x = keras.layers.Reshape((NUM_BOXES, BOX_FEATURES + 2))(x)
+        x = keras.layers.Lambda(
+            lambda x: {
+                "3d_boxes": {
+                    "boxes": x[:, :, :7],
+                    "classes": tf.abs(x[:, :, 7]),
+                    "confidence": x[:, :, 8],
+                }
+            }
+        )(x)
 
         class MeanLoss(keras.losses.Loss):
             def call(self, y_true, y_pred):
                 return tf.reduce_mean(y_pred, axis=-1)
 
-        model = tf.keras.Model(inputs=inputs, outputs=x)
+        model = keras.Model(inputs=inputs, outputs=x)
         model.compile(loss=MeanLoss())
 
         return model
```

## keras_cv/core/__init__.py

```diff
@@ -7,11 +7,17 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from keras_cv.core.factor_sampler.constant_factor_sampler import ConstantFactorSampler
+from keras_cv.core.factor_sampler.constant_factor_sampler import (
+    ConstantFactorSampler,
+)
 from keras_cv.core.factor_sampler.factor_sampler import FactorSampler
-from keras_cv.core.factor_sampler.normal_factor_sampler import NormalFactorSampler
-from keras_cv.core.factor_sampler.uniform_factor_sampler import UniformFactorSampler
+from keras_cv.core.factor_sampler.normal_factor_sampler import (
+    NormalFactorSampler,
+)
+from keras_cv.core.factor_sampler.uniform_factor_sampler import (
+    UniformFactorSampler,
+)
```

## keras_cv/core/factor_sampler/constant_factor_sampler.py

```diff
@@ -7,25 +7,28 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.core.factor_sampler.factor_sampler import FactorSampler
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class ConstantFactorSampler(FactorSampler):
-    """ConstantFactorSampler samples the same factor for every call to `__call__()`.
+    """ConstantFactorSampler samples the same factor for every call to
+    `__call__()`.
 
-    This is useful in cases where a user wants to always ensure that an augmentation
-    layer performs augmentations of the same strength.
+    This is useful in cases where a user wants to always ensure that an
+    augmentation layer performs augmentations of the same strength.
 
     Args:
         value: the value to return from `__call__()`.
 
     Usage:
     ```python
     constant_factor = keras_cv.ConstantFactorSampler(0.5)
@@ -38,7 +41,11 @@
         self.value = value
 
     def __call__(self, shape=(), dtype="float32"):
         return tf.ones(shape=shape, dtype=dtype) * self.value
 
     def get_config(self):
         return {"value": self.value}
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/core/factor_sampler/factor_sampler.py

```diff
@@ -8,25 +8,27 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
+from tensorflow import keras
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class FactorSampler:
-    """FactorSampler represents a strength factor for use in an augmentation layer.
+    """FactorSampler represents a strength factor for use in an augmentation
+    layer.
 
-    FactorSampler should be subclassed and implement a `__call__()` method that returns
-    a tf.float32, or a float.  This method will be used by preprocessing layers to
-    determine the strength of their augmentation.  The specific range of values
-    supported may vary by layer, but for most layers is the range [0, 1].
+    FactorSampler should be subclassed and implement a `__call__()` method that
+    returns a tf.float32, or a float. This method will be used by preprocessing
+    layers to determine the strength of their augmentation. The specific range
+    of values supported may vary by layer, but for most layers is the range
+    [0, 1].
     """
 
     def __call__(self, shape=None, dtype="float32"):
         raise NotImplementedError(
             "FactorSampler subclasses must implement a `__call__()` method."
         )
```

## keras_cv/core/factor_sampler/normal_factor_sampler.py

```diff
@@ -7,25 +7,27 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.core.factor_sampler.factor_sampler import FactorSampler
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class NormalFactorSampler(FactorSampler):
     """NormalFactorSampler samples factors from a normal distribution.
 
-    This is useful in cases where a user wants to always ensure that an augmentation
-    layer performs augmentations of the same strength.
+    This is useful in cases where a user wants to always ensure that an
+    augmentation layer performs augmentations of the same strength.
 
     Args:
         mean: mean value for the distribution.
         stddev: standard deviation of the distribution.
         min_value: values below min_value are clipped to min_value.
         max_value: values above max_value are clipped to max_value.
 
@@ -34,16 +36,16 @@
     factor = keras_cv.core.NormalFactor(
         mean=0.5,
         stddev=0.1,
         lower=0,
         upper=1
     )
     random_sharpness = keras_cv.layers.RandomSharpness(factor=factor)
-    # random_sharpness will now sample normally around 0.5, with a lower of 0 and upper
-    # bound of 1.
+    # random_sharpness will now sample normally around 0.5, with a lower of 0
+    # and upper bound of 1.
     ```
     """
 
     def __init__(self, mean, stddev, min_value, max_value, seed=None):
         self.mean = mean
         self.stddev = stddev
         self.min_value = min_value
@@ -67,7 +69,11 @@
         return {
             "mean": self.mean,
             "stddev": self.stddev,
             "min_value": self.min_value,
             "max_value": self.max_value,
             "seed": self.seed,
         }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/core/factor_sampler/normal_factor_sampler_test_.py

```diff
@@ -15,15 +15,19 @@
 import tensorflow as tf
 
 from keras_cv import core
 
 
 class NormalFactorTest(tf.test.TestCase):
     def test_sample(self):
-        factor = core.NormalFactor(mean=0.5, stddev=0.2, min_value=0, max_value=1)
+        factor = core.NormalFactor(
+            mean=0.5, stddev=0.2, min_value=0, max_value=1
+        )
         self.assertTrue(0 <= factor() <= 1)
 
     def test_config(self):
-        factor = core.NormalFactor(mean=0.5, stddev=0.2, min_value=0, max_value=1)
+        factor = core.NormalFactor(
+            mean=0.5, stddev=0.2, min_value=0, max_value=1
+        )
         config = factor.get_config()
         self.assertEqual(config["mean"], 0.5)
         self.assertEqual(config["stddev"], 0.2)
```

## keras_cv/core/factor_sampler/uniform_factor_sampler.py

```diff
@@ -7,31 +7,34 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.core.factor_sampler.factor_sampler import FactorSampler
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class UniformFactorSampler(FactorSampler):
     """UniformFactorSampler samples factors uniformly from a range.
 
-    This is useful in cases where a user wants to always ensure that an augmentation
-    layer performs augmentations of the same strength.
+    This is useful in cases where a user wants to always ensure that an
+    augmentation layer performs augmentations of the same strength.
 
     Args:
         lower: the lower bound of values returned from `__call__()`.
         upper: the upper bound of values returned from `__call__()`.
-        seed: A shape int or Tensor, the seed to the random number generator. Must have
-            dtype int32 or int64. (When using XLA, only int32 is allowed.)
+        seed: A shape int or Tensor, the seed to the random number generator.
+            Must have dtype int32 or int64. (When using XLA, only int32 is
+            allowed.)
     Usage:
     ```python
     uniform_factor = keras_cv.UniformFactorSampler(0, 0.5)
     random_sharpness = keras_cv.layers.RandomSharpness(factor=uniform_factor)
     # random_sharpness will now sample factors between 0, and 0.5
     ```
     """
@@ -39,16 +42,24 @@
     def __init__(self, lower, upper, seed=None):
         self.lower = lower
         self.upper = upper
         self.seed = seed
 
     def __call__(self, shape=(), dtype="float32"):
         return tf.random.uniform(
-            shape, seed=self.seed, minval=self.lower, maxval=self.upper, dtype=dtype
+            shape,
+            seed=self.seed,
+            minval=self.lower,
+            maxval=self.upper,
+            dtype=dtype,
         )
 
     def get_config(self):
         return {
             "lower": self.lower,
             "upper": self.upper,
             "seed": self.seed,
         }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/datasets/imagenet/load.py

```diff
@@ -40,15 +40,17 @@
         # Decode and resize image
         image_bytes = tf.reshape(parsed[image_key], shape=[])
         image = tf.io.decode_jpeg(image_bytes, channels=3)
         if resizing:
             image = resizing(image)
 
         # Decode label
-        label = tf.cast(tf.reshape(parsed[label_key], shape=()), dtype=tf.int32) - 1
+        label = (
+            tf.cast(tf.reshape(parsed[label_key], shape=()), dtype=tf.int32) - 1
+        )
         label = tf.one_hot(label, 1000)
 
         return image, label
 
     return apply
 
 
@@ -68,35 +70,38 @@
     ```python
     dataset, ds_info = keras_cv.datasets.imagenet.load(
         split="train", tfrecord_path="gs://my-bucket/imagenet-tfrecords"
     )
     ```
 
     Args:
-        split: the split to load.  Should be one of "train" or "validation."
+        split: the split to load. Should be one of "train" or "validation."
         tfrecord_path: the path to your preprocessed ImageNet TFRecords.
-            See keras_cv/datasets/imagenet/README.md for preprocessing instructions.
+            See keras_cv/datasets/imagenet/README.md for preprocessing
+            instructions.
         batch_size: how many instances to include in batches after loading.
             Should only be specified if img_size is specified (so that images
             can be resized to the same size before batching).
-        shuffle: whether or not to shuffle the dataset.  Defaults to True.
+        shuffle: whether to shuffle the dataset, defaults to True.
         shuffle_buffer: the size of the buffer to use in shuffling.
-        reshuffle_each_iteration: whether to reshuffle the dataset on every epoch.
-            Defaults to False.
-        img_size: the size to resize the images to. Defaults to None, indicating
+        reshuffle_each_iteration: whether to reshuffle the dataset on every
+            epoch, defaults to False.
+        img_size: the size to resize the images to, defaults to None, indicating
             that images should not be resized.
 
     Returns:
-        tf.data.Dataset containing ImageNet.  Each entry is a dictionary containing
-        keys {"image": image, "label": label} where images is a Tensor of shape
-        [H, W, 3] and label is a Tensor of shape [1000].
+        tf.data.Dataset containing ImageNet. Each entry is a dictionary
+        containing keys {"image": image, "label": label} where images is a
+        Tensor of shape [H, W, 3] and label is a Tensor of shape [1000].
     """
 
     if batch_size is not None and img_size is None:
-        raise ValueError("Batching can only be performed if images are resized.")
+        raise ValueError(
+            "Batching can only be performed if images are resized."
+        )
 
     num_splits = 1024 if split == "train" else 128
     filenames = [
         f"{tfrecord_path}/{split}-{i:05d}-of-{num_splits:05d}"
         for i in range(0, num_splits)
     ]
 
@@ -108,16 +113,16 @@
         parse_imagenet_example(img_size, crop_to_aspect_ratio),
         num_parallel_calls=tf.data.AUTOTUNE,
     )
 
     if shuffle:
         if not batch_size and not shuffle_buffer:
             raise ValueError(
-                "If `shuffle=True`, either a `batch_size` or `shuffle_buffer` must be "
-                "provided to `keras_cv.datasets.imagenet.load().`"
+                "If `shuffle=True`, either a `batch_size` or `shuffle_buffer` "
+                "must be provided to `keras_cv.datasets.imagenet.load().`"
             )
         shuffle_buffer = shuffle_buffer or 8 * batch_size
         dataset = dataset.shuffle(
             shuffle_buffer, reshuffle_each_iteration=reshuffle_each_iteration
         )
 
     if batch_size is not None:
```

## keras_cv/datasets/pascal_voc/load.py

```diff
@@ -54,33 +54,33 @@
     ```python
     dataset, ds_info = keras_cv.datasets.pascal_voc.load(
         split="train", bounding_box_format="xywh", batch_size=9
     )
     ```
 
     Args:
-        split: the split string passed to the `tensorflow_datasets.load()` call.  Should
-            be one of "train", "test", or "validation."
-        bounding_box_format: the keras_cv bounding box format to load the boxes into.
-            For a list of supported formats, please  Refer
+        split: the split string passed to the `tensorflow_datasets.load()` call.
+            Should be one of "train", "test", or "validation."
+        bounding_box_format: the keras_cv bounding box format to load the boxes
+            into. For a list of supported formats, please refer
             [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
             for more details on supported bounding box formats.
         batch_size: how many instances to include in batches after loading
-        shuffle: whether or not to shuffle the dataset.  Defaults to True.
         shuffle_buffer: the size of the buffer to use in shuffling.
-        shuffle_files: (Optional) whether or not to shuffle files, defaults to True.
-        dataset: (Optional) the PascalVOC dataset to load from.  Should be either
-            'voc/2007' or 'voc/2012'. Defaults to 'voc/2007'.
+        shuffle_files: (Optional) whether to shuffle files, defaults to
+            True.
+        dataset: (Optional) the PascalVOC dataset to load from. Should be either
+            'voc/2007' or 'voc/2012', defaults to 'voc/2007'.
 
     Returns:
-        tf.data.Dataset containing PascalVOC.  Each entry is a dictionary containing
-        keys {"images": images, "bounding_boxes": bounding_boxes} where images is a
-        Tensor of shape [batch, H, W, 3] and bounding_boxes is a `tf.RaggedTensor` of
-        shape [batch, None, 5].
-    """
+        tf.data.Dataset containing PascalVOC. Each entry is a dictionary
+        containing keys {"images": images, "bounding_boxes": bounding_boxes}
+        where images is a Tensor of shape [batch, H, W, 3] and bounding_boxes is
+        a `tf.RaggedTensor` of shape [batch, None, 5].
+    """  # noqa: E501
     if dataset not in ["voc/2007", "voc/2012"]:
         raise ValueError(
             "keras_cv.datasets.pascal_voc.load() expects the `dataset` "
             "argument to be either 'voc/2007' or 'voc/2012', but got "
             f"`dataset={dataset}`."
         )
     dataset, dataset_info = tfds.load(
```

## keras_cv/datasets/pascal_voc/segmentation.py

```diff
@@ -10,58 +10,63 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Data loader for Pascal VOC 2012 segmentation dataset.
 
-The image classification and object detection (bounding box) data is covered by existing
-TF datasets in https://www.tensorflow.org/datasets/catalog/voc. The segmentation data (
-both class segmentation and instance segmentation) are included in the VOC 2012, but not
-offered by TF-DS yet. This module is trying to fill this gap while TFDS team can
-address this feature (b/252870855, https://github.com/tensorflow/datasets/issues/27 and
+The image classification and object detection (bounding box) data is covered by
+existing TF datasets in https://www.tensorflow.org/datasets/catalog/voc. The
+segmentation data (both class segmentation and instance segmentation) are
+included in the VOC 2012, but not offered by TF-DS yet. This module is trying to
+fill this gap while TFDS team can address this feature (b/252870855,
+https://github.com/tensorflow/datasets/issues/27 and
 https://github.com/tensorflow/datasets/pull/1198).
 
-The schema design is similar to the existing design of TFDS, but trimmed to fit the need
-of Keras CV models.
+The schema design is similar to the existing design of TFDS, but trimmed to fit
+the need of Keras CV models.
 
 This module contains following functionalities:
 
 1. Download and unpack original data from Pascal VOC.
-2. Reprocess and build up dataset that include image, class label, object bounding boxes,
+2. Reprocess and build up dataset that include image, class label, object
+   bounding boxes,
    class and instance segmentation masks.
 3. Produce tfrecords from the dataset.
 4. Load existing tfrecords from result in 3.
 """
+
 import logging
 import multiprocessing
 import os.path
 import random
 import tarfile
 import xml
 
 import numpy as np
 import tensorflow as tf
 import tensorflow_datasets as tfds
+from tensorflow import keras
 
-VOC_URL = "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"
+VOC_URL = "https://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"  # noqa: E501
 
 """
 @InProceedings{{BharathICCV2011,
     author = "Bharath Hariharan and Pablo Arbelaez and Lubomir Bourdev and Subhransu Maji and Jitendra Malik",
     title = "Semantic Contours from Inverse Detectors",
     booktitle = "International Conference on Computer Vision (ICCV)",
     year = "2011"}}
-"""
-SBD_URL = "http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz"
+"""  # noqa: E501
+SBD_URL = "https://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz"  # noqa: E501
 
 
-# Note that this list doesn't contain the background class. In the classification use
-# case, the label is 0 based (aeroplane -> 0), whereas in segmentation use case, the 0 is
-# reserved for background, so aeroplane maps to 1.
+# Note that this list doesn't contain the background class. In the
+# classification use case, the label is 0 based (aeroplane -> 0), whereas in
+# segmentation use case, the 0 is reserved for background, so aeroplane maps to
+# 1.
 CLASSES = [
     "aeroplane",
     "bicycle",
     "bird",
     "boat",
     "bottle",
     "bus",
@@ -79,19 +84,19 @@
     "sofa",
     "train",
     "tvmonitor",
 ]
 # This is used to map between string class to index.
 CLASS_TO_INDEX = {name: index for index, name in enumerate(CLASSES)}
 
-# For the mask data in the PNG file, the encoded raw pixel value need be to converted
-# to the proper class index. In the following map, [0, 0, 0] will be convert to 0, and
-# [128, 0, 0] will be conveted to 1, so on so forth. Also note that the mask class is 1
-# base since class 0 is reserved for the background. The [128, 0, 0] (class 1) is mapped
-# to `aeroplane`.
+# For the mask data in the PNG file, the encoded raw pixel value need to be
+# converted to the proper class index. In the following map, [0, 0, 0] will be
+# convert to 0, and [128, 0, 0] will be converted to 1, so on so forth. Also
+# note that the mask class is 1 base since class 0 is reserved for the
+# background. The [128, 0, 0] (class 1) is mapped to `aeroplane`.
 VOC_PNG_COLOR_VALUE = [
     [0, 0, 0],
     [128, 0, 0],
     [0, 128, 0],
     [128, 128, 0],
     [0, 0, 128],
     [128, 0, 128],
@@ -132,15 +137,16 @@
 
 def _download_data_file(
     data_url, extracted_dir, local_dir_path=None, override_extract=False
 ):
     """Fetch the original VOC or Semantic Boundaries Dataset from remote URL.
 
     Args:
-        data_url: string, the URL for the data to be downloaded, should be in a zipped tar package.
+        data_url: string, the URL for the data to be downloaded, should be in a
+            zipped tar package.
         local_dir_path: string, the local directory path to save the data.
     Returns:
         the path to the folder of extracted data.
     """
     if not local_dir_path:
         # download to ~/.keras/datasets/fname
         cache_dir = os.path.join(os.path.expanduser("~"), ".keras/datasets")
@@ -151,27 +157,28 @@
             os.makedirs(local_dir_path, exist_ok=True)
         # download to local_dir_path/fname
         fname = os.path.join(local_dir_path, os.path.basename(data_url))
     data_directory = os.path.join(os.path.dirname(fname), extracted_dir)
     if not override_extract and os.path.exists(data_directory):
         logging.info("data directory %s already exist", data_directory)
         return data_directory
-    data_file_path = tf.keras.utils.get_file(fname=fname, origin=data_url)
+    data_file_path = keras.utils.get_file(fname=fname, origin=data_url)
     # Extra the data into the same directory as the tar file.
     data_directory = os.path.dirname(data_file_path)
     logging.info("Extract data into %s", data_directory)
     with tarfile.open(data_file_path) as f:
         f.extractall(data_directory)
     return os.path.join(data_directory, extracted_dir)
 
 
 def _parse_annotation_data(annotation_file_path):
     """Parse the annotation XML file for the image.
 
-    The annotation contains the metadata, as well as the object bounding box information.
+    The annotation contains the metadata, as well as the object bounding box
+    information.
 
     """
     with tf.io.gfile.GFile(annotation_file_path, "r") as f:
         root = xml.etree.ElementTree.parse(f).getroot()
 
         size = root.find("size")
         width = int(size.find("width").text)
@@ -208,15 +215,17 @@
         "train": "train.txt",
         "eval": "val.txt",
         "trainval": "trainval.txt",
         # TODO(tanzhenyu): add diff dataset
         # "diff": "diff.txt",
     }
     with tf.io.gfile.GFile(
-        os.path.join(data_dir, "ImageSets", "Segmentation", data_file_mapping[split]),
+        os.path.join(
+            data_dir, "ImageSets", "Segmentation", data_file_mapping[split]
+        ),
         "r",
     ) as f:
         image_ids = f.read().splitlines()
         logging.info(f"Received {len(image_ids)} images for {split} dataset.")
         return image_ids
 
 
@@ -237,15 +246,17 @@
     image_id, _ = os.path.splitext(image_file_name)
     class_segmentation_file_path = os.path.join(
         data_dir, "SegmentationClass", image_id + ".png"
     )
     object_segmentation_file_path = os.path.join(
         data_dir, "SegmentationObject", image_id + ".png"
     )
-    annotation_file_path = os.path.join(data_dir, "Annotations", image_id + ".xml")
+    annotation_file_path = os.path.join(
+        data_dir, "Annotations", image_id + ".xml"
+    )
     image_annotations = _parse_annotation_data(annotation_file_path)
 
     result = {
         "image/filename": image_id + ".jpg",
         "image/file_path": image_file_path,
         "segmentation/class/file_path": class_segmentation_file_path,
         "segmentation/object/file_path": object_segmentation_file_path,
@@ -257,16 +268,20 @@
     return result
 
 
 def _parse_single_sbd_image(image_file_path):
     data_dir, image_file_name = os.path.split(image_file_path)
     data_dir = os.path.normpath(os.path.join(data_dir, os.path.pardir))
     image_id, _ = os.path.splitext(image_file_name)
-    class_segmentation_file_path = os.path.join(data_dir, "cls", image_id + ".mat")
-    object_segmentation_file_path = os.path.join(data_dir, "inst", image_id + ".mat")
+    class_segmentation_file_path = os.path.join(
+        data_dir, "cls", image_id + ".mat"
+    )
+    object_segmentation_file_path = os.path.join(
+        data_dir, "inst", image_id + ".mat"
+    )
     result = {
         "image/filename": image_id + ".jpg",
         "image/file_path": image_file_path,
         "segmentation/class/file_path": class_segmentation_file_path,
         "segmentation/object/file_path": object_segmentation_file_path,
     }
     return result
@@ -304,15 +319,17 @@
             values.append([o[key] for o in object])
         result["objects/" + key] = values
     return result
 
 
 def _build_sbd_metadata(data_dir, image_ids):
     # Parallel process all the images.
-    image_file_paths = [os.path.join(data_dir, "img", i + ".jpg") for i in image_ids]
+    image_file_paths = [
+        os.path.join(data_dir, "img", i + ".jpg") for i in image_ids
+    ]
     pool_size = 10 if len(image_ids) > 10 else len(image_ids)
     with multiprocessing.Pool(pool_size) as p:
         metadata = p.map(_parse_single_sbd_image, image_file_paths)
 
     keys = [
         "image/filename",
         "image/file_path",
@@ -322,21 +339,24 @@
     result = {}
     for key in keys:
         values = [value[key] for value in metadata]
         result[key] = values
     return result
 
 
-# With jit_compile=True, there will be 0.4 sec compilation overhead, but save about 0.2
-# sec per 1000 images. See https://github.com/keras-team/keras-cv/pull/943#discussion_r1001092882
+# With jit_compile=True, there will be 0.4 sec compilation overhead, but save
+# about 0.2 sec per 1000 images. See
+# https://github.com/keras-team/keras-cv/pull/943#discussion_r1001092882
 # for more details.
 @tf.function(jit_compile=True)
 def _decode_png_mask(mask):
-    """Decode the raw PNG image and convert it to 2D tensor with probably class."""
-    # Cast the mask to int32 since the original uint8 will overflow when multiple with 256
+    """Decode the raw PNG image and convert it to 2D tensor with probably
+    class."""
+    # Cast the mask to int32 since the original uint8 will overflow when
+    # multiplied with 256
     mask = tf.cast(mask, tf.int32)
     mask = mask[:, :, 0] * 256 * 256 + mask[:, :, 1] * 256 + mask[:, :, 2]
     mask = tf.expand_dims(tf.gather(VOC_PNG_COLOR_MAPPING, mask), -1)
     mask = tf.cast(mask, tf.uint8)
     return mask
 
 
@@ -365,22 +385,28 @@
     return example
 
 
 def _load_sbd_images(image_file_path, seg_cls_file_path, seg_obj_file_path):
     image = tf.io.read_file(image_file_path)
     image = tf.image.decode_jpeg(image)
 
-    segmentation_class_mask = tfds.core.lazy_imports.scipy.io.loadmat(seg_cls_file_path)
-    segmentation_class_mask = segmentation_class_mask["GTcls"]["Segmentation"][0][0]
+    segmentation_class_mask = tfds.core.lazy_imports.scipy.io.loadmat(
+        seg_cls_file_path
+    )
+    segmentation_class_mask = segmentation_class_mask["GTcls"]["Segmentation"][
+        0
+    ][0]
     segmentation_class_mask = segmentation_class_mask[..., np.newaxis]
 
     segmentation_object_mask = tfds.core.lazy_imports.scipy.io.loadmat(
         seg_obj_file_path
     )
-    segmentation_object_mask = segmentation_object_mask["GTinst"]["Segmentation"][0][0]
+    segmentation_object_mask = segmentation_object_mask["GTinst"][
+        "Segmentation"
+    ][0][0]
     segmentation_object_mask = segmentation_object_mask[..., np.newaxis]
 
     return {
         "image": image,
         "class_segmentation": segmentation_class_mask,
         "object_segmentation": segmentation_object_mask,
     }
@@ -439,31 +465,39 @@
 
 def load(
     split="sbd_train",
     data_dir=None,
 ):
     """Load the Pacal VOC 2012 dataset.
 
-    This function will download the data tar file from remote if needed, and untar to
-    the local `data_dir`, and build dataset from it.
+    This function will download the data tar file from remote if needed, and
+    untar to the local `data_dir`, and build dataset from it.
 
     It supports both VOC2012 and Semantic Boundaries Dataset (SBD).
 
-    The returned segmentation masks will be int ranging from [0, num_classes), as well as
-    255 which is the boundary mask.
+    The returned segmentation masks will be int ranging from [0, num_classes),
+    as well as 255 which is the boundary mask.
 
     Args:
-        split: string, can be 'train', 'eval', 'trainval", 'sbd_train', or 'sbd_eval'.
-            'sbd_train' represents the training dataset for SBD dataset, while 'train' represents
-            the training dataset for VOC2012 dataset. Default to `sbd_train`.
-        data_dir: string, local directory path for the loaded data. This will be used to
-            download the data file, and unzip. It will be used as a cach directory.
-            Default to None, and `~/.keras/pascal_voc_2012` will be used.
+        split: string, can be 'train', 'eval', 'trainval", 'sbd_train', or
+            'sbd_eval'. 'sbd_train' represents the training dataset for SBD
+            dataset, while 'train' represents the training dataset for VOC2012
+            dataset. Defaults to `sbd_train`.
+        data_dir: string, local directory path for the loaded data. This will be
+            used to download the data file, and unzip. It will be used as a
+            cache directory. Defaults to None, and `~/.keras/pascal_voc_2012`
+            will be used.
     """
-    supported_split_value = ["train", "eval", "trainval", "sbd_train", "sbd_eval"]
+    supported_split_value = [
+        "train",
+        "eval",
+        "trainval",
+        "sbd_train",
+        "sbd_eval",
+    ]
     if split not in supported_split_value:
         raise ValueError(
             f"The support value for `split` are {supported_split_value}. "
             f"Got: {split}"
         )
 
     if data_dir is not None:
```

## keras_cv/datasets/pascal_voc/segmentation_test.py

```diff
@@ -23,98 +23,112 @@
 extracted_dir = os.path.join("VOCdevkit", "VOC2012")
 
 
 class PascalVocSegmentationDataTest(tf.test.TestCase):
     def setUp(self):
         super().setUp()
         self.tempdir = self.get_tempdir()
-        # Note that this will not work with bazel, need to be rewrite into relying on
-        # FLAGS.test_srcdir
+        # Note that this will not work with bazel, need to be rewritten into
+        # relying on FLAGS.test_srcdir
         self.test_data_tar_path = os.path.abspath(
             os.path.join(
-                os.path.abspath(__file__), os.path.pardir, "test_data", "VOC_mini.tar"
+                os.path.abspath(__file__),
+                os.path.pardir,
+                "test_data",
+                "VOC_mini.tar",
             )
         )
 
     def get_tempdir(self):
         try:
             flags.FLAGS.test_tmpdir
         except flags.UnparsedFlagAccessError:
             # Need to initialize flags when running `pytest`.
             flags.FLAGS(sys.argv, known_only=True)
         return self.create_tempdir().full_path
 
     def test_download_data(self):
-        # Since the original data package is too large, we use a small package as a
-        # replacement.
+        # Since the original data package is too large, we use a small package
+        # as a replacement.
         local_data_dir = os.path.join(self.tempdir, "pascal_voc_2012/")
         test_data_dir = segmentation._download_data_file(
             data_url=pathlib.Path(self.test_data_tar_path).as_uri(),
             extracted_dir=extracted_dir,
             local_dir_path=local_data_dir,
         )
 
         self.assertTrue(os.path.exists(test_data_dir))
-        # Make sure the data is unzipped correctly and populated with correct content
+        # Make sure the data is unzipped correctly and populated with correct
+        # content.
         expected_subdirs = [
             "Annotations",
             "ImageSets",
             "JPEGImages",
             "SegmentationClass",
             "SegmentationObject",
         ]
         for sub_dir in expected_subdirs:
-            self.assertTrue(os.path.exists(os.path.join(test_data_dir, sub_dir)))
+            self.assertTrue(
+                os.path.exists(os.path.join(test_data_dir, sub_dir))
+            )
 
     def test_skip_download_and_override(self):
         local_data_dir = os.path.join(self.tempdir, "pascal_voc_2012/")
         test_data_dir = segmentation._download_data_file(
             data_url=pathlib.Path(self.test_data_tar_path).as_uri(),
             extracted_dir=extracted_dir,
             local_dir_path=local_data_dir,
         )
 
-        # Touch a file in the test_data_dir and make sure it exists (not being override)
-        # when invoke the _download_data_file again
+        # Touch a file in the test_data_dir and make sure it exists (not being
+        # overridden) when invoking the _download_data_file again
         os.makedirs(os.path.join(test_data_dir, "Annotations", "dummy_dir"))
         segmentation._download_data_file(
             data_url=pathlib.Path(self.test_data_tar_path).as_uri(),
             extracted_dir=extracted_dir,
             local_dir_path=local_data_dir,
             override_extract=False,
         )
         self.assertTrue(
-            os.path.exists(os.path.join(test_data_dir, "Annotations", "dummy_dir"))
+            os.path.exists(
+                os.path.join(test_data_dir, "Annotations", "dummy_dir")
+            )
         )
 
     def test_get_image_ids(self):
         local_data_dir = os.path.join(self.tempdir, "pascal_voc_2012/")
         data_dir = segmentation._download_data_file(
             data_url=pathlib.Path(self.test_data_tar_path).as_uri(),
             extracted_dir=extracted_dir,
             local_dir_path=local_data_dir,
         )
         train_ids = ["2007_000032", "2007_000039", "2007_000063"]
         eval_ids = ["2007_000033"]
         train_eval_ids = train_ids + eval_ids
-        self.assertEquals(segmentation._get_image_ids(data_dir, "train"), train_ids)
-        self.assertEquals(segmentation._get_image_ids(data_dir, "eval"), eval_ids)
+        self.assertEquals(
+            segmentation._get_image_ids(data_dir, "train"), train_ids
+        )
+        self.assertEquals(
+            segmentation._get_image_ids(data_dir, "eval"), eval_ids
+        )
         self.assertEquals(
             segmentation._get_image_ids(data_dir, "trainval"), train_eval_ids
         )
 
     def test_parse_annotation_file(self):
         local_data_dir = os.path.join(self.tempdir, "pascal_voc_2012/")
         data_dir = segmentation._download_data_file(
             data_url=pathlib.Path(self.test_data_tar_path).as_uri(),
             extracted_dir=extracted_dir,
             local_dir_path=local_data_dir,
         )
         # One of the train file.
-        annotation_file = os.path.join(data_dir, "Annotations", "2007_000032.xml")
+        annotation_file = os.path.join(
+            data_dir, "Annotations", "2007_000032.xml"
+        )
         metadata = segmentation._parse_annotation_data(annotation_file)
         expected_result = {
             "height": 281,
             "width": 500,
             "objects": [
                 {
                     "label": 0,
@@ -151,26 +165,36 @@
     def test_decode_png_mask(self):
         local_data_dir = os.path.join(self.tempdir, "pascal_voc_2012/")
         data_dir = segmentation._download_data_file(
             data_url=pathlib.Path(self.test_data_tar_path).as_uri(),
             extracted_dir=extracted_dir,
             local_dir_path=local_data_dir,
         )
-        mask_file = os.path.join(data_dir, "SegmentationClass", "2007_000032.png")
+        mask_file = os.path.join(
+            data_dir, "SegmentationClass", "2007_000032.png"
+        )
         mask = tf.io.decode_png(tf.io.read_file(mask_file))
         segmentation._maybe_populate_voc_color_mapping()
         mask = segmentation._decode_png_mask(mask)
 
         self.assertEquals(mask.shape, (281, 500, 1))
-        self.assertEquals(tf.reduce_max(mask), 255)  # The 255 value is for the boundary
-        self.assertEquals(tf.reduce_min(mask), 0)  # The 0 value is for the background
-        # The mask contains two classes, 1 and 15, see the label section in the previous
-        # test case.
-        self.assertEquals(tf.reduce_sum(tf.cast(tf.equal(mask, 1), tf.int32)), 4734)
-        self.assertEquals(tf.reduce_sum(tf.cast(tf.equal(mask, 15), tf.int32)), 866)
+        self.assertEquals(
+            tf.reduce_max(mask), 255
+        )  # The 255 value is for the boundary
+        self.assertEquals(
+            tf.reduce_min(mask), 0
+        )  # The 0 value is for the background
+        # The mask contains two classes, 1 and 15, see the label section in the
+        # previous test case.
+        self.assertEquals(
+            tf.reduce_sum(tf.cast(tf.equal(mask, 1), tf.int32)), 4734
+        )
+        self.assertEquals(
+            tf.reduce_sum(tf.cast(tf.equal(mask, 15), tf.int32)), 866
+        )
 
     def test_parse_single_image(self):
         local_data_dir = os.path.join(self.tempdir, "pascal_voc_2012/")
         data_dir = segmentation._download_data_file(
             data_url=pathlib.Path(self.test_data_tar_path).as_uri(),
             extracted_dir=extracted_dir,
             local_dir_path=local_data_dir,
@@ -288,13 +312,21 @@
         ]
         for key in expected_keys:
             self.assertIn(key, entry)
 
         # Check the mask png content
         png = entry["class_segmentation"]
         self.assertEquals(png.shape, (281, 500, 1))
-        self.assertEquals(tf.reduce_max(png), 255)  # The 255 value is for the boundary
-        self.assertEquals(tf.reduce_min(png), 0)  # The 0 value is for the background
-        # The mask contains two classes, 1 and 15, see the label section in the previous
-        # test case.
-        self.assertEquals(tf.reduce_sum(tf.cast(tf.equal(png, 1), tf.int32)), 4734)
-        self.assertEquals(tf.reduce_sum(tf.cast(tf.equal(png, 15), tf.int32)), 866)
+        self.assertEquals(
+            tf.reduce_max(png), 255
+        )  # The 255 value is for the boundary
+        self.assertEquals(
+            tf.reduce_min(png), 0
+        )  # The 0 value is for the background
+        # The mask contains two classes, 1 and 15, see the label section in the
+        # previous test case.
+        self.assertEquals(
+            tf.reduce_sum(tf.cast(tf.equal(png, 1), tf.int32)), 4734
+        )
+        self.assertEquals(
+            tf.reduce_sum(tf.cast(tf.equal(png, 15), tf.int32)), 866
+        )
```

## keras_cv/datasets/waymo/__init__.py

```diff
@@ -10,13 +10,13 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 # Following symbols are only available when Waymo Open Dataset dependencies are
 # installed.
-try:
-    from keras_cv.datasets.waymo.load import load
-    from keras_cv.datasets.waymo.transformer import build_tensors_for_augmentation
-    from keras_cv.datasets.waymo.transformer import build_tensors_from_wod_frame
-except ImportError:
-    pass
+from keras_cv.datasets.waymo.load import load
+from keras_cv.datasets.waymo.transformer import build_tensors_for_augmentation
+from keras_cv.datasets.waymo.transformer import build_tensors_from_wod_frame
+from keras_cv.datasets.waymo.transformer import convert_to_center_pillar_inputs
+from keras_cv.datasets.waymo.transformer import pad_or_trim_tensors
+from keras_cv.datasets.waymo.transformer import transform_to_vehicle_frame
```

## keras_cv/datasets/waymo/load.py

```diff
@@ -11,25 +11,29 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Data loader for the Waymo Open Dataset."""
 import os
 
 import tensorflow as tf
-import tensorflow_datasets as tfds
-from waymo_open_dataset import dataset_pb2
 
 from keras_cv.datasets.waymo import transformer
+from keras_cv.utils import assert_waymo_open_dataset_installed
+
+try:
+    import waymo_open_dataset
+except ImportError:
+    waymo_open_dataset = None
 
 
 def _generate_frames(segments, transformer):
     def _generator():
-        for record in tfds.as_numpy(segments):
-            frame = dataset_pb2.Frame()
-            frame.ParseFromString(record)
+        for record in segments:
+            frame = waymo_open_dataset.dataset_pb2.Frame()
+            frame.ParseFromString(record.numpy())
             yield transformer(frame)
 
     return _generator
 
 
 def load(
     tfrecord_path,
@@ -40,19 +44,19 @@
     Loads the Waymo Open Dataset and transforms frames into features as
     tensors.
     Args:
         tfrecord_path: a string pointing to the directory containing the raw
             tfrecords in the Waymo Open Dataset, or a list of strings pointing
             to the tfrecords themselves
         transformer: a Python function which transforms a Waymo Open Dataset
-          Frame object into tensors. Default to convert range image to point
+          Frame object into tensors, defaults to convert range image to point
           cloud.
         output_signature: the type specification of the tensors created by the
           transformer. This is often a dictionary from feature column names to
-          tf.TypeSpecs. Default to point cloud representations of Waymo Open
+          tf.TypeSpecs, defaults to point cloud representations of Waymo Open
           Dataset data.
 
     Returns:
         tf.data.Dataset containing the features extracted from Frames using the
         provided transformer.
 
     Example:
@@ -63,17 +67,19 @@
     def simple_transformer(frame):
         return {"timestamp_micros": frame.timestamp_micros}
 
     output_signature = {"timestamp_micros": tf.TensorSpec((), tf.int64)}
     load("/path/to/tfrecords", simple_transformer, output_signature)
     ```
     """
+    assert_waymo_open_dataset_installed("keras_cv.datasets.waymo.load()")
     if type(tfrecord_path) == list:
         filenames = tfrecord_path
     else:
         filenames = tf.data.TFRecordDataset.list_files(
             os.path.join(tfrecord_path, "*.tfrecord")
         )
     segments = tf.data.TFRecordDataset(filenames)
     return tf.data.Dataset.from_generator(
-        _generate_frames(segments, transformer), output_signature=output_signature
+        _generate_frames(segments, transformer),
+        output_signature=output_signature,
     )
```

## keras_cv/datasets/waymo/load_test.py

```diff
@@ -29,28 +29,30 @@
         super().setUp()
         self.test_data_path = os.path.abspath(
             os.path.join(os.path.abspath(__file__), os.path.pardir, "test_data")
         )
         self.test_data_file = "wod_one_frame.tfrecord"
 
     @pytest.mark.skipif(
-        "TEST_WAYMO_DEPS" not in os.environ or os.environ["TEST_WAYMO_DEPS"] != "true",
+        "TEST_WAYMO_DEPS" not in os.environ
+        or os.environ["TEST_WAYMO_DEPS"] != "true",
         reason="Requires Waymo Open Dataset package",
     )
     def test_load_from_directory(self):
         dataset = load(self.test_data_path)
 
         # Extract records into a list
         dataset = [record for record in dataset]
 
         self.assertEquals(len(dataset), 1)
         self.assertNotEqual(dataset[0]["timestamp_micros"], 0)
 
     @pytest.mark.skipif(
-        "TEST_WAYMO_DEPS" not in os.environ or os.environ["TEST_WAYMO_DEPS"] != "true",
+        "TEST_WAYMO_DEPS" not in os.environ
+        or os.environ["TEST_WAYMO_DEPS"] != "true",
         reason="Requires Waymo Open Dataset package",
     )
     def test_load_from_files(self):
         dataset = load([os.path.join(self.test_data_path, self.test_data_file)])
 
         # Extract records into a list
         dataset = [record for record in dataset]
```

## keras_cv/datasets/waymo/transformer.py

```diff
@@ -9,26 +9,33 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Transformer to convert Waymo Open Dataset proto to model inputs."""
 
+from typing import Any
 from typing import Dict
 from typing import List
 from typing import Sequence
 from typing import Tuple
 
 import numpy as np
 import tensorflow as tf
-from waymo_open_dataset import dataset_pb2
-from waymo_open_dataset.utils import box_utils
-from waymo_open_dataset.utils import frame_utils
-from waymo_open_dataset.utils import range_image_utils
-from waymo_open_dataset.utils import transform_utils
+
+from keras_cv.utils import assert_waymo_open_dataset_installed
+
+try:
+    from waymo_open_dataset import dataset_pb2
+    from waymo_open_dataset.utils import box_utils
+    from waymo_open_dataset.utils import frame_utils
+    from waymo_open_dataset.utils import range_image_utils
+    from waymo_open_dataset.utils import transform_utils
+except ImportError:
+    waymo_open_dataset = None
 
 from keras_cv.datasets.waymo import struct
 from keras_cv.layers.object_detection_3d import voxel_utils
 
 WOD_FRAME_OUTPUT_SIGNATURE = {
     "frame_id": tf.TensorSpec((), tf.int64),
     "timestamp_offset": tf.TensorSpec((), tf.float32),
@@ -46,20 +53,20 @@
     "label_box_density": tf.TensorSpec([None], tf.int32),
     "label_box_detection_difficulty": tf.TensorSpec([None], tf.int32),
     "label_box_mask": tf.TensorSpec([None], tf.bool),
     "label_point_class": tf.TensorSpec([None], tf.int32),
     "label_point_nlz": tf.TensorSpec([None], tf.int32),
 }
 
-# Maximum number of points from all lidars excluding the top lidar.
-# Please refer to https://arxiv.org/pdf/1912.04838.pdf Figure 1 for sensor layouts.
+# Maximum number of points from all lidars excluding the top lidar. Please refer
+# to https://arxiv.org/pdf/1912.04838.pdf Figure 1 for sensor layouts.
 _MAX_NUM_NON_TOP_LIDAR_POINTS = 30000
 
 
-def _decode_range_images(frame: dataset_pb2.Frame) -> Dict[int, List[tf.Tensor]]:
+def _decode_range_images(frame) -> Dict[int, List[tf.Tensor]]:
     """Decodes range images from a Waymo Open Dataset frame.
 
     Please refer to https://arxiv.org/pdf/1912.04838.pdf for more details.
 
     Args:
       frame: a Waymo Open Dataset frame.
 
@@ -82,30 +89,33 @@
         if lidar.name == dataset_pb2.LaserName.TOP:
             range_image_str_tensor = tf.io.decode_compressed(
                 lidar.ri_return2.range_image_compressed, "ZLIB"
             )
             ri = dataset_pb2.MatrixFloat()
             ri.ParseFromString(bytearray(range_image_str_tensor.numpy()))
             ri_tensor = tf.reshape(
-                tf.convert_to_tensor(value=ri.data, dtype=tf.float32), ri.shape.dims
+                tf.convert_to_tensor(value=ri.data, dtype=tf.float32),
+                ri.shape.dims,
             )
             range_images[lidar.name].append(ri_tensor)
     return range_images
 
 
-def _get_range_image_top_pose(frame: dataset_pb2.Frame) -> tf.Tensor:
+def _get_range_image_top_pose(frame) -> tf.Tensor:
     """Extracts range image pose tensor.
 
     Args:
       frame: a Waymo Open Dataset frame.
 
     Returns:
       Pose tensors for the range image.
     """
-    _, _, _, ri_pose = frame_utils.parse_range_image_and_camera_projection(frame)
+    _, _, _, ri_pose = frame_utils.parse_range_image_and_camera_projection(
+        frame
+    )
     assert ri_pose
     ri_pose_tensor = tf.reshape(
         tf.convert_to_tensor(value=ri_pose.data), ri_pose.shape.dims
     )
     # [H, W, 3, 3]
     ri_pose_tensor_rotation = transform_utils.get_rotation_matrix(
         ri_pose_tensor[..., 0], ri_pose_tensor[..., 1], ri_pose_tensor[..., 2]
@@ -114,15 +124,15 @@
     ri_pose_tensor = transform_utils.get_transform(
         ri_pose_tensor_rotation, ri_pose_tensor_translation
     )
     return ri_pose_tensor
 
 
 def _get_point_top_lidar(
-    range_image: Sequence[tf.Tensor], frame: dataset_pb2.Frame
+    range_image: Sequence[tf.Tensor], frame
 ) -> struct.PointTensors:
     """Gets point related tensors for the top lidar.
 
     Please refer to https://arxiv.org/pdf/1912.04838.pdf Table 2 for lidar
     specifications.
 
     Args:
@@ -174,19 +184,27 @@
         xyz_list.append(xyz)
         feature_list.append(feature)
         nlz_list.append(nlz)
         row_col_list.append(mask_idx)
 
         if i == 0:
             has_second_return = range_image[1][:, :, 0] > 0
-            has_second_return_list.append(tf.gather_nd(has_second_return, mask_idx))
-            is_second_return_list.append(tf.zeros([mask_idx.shape[0]], dtype=tf.bool))
+            has_second_return_list.append(
+                tf.gather_nd(has_second_return, mask_idx)
+            )
+            is_second_return_list.append(
+                tf.zeros([mask_idx.shape[0]], dtype=tf.bool)
+            )
         else:
-            has_second_return_list.append(tf.zeros([mask_idx.shape[0]], dtype=tf.bool))
-            is_second_return_list.append(tf.ones([mask_idx.shape[0]], dtype=tf.bool))
+            has_second_return_list.append(
+                tf.zeros([mask_idx.shape[0]], dtype=tf.bool)
+            )
+            is_second_return_list.append(
+                tf.ones([mask_idx.shape[0]], dtype=tf.bool)
+            )
 
     xyz = tf.concat(xyz_list, axis=0)
     feature = tf.concat(feature_list, axis=0)
     row_col = tf.concat(row_col_list, axis=0)
     nlz = tf.concat(nlz_list, axis=0)
     has_second_return = tf.cast(
         tf.concat(has_second_return_list, axis=0), dtype=tf.float32
@@ -199,28 +217,28 @@
         [
             feature,
             has_second_return[:, tf.newaxis],
             is_second_return[:, tf.newaxis],
         ],
         axis=-1,
     )
-    sensor_id = tf.ones([xyz.shape[0], 1], dtype=tf.int32) * dataset_pb2.LaserName.TOP
+    sensor_id = (
+        tf.ones([xyz.shape[0], 1], dtype=tf.int32) * dataset_pb2.LaserName.TOP
+    )
     ri_row_col_sensor_id = tf.concat([row_col, sensor_id], axis=-1)
 
     return struct.PointTensors(
         point_xyz=xyz,
         point_feature=feature,
         point_range_image_row_col_sensor_id=ri_row_col_sensor_id,
         label_point_nlz=nlz,
     )
 
 
-def _get_lidar_calibration(
-    frame: dataset_pb2.Frame, name: int
-) -> dataset_pb2.LaserCalibration:
+def _get_lidar_calibration(frame, name: int):
     """Gets lidar calibration for a given lidar."""
     calibration = None
     for c in frame.context.laser_calibrations:
         if c.name == name:
             calibration = c
     assert calibration is not None
     return calibration
@@ -240,45 +258,50 @@
 
     tensors = {key: _gather(value) for key, value in vars(point).items()}
     return struct.PointTensors(**tensors)
 
 
 def _get_point_lidar(
     ris: Dict[int, List[tf.Tensor]],
-    frame: dataset_pb2.Frame,
+    frame,
     max_num_points: int,
 ) -> struct.PointTensors:
-    """Gets point related tensors for non top lidar.
+    """Gets point related tensors for non-top lidar.
 
-    The main differences from top lidar extraction are related to second return and
-    point down sampling.
+    The main differences from top lidar extraction are related to second return
+    and point down sampling.
 
     Args:
       ris: Mapping from lidar ID to range image tensor. The ri format is [range,
         intensity, elongation, is_in_nlz].
       frame: a Waymo Open Dataset frame.
-      max_num_points: maximum number of points from non top lidar.
+      max_num_points: maximum number of points from non-top lidar.
 
     Returns:
       Point related tensors.
     """
     xyz_list = []
     feature_list = []
     nlz_list = []
     ri_row_col_sensor_id_list = []
 
     for sensor_id in ris.keys():
         ri_tensor = ris[sensor_id]
         assert len(ri_tensor) == 1, f"{sensor_id}"
         ri_tensor = ri_tensor[0]
         calibration = _get_lidar_calibration(frame, sensor_id)
-        extrinsic = tf.reshape(np.array(calibration.extrinsic.transform), [4, 4])
+        extrinsic = tf.reshape(
+            np.array(calibration.extrinsic.transform), [4, 4]
+        )
         beam_inclinations = range_image_utils.compute_inclination(
             tf.constant(
-                [calibration.beam_inclination_min, calibration.beam_inclination_max]
+                [
+                    calibration.beam_inclination_min,
+                    calibration.beam_inclination_max,
+                ]
             ),
             height=ri_tensor.shape[0],
         )
         beam_inclinations = tf.reverse(beam_inclinations, axis=[-1])
         xyz = range_image_utils.extract_point_cloud_from_range_image(
             tf.expand_dims(ri_tensor[..., 0], axis=0),
             tf.expand_dims(extrinsic, axis=0),
@@ -286,24 +309,28 @@
         )
         mask = ri_tensor[:, :, 0] > 0
         mask_idx = tf.cast(tf.where(mask), dtype=tf.int32)
 
         xyz = tf.gather_nd(tf.squeeze(xyz, axis=0), mask_idx)
         feature = tf.gather_nd(ri_tensor[:, :, 1:3], mask_idx)
         feature = tf.concat(
-            [feature, tf.zeros([feature.shape[0], 2], dtype=tf.float32)], axis=-1
+            [feature, tf.zeros([feature.shape[0], 2], dtype=tf.float32)],
+            axis=-1,
         )
         nlz = tf.gather_nd(ri_tensor[:, :, -1] > 0, mask_idx)
 
         xyz_list.append(xyz)
         feature_list.append(feature)
         nlz_list.append(nlz)
         ri_row_col_sensor_id_list.append(
             tf.concat(
-                [mask_idx, sensor_id * tf.ones([nlz.shape[0], 1], dtype=tf.int32)],
+                [
+                    mask_idx,
+                    sensor_id * tf.ones([nlz.shape[0], 1], dtype=tf.int32),
+                ],
                 axis=-1,
             )
         )
 
     xyz = tf.concat(xyz_list, axis=0)
     feature = tf.concat(feature_list, axis=0)
     nlz = tf.concat(nlz_list, axis=0)
@@ -315,44 +342,45 @@
         point_range_image_row_col_sensor_id=ri_row_col_sensor_id,
         label_point_nlz=nlz,
     )
     point_tensors = _downsample(point_tensors, max_num_points)
     return point_tensors
 
 
-def _get_point(
-    frame: dataset_pb2.Frame, max_num_lidar_points: int
-) -> struct.PointTensors:
+def _get_point(frame, max_num_lidar_points: int) -> struct.PointTensors:
     """Gets point related tensors from a Waymo Open Dataset frame.
 
     Args:
       frame: a Waymo Open Dataset frame.
-      max_num_lidar_points: maximum number of points from non top lidars.
+      max_num_lidar_points: maximum number of points from non-top lidars.
 
     Returns:
       Point related tensors.
     """
     range_images = _decode_range_images(frame)
     point_top_lidar = _get_point_top_lidar(
         range_images[dataset_pb2.LaserName.TOP], frame
     )
 
     range_images.pop(dataset_pb2.LaserName.TOP)
-    point_tensors_lidar = _get_point_lidar(range_images, frame, max_num_lidar_points)
+    point_tensors_lidar = _get_point_lidar(
+        range_images, frame, max_num_lidar_points
+    )
 
     merged = {}
     for key in vars(point_tensors_lidar).keys():
         merged[key] = tf.concat(
-            [getattr(point_tensors_lidar, key), getattr(point_top_lidar, key)], axis=0
+            [getattr(point_tensors_lidar, key), getattr(point_top_lidar, key)],
+            axis=0,
         )
     return struct.PointTensors(**merged)
 
 
 def _get_point_label_box(
-    frame: dataset_pb2.Frame,
+    frame,
 ) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:
     """Extracts 3D box labels from a Waymo Open Dataset frame.
 
     Args:
       frame: a Waymo Open Dataset frame.
 
     Returns:
@@ -396,30 +424,39 @@
                 meta.speed_y,
                 meta.accel_x,
                 meta.accel_y,
             ]
         )
         box_class_list.append(model_object_type)
         box_id = tf.bitcast(
-            tf.fingerprint(tf.expand_dims(label.id.encode(encoding="ascii"), 0))[0],
+            tf.fingerprint(
+                tf.expand_dims(label.id.encode(encoding="ascii"), 0)
+            )[0],
             tf.int64,
         )
         box_id_list.append(box_id)
         box_density_list.append(density)
         box_detection_difficulty_list.append(detection_difficulty)
 
     box_3d = tf.reshape(tf.constant(box_3d_list, dtype=tf.float32), [-1, 7])
     box_meta = tf.reshape(tf.constant(box_meta_list, dtype=tf.float32), [-1, 4])
     box_class = tf.constant(box_class_list, dtype=tf.int32)
     box_id = tf.stack(box_id_list)
     box_density = tf.constant(box_density_list, dtype=tf.int32)
     box_detection_difficulty = tf.constant(
         box_detection_difficulty_list, dtype=tf.int32
     )
-    return (box_3d, box_meta, box_class, box_id, box_density, box_detection_difficulty)
+    return (
+        box_3d,
+        box_meta,
+        box_class,
+        box_id,
+        box_density,
+        box_detection_difficulty,
+    )
 
 
 def _get_box_class_per_point(
     box: tf.Tensor, box_class: tf.Tensor, point_xyz: tf.Tensor
 ) -> tf.Tensor:
     """Extracts point labels.
 
@@ -439,22 +476,22 @@
     # [N, M]
     point_in_box = box_utils.is_within_box_3d(point_xyz, box)
     # [N]
     point_in_any_box = tf.math.reduce_any(point_in_box, axis=-1)
     # [N]
     point_box_idx = tf.math.argmax(point_in_box, axis=-1, output_type=tf.int32)
     # [N]
-    point_box_class = tf.where(point_in_any_box, tf.gather(box_class, point_box_idx), 0)
+    point_box_class = tf.where(
+        point_in_any_box, tf.gather(box_class, point_box_idx), 0
+    )
 
     return point_box_class
 
 
-def _get_point_label(
-    frame: dataset_pb2.Frame, point_xyz: tf.Tensor
-) -> struct.LabelTensors:
+def _get_point_label(frame, point_xyz: tf.Tensor) -> struct.LabelTensors:
     """Extracts labels.
 
     Args:
       frame: an open dataset frame.
       point_xyz: [N, 3] tensor representing point xyz.
 
     Returns:
@@ -498,15 +535,17 @@
     loc = sdc_pose[..., 0:3, 3]
     return (
         tf.linalg.matmul(point_vehicle_xyz, rot, transpose_b=True)
         + loc[..., tf.newaxis, :]
     )
 
 
-def _point_global_to_vehicle(point_xyz: tf.Tensor, sdc_pose: tf.Tensor) -> tf.Tensor:
+def _point_global_to_vehicle(
+    point_xyz: tf.Tensor, sdc_pose: tf.Tensor
+) -> tf.Tensor:
     """Transforms points from global to vehicle frame.
 
     Args:
       point_xyz: [..., N, 3] global xyz.
       sdc_pose: [..., 4, 4] the SDC pose.
 
     Returns:
@@ -516,15 +555,17 @@
     loc = sdc_pose[..., 0:3, 3]
     return (
         tf.linalg.matmul(point_xyz, rot)
         + voxel_utils.inv_loc(rot, loc)[..., tf.newaxis, :]
     )
 
 
-def _box_3d_vehicle_to_global(box_3d: tf.Tensor, sdc_pose: tf.Tensor) -> tf.Tensor:
+def _box_3d_vehicle_to_global(
+    box_3d: tf.Tensor, sdc_pose: tf.Tensor
+) -> tf.Tensor:
     """Transforms 3D boxes from vehicle to global frame.
 
     Args:
       box_3d: [..., N, 7] 3d boxes in vehicle frame.
       sdc_pose: [..., 4, 4] the SDC pose.
 
     Returns:
@@ -532,21 +573,24 @@
     """
     center = box_3d[..., 0:3]
     dim = box_3d[..., 3:6]
     heading = box_3d[..., 6]
 
     new_center = _point_vehicle_to_global(center, sdc_pose)
     new_heading = (
-        heading + tf.atan2(sdc_pose[..., 1, 0], sdc_pose[..., 0, 0])[..., tf.newaxis]
+        heading
+        + tf.atan2(sdc_pose[..., 1, 0], sdc_pose[..., 0, 0])[..., tf.newaxis]
     )
 
     return tf.concat([new_center, dim, new_heading[..., tf.newaxis]], axis=-1)
 
 
-def _box_3d_global_to_vehicle(box_3d: tf.Tensor, sdc_pose: tf.Tensor) -> tf.Tensor:
+def _box_3d_global_to_vehicle(
+    box_3d: tf.Tensor, sdc_pose: tf.Tensor
+) -> tf.Tensor:
     """Transforms 3D boxes from global to vehicle frame.
 
     Args:
       box_3d: [..., N, 7] 3d boxes in global frame.
       sdc_pose: [..., 4, 4] the SDC pose.
 
     Returns:
@@ -554,71 +598,79 @@
     """
     center = box_3d[..., 0:3]
     dim = box_3d[..., 3:6]
     heading = box_3d[..., 6]
 
     new_center = _point_global_to_vehicle(center, sdc_pose)
     new_heading = (
-        heading + tf.atan2(sdc_pose[..., 0, 1], sdc_pose[..., 0, 0])[..., tf.newaxis]
+        heading
+        + tf.atan2(sdc_pose[..., 0, 1], sdc_pose[..., 0, 0])[..., tf.newaxis]
     )
 
     return tf.concat([new_center, dim, new_heading[..., tf.newaxis]], axis=-1)
 
 
-def build_tensors_from_wod_frame(frame: dataset_pb2.Frame) -> Dict[str, tf.Tensor]:
+def build_tensors_from_wod_frame(frame) -> Dict[str, tf.Tensor]:
     """Builds tensors from a Waymo Open Dataset frame.
 
-    This function is to convert range image to point cloud. User can also work with
-    range image directly with frame_utils functions from waymo_open_dataset.
+    This function is to convert range image to point cloud. User can also work
+    with range image directly with frame_utils functions from
+    waymo_open_dataset.
 
     Args:
       frame: a Waymo Open Dataset frame.
 
     Returns:
       Flat dictionary of tensors.
     """
-
-    frame_id_bytes = "{}_{}".format(frame.context.name, frame.timestamp_micros).encode(
-        encoding="ascii"
+    assert_waymo_open_dataset_installed(
+        "keras_cv.datasets.waymo.build_tensors_from_wod_frame()"
     )
+
+    frame_id_bytes = "{}_{}".format(
+        frame.context.name, frame.timestamp_micros
+    ).encode(encoding="ascii")
     frame_id = tf.bitcast(
         tf.fingerprint(tf.expand_dims(frame_id_bytes, 0))[0], tf.int64
     )
 
     timestamp_micros = tf.constant(frame.timestamp_micros, dtype=tf.int64)
     pose = tf.convert_to_tensor(
-        value=np.reshape(np.array(frame.pose.transform), [4, 4]), dtype_hint=tf.float32
+        value=np.reshape(np.array(frame.pose.transform), [4, 4]),
+        dtype_hint=tf.float32,
     )
 
     point_tensors = _get_point(frame, _MAX_NUM_NON_TOP_LIDAR_POINTS)
     point_label_tensors = _get_point_label(frame, point_tensors.point_xyz)
 
     # Transforms lidar frames to global coordinates.
-    point_tensors.point_xyz = _point_vehicle_to_global(point_tensors.point_xyz, pose)
+    point_tensors.point_xyz = _point_vehicle_to_global(
+        point_tensors.point_xyz, pose
+    )
     point_label_tensors.label_box = _box_3d_vehicle_to_global(
         point_label_tensors.label_box, pose
     )
 
     # Constructs final results.
     num_points = point_tensors.point_xyz.shape[0]
     return {
         "frame_id": frame_id,
         "timestamp_offset": tf.constant(0.0, dtype=tf.float32),
         "timestamp_micros": timestamp_micros,
         "pose": pose,
         "point_xyz": point_tensors.point_xyz,
         "point_feature": point_tensors.point_feature,
         "point_mask": tf.ones([num_points], dtype=tf.bool),
-        "point_range_image_row_col_sensor_id": point_tensors.point_range_image_row_col_sensor_id,
+        "point_range_image_row_col_sensor_id": point_tensors.point_range_image_row_col_sensor_id,  # noqa: E501
         "label_box": point_label_tensors.label_box,
         "label_box_id": point_label_tensors.label_box_id,
         "label_box_meta": point_label_tensors.label_box_meta,
         "label_box_class": point_label_tensors.label_box_class,
         "label_box_density": point_label_tensors.label_box_density,
-        "label_box_detection_difficulty": point_label_tensors.label_box_detection_difficulty,
+        "label_box_detection_difficulty": point_label_tensors.label_box_detection_difficulty,  # noqa: E501
         "label_box_mask": point_label_tensors.label_box_mask,
         "label_point_class": point_label_tensors.label_point_class,
         "label_point_nlz": point_tensors.label_point_nlz,
     }
 
 
 def pad_or_trim_tensors(
@@ -664,24 +716,31 @@
     for key in box_tensor_keys:
         t = frame[key]
         if t is not None:
             frame[key] = _pad_fn(t, max_num_label_box)
     return frame
 
 
-def transform_to_vehicle_frame(frame: Dict[str, tf.Tensor]) -> Dict[str, tf.Tensor]:
-    """Transform tensors in a frame from global coordinates to vehicle coordinates.
+def transform_to_vehicle_frame(
+    frame: Dict[str, tf.Tensor]
+) -> Dict[str, tf.Tensor]:
+    """Transform tensors in a frame from global coordinates to vehicle
+    coordinates.
 
     Args:
-      frame: a dictionary of feature tensors from a Waymo Open Dataset frame in global frame.
+      frame: a dictionary of feature tensors from a Waymo Open Dataset frame in
+        global frame.
 
 
     Returns:
       A dictionary of feature tensors in vehicle frame.
     """
+    assert_waymo_open_dataset_installed(
+        "keras_cv.datasets.waymo.transform_to_vehicle_frame()"
+    )
 
     def _transform_to_vehicle_frame(
         point_global_xyz: tf.Tensor,
         point_mask: tf.Tensor,
         box_global: tf.Tensor,
         box_mask: tf.Tensor,
         sdc_pose: tf.Tensor,
@@ -709,41 +768,80 @@
         frame["point_mask"] = tf.logical_and(
             frame["point_mask"],
             tf.logical_not(tf.cast(frame["label_point_nlz"], tf.bool)),
         )
     return frame
 
 
+def convert_to_center_pillar_inputs(
+    frame: Dict[str, tf.Tensor]
+) -> Dict[str, Any]:
+    """Converts an input frame into CenterPillar input format.
+
+    Args:
+      frame: a dictionary of feature tensors from a Waymo Open Dataset frame
+
+    Returns:
+      A dictionary of two tensor dictionaries with keys "point_clouds"
+      and "3d_boxes".
+    """
+    point_clouds = {
+        "point_xyz": frame["point_xyz"],
+        "point_feature": frame["point_feature"],
+        "point_mask": frame["point_mask"],
+    }
+    boxes = {
+        "boxes": frame["label_box"],
+        "classes": frame["label_box_class"],
+        "difficulty": frame["label_box_detection_difficulty"],
+        "mask": frame["label_box_mask"],
+    }
+    y = {
+        "point_clouds": point_clouds,
+        "3d_boxes": boxes,
+    }
+    return y
+
+
 def build_tensors_for_augmentation(
     frame: Dict[str, tf.Tensor]
 ) -> Tuple[tf.Tensor, tf.Tensor]:
     """Builds tensors for data augmentation from an input frame.
 
     Args:
       frame: a dictionary of feature tensors from a Waymo Open Dataset frame
 
     Returns:
       A dictionary of two tensors with keys "point_clouds" and "bounding_boxes"
       and values which are tensors of shapes [num points, num features] and
       [num boxes, num features]).
     """
+    assert_waymo_open_dataset_installed(
+        "keras_cv.datasets.waymo.build_tensors_for_augmentation()"
+    )
     point_cloud = tf.concat(
         [
             frame["point_xyz"][tf.newaxis, ...],
             frame["point_feature"][tf.newaxis, ...],
             tf.cast(frame["point_mask"], tf.float32)[tf.newaxis, :, tf.newaxis],
         ],
         axis=-1,
     )
     boxes = tf.concat(
         [
             frame["label_box"][tf.newaxis, :],
-            tf.cast(frame["label_box_class"], tf.float32)[tf.newaxis, :, tf.newaxis],
-            tf.cast(frame["label_box_mask"], tf.float32)[tf.newaxis, :, tf.newaxis],
-            tf.cast(frame["label_box_density"], tf.float32)[tf.newaxis, :, tf.newaxis],
+            tf.cast(frame["label_box_class"], tf.float32)[
+                tf.newaxis, :, tf.newaxis
+            ],
+            tf.cast(frame["label_box_mask"], tf.float32)[
+                tf.newaxis, :, tf.newaxis
+            ],
+            tf.cast(frame["label_box_density"], tf.float32)[
+                tf.newaxis, :, tf.newaxis
+            ],
             tf.cast(frame["label_box_detection_difficulty"], tf.float32)[
                 tf.newaxis, :, tf.newaxis
             ],
         ],
         axis=-1,
     )
     return {
```

## keras_cv/datasets/waymo/transformer_test.py

```diff
@@ -29,15 +29,16 @@
     def setUp(self):
         super().setUp()
         self.test_data_path = os.path.abspath(
             os.path.join(os.path.abspath(__file__), os.path.pardir, "test_data")
         )
 
     @pytest.mark.skipif(
-        "TEST_WAYMO_DEPS" not in os.environ or os.environ["TEST_WAYMO_DEPS"] != "true",
+        "TEST_WAYMO_DEPS" not in os.environ
+        or os.environ["TEST_WAYMO_DEPS"] != "true",
         reason="Requires Waymo Open Dataset package",
     )
     def test_load_and_transform(self):
         tf_dataset = load(self.test_data_path)
 
         # Extract records into a list.
         dataset = list(tf_dataset)
@@ -52,38 +53,45 @@
         self.assertAllEqual(
             lidar_tensors["label_box_detection_difficulty"],
             tf.zeros(num_boxes, dtype=tf.int32),
         )
 
         # Laser points.
         point_xyz_mean = tf.reduce_mean(lidar_tensors["point_xyz"], axis=0)
-        self.assertAllClose(point_xyz_mean, lidar_tensors["pose"][:3, 3], atol=100)
-        point_feature_mean = tf.reduce_mean(lidar_tensors["point_feature"], axis=0)
+        self.assertAllClose(
+            point_xyz_mean, lidar_tensors["pose"][:3, 3], atol=100
+        )
+        point_feature_mean = tf.reduce_mean(
+            lidar_tensors["point_feature"], axis=0
+        )
         self.assertAllGreater(point_feature_mean[0], 0)
         self.assertAllGreater(tf.abs(point_feature_mean[1]), 1e-6)
         self.assertAllGreater(point_feature_mean[2:4], 0)
         self.assertTrue(tf.math.reduce_all(lidar_tensors["point_mask"]))
 
         # Laser labels.
         self.assertEqual(lidar_tensors["label_box_id"].shape[0], num_boxes)
         self.assertEqual(lidar_tensors["label_box_meta"].shape[0], num_boxes)
         self.assertEqual(lidar_tensors["label_box_class"].shape[0], num_boxes)
         self.assertEqual(lidar_tensors["label_box_density"].shape[0], num_boxes)
         self.assertTrue(tf.math.reduce_all(lidar_tensors["label_box_mask"]))
-        self.assertAllGreater(tf.math.reduce_max(lidar_tensors["label_point_class"]), 0)
+        self.assertAllGreater(
+            tf.math.reduce_max(lidar_tensors["label_point_class"]), 0
+        )
 
         # Multi-frame tensors for augmentation.
         augmented_example = next(
             iter(tf_dataset.map(transformer.build_tensors_for_augmentation))
         )
         self.assertEqual(augmented_example["point_clouds"].shape, [183142, 8])
         self.assertEqual(augmented_example["bounding_boxes"].shape, [16, 11])
 
     @pytest.mark.skipif(
-        "TEST_WAYMO_DEPS" not in os.environ or os.environ["TEST_WAYMO_DEPS"] != "true",
+        "TEST_WAYMO_DEPS" not in os.environ
+        or os.environ["TEST_WAYMO_DEPS"] != "true",
         reason="Requires Waymo Open Dataset package",
     )
     def test_pad_and_transform_to_vehicle(self):
         dataset = load(self.test_data_path)
         dataset = dataset.map(
             lambda x: (
                 transformer.pad_or_trim_tensors(
@@ -108,8 +116,50 @@
         # Laser labels.
         self.assertEqual(example["label_box_id"].shape[0], 1000)
         self.assertEqual(example["label_box_meta"].shape[0], 1000)
         self.assertEqual(example["label_box_class"].shape[0], 1000)
         self.assertEqual(example["label_box_density"].shape[0], 1000)
         self.assertEqual(example["label_box_mask"].shape, [1000])
         self.assertTrue(tf.math.reduce_any(example["label_box_mask"]))
-        self.assertAllGreater(tf.math.reduce_max(example["label_point_class"]), 0)
+        self.assertAllGreater(
+            tf.math.reduce_max(example["label_point_class"]), 0
+        )
+
+    @pytest.mark.skipif(
+        "TEST_WAYMO_DEPS" not in os.environ
+        or os.environ["TEST_WAYMO_DEPS"] != "true",
+        reason="Requires Waymo Open Dataset package",
+    )
+    def test_convert_to_center_pillar_inputs(self):
+        dataset = load(self.test_data_path)
+        dataset = dataset.map(
+            lambda x: (
+                transformer.convert_to_center_pillar_inputs(
+                    transformer.pad_or_trim_tensors(
+                        transformer.transform_to_vehicle_frame(x)
+                    )
+                )
+            )
+        )
+        example = next(iter(dataset))
+
+        # Laser points.
+        point_clouds = example["point_clouds"]
+        self.assertEqual(point_clouds["point_xyz"].shape, [199600, 3])
+        self.assertEqual(point_clouds["point_feature"].shape, [199600, 4])
+        self.assertEqual(point_clouds["point_mask"].shape, [199600])
+        point_feature_mean = tf.reduce_mean(
+            point_clouds["point_feature"], axis=0
+        )
+        self.assertAllGreater(point_feature_mean[0], 0)
+        self.assertAllGreater(tf.abs(point_feature_mean[1]), 1e-6)
+        self.assertAllGreater(point_feature_mean[2:4], 0)
+        self.assertTrue(tf.math.reduce_any(point_clouds["point_mask"]))
+
+        # Laser labels.
+        boxes = example["3d_boxes"]
+        self.assertEqual(boxes["boxes"].shape[0], 1000)
+        self.assertEqual(boxes["classes"].shape[0], 1000)
+        self.assertEqual(boxes["difficulty"].shape[0], 1000)
+        self.assertEqual(boxes["mask"].shape, [1000])
+        self.assertTrue(tf.math.reduce_any(boxes["mask"]))
+        self.assertAllGreater(tf.math.reduce_max(boxes["classes"]), 0)
```

## keras_cv/keypoint/converters.py

```diff
@@ -59,24 +59,24 @@
 
 
 def convert_format(keypoints, source, target, images=None, dtype=None):
     """Converts keypoints from one format to another.
 
     Supported formats are:
     - `"xy"`, absolute pixel positions.
-    - `"rel_xyxy"`.  relative pixel positions.
+    - `"rel_xyxy"`. relative pixel positions.
 
-    Formats are case insensitive.  It is recommended that you
+    Formats are case-insensitive. It is recommended that you
     capitalize width and height to maximize the visual difference
     between `"xyWH"` and `"xyxy"`.
 
     Relative formats, abbreviated `rel`, make use of the shapes of the
-    `images` passsed.  In these formats, the coordinates, widths, and
+    `images` passed. In these formats, the coordinates, widths, and
     heights are all specified as percentages of the host image.
-    `images` may be a ragged Tensor.  Note that using a ragged Tensor
+    `images` may be a ragged Tensor. Note that using a ragged Tensor
     for images may cause a substantial performance loss, as each image
     will need to be processed separately due to the mismatching image
     shapes.
 
     Usage:
 
     ```python
@@ -89,26 +89,26 @@
     )
     ```
 
     Args:
         keypoints: tf.Tensor or tf.RaggedTensor representing keypoints
             in the format specified in the `source` parameter.
             `keypoints` can optionally have extra dimensions stacked
-            on the final axis to store metadata.  keypoints should
+            on the final axis to store metadata. keypoints should
             have a rank between 2 and 4, with the shape
             `[num_boxes,*]`, `[batch_size, num_boxes, *]` or
             `[batch_size, num_groups, num_keypoints,*]`.
         source: One of {" ".join([f'"{f}"' for f in
-            TO_XY_CONVERTERS.keys()])}.  Used to specify the original
+            TO_XY_CONVERTERS.keys()])}. Used to specify the original
             format of the `boxes` parameter.
         target: One of {" ".join([f'"{f}"' for f in
-            TO_XY_CONVERTERS.keys()])}.  Used to specify the
+            TO_XY_CONVERTERS.keys()])}. Used to specify the
             destination format of the `boxes` parameter.
         images: (Optional) a batch of images aligned with `boxes` on
-            the first axis.  Should be rank 3 (`HWC` format) or 4
+            the first axis. Should be rank 3 (`HWC` format) or 4
             (`BHWC` format). Used in some converters to compute
             relative pixel values of the bounding box dimensions.
             Required when transforming from a rel format to a non-rel
             format.
         dtype: the data type to use when transforming the boxes.
             Defaults to None, i.e. `keypoints` dtype.
     """
@@ -165,16 +165,17 @@
             raise ValueError(
                 "Expected images rank to be 3 or 4, got "
                 f"len(images.shape)={images_rank}."
             )
         images_include_batch = images_rank == 4
         if keypoints_includes_batch != images_include_batch:
             raise ValueError(
-                "convert_format() expects both `keypoints` and `images` to be batched "
-                f"or both unbatched. Received len(keypoints.shape)={keypoints_rank}, "
+                "convert_format() expects both `keypoints` and `images` to be "
+                "batched or both unbatched. Received "
+                f"len(keypoints.shape)={keypoints_rank}, "
                 f"len(images.shape)={images_rank}. Expected either "
                 "len(keypoints.shape)=2 and len(images.shape)=3, or "
                 "len(keypoints.shape)>=3 and len(images.shape)=4."
             )
         if not images_include_batch:
             images = tf.expand_dims(images, axis=0)
```

## keras_cv/keypoint/converters_test.py

```diff
@@ -105,44 +105,48 @@
                 source_keypoints, source=source, target=target, images=images
             ),
             target_keypoints,
         )
 
     def test_raise_errors_when_missing_shape(self):
         with self.assertRaises(ValueError) as e:
-            keypoint.convert_format(keypoints["xy"], source="xy", target="rel_xy")
+            keypoint.convert_format(
+                keypoints["xy"], source="xy", target="rel_xy"
+            )
 
         self.assertEqual(
             str(e.exception),
             "convert_format() must receive `images` when transforming "
             "between relative and absolute formats. convert_format() "
             "received source=`xy`, target=`rel_xy`, but images=None",
         )
 
     @parameterized.named_parameters(
         (
             "keypoint_rank",
             tf.ones([2, 3, 4, 2, 1]),
             None,
-            "Expected keypoints rank to be in [2, 4], got len(keypoints.shape)=5.",
+            "Expected keypoints rank to be in [2, 4], got "
+            "len(keypoints.shape)=5.",
         ),
         (
             "images_rank",
             tf.ones([4, 2]),
             tf.ones([35, 35]),
             "Expected images rank to be 3 or 4, got len(images.shape)=2.",
         ),
         (
             "batch_mismatch",
             tf.ones([2, 4, 2]),
             tf.ones([35, 35, 3]),
-            "convert_format() expects both `keypoints` and `images` to be batched or "
-            "both unbatched. Received len(keypoints.shape)=3, len(images.shape)=3. "
-            "Expected either len(keypoints.shape)=2 and len(images.shape)=3, or "
-            "len(keypoints.shape)>=3 and len(images.shape)=4.",
+            "convert_format() expects both `keypoints` and `images` to be "
+            "batched or both unbatched. Received len(keypoints.shape)=3, "
+            "len(images.shape)=3. Expected either len(keypoints.shape)=2 and "
+            "len(images.shape)=3, or len(keypoints.shape)>=3 and "
+            "len(images.shape)=4.",
         ),
     )
     def test_input_format_exception(self, keypoints, images, expected):
         with self.assertRaises(ValueError) as e:
             keypoint.convert_format(
                 keypoints, source="xy", target="rel_xy", images=images
             )
```

## keras_cv/keypoint/formats.py

```diff
@@ -39,15 +39,15 @@
 
 
 class REL_XY:
     """REL_XY contains axis indices for the REL_XY format.
 
 
     REL_XY is like XY, but each value is relative to the width and height of the
-    origin image.  Values are percentages of the origin images' width and height
+    origin image. Values are percentages of the origin images' width and height
     respectively.
 
     The REL_XY format consists of the following required indices:
 
     - X: the width position
     - Y: the height position
```

## keras_cv/keypoint/utils_test.py

```diff
@@ -34,18 +34,22 @@
         ),
         (
             "ragged input",
             tf.RaggedTensor.from_row_lengths(
                 [[10.0, 20.0], [30.0, 40.0], [50.0, 50.0]], [2, 1]
             ),
             tf.zeros([50, 50, 3]),
-            tf.RaggedTensor.from_row_lengths([[10.0, 20.0], [30.0, 40.0]], [2, 0]),
+            tf.RaggedTensor.from_row_lengths(
+                [[10.0, 20.0], [30.0, 40.0]], [2, 0]
+            ),
         ),
         (
             "height - width confusion",
             tf.constant([[[10.0, 20.0]], [[40.0, 30.0]], [[30.0, 40.0]]]),
             tf.zeros((50, 40, 3)),
-            tf.ragged.constant([[[10.0, 20.0]], [], [[30.0, 40.0]]], ragged_rank=1),
+            tf.ragged.constant(
+                [[[10.0, 20.0]], [], [[30.0, 40.0]]], ragged_rank=1
+            ),
         ),
     )
     def test_result(self, keypoints, image, expected):
         self.assertAllClose(filter_out_of_image(keypoints, image), expected)
```

## keras_cv/layers/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -20,20 +20,19 @@
 from keras_cv.layers.fusedmbconv import FusedMBConvBlock
 from keras_cv.layers.mbconv import MBConvBlock
 from keras_cv.layers.object_detection.anchor_generator import AnchorGenerator
 from keras_cv.layers.object_detection.box_matcher import BoxMatcher
 from keras_cv.layers.object_detection.multi_class_non_max_suppression import (
     MultiClassNonMaxSuppression,
 )
-from keras_cv.layers.object_detection.retina_net_label_encoder import (
-    RetinaNetLabelEncoder,
+from keras_cv.layers.object_detection_3d.centernet_label_encoder import (
+    CenterNetLabelEncoder,
 )
 from keras_cv.layers.object_detection_3d.voxelization import DynamicVoxelization
 from keras_cv.layers.preprocessing.aug_mix import AugMix
-from keras_cv.layers.preprocessing.augmenter import Augmenter
 from keras_cv.layers.preprocessing.auto_contrast import AutoContrast
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.layers.preprocessing.channel_shuffle import ChannelShuffle
 from keras_cv.layers.preprocessing.cut_mix import CutMix
 from keras_cv.layers.preprocessing.equalization import Equalization
@@ -47,51 +46,68 @@
 from keras_cv.layers.preprocessing.posterization import Posterization
 from keras_cv.layers.preprocessing.rand_augment import RandAugment
 from keras_cv.layers.preprocessing.random_aspect_ratio import RandomAspectRatio
 from keras_cv.layers.preprocessing.random_augmentation_pipeline import (
     RandomAugmentationPipeline,
 )
 from keras_cv.layers.preprocessing.random_brightness import RandomBrightness
-from keras_cv.layers.preprocessing.random_channel_shift import RandomChannelShift
+from keras_cv.layers.preprocessing.random_channel_shift import (
+    RandomChannelShift,
+)
 from keras_cv.layers.preprocessing.random_choice import RandomChoice
 from keras_cv.layers.preprocessing.random_color_degeneration import (
     RandomColorDegeneration,
 )
 from keras_cv.layers.preprocessing.random_color_jitter import RandomColorJitter
 from keras_cv.layers.preprocessing.random_contrast import RandomContrast
 from keras_cv.layers.preprocessing.random_crop import RandomCrop
-from keras_cv.layers.preprocessing.random_crop_and_resize import RandomCropAndResize
+from keras_cv.layers.preprocessing.random_crop_and_resize import (
+    RandomCropAndResize,
+)
 from keras_cv.layers.preprocessing.random_cutout import RandomCutout
 from keras_cv.layers.preprocessing.random_flip import RandomFlip
-from keras_cv.layers.preprocessing.random_gaussian_blur import RandomGaussianBlur
+from keras_cv.layers.preprocessing.random_gaussian_blur import (
+    RandomGaussianBlur,
+)
 from keras_cv.layers.preprocessing.random_hue import RandomHue
 from keras_cv.layers.preprocessing.random_jpeg_quality import RandomJpegQuality
 from keras_cv.layers.preprocessing.random_rotation import RandomRotation
 from keras_cv.layers.preprocessing.random_saturation import RandomSaturation
 from keras_cv.layers.preprocessing.random_sharpness import RandomSharpness
 from keras_cv.layers.preprocessing.random_shear import RandomShear
 from keras_cv.layers.preprocessing.random_translation import RandomTranslation
 from keras_cv.layers.preprocessing.random_zoom import RandomZoom
-from keras_cv.layers.preprocessing.randomly_zoomed_crop import RandomlyZoomedCrop
-from keras_cv.layers.preprocessing.repeated_augmentation import RepeatedAugmentation
+from keras_cv.layers.preprocessing.randomly_zoomed_crop import (
+    RandomlyZoomedCrop,
+)
+from keras_cv.layers.preprocessing.repeated_augmentation import (
+    RepeatedAugmentation,
+)
 from keras_cv.layers.preprocessing.rescaling import Rescaling
 from keras_cv.layers.preprocessing.resizing import Resizing
 from keras_cv.layers.preprocessing.solarization import Solarization
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
+)
 from keras_cv.layers.preprocessing_3d.frustum_random_dropping_points import (
     FrustumRandomDroppingPoints,
 )
-from keras_cv.layers.preprocessing_3d.frustum_random_point_feature_noise import (
+from keras_cv.layers.preprocessing_3d.frustum_random_point_feature_noise import (  # noqa: E501
     FrustumRandomPointFeatureNoise,
 )
 from keras_cv.layers.preprocessing_3d.global_random_dropping_points import (
     GlobalRandomDroppingPoints,
 )
 from keras_cv.layers.preprocessing_3d.global_random_flip import GlobalRandomFlip
-from keras_cv.layers.preprocessing_3d.global_random_rotation import GlobalRandomRotation
-from keras_cv.layers.preprocessing_3d.global_random_scaling import GlobalRandomScaling
+from keras_cv.layers.preprocessing_3d.global_random_rotation import (
+    GlobalRandomRotation,
+)
+from keras_cv.layers.preprocessing_3d.global_random_scaling import (
+    GlobalRandomScaling,
+)
 from keras_cv.layers.preprocessing_3d.global_random_translation import (
     GlobalRandomTranslation,
 )
 from keras_cv.layers.preprocessing_3d.group_points_by_bounding_boxes import (
     GroupPointsByBoundingBoxes,
 )
 from keras_cv.layers.preprocessing_3d.random_copy_paste import RandomCopyPaste
```

## keras_cv/layers/feature_pyramid.py

```diff
@@ -8,82 +8,92 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
+from tensorflow import keras
 
 
-# TODO(scottzhu): Register it later due to the conflict in the retina_net
-# @tf.keras.utils.register_keras_serializable(package="keras_cv")
-class FeaturePyramid(tf.keras.layers.Layer):
+# TODO(scottzhu): Register it later due to the conflict in the retinanet
+# @keras.utils.register_keras_serializable(package="keras_cv")
+class FeaturePyramid(keras.layers.Layer):
     """Implements a Feature Pyramid Network.
 
     This implements the paper:
-      Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and
-      Serge Belongie.
-      Feature Pyramid Networks for Object Detection.
+      Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan,
+      and Serge Belongie. Feature Pyramid Networks for Object Detection.
       (https://arxiv.org/pdf/1612.03144)
 
     Feature Pyramid Networks (FPNs) are basic components that are added to an
-    existing feature extractor (CNN) to combine features at different scales. For the
-    basic FPN, the inputs are features `Ci` from different levels of a CNN, which is
-    usually the last block for each level, where the feature is scaled from the image
-    by a factor of `1/2^i`.
-
-    There is an output associated with each level in the basic FPN. The output Pi
-    at level `i` (corresponding to Ci) is given by performing a merge operation on
-    the outputs of:
+    existing feature extractor (CNN) to combine features at different scales.
+    For the basic FPN, the inputs are features `Ci` from different levels of a
+    CNN, which is usually the last block for each level, where the feature is
+    scaled from the image by a factor of `1/2^i`.
+
+    There is an output associated with each level in the basic FPN. The output
+    Pi at level `i` (corresponding to Ci) is given by performing a merge
+    operation on the outputs of:
 
-    1) a lateral operation on Ci (usually a conv2D layer with kernel = 1 and strides = 1)
+    1) a lateral operation on Ci (usually a conv2D layer with kernel = 1 and
+       strides = 1)
     2) a top-down upsampling operation from Pi+1 (except for the top most level)
 
     The final output of each level will also have a conv2D operation
-    (usually with kernel = 3 and strides = 1).
+    (typically with kernel = 3 and strides = 1).
 
     The inputs to the layer should be a dict with int keys should match the
-    pyramid_levels, e.g. for `pyramid_levels` = [2,3,4,5], the expected input dict should
-    be `{2:c2, 3:c3, 4:c4, 5:c5}`.
+    pyramid_levels, e.g. for `pyramid_levels` = [2,3,4,5], the expected input
+    dict should be `{2:c2, 3:c3, 4:c4, 5:c5}`.
 
-    The output of the layer will have same structures as the inputs, a dict with int keys
-    and value for each of the level.
+    The output of the layer will have same structures as the inputs, a dict with
+    int keys and value for each of the level.
 
     Args:
         min_level: a python int for the lowest level of the pyramid for
             feature extraction.
         max_level: a python int for the highest level of the pyramid for
             feature extraction.
         num_channels: an integer representing the number of channels for the FPN
-            operations. Defaults to 256.
-        lateral_layers: a python dict with int keys that matches to each of the pyramid
-            level. The values of the dict should be `keras.Layer`, which will be called
-            with feature activation outputs from backbone at each level. Default to
-            None, and a `keras.Conv2D` layer with kernel 1x1 will be created for each
-            pyramid level.
-        output_layers: a python dict with int keys that matches to each of the pyramid
-            level. The values of the dict should be `keras.Layer`, which will be called
-            with feature inputs and merged result from upstream levels. Default to None,
-            and a `keras.Conv2D` layer with kernel 3x3 will be created for each pyramid
-            level.
+            operations, defaults to 256.
+        lateral_layers: a python dict with int keys that matches to each of the
+            pyramid level. The values of the dict should be `keras.Layer`, which
+            will be called with feature activation outputs from backbone at each
+            level. Defaults to None, and a `keras.Conv2D` layer with kernel 1x1
+            will be created for each pyramid level.
+        output_layers: a python dict with int keys that matches to each of the
+            pyramid level. The values of the dict should be `keras.Layer`, which
+            will be called with feature inputs and merged result from upstream
+            levels. Defaults to None, and a `keras.Conv2D` layer with kernel 3x3
+            will be created for each pyramid level.
 
     Sample Usage:
     ```python
 
-    inp = tf.keras.layers.Input((384, 384, 3))
-    backbone = tf.keras.applications.EfficientNetB0(input_tensor=inp, include_top=False)
-    layer_names = ['block2b_add', 'block3b_add', 'block5c_add', 'top_activation']
+    inp = keras.layers.Input((384, 384, 3))
+    backbone = keras.applications.EfficientNetB0(
+        input_tensor=inp,
+        include_top=False
+    )
+    layer_names = ['block2b_add',
+        'block3b_add',
+        'block5c_add',
+        'top_activation'
+    ]
 
     backbone_outputs = {}
     for i, layer_name in enumerate(layer_names):
         backbone_outputs[i+2] = backbone.get_layer(layer_name).output
 
     # output_dict is a dict with 2, 3, 4, 5 as keys
-    output_dict = keras_cv.layers.FeaturePyramid(min_level=2, max_level=5)(backbone_outputs)
+    output_dict = keras_cv.layers.FeaturePyramid(
+        min_level=2,
+        max_level=5
+    )(backbone_outputs)
     ```
     """
 
     def __init__(
         self,
         min_level,
         max_level,
@@ -102,72 +112,73 @@
         self.lateral_layers_passed = lateral_layers
         self.output_layers_passed = output_layers
 
         if not lateral_layers:
             # populate self.lateral_ops with default FPN Conv2D 1X1 layers
             self.lateral_layers = {}
             for i in self.pyramid_levels:
-                self.lateral_layers[i] = tf.keras.layers.Conv2D(
+                self.lateral_layers[i] = keras.layers.Conv2D(
                     self.num_channels,
                     kernel_size=1,
                     strides=1,
                     padding="same",
                     name=f"lateral_P{i}",
                 )
         else:
             self._validate_user_layers(lateral_layers, "lateral_layers")
             self.lateral_layers = lateral_layers
 
         # Output conv2d layers.
         if not output_layers:
             self.output_layers = {}
             for i in self.pyramid_levels:
-                self.output_layers[i] = tf.keras.layers.Conv2D(
+                self.output_layers[i] = keras.layers.Conv2D(
                     self.num_channels,
                     kernel_size=3,
                     strides=1,
                     padding="same",
                     name=f"output_P{i}",
                 )
         else:
             self._validate_user_layers(output_layers, "output_layers")
             self.output_layers = output_layers
 
         # the same upsampling layer is used for all levels
-        self.top_down_op = tf.keras.layers.UpSampling2D(size=2)
+        self.top_down_op = keras.layers.UpSampling2D(size=2)
         # the same merge layer is used for all levels
-        self.merge_op = tf.keras.layers.Add()
+        self.merge_op = keras.layers.Add()
 
     def _validate_user_layers(self, user_input, param_name):
         if (
             not isinstance(user_input, dict)
             or sorted(user_input.keys()) != self.pyramid_levels
         ):
             raise ValueError(
                 f"Expect {param_name} to be a dict with keys as "
                 f"{self.pyramid_levels}, got {user_input}"
             )
 
     def call(self, features):
-        # Note that this assertion might not be true for all the subclasses. It is
-        # possible to have FPN that has high levels than the height of backbone outputs.
+        # Note that this assertion might not be true for all the subclasses. It
+        # is possible to have FPN that has high levels than the height of
+        # backbone outputs.
         if (
             not isinstance(features, dict)
             or sorted(features.keys()) != self.pyramid_levels
         ):
             raise ValueError(
-                "FeaturePyramid expects input features to be a dict with int keys "
-                "that match the values provided in pyramid_levels. "
+                "FeaturePyramid expects input features to be a dict with int "
+                "keys that match the values provided in pyramid_levels. "
                 f"Expect feature keys: {self.pyramid_levels}, got: {features}"
             )
         return self.build_feature_pyramid(features)
 
     def build_feature_pyramid(self, input_features):
-        # To illustrate the connection/topology, the basic flow for a FPN with level
-        # 3, 4, 5 is like below:
+        # To illustrate the connection/topology, the basic flow for a FPN with
+        # level 3, 4, 5 is like below:
         #
         # input_l5 -> conv2d_1x1_l5 ----V---> conv2d_3x3_l5 -> output_l5
         #                               V
         #                          upsample2d
         #                               V
         # input_l4 -> conv2d_1x1_l4 -> Add -> conv2d_3x3_l4 -> output_l4
         #                               V
@@ -177,23 +188,26 @@
 
         output_features = {}
         reversed_levels = list(sorted(input_features.keys(), reverse=True))
         top_level = reversed_levels[0]
         for level in reversed_levels:
             output = self.lateral_layers[level](input_features[level])
             if level < top_level:
-                # for the top most output, it doesn't need to merge with any upper stream
-                # outputs
+                # for the top most output, it doesn't need to merge with any
+                # upper stream outputs
                 upstream_output = self.top_down_op(output_features[level + 1])
                 output = self.merge_op([output, upstream_output])
             output_features[level] = output
 
-        # Post apply the output layers so that we don't leak them to the down stream level
+        # Post apply the output layers so that we don't leak them to the down
+        # stream level
         for level in reversed_levels:
-            output_features[level] = self.output_layers[level](output_features[level])
+            output_features[level] = self.output_layers[level](
+                output_features[level]
+            )
 
         return output_features
 
     def get_config(self):
         config = {
             "min_level": self.min_level,
             "max_level": self.max_level,
```

## keras_cv/layers/feature_pyramid_test.py

```diff
@@ -9,14 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers import FeaturePyramid
 
 
 class FeaturePyramidTest(tf.test.TestCase):
     def test_return_type_dict(self):
         layer = FeaturePyramid(min_level=2, max_level=5)
@@ -57,57 +58,77 @@
             self.assertEquals(output[level].shape[1], inputs[level].shape[1])
             self.assertEquals(output[level].shape[2], inputs[level].shape[2])
             self.assertEquals(output[level].shape[3], layer.num_channels)
 
     def test_with_keras_input_tensor(self):
         # This mimic the model building with Backbone network
         layer = FeaturePyramid(min_level=2, max_level=5)
-        c2 = tf.keras.layers.Input([64, 64, 3])
-        c3 = tf.keras.layers.Input([32, 32, 3])
-        c4 = tf.keras.layers.Input([16, 16, 3])
-        c5 = tf.keras.layers.Input([8, 8, 3])
+        c2 = keras.layers.Input([64, 64, 3])
+        c3 = keras.layers.Input([32, 32, 3])
+        c4 = keras.layers.Input([16, 16, 3])
+        c5 = keras.layers.Input([8, 8, 3])
 
         inputs = {2: c2, 3: c3, 4: c4, 5: c5}
         output = layer(inputs)
         for level in inputs.keys():
             self.assertEquals(output[level].shape[1], inputs[level].shape[1])
             self.assertEquals(output[level].shape[2], inputs[level].shape[2])
             self.assertEquals(output[level].shape[3], layer.num_channels)
 
     def test_invalid_lateral_layers(self):
-        lateral_layers = [tf.keras.layers.Conv2D(256, 1)] * 3
-        with self.assertRaisesRegexp(ValueError, "Expect lateral_layers to be a dict"):
-            _ = FeaturePyramid(min_level=2, max_level=5, lateral_layers=lateral_layers)
+        lateral_layers = [keras.layers.Conv2D(256, 1)] * 3
+        with self.assertRaisesRegexp(
+            ValueError, "Expect lateral_layers to be a dict"
+        ):
+            _ = FeaturePyramid(
+                min_level=2, max_level=5, lateral_layers=lateral_layers
+            )
         lateral_layers = {
-            2: tf.keras.layers.Conv2D(256, 1),
-            3: tf.keras.layers.Conv2D(256, 1),
-            4: tf.keras.layers.Conv2D(256, 1),
+            2: keras.layers.Conv2D(256, 1),
+            3: keras.layers.Conv2D(256, 1),
+            4: keras.layers.Conv2D(256, 1),
         }
-        with self.assertRaisesRegexp(ValueError, "with keys as .* [2, 3, 4, 5]"):
-            _ = FeaturePyramid(min_level=2, max_level=5, lateral_layers=lateral_layers)
+        with self.assertRaisesRegexp(
+            ValueError, "with keys as .* [2, 3, 4, 5]"
+        ):
+            _ = FeaturePyramid(
+                min_level=2, max_level=5, lateral_layers=lateral_layers
+            )
 
     def test_invalid_output_layers(self):
-        output_layers = [tf.keras.layers.Conv2D(256, 3)] * 3
-        with self.assertRaisesRegexp(ValueError, "Expect output_layers to be a dict"):
-            _ = FeaturePyramid(min_level=2, max_level=5, output_layers=output_layers)
+        output_layers = [keras.layers.Conv2D(256, 3)] * 3
+        with self.assertRaisesRegexp(
+            ValueError, "Expect output_layers to be a dict"
+        ):
+            _ = FeaturePyramid(
+                min_level=2, max_level=5, output_layers=output_layers
+            )
         output_layers = {
-            2: tf.keras.layers.Conv2D(256, 3),
-            3: tf.keras.layers.Conv2D(256, 3),
-            4: tf.keras.layers.Conv2D(256, 3),
+            2: keras.layers.Conv2D(256, 3),
+            3: keras.layers.Conv2D(256, 3),
+            4: keras.layers.Conv2D(256, 3),
         }
-        with self.assertRaisesRegexp(ValueError, "with keys as .* [2, 3, 4, 5]"):
-            _ = FeaturePyramid(min_level=2, max_level=5, output_layers=output_layers)
+        with self.assertRaisesRegexp(
+            ValueError, "with keys as .* [2, 3, 4, 5]"
+        ):
+            _ = FeaturePyramid(
+                min_level=2, max_level=5, output_layers=output_layers
+            )
 
     def test_invalid_input_features(self):
         layer = FeaturePyramid(min_level=2, max_level=5)
 
         c2 = tf.ones([2, 64, 64, 3])
         c3 = tf.ones([2, 32, 32, 3])
         c4 = tf.ones([2, 16, 16, 3])
         c5 = tf.ones([2, 8, 8, 3])
         list_input = [c2, c3, c4, c5]
-        with self.assertRaisesRegexp(ValueError, "expects input features to be a dict"):
+        with self.assertRaisesRegexp(
+            ValueError, "expects input features to be a dict"
+        ):
             layer(list_input)
 
         dict_input_with_missing_feature = {2: c2, 3: c3, 4: c4}
-        with self.assertRaisesRegexp(ValueError, "Expect feature keys.*[2, 3, 4, 5]"):
+        with self.assertRaisesRegexp(
+            ValueError, "Expect feature keys.*[2, 3, 4, 5]"
+        ):
             layer(dict_input_with_missing_feature)
```

## keras_cv/layers/fusedmbconv.py

```diff
@@ -9,76 +9,86 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-import tensorflow as tf
 from keras import backend
+from tensorflow import keras
 from tensorflow.keras import layers
 
 BN_AXIS = 3
 
 CONV_KERNEL_INITIALIZER = {
     "class_name": "VarianceScaling",
     "config": {
         "scale": 2.0,
         "mode": "fan_out",
         "distribution": "truncated_normal",
     },
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class FusedMBConvBlock(layers.Layer):
     """
-    Implementation of the FusedMBConv block (Fused Mobile Inverted Residual Bottleneck) from:
-        (EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML)[https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html]
-        (EfficientNetV2: Smaller Models and Faster Training)[https://arxiv.org/abs/2104.00298v3].
-
-    FusedMBConv blocks are based on MBConv blocks, and replace the depthwise and 1x1 output convolution
-    blocks with a single 3x3 convolution block, fusing them together - hence the name "FusedMBConv".
-    Alongside MBConv blocks, they can be used in mobile-oriented and efficient architectures,
-    and are present in architectures EfficientNet.
-
-    FusedMBConv blocks follow a narrow-wide-narrow structure - expanding a 1x1 convolution, performing
-    Squeeze-Excitation and then applying a 3x3 convolution, which is a more efficient operation than
-    conventional wide-narrow-wide structures.
+    Implementation of the FusedMBConv block (Fused Mobile Inverted Residual
+    Bottleneck) from:
+        [EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML](https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html)
+        [EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298v3).
+
+    FusedMBConv blocks are based on MBConv blocks, and replace the depthwise and
+    1x1 output convolution blocks with a single 3x3 convolution block, fusing
+    them together - hence the name "FusedMBConv". Alongside MBConv blocks, they
+    can be used in mobile-oriented and efficient architectures, and are present
+    in architectures EfficientNet.
+
+    FusedMBConv blocks follow a narrow-wide-narrow structure - expanding a 1x1
+    convolution, performing Squeeze-Excitation and then applying a 3x3
+    convolution, which is a more efficient operation than conventional
+    wide-narrow-wide structures.
 
-    As they're frequently used for models to be deployed to edge devices, they're
-    implemented as a layer for ease of use and re-use.
+    As they're frequently used for models to be deployed to edge devices,
+    they're implemented as a layer for ease of use and re-use.
 
     Args:
         input_filters: int, the number of input filters
         output_filters: int, the number of output filters
-        expand_ratio: default 1, the ratio by which input_filters are multiplied to expand
-            the structure in the middle expansion phase
-        kernel_size: default 3, the kernel_size to apply to the expansion phase convolutions
-        strides: default 1, the strides to apply to the expansion phase convolutions
-        se_ratio: default 0.0, The filters used in the Squeeze-Excitation phase, and are chosen as
-            the maximum between 1 and input_filters*se_ratio
+        expand_ratio: default 1, the ratio by which input_filters are multiplied
+            to expand the structure in the middle expansion phase
+        kernel_size: default 3, the kernel_size to apply to the expansion phase
+            convolutions
+        strides: default 1, the strides to apply to the expansion phase
+            convolutions
+        se_ratio: default 0.0, The filters used in the Squeeze-Excitation phase,
+            and are chosen as the maximum between 1 and input_filters*se_ratio
         bn_momentum: default 0.9, the BatchNormalization momentum
-        activation: default "swish", the activation function used between convolution operations
-        survival_probability: float, default 0.8, the optional dropout rate to apply before the output
-            convolution
+        activation: default "swish", the activation function used between
+            convolution operations
+        survival_probability: float, the optional dropout rate to apply before
+            the output convolution, defaults to 0.8
 
     Returns:
-        A `tf.Tensor` representing a feature map, passed through the FusedMBConv block
+        A `tf.Tensor` representing a feature map, passed through the FusedMBConv
+        block
 
 
     Example usage:
 
     ```
     inputs = tf.random.normal(shape=(1, 64, 64, 32), dtype=tf.float32)
-    layer = keras_cv.layers.FusedMBConvBlock(input_filters=32, output_filters=32)
+    layer = keras_cv.layers.FusedMBConvBlock(
+        input_filters=32,
+        output_filters=32
+    )
     output = layer(inputs)
     output.shape # TensorShape([1, 224, 224, 48])
     ```
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         input_filters: int,
         output_filters: int,
         expand_ratio=1,
         kernel_size=3,
@@ -151,15 +161,17 @@
             padding="same",
             data_format="channels_last",
             use_bias=False,
             name=self.name + "project_conv",
         )
 
         self.bn3 = layers.BatchNormalization(
-            axis=BN_AXIS, momentum=self.bn_momentum, name=self.name + "project_bn"
+            axis=BN_AXIS,
+            momentum=self.bn_momentum,
+            name=self.name + "project_bn",
         )
 
     def build(self, input_shape):
         if self.name is None:
             self.name = backend.get_uid("block0")
 
     def call(self, inputs):
```

## keras_cv/layers/fusedmbconv_test.py

```diff
@@ -7,29 +7,32 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import pytest
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.fusedmbconv import FusedMBConvBlock
 
 
 class FusedMBConvBlockTest(tf.test.TestCase):
     @pytest.fixture(autouse=True)
     def cleanup_global_session(self):
         # Code before yield runs before the test
         tf.config.set_soft_device_placement(False)
         yield
-        # Reset soft device placement to not interfere with other unit test files
+        # Reset soft device placement to not interfere with other unit test
+        # files
         tf.config.set_soft_device_placement(True)
-        tf.keras.backend.clear_session()
+        keras.backend.clear_session()
 
     def test_same_input_output_shapes(self):
         inputs = tf.random.normal(shape=(1, 64, 64, 32), dtype=tf.float32)
         layer = FusedMBConvBlock(input_filters=32, output_filters=32)
 
         output = layer(inputs)
         self.assertEquals(output.shape, [1, 64, 64, 32])
@@ -43,13 +46,15 @@
         output = layer(inputs)
         self.assertEquals(output.shape, [1, 64, 64, 48])
         self.assertLen(output, 1)
         self.assertTrue(isinstance(output, tf.Tensor))
 
     def test_squeeze_excitation_ratio(self):
         inputs = tf.random.normal(shape=(1, 64, 64, 32), dtype=tf.float32)
-        layer = FusedMBConvBlock(input_filters=32, output_filters=48, se_ratio=0.25)
+        layer = FusedMBConvBlock(
+            input_filters=32, output_filters=48, se_ratio=0.25
+        )
 
         output = layer(inputs)
         self.assertEquals(output.shape, [1, 64, 64, 48])
         self.assertLen(output, 1)
         self.assertTrue(isinstance(output, tf.Tensor))
```

## keras_cv/layers/mbconv.py

```diff
@@ -9,31 +9,31 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-import tensorflow as tf
 from keras import backend
+from tensorflow import keras
 from tensorflow.keras import layers
 
 BN_AXIS = 3
 
 CONV_KERNEL_INITIALIZER = {
     "class_name": "VarianceScaling",
     "config": {
         "scale": 2.0,
         "mode": "fan_out",
         "distribution": "truncated_normal",
     },
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class MBConvBlock(layers.Layer):
     def __init__(
         self,
         input_filters: int,
         output_filters: int,
         expand_ratio=1,
         kernel_size=3,
@@ -41,56 +41,66 @@
         se_ratio=0.0,
         bn_momentum=0.9,
         activation="swish",
         survival_probability: float = 0.8,
         **kwargs
     ):
         """
-        Implementation of the MBConv block (Mobile Inverted Residual Bottleneck) from:
-            (MobileNetV2: Inverted Residuals and Linear Bottlenecks)[https://arxiv.org/abs/1801.04381v4].
+        Implementation of the MBConv block (Mobile Inverted Residual Bottleneck)
+        from:
+            [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381v4).
+
+        MBConv blocks are common blocks used in mobile-oriented and efficient
+        architectures, present in architectures such as MobileNet, EfficientNet,
+        MaxViT, etc.
+
+        MBConv blocks follow a narrow-wide-narrow structure - expanding a 1x1
+        convolution, applying depthwise convolution, and narrowing back to a 1x1
+        convolution, which is a more efficient operation than conventional
+        wide-narrow-wide structures.
 
-        MBConv blocks are common blocks used in mobile-oriented and efficient architectures,
-        present in architectures such as MobileNet, EfficientNet, MaxViT, etc.
-
-        MBConv blocks follow a narrow-wide-narrow structure - expanding a 1x1 convolution, applying
-        depthwise convolution, and narrowing back to a 1x1 convolution, which is a more efficient operation
-        than conventional wide-narrow-wide structures.
-
-        As they're frequently used for models to be deployed to edge devices, they're
-        implemented as a layer for ease of use and re-use.
+        As they're frequently used for models to be deployed to edge devices,
+        they're implemented as a layer for ease of use and re-use.
 
         Args:
             input_filters: int, the number of input filters
-            output_filters: int, the optional number of output filters after Squeeze-Excitation
-            expand_ratio: default 1, the ratio by which input_filters are multiplied to expand
-                the structure in the middle expansion phase
-            kernel_size: default 3, the kernel_size to apply to the expansion phase convolutions
-            strides: default 1, the strides to apply to the expansion phase convolutions
-            se_ratio: default 0.0, Squeeze-Excitation happens before depthwise convolution
-                and before output convolution only if the se_ratio is above 0.
-                The filters used in this phase are chosen as the maximum between 1 and input_filters*se_ratio
+            output_filters: int, the optional number of output filters after
+                Squeeze-Excitation
+            expand_ratio: default 1, the ratio by which input_filters are
+                multiplied to expand the structure in the middle expansion phase
+            kernel_size: default 3, the kernel_size to apply to the expansion
+                phase convolutions
+            strides: default 1, the strides to apply to the expansion phase
+                convolutions
+            se_ratio: default 0.0, Squeeze-Excitation happens before depthwise
+                convolution and before output convolution only if the se_ratio
+                is above 0. The filters used in this phase are chosen as the
+                maximum between 1 and input_filters*se_ratio
             bn_momentum: default 0.9, the BatchNormalization momentum
-            activation: default "swish", the activation function used between convolution operations
-            survival_probability: float, default 0.8, the optional dropout rate to apply before the output
-                convolution
+            activation: default "swish", the activation function used between
+                convolution operations
+            survival_probability: float, the optional dropout rate to apply
+                before the output convolution, defaults to 0.8
 
         Returns:
-            A `tf.Tensor` representing a feature map, passed through the MBConv block
+            A `tf.Tensor` representing a feature map, passed through the MBConv
+            block
 
 
         Example usage:
 
         ```
         inputs = tf.random.normal(shape=(1, 64, 64, 32), dtype=tf.float32)
         layer = keras_cv.layers.MBConvBlock(input_filters=32, output_filters=32)
 
         output = layer(inputs)
         output.shape # TensorShape([1, 64, 64, 32])
         ```
-        """
+        """  # noqa: E501
+
         super().__init__(**kwargs)
         self.input_filters = input_filters
         self.output_filters = output_filters
         self.expand_ratio = expand_ratio
         self.kernel_size = kernel_size
         self.strides = strides
         self.se_ratio = se_ratio
@@ -111,15 +121,17 @@
             name=self.name + "expand_conv",
         )
         self.bn1 = layers.BatchNormalization(
             axis=BN_AXIS,
             momentum=self.bn_momentum,
             name=self.name + "expand_bn",
         )
-        self.act = layers.Activation(self.activation, name=self.name + "activation")
+        self.act = layers.Activation(
+            self.activation, name=self.name + "activation"
+        )
         self.depthwise = layers.DepthwiseConv2D(
             kernel_size=self.kernel_size,
             strides=self.strides,
             depthwise_initializer=CONV_KERNEL_INITIALIZER,
             padding="same",
             data_format="channels_last",
             use_bias=False,
@@ -156,15 +168,17 @@
             padding="same",
             data_format="channels_last",
             use_bias=False,
             name=self.name + "project_conv",
         )
 
         self.bn3 = layers.BatchNormalization(
-            axis=BN_AXIS, momentum=self.bn_momentum, name=self.name + "project_bn"
+            axis=BN_AXIS,
+            momentum=self.bn_momentum,
+            name=self.name + "project_bn",
         )
 
     def build(self, input_shape):
         if self.name is None:
             self.name = backend.get_uid("block0")
 
     def call(self, inputs):
```

## keras_cv/layers/mbconv_test.py

```diff
@@ -7,29 +7,32 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import pytest
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.mbconv import MBConvBlock
 
 
 class MBConvTest(tf.test.TestCase):
     @pytest.fixture(autouse=True)
     def cleanup_global_session(self):
         # Code before yield runs before the test
         tf.config.set_soft_device_placement(False)
         yield
-        # Reset soft device placement to not interfere with other unit test files
+        # Reset soft device placement to not interfere with other unit test
+        # files
         tf.config.set_soft_device_placement(True)
-        tf.keras.backend.clear_session()
+        keras.backend.clear_session()
 
     def test_same_input_output_shapes(self):
         inputs = tf.random.normal(shape=(1, 64, 64, 32), dtype=tf.float32)
         layer = MBConvBlock(input_filters=32, output_filters=32)
 
         output = layer(inputs)
         self.assertEquals(output.shape, [1, 64, 64, 32])
```

## keras_cv/layers/serialization_test.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,23 +12,23 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import inspect
 
 import tensorflow as tf
 from absl.testing import parameterized
+from tensorflow import keras
 
 from keras_cv import layers as cv_layers
 from keras_cv.layers.vit_layers import PatchingAndEmbedding
 from keras_cv.utils import test_utils
 
 
 class SerializationTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(
-        ("Augmenter", cv_layers.Augmenter, {"layers": [cv_layers.Grayscale()]}),
         ("AutoContrast", cv_layers.AutoContrast, {"value_range": (0, 255)}),
         ("ChannelShuffle", cv_layers.ChannelShuffle, {"seed": 1}),
         ("CutMix", cv_layers.CutMix, {"seed": 1}),
         ("Equalization", cv_layers.Equalization, {"value_range": (0, 255)}),
         ("Grayscale", cv_layers.Grayscale, {}),
         ("GridMask", cv_layers.GridMask, {"seed": 1}),
         ("MixUp", cv_layers.MixUp, {"seed": 1}),
@@ -135,15 +135,19 @@
                 "brightness_factor": (-0.2, 0.5),
                 "contrast_factor": (0.5, 0.9),
                 "saturation_factor": (0.5, 0.9),
                 "hue_factor": (0.5, 0.9),
                 "seed": 1,
             },
         ),
-        ("RandomContrast", cv_layers.RandomContrast, {"factor": 0.5}),
+        (
+            "RandomContrast",
+            cv_layers.RandomContrast,
+            {"value_range": (0, 255), "factor": 0.5},
+        ),
         (
             "RandomCropAndResize",
             cv_layers.RandomCropAndResize,
             {
                 "target_size": (224, 224),
                 "crop_area_factor": (0.8, 1.0),
                 "aspect_ratio_factor": (3 / 4, 4 / 3),
@@ -170,17 +174,17 @@
             {"rate": 0.1},
         ),
         (
             "SqueezeAndExcite2D",
             cv_layers.SqueezeAndExcite2D,
             {
                 "filters": 16,
-                "ratio": 0.25,
-                "squeeze_activation": tf.keras.layers.ReLU(),
-                "excite_activation": tf.keras.activations.relu,
+                "bottleneck_filters": 4,
+                "squeeze_activation": keras.layers.ReLU(),
+                "excite_activation": keras.activations.relu,
             },
         ),
         (
             "DropPath",
             cv_layers.DropPath,
             {
                 "rate": 0.2,
@@ -384,18 +388,18 @@
         ),
     )
     def test_layer_serialization(self, layer_cls, init_args):
         layer = layer_cls(**init_args)
         config = layer.get_config()
         self.assertAllInitParametersAreInConfig(layer_cls, config)
 
-        model = tf.keras.models.Sequential(layer)
+        model = keras.models.Sequential(layer)
         model_config = model.get_config()
 
-        reconstructed_model = tf.keras.Sequential().from_config(model_config)
+        reconstructed_model = keras.Sequential().from_config(model_config)
         reconstructed_layer = reconstructed_model.layers[0]
 
         self.assertTrue(
             test_utils.config_equals(
                 layer.get_config(), reconstructed_layer.get_config()
             )
         )
@@ -404,10 +408,12 @@
         excluded_name = ["args", "kwargs", "*"]
         parameter_names = {
             v
             for v in inspect.signature(layer_cls).parameters.keys()
             if v not in excluded_name
         }
 
-        intersection_with_config = {v for v in config.keys() if v in parameter_names}
+        intersection_with_config = {
+            v for v in config.keys() if v in parameter_names
+        }
 
         self.assertSetEqual(parameter_names, intersection_with_config)
```

## keras_cv/layers/spatial_pyramid.py

```diff
@@ -7,136 +7,138 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 from typing import Any
 from typing import List
 from typing import Mapping
 
 import tensorflow as tf
+from tensorflow import keras
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class SpatialPyramidPooling(tf.keras.layers.Layer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class SpatialPyramidPooling(keras.layers.Layer):
     """Implements the Atrous Spatial Pyramid Pooling.
 
     References:
-        [Rethinking Atrous Convolution for Semantic Image Segmentation](
-          https://arxiv.org/pdf/1706.05587.pdf)
-    [Encoder-Decoder with Atrous Separable Convolution for Semantic Image
-        Segmentation](https://arxiv.org/pdf/1802.02611.pdf)
+        [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/pdf/1706.05587.pdf)
+        [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/pdf/1802.02611.pdf)
 
-    inp = tf.keras.layers.Input((384, 384, 3))
-    backbone = tf.keras.applications.EfficientNetB0(input_tensor=inp, include_top=False)
+    inp = keras.layers.Input((384, 384, 3))
+    backbone = keras.applications.EfficientNetB0(
+        input_tensor=inp,
+        include_top=False)
     output = backbone(inp)
     output = keras_cv.layers.SpatialPyramidPooling(
         dilation_rates=[6, 12, 18])(output)
 
     # output[4].shape = [None, 16, 16, 256]
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         dilation_rates: List[int],
         num_channels: int = 256,
         activation: str = "relu",
         dropout: float = 0.0,
         **kwargs,
     ):
         """Initializes an Atrous Spatial Pyramid Pooling layer.
 
         Args:
-            dilation_rates: A `list` of integers for parallel dilated conv. Usually a
-                sample choice of rates are [6, 12, 18].
-            num_channels: An `int` number of output channels. Default to 256.
-            activation: A `str` activation to be used. Default to 'relu'.
-            dropout: A `float` for the dropout rate of the final projection output after
-                the activations and batch norm. Default to 0.0, which means no dropout is
-                applied to the output.
+            dilation_rates: A `list` of integers for parallel dilated conv.
+                Usually a sample choice of rates are [6, 12, 18].
+            num_channels: An `int` number of output channels, defaults to 256.
+            activation: A `str` activation to be used, defaults to 'relu'.
+            dropout: A `float` for the dropout rate of the final projection
+                output after the activations and batch norm, defaults to 0.0,
+                which means no dropout is applied to the output.
             **kwargs: Additional keyword arguments to be passed.
         """
         super().__init__(**kwargs)
         self.dilation_rates = dilation_rates
         self.num_channels = num_channels
         self.activation = activation
         self.dropout = dropout
 
     def build(self, input_shape):
         height = input_shape[1]
         width = input_shape[2]
         channels = input_shape[3]
 
-        # This is the parallel networks that process the input features with different
-        # dilation rates. The output from each channel will be merged together and feed
-        # to the output.
+        # This is the parallel networks that process the input features with
+        # different dilation rates. The output from each channel will be merged
+        # together and feed to the output.
         self.aspp_parallel_channels = []
 
         # Channel1 with Conv2D and 1x1 kernel size.
-        conv_sequential = tf.keras.Sequential(
+        conv_sequential = keras.Sequential(
             [
-                tf.keras.layers.Conv2D(
+                keras.layers.Conv2D(
                     filters=self.num_channels,
                     kernel_size=(1, 1),
                     use_bias=False,
                 ),
-                tf.keras.layers.BatchNormalization(),
-                tf.keras.layers.Activation(self.activation),
+                keras.layers.BatchNormalization(),
+                keras.layers.Activation(self.activation),
             ]
         )
         self.aspp_parallel_channels.append(conv_sequential)
 
-        # Channel 2 and afterwards are based on self.dilation_rates, and each of them
-        # will have conv2D with 3x3 kernel size.
+        # Channel 2 and afterwards are based on self.dilation_rates, and each of
+        # them will have conv2D with 3x3 kernel size.
         for dilation_rate in self.dilation_rates:
-            conv_sequential = tf.keras.Sequential(
+            conv_sequential = keras.Sequential(
                 [
-                    tf.keras.layers.Conv2D(
+                    keras.layers.Conv2D(
                         filters=self.num_channels,
                         kernel_size=(3, 3),
                         padding="same",
                         dilation_rate=dilation_rate,
                         use_bias=False,
                     ),
-                    tf.keras.layers.BatchNormalization(),
-                    tf.keras.layers.Activation(self.activation),
+                    keras.layers.BatchNormalization(),
+                    keras.layers.Activation(self.activation),
                 ]
             )
             self.aspp_parallel_channels.append(conv_sequential)
 
         # Last channel is the global average pooling with conv2D 1x1 kernel.
-        pool_sequential = tf.keras.Sequential(
+        pool_sequential = keras.Sequential(
             [
-                tf.keras.layers.GlobalAveragePooling2D(),
-                tf.keras.layers.Reshape((1, 1, channels)),
-                tf.keras.layers.Conv2D(
+                keras.layers.GlobalAveragePooling2D(),
+                keras.layers.Reshape((1, 1, channels)),
+                keras.layers.Conv2D(
                     filters=self.num_channels,
                     kernel_size=(1, 1),
                     use_bias=False,
                 ),
-                tf.keras.layers.BatchNormalization(),
-                tf.keras.layers.Activation(self.activation),
-                tf.keras.layers.Resizing(height, width, interpolation="bilinear"),
+                keras.layers.BatchNormalization(),
+                keras.layers.Activation(self.activation),
+                keras.layers.Resizing(height, width, interpolation="bilinear"),
             ]
         )
         self.aspp_parallel_channels.append(pool_sequential)
 
         # Final projection layers
-        self.projection = tf.keras.Sequential(
+        self.projection = keras.Sequential(
             [
-                tf.keras.layers.Conv2D(
+                keras.layers.Conv2D(
                     filters=self.num_channels,
                     kernel_size=(1, 1),
                     use_bias=False,
                 ),
-                tf.keras.layers.BatchNormalization(),
-                tf.keras.layers.Activation(self.activation),
-                tf.keras.layers.Dropout(rate=self.dropout),
+                keras.layers.BatchNormalization(),
+                keras.layers.Activation(self.activation),
+                keras.layers.Dropout(rate=self.dropout),
             ],
         )
 
     def call(self, inputs, training=None):
         """Calls the Atrous Spatial Pyramid Pooling layer on an input.
 
         Args:
```

## keras_cv/layers/spatial_pyramid_test.py

```diff
@@ -9,14 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers import SpatialPyramidPooling
 
 
 class SpatialPyramidPoolingTest(tf.test.TestCase):
     def test_return_type_and_shape(self):
         layer = SpatialPyramidPooling(dilation_rates=[6, 12, 18])
@@ -24,12 +25,12 @@
 
         inputs = c4
         output = layer(inputs, training=True)
         self.assertEquals(output.shape, [2, 16, 16, 256])
 
     def test_with_keras_tensor(self):
         layer = SpatialPyramidPooling(dilation_rates=[6, 12, 18])
-        c4 = tf.keras.layers.Input([16, 16, 3])
+        c4 = keras.layers.Input([16, 16, 3])
 
         inputs = c4
         output = layer(inputs, training=True)
         self.assertEquals(output.shape, [None, 16, 16, 256])
```

## keras_cv/layers/transformer_encoder.py

```diff
@@ -8,71 +8,83 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras import layers
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class TransformerEncoder(layers.Layer):
     """
     Transformer encoder block implementation as a Keras Layer.
 
     Args:
-        project_dim: the dimensionality of the projection of the encoder, and output of the `MultiHeadAttention`
-        mlp_dim: the intermediate dimensionality of the MLP head before projecting to `project_dim`
+        project_dim: the dimensionality of the projection of the encoder, and
+            output of the `MultiHeadAttention`
+        mlp_dim: the intermediate dimensionality of the MLP head before
+            projecting to `project_dim`
         num_heads: the number of heads for the `MultiHeadAttention` layer
-        mlp_dropout: default 0.1, the dropout rate to apply between the layers of the MLP head of the encoder
-        attention_dropout: default 0.1, the dropout rate to apply in the MultiHeadAttention layer
-        activation: default 'tf.activations.gelu', the activation function to apply in the MLP head - should be a function
-        layer_norm_epsilon: default 1e-06, the epsilon for `LayerNormalization` layers
+        mlp_dropout: default 0.1, the dropout rate to apply between the layers
+            of the MLP head of the encoder
+        attention_dropout: default 0.1, the dropout rate to apply in the
+            MultiHeadAttention layer
+        activation: default 'tf.activations.gelu', the activation function to
+            apply in the MLP head - should be a function
+        layer_norm_epsilon: default 1e-06, the epsilon for `LayerNormalization`
+            layers
 
     Basic usage:
 
     ```
     project_dim = 1024
     mlp_dim = 3072
     num_heads = 4
 
-    encoded_patches = keras_cv.layers.PatchingAndEmbedding(project_dim=project_dim, patch_size=16)(img_batch)
+    encoded_patches = keras_cv.layers.PatchingAndEmbedding(
+        project_dim=project_dim,
+        patch_size=16)(img_batch)
     trans_encoded = keras_cv.layers.TransformerEncoder(project_dim=project_dim,
-                                                       mlp_dim = mlp_dim,
-                                                       num_heads=num_heads)(encoded_patches)
+        mlp_dim = mlp_dim,
+        num_heads=num_heads)(encoded_patches)
 
     print(trans_encoded.shape) # (1, 197, 1024)
     ```
     """
 
     def __init__(
         self,
         project_dim,
         num_heads,
         mlp_dim,
         mlp_dropout=0.1,
         attention_dropout=0.1,
-        activation=tf.keras.activations.gelu,
+        activation=keras.activations.gelu,
         layer_norm_epsilon=1e-06,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.project_dim = project_dim
         self.mlp_dim = mlp_dim
         self.num_heads = num_heads
         self.mlp_dropout = mlp_dropout
         self.attention_dropout = attention_dropout
         self.activation = activation
         self.layer_norm_epsilon = layer_norm_epsilon
         self.mlp_units = [mlp_dim, project_dim]
 
-        self.layer_norm1 = layers.LayerNormalization(epsilon=self.layer_norm_epsilon)
-        self.layer_norm2 = layers.LayerNormalization(epsilon=self.layer_norm_epsilon)
+        self.layer_norm1 = layers.LayerNormalization(
+            epsilon=self.layer_norm_epsilon
+        )
+        self.layer_norm2 = layers.LayerNormalization(
+            epsilon=self.layer_norm_epsilon
+        )
         self.attn = layers.MultiHeadAttention(
             num_heads=self.num_heads,
             key_dim=self.project_dim // self.num_heads,
             dropout=self.attention_dropout,
         )
         self.dense1 = layers.Dense(self.mlp_units[0])
         self.dense2 = layers.Dense(self.mlp_units[1])
@@ -84,50 +96,56 @@
 
         Returns:
             `A tf.Tensor` of shape [batch, patch_num+1, embedding_dim]
         """
 
         if inputs.shape[-1] != self.project_dim:
             raise ValueError(
-                f"The input and output dimensionality must be the same, but the TransformerEncoder was provided with {inputs.shape[-1]} and {self.project_dim}"
+                "The input and output dimensionality must be the same, but the "
+                f"TransformerEncoder was provided with {inputs.shape[-1]} and "
+                f"{self.project_dim}"
             )
 
         x = self.layer_norm1(inputs)
         x = self.attn(x, x)
         x = layers.Dropout(self.mlp_dropout)(x)
         x = layers.Add()([x, inputs])
 
         y = self.layer_norm2(x)
 
         y = self.dense1(y)
-        if self.activation == tf.keras.activations.gelu:
+        if self.activation == keras.activations.gelu:
             y = self.activation(y, approximate=True)
         else:
             y = self.activation(y)
         y = layers.Dropout(self.mlp_dropout)(y)
         y = self.dense2(y)
         y = layers.Dropout(self.mlp_dropout)(y)
 
         output = layers.Add()([x, y])
 
         return output
 
     def get_config(self):
         config = super().get_config()
+        activation = self.activation
+        if not isinstance(activation, (str, dict)):
+            activation = keras.activations.serialize(activation)
         config.update(
             {
                 "project_dim": self.project_dim,
                 "mlp_dim": self.mlp_dim,
                 "num_heads": self.num_heads,
                 "attention_dropout": self.attention_dropout,
                 "mlp_dropout": self.mlp_dropout,
-                "activation": self.activation,
+                "activation": activation,
                 "layer_norm_epsilon": self.layer_norm_epsilon,
             }
         )
         return config
 
     @classmethod
     def from_config(cls, config, custom_objects=None):
         activation = config.pop("activation")
-        activation = tf.keras.activations.deserialize(activation)
+        if isinstance(activation, (str, dict)):
+            activation = keras.activations.deserialize(activation)
         return cls(activation=activation, **config)
```

## keras_cv/layers/transformer_encoder_test.py

```diff
@@ -30,21 +30,23 @@
     def test_wrong_input_dims(self):
         layer = TransformerEncoder(project_dim=128, num_heads=2, mlp_dim=128)
         # Input dims must equal output dims because of the addition
         # of the residual to the final layer
         inputs = tf.random.normal([1, 197, 256])
         with self.assertRaisesRegexp(
             ValueError,
-            "The input and output dimensionality must be the same, but the TransformerEncoder was provided with 256 and 128",
+            "The input and output dimensionality must be the same, but the "
+            "TransformerEncoder was provided with 256 and 128",
         ):
             layer(inputs, training=True)
 
     def test_wrong_project_dims(self):
         layer = TransformerEncoder(project_dim=256, num_heads=2, mlp_dim=128)
         # Input dims must equal output dims because of the addition
         # of the residual to the final layer
         inputs = tf.random.normal([1, 197, 128])
         with self.assertRaisesRegexp(
             ValueError,
-            "The input and output dimensionality must be the same, but the TransformerEncoder was provided with 128 and 256",
+            "The input and output dimensionality must be the same, but the "
+            "TransformerEncoder was provided with 128 and 256",
         ):
             layer(inputs, training=True)
```

## keras_cv/layers/vit_layers.py

```diff
@@ -11,77 +11,85 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import math
 
 import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras import layers
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class PatchingAndEmbedding(layers.Layer):
     """
 
     Layer to patchify images, prepend a class token, positionally embed and
     create a projection of patches for Vision Transformers
 
-    The layer expects a batch of input images and returns batches of patches, flattened as a sequence
-    and projected onto `project_dims`. If the height and width of the images
-    aren't divisible by the patch size, the supplied padding type is used (or 'VALID' by default).
+    The layer expects a batch of input images and returns batches of patches,
+    flattened as a sequence and projected onto `project_dims`. If the height and
+    width of the images aren't divisible by the patch size, the supplied padding
+    type is used (or 'VALID' by default).
 
     Reference:
-        An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
-        by Alexey Dosovitskiy et al. (https://arxiv.org/abs/2010.11929)
+        An Image is Worth 16x16 Words: Transformers for Image Recognition at
+        Scale by Alexey Dosovitskiy et al. (https://arxiv.org/abs/2010.11929)
 
     Args:
         project_dim: the dimensionality of the project_dim
         patch_size: the patch size
         padding: default 'VALID', the padding to apply for patchifying images
 
     Returns:
-        Patchified and linearly projected input images, including a prepended learnable class token
-        with shape (batch, num_patches+1, project_dim)
+        Patchified and linearly projected input images, including a prepended
+        learnable class token with shape (batch, num_patches+1, project_dim)
 
     Basic usage:
 
     ```
     images = #... batch of images
-    encoded_patches = keras_cv.layers.PatchingAndEmbedding(project_dim=project_dim
-                                                          patch_size=patch_size)(patches)
+    encoded_patches = keras_cv.layers.PatchingAndEmbedding(
+        project_dim=project_dim,
+        patch_size=patch_size)(patches)
     print(encoded_patches.shape) # (1, 197, 1024)
     ```
     """
 
     def __init__(self, project_dim, patch_size, padding="VALID", **kwargs):
         super().__init__(**kwargs)
         self.project_dim = project_dim
         self.patch_size = patch_size
         self.padding = padding
         if patch_size < 0:
             raise ValueError(
-                f"The patch_size cannot be a negative number. Received {patch_size}"
+                "The patch_size cannot be a negative number. Received "
+                f"{patch_size}"
             )
         if padding not in ["VALID", "SAME"]:
             raise ValueError(
-                f"Padding must be either 'SAME' or 'VALID', but {padding} was passed."
+                f"Padding must be either 'SAME' or 'VALID', but {padding} was "
+                "passed."
             )
         self.projection = layers.Conv2D(
             filters=self.project_dim,
             kernel_size=self.patch_size,
             strides=self.patch_size,
             padding=self.padding,
         )
 
     def build(self, input_shape):
         self.class_token = self.add_weight(
             shape=[1, 1, self.project_dim], name="class_token", trainable=True
         )
         self.num_patches = (
-            input_shape[1] // self.patch_size * input_shape[2] // self.patch_size
+            input_shape[1]
+            // self.patch_size
+            * input_shape[2]
+            // self.patch_size
         )
         self.position_embedding = layers.Embedding(
             input_dim=self.num_patches + 1, output_dim=self.project_dim
         )
 
     def call(
         self,
@@ -93,15 +101,16 @@
     ):
         """Calls the PatchingAndEmbedding layer on a batch of images.
         Args:
             images: A `tf.Tensor` of shape [batch, width, height, depth]
             interpolate: A `bool` to enable or disable interpolation
             interpolate_height: An `int` representing interpolated height
             interpolate_width: An `int` representing interpolated width
-            patch_size: An `int` representing the new patch size if interpolation is used
+            patch_size: An `int` representing the new patch size if
+                interpolation is used
 
         Returns:
             `A tf.Tensor` of shape [batch, patch_num+1, embedding_dim]
         """
         # Turn images into patches and project them onto `project_dim`
         patches = self.projection(images)
         patch_shapes = tf.shape(patches)
@@ -110,24 +119,27 @@
             shape=(
                 patch_shapes[0],
                 patch_shapes[-2] * patch_shapes[-2],
                 patch_shapes[-1],
             ),
         )
 
-        # Add learnable class token before linear projection and positional embedding
+        # Add learnable class token before linear projection and positional
+        # embedding
         flattened_shapes = tf.shape(patches_flattened)
         class_token_broadcast = tf.cast(
             tf.broadcast_to(
                 self.class_token,
                 [flattened_shapes[0], 1, flattened_shapes[-1]],
             ),
             dtype=patches_flattened.dtype,
         )
-        patches_flattened = tf.concat([class_token_broadcast, patches_flattened], 1)
+        patches_flattened = tf.concat(
+            [class_token_broadcast, patches_flattened], 1
+        )
         positions = tf.range(start=0, limit=self.num_patches + 1, delta=1)
 
         if interpolate and None not in (
             interpolate_width,
             interpolate_height,
             patch_size,
         ):
@@ -144,24 +156,28 @@
             encoded = tf.concat([class_token, addition], 1)
         elif interpolate and None in (
             interpolate_width,
             interpolate_height,
             patch_size,
         ):
             raise ValueError(
-                "`None of `interpolate_width`, `interpolate_height` and `patch_size` cannot be None if `interpolate` is True"
+                "`None of `interpolate_width`, `interpolate_height` and "
+                "`patch_size` cannot be None if `interpolate` is True"
             )
         else:
             encoded = patches_flattened + self.position_embedding(positions)
         return encoded
 
-    def __interpolate_positional_embeddings(self, embedding, height, width, patch_size):
+    def __interpolate_positional_embeddings(
+        self, embedding, height, width, patch_size
+    ):
         """
-        Allows for pre-trained position embedding interpolation. This trick allows you to fine-tune a ViT
-        on higher resolution images than it was trained on.
+        Allows for pre-trained position embedding interpolation. This trick
+        allows you to fine-tune a ViT on higher resolution images than it was
+        trained on.
 
         Based on:
         https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_tf_vit.py
         """
 
         dimensionality = embedding.shape[-1]
```

## keras_cv/layers/vit_layers_test.py

```diff
@@ -15,24 +15,27 @@
 
 from keras_cv.layers.vit_layers import PatchingAndEmbedding
 
 
 class ViTLayersTest(tf.test.TestCase):
     def test_patching_wrong_patch_size(self):
         with self.assertRaisesRegexp(
-            ValueError, "The patch_size cannot be a negative number. Received -16"
+            ValueError,
+            "The patch_size cannot be a negative number. Received -16",
         ):
             PatchingAndEmbedding(project_dim=16, patch_size=-16)
 
     def test_patching_wrong_padding(self):
         with self.assertRaisesRegexp(
             ValueError,
             "Padding must be either 'SAME' or 'VALID', but REFLECT was passed.",
         ):
-            PatchingAndEmbedding(project_dim=16, patch_size=16, padding="REFLECT")
+            PatchingAndEmbedding(
+                project_dim=16, patch_size=16, padding="REFLECT"
+            )
 
     def test_patch_embedding_return_type_and_shape(self):
         layer = PatchingAndEmbedding(project_dim=128, patch_size=16)
         inputs = tf.random.normal([1, 224, 224, 3])
         output = layer(inputs)
         self.assertTrue(isinstance(output, tf.Tensor))
         self.assertLen(output, 1)
@@ -43,15 +46,15 @@
         patch_embedding = PatchingAndEmbedding(project_dim=128, patch_size=16)
         patch_embedding.build(inputs.shape)
 
         positional_embeddings = tf.ones([197, 128])
         (
             output,
             cls,
-        ) = patch_embedding._PatchingAndEmbedding__interpolate_positional_embeddings(
+        ) = patch_embedding._PatchingAndEmbedding__interpolate_positional_embeddings(  # noqa: E501
             positional_embeddings, height=450, width=450, patch_size=12
         )
 
         self.assertTrue(isinstance(output, tf.Tensor))
         self.assertLen(output, 1)
         self.assertEquals(output.shape, [1, 1369, 128])
 
@@ -60,12 +63,14 @@
         patch_embedding = PatchingAndEmbedding(project_dim=4, patch_size=1)
         patch_embedding.build(inputs.shape)
 
         positional_embeddings = tf.ones([17, 4])
         (
             output,
             cls_token,
-        ) = patch_embedding._PatchingAndEmbedding__interpolate_positional_embeddings(
+        ) = patch_embedding._PatchingAndEmbedding__interpolate_positional_embeddings(  # noqa: E501
             positional_embeddings, height=8, width=8, patch_size=2
         )
 
-        self.assertTrue(tf.reduce_all(tf.equal(output, tf.ones([1, 16, 4]))).numpy())
+        self.assertTrue(
+            tf.reduce_all(tf.equal(output, tf.ones([1, 16, 4]))).numpy()
+        )
```

## keras_cv/layers/object_detection/anchor_generator.py

```diff
@@ -14,41 +14,44 @@
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv import bounding_box
 
 
+@keras.utils.register_keras_serializable(package="keras_cv")
 class AnchorGenerator(keras.layers.Layer):
     """AnchorGenerator generates anchors for multiple feature maps.
 
-    AnchorGenerator takes multiple scales and generates anchor boxes based on the anchor
-    sizes, scales, aspect ratios, and strides provided.  To invoke AnchorGenerator, call
-    it on the image that needs anchor boxes.
-
-    `sizes` and `strides` must match structurally - they are pairs.  Scales and
-    aspect ratios can either be a list, that is then used for all of the sizes
-    (aka levels), or a dictionary from `{'level_{number}': [parameters at scale...]}`.
+    AnchorGenerator takes multiple scales and generates anchor boxes based on
+    the anchor sizes, scales, aspect ratios, and strides provided. To invoke
+    AnchorGenerator, call it on the image that needs anchor boxes.
+
+    `sizes` and `strides` must match structurally - they are pairs. Scales and
+    aspect ratios can either be a list, that is then used for all the sizes (aka
+    levels), or a dictionary from `{'level_{number}': [parameters at scale...]}`
 
     Args:
       bounding_box_format: The format of bounding boxes to generate. Refer
         [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
         for more details on supported bounding box formats.
       sizes: A list of integers that represent the anchor sizes for each level,
         or a dictionary of integer lists with each key representing a level.
-        For each anchor size, anchor height will be `anchor_size / sqrt(aspect_ratio)`,
-        and anchor width will be `anchor_size * sqrt(aspect_ratio)`.  This is repeated
-        for each scale and aspect ratio.
+        For each anchor size, anchor height will be
+        `anchor_size / sqrt(aspect_ratio)`, and anchor width will be
+        `anchor_size * sqrt(aspect_ratio)`. This is repeated for each scale and
+        aspect ratio.
       scales: A list of floats corresponding to multipliers that will be
         multiplied by each `anchor_size` to generate a level.
-      aspect_ratios: A list of floats representing the ratio of anchor width to height.
+      aspect_ratios: A list of floats representing the ratio of anchor width to
+        height.
       strides: iterable of ints that represent the anchor stride size between
         center of anchors at each scale.
-      clip_boxes: Whether or not to clip generated anchor boxes to the image size.
-        Defaults to `False`.
+      clip_boxes: whether to clip generated anchor boxes to the image
+        size, defaults to `False`.
 
     Usage:
     ```python
     strides = [8, 16, 32]
     scales = [1, 1.2599210498948732, 1.5874010519681994]
     sizes = [32.0, 64.0, 128.0]
     aspect_ratios = [0.5, 1.0, 2.0]
@@ -64,18 +67,18 @@
     )
     anchors = anchor_generator(image)
     print(anchors)
     # > {0: ..., 1: ..., 2: ...}
     ```
 
     Input shape: an image with shape `[H, W, C]`
-    Output: a dictionary with integer keys corresponding to each level of the feature
-        pyramid.  The size of the anchors at each level will be
+    Output: a dictionary with integer keys corresponding to each level of the
+        feature pyramid. The size of the anchors at each level will be
         `(H/strides[i] * W/strides[i] * len(scales) * len(aspect_ratios), 4)`.
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         bounding_box_format,
         sizes,
         scales,
         aspect_ratios,
@@ -83,15 +86,17 @@
         clip_boxes=False,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         # aspect_ratio is a single list that is the same across all levels.
         sizes, strides = self._format_sizes_and_strides(sizes, strides)
-        aspect_ratios = self._match_param_structure_to_sizes(aspect_ratios, sizes)
+        aspect_ratios = self._match_param_structure_to_sizes(
+            aspect_ratios, sizes
+        )
         scales = self._match_param_structure_to_sizes(scales, sizes)
 
         self.anchor_generators = {}
         for k in sizes.keys():
             self.anchor_generators[k] = _SingleAnchorGenerator(
                 bounding_box_format,
                 sizes[k],
@@ -101,23 +106,25 @@
                 clip_boxes,
                 dtype=self.compute_dtype,
             )
         self.built = True
 
     @staticmethod
     def _format_sizes_and_strides(sizes, strides):
-        result_sizes = AnchorGenerator._ensure_param_is_levels_dict(sizes, "sizes")
+        result_sizes = AnchorGenerator._ensure_param_is_levels_dict(
+            sizes, "sizes"
+        )
         result_strides = AnchorGenerator._ensure_param_is_levels_dict(
             strides, "strides"
         )
 
         if sorted(result_strides.keys()) != sorted(result_sizes.keys()):
             raise ValueError(
                 "Expected sizes and strides to be either lists of"
-                "the same length, or dictionaries with the same keys.  Received "
+                "the same length, or dictionaries with the same keys. Received "
                 f"sizes={sizes}, strides={strides}"
             )
 
         return result_sizes, result_strides
 
     @staticmethod
     def _ensure_param_is_levels_dict(param, param_name):
@@ -143,27 +150,30 @@
     @staticmethod
     def _match_param_structure_to_sizes(params, sizes):
         """broadcast the params to match sizes."""
         # if isinstance(sizes, (tuple, list)):
         #     return [params] * len(sizes)
         if not isinstance(sizes, dict):
             raise ValueError(
-                "the structure of `sizes` must be a dict, " f"received sizes={sizes}"
+                "the structure of `sizes` must be a dict, "
+                f"received sizes={sizes}"
             )
 
         return tf.nest.map_structure(lambda _: params, sizes)
 
     def __call__(self, image=None, image_shape=None):
         if image is None and image_shape is None:
-            raise ValueError("AnchorGenerator() requires `images` or `image_shape`.")
+            raise ValueError(
+                "AnchorGenerator() requires `images` or `image_shape`."
+            )
 
         if image is not None:
             if image.shape.rank != 3:
                 raise ValueError(
-                    "Expected `image` to be a Tensor of rank 3.  Got "
+                    "Expected `image` to be a Tensor of rank 3. Got "
                     f"image.shape.rank={image.shape.rank}"
                 )
             image_shape = tf.shape(image)
 
         anchor_generators = tf.nest.flatten(self.anchor_generators)
         results = [anchor_gen(image_shape) for anchor_gen in anchor_generators]
         results = tf.nest.pack_sequence_as(self.anchor_generators, results)
@@ -175,15 +185,16 @@
                 image_shape=image_shape,
             )
         return results
 
 
 # TODO(tanzheny): consider having customized anchor offset.
 class _SingleAnchorGenerator:
-    """Internal utility to generate anchors for a single feature map in `yxyx` format.
+    """Internal utility to generate anchors for a single feature map in `yxyx`
+    format.
 
     Example:
     ```python
     anchor_gen = _SingleAnchorGenerator(32, [.5, 1., 2.], stride=16)
     anchors = anchor_gen([512, 512, 3])
     ```
 
@@ -198,16 +209,16 @@
       scales: A list/tuple, or a list/tuple of a list/tuple of positive
         floats representing the actual anchor size to the base `anchor_size`.
       aspect_ratios: a list/tuple of positive floats representing the ratio of
         anchor width to anchor height.
       stride: A single int represents the anchor stride size between center of
         each anchor.
       clip_boxes: Boolean to represent whether the anchor coordinates should be
-        clipped to the image size. Defaults to `False`.
-      dtype: (Optional) The data type to use for the output anchors.  Defaults to
+        clipped to the image size, defaults to `False`.
+      dtype: (Optional) The data type to use for the output anchors, defaults to
         'float32'.
 
     """
 
     def __init__(
         self,
         bounding_box_format,
@@ -244,20 +255,20 @@
             anchor_widths.append(anchor_width)
         anchor_heights = tf.concat(anchor_heights, axis=0)
         anchor_widths = tf.concat(anchor_widths, axis=0)
         half_anchor_heights = tf.reshape(0.5 * anchor_heights, [1, 1, -1])
         half_anchor_widths = tf.reshape(0.5 * anchor_widths, [1, 1, -1])
 
         stride = tf.cast(self.stride, tf.float32)
-        # make sure range of `cx` is within limit of `image_width` with `stride`,
-        # also for sizes where `image_width % stride != 0`.
+        # make sure range of `cx` is within limit of `image_width` with
+        # `stride`, also for sizes where `image_width % stride != 0`.
         # [W]
         cx = tf.range(0.5 * stride, (image_width // stride) * stride, stride)
-        # make sure range of `cy` is within limit of `image_height` with `stride`,
-        # also for sizes where `image_height % stride != 0`.
+        # make sure range of `cy` is within limit of `image_height` with
+        # `stride`, also for sizes where `image_height % stride != 0`.
         # [H]
         cy = tf.range(0.5 * stride, (image_height // stride) * stride, stride)
         # [H, W]
         cx_grid, cy_grid = tf.meshgrid(cx, cy)
         # [H, W, 1]
         cx_grid = tf.expand_dims(cx_grid, axis=-1)
         cy_grid = tf.expand_dims(cy_grid, axis=-1)
@@ -276,8 +287,10 @@
         if self.clip_boxes:
             y_min = tf.maximum(tf.minimum(y_min, image_height), 0.0)
             y_max = tf.maximum(tf.minimum(y_max, image_height), 0.0)
             x_min = tf.maximum(tf.minimum(x_min, image_width), 0.0)
             x_max = tf.maximum(tf.minimum(x_max, image_width), 0.0)
 
         # [H * W * K, 4]
-        return tf.cast(tf.concat([y_min, x_min, y_max, x_max], axis=-1), self.dtype)
+        return tf.cast(
+            tf.concat([y_min, x_min, y_max, x_max], axis=-1), self.dtype
+        )
```

## keras_cv/layers/object_detection/box_matcher.py

```diff
@@ -7,47 +7,51 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 from typing import List
 from typing import Tuple
 
 import tensorflow as tf
+from tensorflow import keras
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class BoxMatcher(tf.keras.layers.Layer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class BoxMatcher(keras.layers.Layer):
     """Box matching logic based on argmax of highest value (e.g., IOU).
 
     This class computes matches from a similarity matrix. Each row will be
     matched to at least one column, the matched result can either be positive
      / negative, or simply ignored depending on the setting.
 
     The settings include `thresholds` and `match_values`, for example if:
     1) thresholds=[negative_threshold, positive_threshold], and
-       match_values=[negative_value=0, ignore_value=-1, positive_value=1]: the rows will
-       be assigned to positive_value if its argmax result >=
+       match_values=[negative_value=0, ignore_value=-1, positive_value=1]: the
+       rows will be assigned to positive_value if its argmax result >=
        positive_threshold; the rows will be assigned to negative_value if its
-       argmax result < negative_threshold, and the rows will be assigned
-       to ignore_value if its argmax result is between [negative_threshold, positive_threshold).
+       argmax result < negative_threshold, and the rows will be assigned to
+       ignore_value if its argmax result is between [negative_threshold,
+       positive_threshold).
     2) thresholds=[negative_threshold, positive_threshold], and
-       match_values=[ignore_value=-1, negative_value=0, positive_value=1]: the rows will
-       be assigned to positive_value if its argmax result >=
+       match_values=[ignore_value=-1, negative_value=0, positive_value=1]: the
+       rows will be assigned to positive_value if its argmax result >=
        positive_threshold; the rows will be assigned to ignore_value if its
-       argmax result < negative_threshold, and the rows will be assigned
-       to negative_value if its argmax result is between [negative_threshold ,positive_threshold).
-       This is different from case 1) by swapping first two
+       argmax result < negative_threshold, and the rows will be assigned to
+       negative_value if its argmax result is between [negative_threshold,
+       positive_threshold). This is different from case 1) by swapping first two
        values.
     3) thresholds=[positive_threshold], and
-       match_values=[negative_values, positive_value]: the rows will be assigned to
-       positive value if its argmax result >= positive_threshold; the rows
-       will be assigned to negative_value if its argmax result < negative_threshold.
+       match_values=[negative_values, positive_value]: the rows will be assigned
+       to positive value if its argmax result >= positive_threshold; the rows
+       will be assigned to negative_value if its argmax result <
+       negative_threshold.
 
     Args:
         thresholds: A sorted list of floats to classify the matches into
           different results (e.g. positive or negative or ignored match). The
           list will be prepended with -Inf and and appended with +Inf.
         match_values: A list of integers representing matched results (e.g.
           positive or negative or ignored match). len(`match_values`) must
@@ -119,43 +123,52 @@
             similarity_matrix = tf.expand_dims(similarity_matrix, axis=0)
         static_shape = similarity_matrix.shape.as_list()
         num_rows = static_shape[1] or tf.shape(similarity_matrix)[1]
         batch_size = static_shape[0] or tf.shape(similarity_matrix)[0]
 
         def _match_when_cols_are_empty():
             """Performs matching when the rows of similarity matrix are empty.
-            When the rows are empty, all detections are false positives. So we return
-            a tensor of -1's to indicate that the rows do not match to any columns.
+            When the rows are empty, all detections are false positives. So we
+            return a tensor of -1's to indicate that the rows do not match to
+            any columns.
             Returns:
-                matched_columns: An integer tensor of shape [batch_size, num_rows]
-                  storing the index of the matched column for each row.
-                matched_values: An integer tensor of shape [batch_size, num_rows]
-                  storing the match type indicator (e.g. positive or negative
-                  or ignored match).
+                matched_columns: An integer tensor of shape [batch_size,
+                    num_rows] storing the index of the matched column for each
+                    row.
+                matched_values: An integer tensor of shape [batch_size,
+                    num_rows] storing the match type indicator (e.g. positive or
+                    negative or ignored match).
             """
             with tf.name_scope("empty_boxes"):
-                matched_columns = tf.zeros([batch_size, num_rows], dtype=tf.int32)
-                matched_values = -tf.ones([batch_size, num_rows], dtype=tf.int32)
+                matched_columns = tf.zeros(
+                    [batch_size, num_rows], dtype=tf.int32
+                )
+                matched_values = -tf.ones(
+                    [batch_size, num_rows], dtype=tf.int32
+                )
                 return matched_columns, matched_values
 
         def _match_when_cols_are_non_empty():
-            """Performs matching when the rows of similarity matrix are non empty.
+            """Performs matching when the rows of similarity matrix are
+            non-empty.
             Returns:
-                matched_columns: An integer tensor of shape [batch_size, num_rows]
-                  storing the index of the matched column for each row.
-                matched_values: An integer tensor of shape [batch_size, num_rows]
-                  storing the match type indicator (e.g. positive or negative
-                  or ignored match).
+                matched_columns: An integer tensor of shape [batch_size,
+                    num_rows] storing the index of the matched column for each
+                    row.
+                matched_values: An integer tensor of shape [batch_size,
+                    num_rows] storing the match type indicator (e.g. positive or
+                    negative or ignored match).
             """
             with tf.name_scope("non_empty_boxes"):
                 matched_columns = tf.argmax(
                     similarity_matrix, axis=-1, output_type=tf.int32
                 )
 
-                # Get logical indices of ignored and unmatched columns as tf.int64
+                # Get logical indices of ignored and unmatched columns as
+                # tf.int64
                 matched_vals = tf.reduce_max(similarity_matrix, axis=-1)
                 matched_values = tf.zeros([batch_size, num_rows], tf.int32)
 
                 match_dtype = matched_vals.dtype
                 for ind, low, high in zip(
                     self.match_values, self.thresholds[:-1], self.thresholds[1:]
                 ):
@@ -166,32 +179,36 @@
                         tf.less(matched_vals, high_threshold),
                     )
                     matched_values = self._set_values_using_indicator(
                         matched_values, mask, ind
                     )
 
                 if self.force_match_for_each_col:
-                    # [batch_size, num_cols], for each column (groundtruth_box), find the
-                    # best matching row (anchor).
+                    # [batch_size, num_cols], for each column (groundtruth_box),
+                    # find the best matching row (anchor).
                     matching_rows = tf.argmax(
                         input=similarity_matrix, axis=1, output_type=tf.int32
                     )
-                    # [batch_size, num_cols, num_rows], a transposed 0-1 mapping matrix M,
-                    # where M[j, i] = 1 means column j is matched to row i.
+                    # [batch_size, num_cols, num_rows], a transposed 0-1 mapping
+                    # matrix M, where M[j, i] = 1 means column j is matched to
+                    # row i.
                     column_to_row_match_mapping = tf.one_hot(
                         matching_rows, depth=num_rows
                     )
-                    # [batch_size, num_rows], for each row (anchor), find the matched
-                    # column (groundtruth_box).
+                    # [batch_size, num_rows], for each row (anchor), find the
+                    # matched column (groundtruth_box).
                     force_matched_columns = tf.argmax(
-                        input=column_to_row_match_mapping, axis=1, output_type=tf.int32
+                        input=column_to_row_match_mapping,
+                        axis=1,
+                        output_type=tf.int32,
                     )
                     # [batch_size, num_rows]
                     force_matched_column_mask = tf.cast(
-                        tf.reduce_max(column_to_row_match_mapping, axis=1), tf.bool
+                        tf.reduce_max(column_to_row_match_mapping, axis=1),
+                        tf.bool,
                     )
                     # [batch_size, num_rows]
                     matched_columns = tf.where(
                         force_matched_column_mask,
                         force_matched_columns,
                         matched_columns,
                     )
@@ -201,15 +218,16 @@
                         * tf.ones([batch_size, num_rows], dtype=tf.int32),
                         matched_values,
                     )
 
                 return matched_columns, matched_values
 
         num_boxes = (
-            similarity_matrix.shape.as_list()[-1] or tf.shape(similarity_matrix)[-1]
+            similarity_matrix.shape.as_list()[-1]
+            or tf.shape(similarity_matrix)[-1]
         )
         matched_columns, matched_values = tf.cond(
             pred=tf.greater(num_boxes, 0),
             true_fn=_match_when_cols_are_non_empty,
             false_fn=_match_when_cols_are_empty,
         )
```

## keras_cv/layers/object_detection/box_matcher_test.py

```diff
@@ -37,15 +37,17 @@
         with self.assertRaisesRegex(ValueError, "must be sorted"):
             _ = BoxMatcher(
                 thresholds=[bg_thresh_hi, bg_thresh_lo, fg_threshold],
                 match_values=[-3, -2, -1, 1],
             )
 
     def test_box_matcher_unbatched(self):
-        sim_matrix = tf.constant([[0.04, 0, 0, 0], [0, 0, 1.0, 0]], dtype=tf.float32)
+        sim_matrix = tf.constant(
+            [[0.04, 0, 0, 0], [0, 0, 1.0, 0]], dtype=tf.float32
+        )
 
         fg_threshold = 0.5
         bg_thresh_hi = 0.2
         bg_thresh_lo = 0.0
 
         matcher = BoxMatcher(
             thresholds=[bg_thresh_lo, bg_thresh_hi, fg_threshold],
@@ -57,15 +59,17 @@
 
         self.assertAllEqual(positive_matches.numpy(), [False, True])
         self.assertAllEqual(negative_matches.numpy(), [True, False])
         self.assertAllEqual(match_indices.numpy(), [0, 2])
         self.assertAllEqual(matched_values.numpy(), [-2, 1])
 
     def test_box_matcher_batched(self):
-        sim_matrix = tf.constant([[[0.04, 0, 0, 0], [0, 0, 1.0, 0]]], dtype=tf.float32)
+        sim_matrix = tf.constant(
+            [[[0.04, 0, 0, 0], [0, 0, 1.0, 0]]], dtype=tf.float32
+        )
 
         fg_threshold = 0.5
         bg_thresh_hi = 0.2
         bg_thresh_lo = 0.0
 
         matcher = BoxMatcher(
             thresholds=[bg_thresh_lo, bg_thresh_hi, fg_threshold],
@@ -96,17 +100,19 @@
             force_match_for_each_col=True,
         )
         match_indices, matched_values = matcher(sim_matrix)
         positive_matches = tf.greater_equal(matched_values, 0)
         negative_matches = tf.equal(matched_values, -2)
 
         self.assertAllEqual(positive_matches.numpy(), [True, True, True, True])
-        self.assertAllEqual(negative_matches.numpy(), [False, False, False, False])
-        # the first anchor cannot be matched to 4th gt box given that is matched to
-        # the last anchor.
+        self.assertAllEqual(
+            negative_matches.numpy(), [False, False, False, False]
+        )
+        # the first anchor cannot be matched to 4th gt box given that is matched
+        # to the last anchor.
         self.assertAllEqual(match_indices.numpy(), [1, 2, 0, 3])
         self.assertAllEqual(matched_values.numpy(), [1, 1, 1, 1])
 
     def test_box_matcher_empty_gt_boxes(self):
         sim_matrix = tf.constant([[], []], dtype=tf.float32)
 
         fg_threshold = 0.5
```

## keras_cv/layers/object_detection/multi_class_non_max_suppression.py

```diff
@@ -9,106 +9,118 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 
 
 # TODO(tanzhenyu): provide a TPU compatible NMS decoder.
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class MultiClassNonMaxSuppression(tf.keras.layers.Layer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class MultiClassNonMaxSuppression(keras.layers.Layer):
     """A Keras layer that decodes predictions of an object detection model.
 
     Arguments:
       bounding_box_format: The format of bounding boxes of input dataset. Refer
         [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
-        for more details on supported bounding box formats.
-      from_logits: boolean, True means input score is logits, False means confidence.
+        for more details on supported bounding box
+        formats.
+      from_logits: boolean, True means input score is logits, False means
+        confidence.
       iou_threshold: a float value in the range [0, 1] representing the minimum
-        IoU threshold for two boxes to be considered same for suppression. Defaults
-        to 0.5.
+        IoU threshold for two boxes to be considered same for suppression.
+        Defaults to 0.5.
       confidence_threshold: a float value in the range [0, 1]. All boxes with
-        confidence below this value will be discarded. Defaults to 0.9.
-      max_detections: the maximum detections to consider after nms is applied. A large
-        number may trigger significant memory overhead. Defaults to 100.
-      max_detections_per_class: the maximum detections to consider per class after
-        nms is applied. Defaults to 100.
-    """
+        confidence below this value will be discarded, defaults to 0.5.
+      max_detections: the maximum detections to consider after nms is applied. A
+        large number may trigger significant memory overhead, defaults to 100.
+      max_detections_per_class: the maximum detections to consider per class
+        after nms is applied, defaults to 100.
+    """  # noqa: E501
 
     def __init__(
         self,
         bounding_box_format,
         from_logits,
         iou_threshold=0.5,
-        confidence_threshold=0.9,
+        confidence_threshold=0.5,
         max_detections=100,
         max_detections_per_class=100,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         self.from_logits = from_logits
         self.iou_threshold = iou_threshold
         self.confidence_threshold = confidence_threshold
         self.max_detections = max_detections
         self.max_detections_per_class = max_detections_per_class
         self.built = True
 
-    def call(self, box_prediction, confidence_prediction):
-        """Accepts images and raw predictions, and returns bounding box predictions.
+    def call(
+        self, box_prediction, class_prediction, images=None, image_shape=None
+    ):
+        """Accepts images and raw predictions, and returns bounding box
+        predictions.
 
         Args:
             box_prediction: Dense Tensor of shape [batch, boxes, 4] in the
                 `bounding_box_format` specified in the constructor.
-            confidence_prediction: Dense Tensor of shape [batch, boxes, num_classes].
+            class_prediction: Dense Tensor of shape [batch, boxes, num_classes].
         """
         target_format = "yxyx"
         if bounding_box.is_relative(self.bounding_box_format):
             target_format = bounding_box.as_relative(target_format)
 
         box_prediction = bounding_box.convert_format(
             box_prediction,
             source=self.bounding_box_format,
             target=target_format,
+            images=images,
+            image_shape=image_shape,
         )
         if self.from_logits:
-            confidence_prediction = tf.nn.softmax(confidence_prediction)
+            class_prediction = tf.math.sigmoid(class_prediction)
 
         box_prediction = tf.expand_dims(box_prediction, axis=-2)
         (
-            box_pred,
-            confidence_pred,
-            class_pred,
+            box_prediction,
+            confidence_prediction,
+            class_prediction,
             valid_det,
         ) = tf.image.combined_non_max_suppression(
             boxes=box_prediction,
-            scores=confidence_prediction,
+            scores=class_prediction,
             max_output_size_per_class=self.max_detections_per_class,
             max_total_size=self.max_detections,
             score_threshold=self.confidence_threshold,
             iou_threshold=self.iou_threshold,
             clip_boxes=False,
         )
-        box_pred = bounding_box.convert_format(
-            box_pred,
+        box_prediction = bounding_box.convert_format(
+            box_prediction,
             source=target_format,
             target=self.bounding_box_format,
+            images=images,
+            image_shape=image_shape,
         )
         bounding_boxes = {
-            "boxes": box_pred,
-            "confidence": confidence_pred,
-            "classes": class_pred,
+            "boxes": box_prediction,
+            "confidence": confidence_prediction,
+            "classes": class_prediction,
             "num_detections": valid_det,
         }
         # this is required to comply with KerasCV bounding box format.
-        return bounding_box.mask_invalid_detections(bounding_boxes)
+        return bounding_box.mask_invalid_detections(
+            bounding_boxes, output_ragged=True
+        )
 
     def get_config(self):
         config = {
             "bounding_box_format": self.bounding_box_format,
             "from_logits": self.from_logits,
             "iou_threshold": self.iou_threshold,
             "confidence_threshold": self.confidence_threshold,
```

## keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py

```diff
@@ -8,66 +8,42 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import unittest
-
 import tensorflow as tf
 
 from keras_cv import layers as cv_layers
 
 
 def decode_predictions_output_shapes():
-    classes = 10
-    predictions_shape = (8, 98208, 4 + classes)
+    num_classes = 10
+    predictions_shape = (8, 98208, 4 + num_classes)
 
     predictions = tf.random.stateless_uniform(
-        shape=predictions_shape, seed=(2, 3), minval=0.0, maxval=1.0, dtype=tf.float32
+        shape=predictions_shape,
+        seed=(2, 3),
+        minval=0.0,
+        maxval=1.0,
+        dtype=tf.float32,
     )
     box_pred = predictions[..., :4]
-    confidence_pred = predictions[..., 4:]
+    class_prediction = predictions[..., 4:]
 
     layer = cv_layers.MultiClassNonMaxSuppression(
         bounding_box_format="xyxy",
         from_logits=True,
         max_detections=100,
     )
 
-    result = layer(box_prediction=box_pred, confidence_prediction=confidence_pred)
+    result = layer(box_prediction=box_pred, class_prediction=class_prediction)
     return result
 
 
 class NmsPredictionDecoderTest(tf.test.TestCase):
     def test_decode_predictions_output_shapes(self):
         result = decode_predictions_output_shapes()
-        self.assertEqual(result["boxes"].shape, [8, 100, 4])
-        self.assertEqual(result["classes"].shape, [8, 100])
-        self.assertEqual(result["confidence"].shape, [8, 100])
-
-
-@unittest.expectedFailure
-class NmsPredictionDecoderTestWithXLA(tf.test.TestCase):
-    def test_decode_predictions_output_shapes(self):
-        xla_function = tf.function(decode_predictions_output_shapes, jit_compile=True)
-        result = xla_function()
-        self.assertEqual(result["boxes"].shape, [8, 100, 4])
-        self.assertEqual(result["classes"].shape, [8, 100])
-        self.assertEqual(result["confidence"].shape, [8, 100])
-
-
-class NmsPredictionDecoderTestWithXLAMlirBridge(tf.test.TestCase):
-    def setUp(self):
-        tf.config.experimental.enable_mlir_bridge()
-
-    def tearDown(self):
-        tf.config.experimental.disable_mlir_bridge()
-
-    # @unittest.expectedFailure
-    def test_decode_predictions_output_shapes(self):
-        xla_function = tf.function(decode_predictions_output_shapes, jit_compile=True)
-        result = xla_function()
-        self.assertEqual(result["boxes"].shape, [8, 100, 4])
-        self.assertEqual(result["classes"].shape, [8, 100])
-        self.assertEqual(result["confidence"].shape, [8, 100])
+        self.assertEqual(result["boxes"].shape, [8, None, 4])
+        self.assertEqual(result["classes"].shape, [8, None])
+        self.assertEqual(result["confidence"].shape, [8, None])
```

## keras_cv/layers/object_detection/roi_align.py

```diff
@@ -14,14 +14,15 @@
 
 from typing import Dict
 from typing import Mapping
 from typing import Optional
 from typing import Tuple
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 
 
 def _feature_bilinear_interpolation(
     features: tf.Tensor, kernel_y: tf.Tensor, kernel_x: tf.Tensor
 ) -> tf.Tensor:
@@ -34,16 +35,16 @@
                           [f10, f11]]
     f(y, x) = (hy*hx)f00 + (hy*lx)f01 + (ly*hx)f10 + (lx*ly)f11
     f(y, x) = w00*f00 + w01*f01 + w10*f10 + w11*f11
     kernel_y = [hy, ly]
     kernel_x = [hx, lx]
 
     Args:
-      features: The features are in shape of [batch_size, num_boxes, output_size *
-        2, output_size * 2, num_filters].
+      features: The features are in shape of [batch_size, num_boxes,
+        output_size * 2, output_size * 2, num_filters].
       kernel_y: Tensor of size [batch_size, boxes, output_size, 2, 1].
       kernel_x: Tensor of size [batch_size, boxes, output_size, 2, 1].
 
     Returns:
       A 5-D tensor representing feature crop of shape
       [batch_size, num_boxes, output_size, output_size, num_filters].
     """
@@ -74,31 +75,35 @@
     features = tf.reshape(
         features, [batch_size, num_boxes, output_size, output_size, num_filters]
     )
     return features
 
 
 def _compute_grid_positions(
-    boxes: tf.Tensor, boundaries: tf.Tensor, output_size: int, sample_offset: float
+    boxes: tf.Tensor,
+    boundaries: tf.Tensor,
+    output_size: int,
+    sample_offset: float,
 ) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:
     """
     Computes the grid position w.r.t. the corresponding feature map.
 
     Args:
       boxes: a 3-D tensor of shape [batch_size, num_boxes, 4] encoding the
         information of each box w.r.t. the corresponding feature map.
         boxes[:, :, 0:2] are the grid position in (y, x) (float) of the top-left
         corner of each box. boxes[:, :, 2:4] are the box sizes in (h, w) (float)
-          in terms of the number of pixels of the corresponding feature map size.
+          in terms of the number of pixels of the corresponding feature map
+          size.
       boundaries: a 3-D tensor of shape [batch_size, num_boxes, 2] representing
         the boundary (in (y, x)) of the corresponding feature map for each box.
-        Any resampled grid points that go beyond the bounary will be clipped.
+        Any resampled grid points that go beyond the boundary will be clipped.
       output_size: a scalar indicating the output crop size.
-      sample_offset: a float number in [0, 1] indicates the subpixel sample offset
-        from grid point.
+      sample_offset: a float number in [0, 1] indicates the subpixel sample
+        offset from grid point.
 
     Returns:
       kernel_y: Tensor of size [batch_size, boxes, output_size, 2, 1].
       kernel_x: Tensor of size [batch_size, boxes, output_size, 2, 1].
       box_grid_y0y1: Tensor of size [batch_size, boxes, output_size, 2]
       box_grid_x0x1: Tensor of size [batch_size, boxes, output_size, 2]
     """
@@ -119,18 +124,26 @@
     box_grid_y = tf.stack(box_grid_y, axis=2)
 
     box_grid_y0 = tf.floor(box_grid_y)
     box_grid_x0 = tf.floor(box_grid_x)
     box_grid_x0 = tf.maximum(tf.cast(0.0, dtype=box_grid_x0.dtype), box_grid_x0)
     box_grid_y0 = tf.maximum(tf.cast(0.0, dtype=box_grid_y0.dtype), box_grid_y0)
 
-    box_grid_x0 = tf.minimum(box_grid_x0, tf.expand_dims(boundaries[:, :, 1], -1))
-    box_grid_x1 = tf.minimum(box_grid_x0 + 1, tf.expand_dims(boundaries[:, :, 1], -1))
-    box_grid_y0 = tf.minimum(box_grid_y0, tf.expand_dims(boundaries[:, :, 0], -1))
-    box_grid_y1 = tf.minimum(box_grid_y0 + 1, tf.expand_dims(boundaries[:, :, 0], -1))
+    box_grid_x0 = tf.minimum(
+        box_grid_x0, tf.expand_dims(boundaries[:, :, 1], -1)
+    )
+    box_grid_x1 = tf.minimum(
+        box_grid_x0 + 1, tf.expand_dims(boundaries[:, :, 1], -1)
+    )
+    box_grid_y0 = tf.minimum(
+        box_grid_y0, tf.expand_dims(boundaries[:, :, 0], -1)
+    )
+    box_grid_y1 = tf.minimum(
+        box_grid_y0 + 1, tf.expand_dims(boundaries[:, :, 0], -1)
+    )
 
     box_gridx0x1 = tf.stack([box_grid_x0, box_grid_x1], axis=-1)
     box_gridy0y1 = tf.stack([box_grid_y0, box_grid_y1], axis=-1)
 
     # The RoIAlign feature f can be computed by bilinear interpolation of four
     # neighboring feature points f0, f1, f2, and f3.
     # f(y, x) = [hy, ly] * [[f00, f01], * [hx, lx]^T
@@ -157,24 +170,25 @@
     sample_offset: float = 0.5,
 ) -> tf.Tensor:
     """
     Crop and resize on multilevel feature pyramid.
 
     Generate the (output_size, output_size) set of pixels for each input box
     by first locating the box into the correct feature level, and then cropping
-    and resizing it using the correspoding feature map of that level.
+    and resizing it using the corresponding feature map of that level.
 
     Args:
-      features: A dictionary with key as pyramid level and value as features. The
-        features are in shape of [batch_size, height_l, width_l, num_filters].
-      boxes: A 3-D Tensor of shape [batch_size, num_boxes, 4]. Each row represents
-        a box with [y1, x1, y2, x2] in un-normalized coordinates.
+      features: A dictionary with key as pyramid level and value as features.
+        The features are in shape of [batch_size, height_l, width_l,
+        num_filters].
+      boxes: A 3-D Tensor of shape [batch_size, num_boxes, 4]. Each row
+        represents a box with [y1, x1, y2, x2] in un-normalized coordinates.
       output_size: A scalar to indicate the output crop size.
-      sample_offset: a float number in [0, 1] indicates the subpixel sample offset
-        from grid point.
+      sample_offset: a float number in [0, 1] indicates the subpixel sample
+        offset from grid point.
 
     Returns:
       A 5-D tensor representing feature crop of shape
       [batch_size, num_boxes, output_size, output_size, num_filters].
     """
 
     with tf.name_scope("multilevel_crop_and_resize"):
@@ -196,24 +210,25 @@
         features_all = []
         feature_heights = []
         feature_widths = []
         for level in range(min_level, max_level + 1):
             shape = features[level].get_shape().as_list()
             feature_heights.append(shape[1])
             feature_widths.append(shape[2])
-            # Concat tensor of [batch_size, height_l * width_l, num_filters] for each
-            # levels.
+            # Concat tensor of [batch_size, height_l * width_l, num_filters] for
+            # each level.
             features_all.append(
                 tf.reshape(features[level], [batch_size, -1, num_filters])
             )
         features_r2 = tf.reshape(tf.concat(features_all, 1), [-1, num_filters])
 
         # Calculate height_l * width_l for each level.
         level_dim_sizes = [
-            feature_widths[i] * feature_heights[i] for i in range(len(feature_widths))
+            feature_widths[i] * feature_heights[i]
+            for i in range(len(feature_widths))
         ]
         # level_dim_offsets is accumulated sum of level_dim_size.
         level_dim_offsets = [0]
         for i in range(len(feature_widths) - 1):
             level_dim_offsets.append(level_dim_offsets[i] + level_dim_sizes[i])
         batch_dim_size = level_dim_offsets[-1] + level_dim_sizes[-1]
         level_dim_offsets = tf.constant(level_dim_offsets, tf.int32)
@@ -225,25 +240,27 @@
         areas_sqrt = tf.sqrt(
             tf.cast(box_height, tf.float32) * tf.cast(box_width, tf.float32)
         )
 
         # following the FPN paper to divide by 224.
         levels = tf.cast(
             tf.math.floordiv(
-                tf.math.log(tf.math.divide_no_nan(areas_sqrt, 224.0)), tf.math.log(2.0)
+                tf.math.log(tf.math.divide_no_nan(areas_sqrt, 224.0)),
+                tf.math.log(2.0),
             )
             + 4.0,
             dtype=tf.int32,
         )
         # Maps levels between [min_level, max_level].
         levels = tf.minimum(max_level, tf.maximum(levels, min_level))
 
         # Projects box location and sizes to corresponding feature levels.
         scale_to_level = tf.cast(
-            tf.pow(tf.constant(2.0), tf.cast(levels, tf.float32)), dtype=boxes.dtype
+            tf.pow(tf.constant(2.0), tf.cast(levels, tf.float32)),
+            dtype=boxes.dtype,
         )
         boxes /= tf.expand_dims(scale_to_level, axis=2)
         box_width /= scale_to_level
         box_height /= scale_to_level
         boxes = tf.concat(
             [
                 boxes[:, :, 0:2],
@@ -256,91 +273,115 @@
         # Maps levels to [0, max_level-min_level].
         levels -= min_level
         level_strides = tf.pow([[2.0]], tf.cast(levels, tf.float32))
         boundary = tf.cast(
             tf.concat(
                 [
                     tf.expand_dims(
-                        [[tf.cast(max_feature_height, tf.float32)]] / level_strides - 1,
+                        [[tf.cast(max_feature_height, tf.float32)]]
+                        / level_strides
+                        - 1,
                         axis=-1,
                     ),
                     tf.expand_dims(
-                        [[tf.cast(max_feature_width, tf.float32)]] / level_strides - 1,
+                        [[tf.cast(max_feature_width, tf.float32)]]
+                        / level_strides
+                        - 1,
                         axis=-1,
                     ),
                 ],
                 axis=-1,
             ),
             boxes.dtype,
         )
 
         # Compute grid positions.
-        kernel_y, kernel_x, box_gridy0y1, box_gridx0x1 = _compute_grid_positions(
-            boxes, boundary, output_size, sample_offset
-        )
+        (
+            kernel_y,
+            kernel_x,
+            box_gridy0y1,
+            box_gridx0x1,
+        ) = _compute_grid_positions(boxes, boundary, output_size, sample_offset)
 
         x_indices = tf.cast(
             tf.reshape(box_gridx0x1, [batch_size, num_boxes, output_size * 2]),
             dtype=tf.int32,
         )
         y_indices = tf.cast(
             tf.reshape(box_gridy0y1, [batch_size, num_boxes, output_size * 2]),
             dtype=tf.int32,
         )
 
         batch_size_offset = tf.tile(
-            tf.reshape(tf.range(batch_size) * batch_dim_size, [batch_size, 1, 1, 1]),
+            tf.reshape(
+                tf.range(batch_size) * batch_dim_size, [batch_size, 1, 1, 1]
+            ),
             [1, num_boxes, output_size * 2, output_size * 2],
         )
         # Get level offset for each box. Each box belongs to one level.
         levels_offset = tf.tile(
             tf.reshape(
-                tf.gather(level_dim_offsets, levels), [batch_size, num_boxes, 1, 1]
+                tf.gather(level_dim_offsets, levels),
+                [batch_size, num_boxes, 1, 1],
             ),
             [1, 1, output_size * 2, output_size * 2],
         )
         y_indices_offset = tf.tile(
             tf.reshape(
-                y_indices * tf.expand_dims(tf.gather(height_dim_sizes, levels), -1),
+                y_indices
+                * tf.expand_dims(tf.gather(height_dim_sizes, levels), -1),
                 [batch_size, num_boxes, output_size * 2, 1],
             ),
             [1, 1, 1, output_size * 2],
         )
         x_indices_offset = tf.tile(
             tf.reshape(x_indices, [batch_size, num_boxes, 1, output_size * 2]),
             [1, 1, output_size * 2, 1],
         )
         indices = tf.reshape(
-            batch_size_offset + levels_offset + y_indices_offset + x_indices_offset,
+            batch_size_offset
+            + levels_offset
+            + y_indices_offset
+            + x_indices_offset,
             [-1],
         )
 
-        # TODO(tanzhenyu): replace tf.gather with tf.gather_nd and try to get similar
-        # performance.
+        # TODO(tanzhenyu): replace tf.gather with tf.gather_nd and try to get
+        #  similar performance.
         features_per_box = tf.reshape(
             tf.gather(features_r2, indices),
-            [batch_size, num_boxes, output_size * 2, output_size * 2, num_filters],
+            [
+                batch_size,
+                num_boxes,
+                output_size * 2,
+                output_size * 2,
+                num_filters,
+            ],
         )
 
         # Bilinear interpolation.
         features_per_box = _feature_bilinear_interpolation(
             features_per_box, kernel_y, kernel_x
         )
         return features_per_box
 
 
-# TODO(tanzhenyu): Remove this implementation once roi_pool has better performance.
-# as this is mostly a duplicate of
-# https://github.com/tensorflow/models/blob/master/official/legacy/detection/ops/spatial_transform_ops.py#L324
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class _ROIAligner(tf.keras.layers.Layer):
+# TODO(tanzhenyu): Remove this implementation once roi_pool has better
+#  performance as this is mostly a duplicate of
+#  https://github.com/tensorflow/models/blob/master/official/legacy/detection/ops/spatial_transform_ops.py#L324
+@keras.utils.register_keras_serializable(package="keras_cv")
+class _ROIAligner(keras.layers.Layer):
     """Performs ROIAlign for the second stage processing."""
 
     def __init__(
-        self, bounding_box_format, target_size=7, sample_offset: float = 0.5, **kwargs
+        self,
+        bounding_box_format,
+        target_size=7,
+        sample_offset: float = 0.5,
+        **kwargs
     ):
         """
         Generates ROI Aligner.
 
         Args:
           bounding_box_format: the input format for boxes.
           crop_size: An `int` of the output size of the cropped features.
@@ -359,27 +400,29 @@
         features: Mapping[str, tf.Tensor],
         boxes: tf.Tensor,
         training: Optional[bool] = None,
     ):
         """
 
         Args:
-          features: A dictionary with key as pyramid level and value as features.
-            The features are in shape of
+          features: A dictionary with key as pyramid level and value as
+            features. The features are in shape of
             [batch_size, height_l, width_l, num_filters].
           boxes: A 3-D `tf.Tensor` of shape [batch_size, num_boxes, 4]. Each row
             represents a box with [y1, x1, y2, x2] in un-normalized coordinates.
             from grid point.
           training: A `bool` of whether it is in training mode.
         Returns:
           A 5-D `tf.Tensor` representing feature crop of shape
           [batch_size, num_boxes, crop_size, crop_size, num_filters].
         """
         boxes = bounding_box.convert_format(
-            boxes, source=self._config_dict["bounding_box_format"], target="yxyx"
+            boxes,
+            source=self._config_dict["bounding_box_format"],
+            target="yxyx",
         )
         roi_features = multilevel_crop_and_resize(
             features,
             boxes,
             output_size=self._config_dict["crop_size"],
             sample_offset=self._config_dict["sample_offset"],
         )
```

## keras_cv/layers/object_detection/roi_generator.py

```diff
@@ -14,20 +14,21 @@
 
 from typing import Mapping
 from typing import Optional
 from typing import Tuple
 from typing import Union
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class ROIGenerator(tf.keras.layers.Layer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class ROIGenerator(keras.layers.Layer):
     """
     Generates region of interests (ROI, or proposal) from scores.
 
     Mainly used in Region CNN (RCNN) networks.
 
     This works for a multi-level input, both boxes and scores are dictionary
     inputs with the same set of keys.
@@ -42,40 +43,48 @@
     3) combined scores and ROIs across all levels
     4) post_nms_topk scores and ROIs sorted and selected
 
     Args:
         bounding_box_format: a case-insensitive string.
             For detailed information on the supported format, see the
             [KerasCV bounding box documentation](https://keras.io/api/keras_cv/bounding_box/formats/).
-        pre_nms_topk_train: int. number of top k scoring proposals to keep before applying NMS in training mode.
-            When RPN is run on multiple feature maps / levels (as in FPN) this number is per
+        pre_nms_topk_train: int. number of top k scoring proposals to keep
+            before applying NMS in training mode. When RPN is run on multiple
+            feature maps / levels (as in FPN) this number is per
             feature map / level.
-        nms_score_threshold_train: float. score threshold to use for NMS in training mode.
-        nms_iou_threshold_train: float. IOU threshold to use for NMS in training mode.
-        post_nms_topk_train: int. number of top k scoring proposals to keep after applying NMS in training mode.
-            When RPN is run on multiple feature maps / levels (as in FPN) this number is per
+        nms_score_threshold_train: float. score threshold to use for NMS in
+            training mode.
+        nms_iou_threshold_train: float. IOU threshold to use for NMS in training
+            mode.
+        post_nms_topk_train: int. number of top k scoring proposals to keep
+            after applying NMS in training mode. When RPN is run on multiple
+            feature maps / levels (as in FPN) this number is per
             feature map / level.
-        pre_nms_topk_test: int. number of top k scoring proposals to keep before applying NMS in inference mode.
-            When RPN is run on multiple feature maps / levels (as in FPN) this number is per
+        pre_nms_topk_test: int. number of top k scoring proposals to keep before
+            applying NMS in inference mode. When RPN is run on multiple
+            feature maps / levels (as in FPN) this number is per
             feature map / level.
-        nms_score_threshold_test: float. score threshold to use for NMS in inference mode.
-        nms_iou_threshold_test: float. IOU threshold to use for NMS in inference mode.
-        post_nms_topk_test: int. number of top k scoring proposals to keep after applying NMS in inference mode.
-            When RPN is run on multiple feature maps / levels (as in FPN) this number is per
+        nms_score_threshold_test: float. score threshold to use for NMS in
+            inference mode.
+        nms_iou_threshold_test: float. IOU threshold to use for NMS in inference
+            mode.
+        post_nms_topk_test: int. number of top k scoring proposals to keep after
+            applying NMS in inference mode. When RPN is run on multiple
+            feature maps / levels (as in FPN) this number is per
             feature map / level.
 
     Usage:
     ```python
     roi_generator = ROIGenerator("xyxy")
     boxes = {2: tf.random.normal([32, 5, 4])}
     scores = {2: tf.random.normal([32, 5])}
     rois, roi_scores = roi_generator(boxes, scores, training=True)
     ```
 
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         bounding_box_format,
         pre_nms_topk_train: int = 2000,
         nms_score_threshold_train: float = 0.0,
         nms_iou_threshold_train: float = 0.7,
@@ -102,20 +111,22 @@
         self,
         multi_level_boxes: Union[tf.Tensor, Mapping[int, tf.Tensor]],
         multi_level_scores: Union[tf.Tensor, Mapping[int, tf.Tensor]],
         training: Optional[bool] = None,
     ) -> Tuple[tf.Tensor, tf.Tensor]:
         """
         Args:
-          multi_level_boxes: float Tensor. A dictionary or single Tensor of boxes, one per level. shape is
-            [batch_size, num_boxes, 4] each level, in `bounding_box_format`.
-            The boxes from RPNs are usually encoded as deltas w.r.t to anchors,
-            they need to be decoded before passing in here.
-          multi_level_scores: float Tensor. A dictionary or single Tensor of scores, usually confidence scores,
-            one per level. shape is [batch_size, num_boxes] each level.
+          multi_level_boxes: float Tensor. A dictionary or single Tensor of
+            boxes, one per level. Shape is [batch_size, num_boxes, 4] each
+            level, in `bounding_box_format`. The boxes from RPNs are usually
+            encoded as deltas w.r.t to anchors, they need to be decoded before
+            passing in here.
+          multi_level_scores: float Tensor. A dictionary or single Tensor of
+            scores, typically confidence scores, one per level. Shape is
+            [batch_size, num_boxes] each level.
 
         Returns:
           rois: float Tensor of [batch_size, post_nms_topk, 4]
           roi_scores: float Tensor of [batch_size, post_nms_topk]
         """
 
         if training:
```

## keras_cv/layers/object_detection/roi_generator_test.py

```diff
@@ -18,108 +18,155 @@
 
 
 class ROIGeneratorTest(tf.test.TestCase):
     def test_single_tensor(self):
         roi_generator = ROIGenerator("xyxy", nms_iou_threshold_train=0.96)
         rpn_boxes = tf.constant(
             [
-                [[0, 0, 10, 10], [0.1, 0.1, 9.9, 9.9], [5, 5, 10, 10], [2, 2, 8, 8]],
+                [
+                    [0, 0, 10, 10],
+                    [0.1, 0.1, 9.9, 9.9],
+                    [5, 5, 10, 10],
+                    [2, 2, 8, 8],
+                ],
             ]
         )
         expected_rois = tf.gather(rpn_boxes, [[1, 3, 2]], batch_dims=1)
         expected_rois = tf.concat([expected_rois, tf.zeros([1, 1, 4])], axis=1)
         rpn_scores = tf.constant(
             [
                 [0.6, 0.9, 0.2, 0.3],
             ]
         )
         # selecting the 1st, then 3rd, then 2nd as they don't overlap
         # 0th box overlaps with 1st box
         expected_roi_scores = tf.gather(rpn_scores, [[1, 3, 2]], batch_dims=1)
-        expected_roi_scores = tf.concat([expected_roi_scores, tf.zeros([1, 1])], axis=1)
+        expected_roi_scores = tf.concat(
+            [expected_roi_scores, tf.zeros([1, 1])], axis=1
+        )
         rois, roi_scores = roi_generator(rpn_boxes, rpn_scores, training=True)
         self.assertAllClose(expected_rois, rois)
         self.assertAllClose(expected_roi_scores, roi_scores)
 
     def test_single_level_single_batch_roi_ignore_box(self):
         roi_generator = ROIGenerator("xyxy", nms_iou_threshold_train=0.96)
         rpn_boxes = tf.constant(
             [
-                [[0, 0, 10, 10], [0.1, 0.1, 9.9, 9.9], [5, 5, 10, 10], [2, 2, 8, 8]],
+                [
+                    [0, 0, 10, 10],
+                    [0.1, 0.1, 9.9, 9.9],
+                    [5, 5, 10, 10],
+                    [2, 2, 8, 8],
+                ],
             ]
         )
         expected_rois = tf.gather(rpn_boxes, [[1, 3, 2]], batch_dims=1)
         expected_rois = tf.concat([expected_rois, tf.zeros([1, 1, 4])], axis=1)
         rpn_boxes = {2: rpn_boxes}
         rpn_scores = tf.constant(
             [
                 [0.6, 0.9, 0.2, 0.3],
             ]
         )
         # selecting the 1st, then 3rd, then 2nd as they don't overlap
         # 0th box overlaps with 1st box
         expected_roi_scores = tf.gather(rpn_scores, [[1, 3, 2]], batch_dims=1)
-        expected_roi_scores = tf.concat([expected_roi_scores, tf.zeros([1, 1])], axis=1)
+        expected_roi_scores = tf.concat(
+            [expected_roi_scores, tf.zeros([1, 1])], axis=1
+        )
         rpn_scores = {2: rpn_scores}
         rois, roi_scores = roi_generator(rpn_boxes, rpn_scores, training=True)
         self.assertAllClose(expected_rois, rois)
         self.assertAllClose(expected_roi_scores, roi_scores)
 
     def test_single_level_single_batch_roi_all_box(self):
         # for iou between 1st and 2nd box is 0.9604, so setting to 0.97 to
         # such that NMS would treat them as different ROIs
         roi_generator = ROIGenerator("xyxy", nms_iou_threshold_train=0.97)
         rpn_boxes = tf.constant(
             [
-                [[0, 0, 10, 10], [0.1, 0.1, 9.9, 9.9], [5, 5, 10, 10], [2, 2, 8, 8]],
+                [
+                    [0, 0, 10, 10],
+                    [0.1, 0.1, 9.9, 9.9],
+                    [5, 5, 10, 10],
+                    [2, 2, 8, 8],
+                ],
             ]
         )
         expected_rois = tf.gather(rpn_boxes, [[1, 0, 3, 2]], batch_dims=1)
         rpn_boxes = {2: rpn_boxes}
         rpn_scores = tf.constant(
             [
                 [0.6, 0.9, 0.2, 0.3],
             ]
         )
         # selecting the 1st, then 0th, then 3rd, then 2nd as they don't overlap
-        expected_roi_scores = tf.gather(rpn_scores, [[1, 0, 3, 2]], batch_dims=1)
+        expected_roi_scores = tf.gather(
+            rpn_scores, [[1, 0, 3, 2]], batch_dims=1
+        )
         rpn_scores = {2: rpn_scores}
         rois, roi_scores = roi_generator(rpn_boxes, rpn_scores, training=True)
         self.assertAllClose(expected_rois, rois)
         self.assertAllClose(expected_roi_scores, roi_scores)
 
     def test_single_level_propose_rois(self):
         roi_generator = ROIGenerator("xyxy")
         rpn_boxes = tf.constant(
             [
-                [[0, 0, 10, 10], [0.1, 0.1, 9.9, 9.9], [5, 5, 10, 10], [2, 2, 8, 8]],
-                [[2, 2, 4, 4], [3, 3, 6, 6], [3.1, 3.1, 6.1, 6.1], [1, 1, 8, 8]],
+                [
+                    [0, 0, 10, 10],
+                    [0.1, 0.1, 9.9, 9.9],
+                    [5, 5, 10, 10],
+                    [2, 2, 8, 8],
+                ],
+                [
+                    [2, 2, 4, 4],
+                    [3, 3, 6, 6],
+                    [3.1, 3.1, 6.1, 6.1],
+                    [1, 1, 8, 8],
+                ],
             ]
         )
-        expected_rois = tf.gather(rpn_boxes, [[1, 3, 2], [1, 3, 0]], batch_dims=1)
+        expected_rois = tf.gather(
+            rpn_boxes, [[1, 3, 2], [1, 3, 0]], batch_dims=1
+        )
         expected_rois = tf.concat([expected_rois, tf.zeros([2, 1, 4])], axis=1)
         rpn_boxes = {2: rpn_boxes}
         rpn_scores = tf.constant([[0.6, 0.9, 0.2, 0.3], [0.1, 0.8, 0.3, 0.5]])
-        # 1st batch -- selecting the 1st, then 3rd, then 2nd as they don't overlap
-        # 2nd batch -- selecting the 1st, then 3rd, then 0th as they don't overlap
+        # 1st batch -- selecting the 1st, then 3rd, then 2nd as they don't
+        #   overlap
+        # 2nd batch -- selecting the 1st, then 3rd, then 0th as they don't
+        #   overlap
         expected_roi_scores = tf.gather(
             rpn_scores, [[1, 3, 2], [1, 3, 0]], batch_dims=1
         )
-        expected_roi_scores = tf.concat([expected_roi_scores, tf.zeros([2, 1])], axis=1)
+        expected_roi_scores = tf.concat(
+            [expected_roi_scores, tf.zeros([2, 1])], axis=1
+        )
         rpn_scores = {2: rpn_scores}
         rois, roi_scores = roi_generator(rpn_boxes, rpn_scores, training=True)
         self.assertAllClose(expected_rois, rois)
         self.assertAllClose(expected_roi_scores, roi_scores)
 
     def test_two_level_single_batch_propose_rois_ignore_box(self):
         roi_generator = ROIGenerator("xyxy")
         rpn_boxes = tf.constant(
             [
-                [[0, 0, 10, 10], [0.1, 0.1, 9.9, 9.9], [5, 5, 10, 10], [2, 2, 8, 8]],
-                [[2, 2, 4, 4], [3, 3, 6, 6], [3.1, 3.1, 6.1, 6.1], [1, 1, 8, 8]],
+                [
+                    [0, 0, 10, 10],
+                    [0.1, 0.1, 9.9, 9.9],
+                    [5, 5, 10, 10],
+                    [2, 2, 8, 8],
+                ],
+                [
+                    [2, 2, 4, 4],
+                    [3, 3, 6, 6],
+                    [3.1, 3.1, 6.1, 6.1],
+                    [1, 1, 8, 8],
+                ],
             ]
         )
         expected_rois = tf.constant(
             [
                 [
                     [0.1, 0.1, 9.9, 9.9],
                     [3, 3, 6, 6],
@@ -130,16 +177,18 @@
                     [0, 0, 0, 0],
                     [0, 0, 0, 0],
                 ]
             ]
         )
         rpn_boxes = {2: rpn_boxes[0:1], 3: rpn_boxes[1:2]}
         rpn_scores = tf.constant([[0.6, 0.9, 0.2, 0.3], [0.1, 0.8, 0.3, 0.5]])
-        # 1st batch -- selecting the 1st, then 3rd, then 2nd as they don't overlap
-        # 2nd batch -- selecting the 1st, then 3rd, then 0th as they don't overlap
+        # 1st batch -- selecting the 1st, then 3rd, then 2nd as they don't
+        #   overlap
+        # 2nd batch -- selecting the 1st, then 3rd, then 0th as they don't
+        #   overlap
         expected_roi_scores = [
             [
                 0.9,
                 0.8,
                 0.5,
                 0.3,
                 0.2,
@@ -153,16 +202,26 @@
         self.assertAllClose(expected_rois, rois)
         self.assertAllClose(expected_roi_scores, roi_scores)
 
     def test_two_level_single_batch_propose_rois_all_box(self):
         roi_generator = ROIGenerator("xyxy", nms_iou_threshold_train=0.99)
         rpn_boxes = tf.constant(
             [
-                [[0, 0, 10, 10], [0.1, 0.1, 9.9, 9.9], [5, 5, 10, 10], [2, 2, 8, 8]],
-                [[2, 2, 4, 4], [3, 3, 6, 6], [3.1, 3.1, 6.1, 6.1], [1, 1, 8, 8]],
+                [
+                    [0, 0, 10, 10],
+                    [0.1, 0.1, 9.9, 9.9],
+                    [5, 5, 10, 10],
+                    [2, 2, 8, 8],
+                ],
+                [
+                    [2, 2, 4, 4],
+                    [3, 3, 6, 6],
+                    [3.1, 3.1, 6.1, 6.1],
+                    [1, 1, 8, 8],
+                ],
             ]
         )
         expected_rois = tf.constant(
             [
                 [
                     [0.1, 0.1, 9.9, 9.9],
                     [3, 3, 6, 6],
@@ -173,16 +232,18 @@
                     [5, 5, 10, 10],
                     [2, 2, 4, 4],
                 ]
             ]
         )
         rpn_boxes = {2: rpn_boxes[0:1], 3: rpn_boxes[1:2]}
         rpn_scores = tf.constant([[0.6, 0.9, 0.2, 0.3], [0.1, 0.8, 0.3, 0.5]])
-        # 1st batch -- selecting the 1st, then 0th, then 3rd, then 2nd as they don't overlap
-        # 2nd batch -- selecting the 1st, then 3rd, then 2nd, then 0th as they don't overlap
+        # 1st batch -- selecting the 1st, then 0th, then 3rd, then 2nd as they
+        #   don't overlap
+        # 2nd batch -- selecting the 1st, then 3rd, then 2nd, then 0th as they
+        #   don't overlap
         expected_roi_scores = [
             [
                 0.9,
                 0.8,
                 0.6,
                 0.5,
                 0.3,
```

## keras_cv/layers/object_detection/roi_pool.py

```diff
@@ -9,78 +9,89 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class ROIPooler(tf.keras.layers.Layer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class ROIPooler(keras.layers.Layer):
     """
-    Pooling feature map of dynamic shape into region of interest (ROI) of fixed shape.
+    Pooling feature map of dynamic shape into region of interest (ROI) of fixed
+    shape.
 
     Mainly used in Region CNN (RCNN) networks. This works for a single-level
     input feature map.
 
-    This layer splits the feature map into [target_size[0], target_size[1]] areas,
-    and performs max pooling for each area. The area coordinates will be quantized.
+    This layer splits the feature map into [target_size[0], target_size[1]]
+    areas, and performs max pooling for each area. The area coordinates will be
+    quantized.
 
     Args:
         bounding_box_format: a case-insensitive string.
             For detailed information on the supported format, see the
             [KerasCV bounding box documentation](https://keras.io/api/keras_cv/bounding_box/formats/).
         target_size: List or Tuple of 2 integers of the pooled shape
-        image_shape: List of Tuple of 3 integers, or `TensorShape` of the input image shape.
+        image_shape: List of Tuple of 3 integers, or `TensorShape` of the input
+            image shape.
 
     Usage:
     ```python
     feature_map = tf.random.normal([2, 16, 16, 512])
     roi_pooler = ROIPooler(bounding_box_format="yxyx", target_size=[7, 7],
       image_shape=[224, 224, 3])
     rois = tf.constant([[[15., 30., 25., 45.]], [[22., 1., 30., 32.]]])
     pooled_feature_map = roi_pooler(feature_map, rois)
     ```
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         bounding_box_format,
         # TODO(consolidate size vs shape for KPL and here)
         target_size,
         image_shape,
         **kwargs,
     ):
         if not isinstance(target_size, (tuple, list)):
             raise ValueError(
-                f"Expected `target_size` to be tuple or list, got {type(target_size)}"
+                "Expected `target_size` to be tuple or list, got "
+                f"{type(target_size)}"
             )
         if len(target_size) != 2:
             raise ValueError(
                 f"Expected `target_size` to be size 2, got {len(target_size)}"
             )
-        if image_shape[0] is None or image_shape[1] is None or image_shape[2] is None:
+        if (
+            image_shape[0] is None
+            or image_shape[1] is None
+            or image_shape[2] is None
+        ):
             raise ValueError(
                 f"`image_shape` cannot have dynamic shape, got {image_shape}"
             )
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         self.target_height = target_size[0]
         self.target_width = target_size[1]
         self.image_shape = image_shape
         self.built = True
 
     def call(self, feature_map, rois):
         """
         Args:
-          feature_map: [batch_size, H, W, C] float Tensor, the feature map extracted from image.
-          rois: [batch_size, N, 4] float Tensor, the region of interests to be pooled.
+          feature_map: [batch_size, H, W, C] float Tensor, the feature map
+            extracted from image.
+          rois: [batch_size, N, 4] float Tensor, the region of interests to be
+            pooled.
         Returns:
           pooled_feature_map: [batch_size, N, target_size, C] float Tensor
         """
         # convert to relative format given feature map shape != image shape
         rois = bounding_box.convert_format(
             rois,
             source=self.bounding_box_format,
@@ -118,28 +129,33 @@
                 for j in range(self.target_width):
                     height_start = y_start + i * h_step
                     height_end = height_start + h_step
                     height_start = tf.cast(height_start, tf.int32)
                     height_end = tf.cast(height_end, tf.int32)
                     # if feature_map shape smaller than roi, h_step would be 0
                     # in this case the result will be feature_map[0, 0, ...]
-                    height_end = height_start + tf.maximum(1, height_end - height_start)
+                    height_end = height_start + tf.maximum(
+                        1, height_end - height_start
+                    )
                     width_start = x_start + j * w_step
                     width_end = width_start + w_step
                     width_start = tf.cast(width_start, tf.int32)
                     width_end = tf.cast(width_end, tf.int32)
-                    width_end = width_start + tf.maximum(1, width_end - width_start)
+                    width_end = width_start + tf.maximum(
+                        1, width_end - width_start
+                    )
                     # [h_step, w_step, C]
                     region = feature_map[
                         height_start:height_end, width_start:width_end, :
                     ]
                     # target_height * target_width * [C]
                     regions.append(tf.reduce_max(region, axis=[0, 1]))
             regions = tf.reshape(
-                tf.stack(regions), [self.target_height, self.target_width, channel]
+                tf.stack(regions),
+                [self.target_height, self.target_width, channel],
             )
             return regions
 
     def get_config(self):
         config = {
             "bounding_box_format": self.bounding_box_format,
             "target_size": [self.target_height, self.target_width],
```

## keras_cv/layers/object_detection/roi_pool_test.py

```diff
@@ -18,74 +18,98 @@
 
 
 class ROIPoolTest(tf.test.TestCase):
     def test_no_quantize(self):
         roi_pooler = ROIPooler(
             "rel_yxyx", target_size=[2, 2], image_shape=[224, 224, 3]
         )
-        feature_map = tf.expand_dims(tf.reshape(tf.range(64), [8, 8, 1]), axis=0)
+        feature_map = tf.expand_dims(
+            tf.reshape(tf.range(64), [8, 8, 1]), axis=0
+        )
         rois = tf.reshape(tf.constant([0.0, 0.0, 1.0, 1.0]), [1, 1, 4])
         pooled_feature_map = roi_pooler(feature_map, rois)
-        # the maximum value would be at bottom-right at each block, roi sharded into 2x2 blocks
+        # the maximum value would be at bottom-right at each block, roi sharded
+        # into 2x2 blocks
         # | 0, 1, 2, 3          | 4, 5, 6, 7            |
         # | 8, 9, 10, 11        | 12, 13, 14, 15        |
         # | 16, 17, 18, 19      | 20, 21, 22, 23        |
         # | 24, 25, 26, 27(max) | 28, 29, 30, 31(max)   |
         # --------------------------------------------
         # | 32, 33, 34, 35      | 36, 37, 38, 39        |
         # | 40, 41, 42, 43      | 44, 45, 46, 47        |
         # | 48, 49, 50, 51      | 52, 53, 54, 55        |
         # | 56, 57, 58, 59(max) | 60, 61, 62, 63(max)   |
         # --------------------------------------------
-        expected_feature_map = tf.reshape(tf.constant([27, 31, 59, 63]), [1, 2, 2, 1])
+        expected_feature_map = tf.reshape(
+            tf.constant([27, 31, 59, 63]), [1, 2, 2, 1]
+        )
         self.assertAllClose(expected_feature_map, pooled_feature_map)
 
     def test_roi_quantize_y(self):
-        roi_pooler = ROIPooler("yxyx", target_size=[2, 2], image_shape=[224, 224, 3])
-        feature_map = tf.expand_dims(tf.reshape(tf.range(64), [8, 8, 1]), axis=0)
+        roi_pooler = ROIPooler(
+            "yxyx", target_size=[2, 2], image_shape=[224, 224, 3]
+        )
+        feature_map = tf.expand_dims(
+            tf.reshape(tf.range(64), [8, 8, 1]), axis=0
+        )
         rois = tf.reshape(tf.constant([0.0, 0.0, 224, 220]), [1, 1, 4])
         pooled_feature_map = roi_pooler(feature_map, rois)
-        # the maximum value would be at bottom-right at each block, roi sharded into 2x2 blocks
+        # the maximum value would be at bottom-right at each block, roi sharded
+        # into 2x2 blocks
         # | 0, 1, 2             | 3, 4, 5, 6            | 7 (removed)
         # | 8, 9, 10            | 11, 12, 13, 14        | 15 (removed)
         # | 16, 17, 18          | 19, 20, 21, 22        | 23 (removed)
         # | 24, 25, 26(max)     | 27, 28, 29, 30(max)   | 31 (removed)
         # --------------------------------------------
         # | 32, 33, 34          | 35, 36, 37, 38        | 39 (removed)
         # | 40, 41, 42          | 43, 44, 45, 46        | 47 (removed)
         # | 48, 49, 50          | 51, 52, 53, 54        | 55 (removed)
         # | 56, 57, 58(max)     | 59, 60, 61, 62(max)   | 63 (removed)
         # --------------------------------------------
-        expected_feature_map = tf.reshape(tf.constant([26, 30, 58, 62]), [1, 2, 2, 1])
+        expected_feature_map = tf.reshape(
+            tf.constant([26, 30, 58, 62]), [1, 2, 2, 1]
+        )
         self.assertAllClose(expected_feature_map, pooled_feature_map)
 
     def test_roi_quantize_x(self):
-        roi_pooler = ROIPooler("yxyx", target_size=[2, 2], image_shape=[224, 224, 3])
-        feature_map = tf.expand_dims(tf.reshape(tf.range(64), [8, 8, 1]), axis=0)
+        roi_pooler = ROIPooler(
+            "yxyx", target_size=[2, 2], image_shape=[224, 224, 3]
+        )
+        feature_map = tf.expand_dims(
+            tf.reshape(tf.range(64), [8, 8, 1]), axis=0
+        )
         rois = tf.reshape(tf.constant([0.0, 0.0, 220, 224]), [1, 1, 4])
         pooled_feature_map = roi_pooler(feature_map, rois)
-        # the maximum value would be at bottom-right at each block, roi sharded into 2x2 blocks
+        # the maximum value would be at bottom-right at each block, roi sharded
+        # into 2x2 blocks
         # | 0, 1, 2, 3          | 4, 5, 6, 7            |
         # | 8, 9, 10, 11        | 12, 13, 14, 15        |
         # | 16, 17, 18, 19(max) | 20, 21, 22, 23(max)   |
         # --------------------------------------------
         # | 24, 25, 26, 27      | 28, 29, 30, 31        |
         # | 32, 33, 34, 35      | 36, 37, 38, 39        |
         # | 40, 41, 42, 43      | 44, 45, 46, 47        |
         # | 48, 49, 50, 51(max) | 52, 53, 54, 55(max)   |
         # --------------------------------------------
-        expected_feature_map = tf.reshape(tf.constant([19, 23, 51, 55]), [1, 2, 2, 1])
+        expected_feature_map = tf.reshape(
+            tf.constant([19, 23, 51, 55]), [1, 2, 2, 1]
+        )
         self.assertAllClose(expected_feature_map, pooled_feature_map)
 
     def test_roi_quantize_h(self):
-        roi_pooler = ROIPooler("yxyx", target_size=[3, 2], image_shape=[224, 224, 3])
-        feature_map = tf.expand_dims(tf.reshape(tf.range(64), [8, 8, 1]), axis=0)
+        roi_pooler = ROIPooler(
+            "yxyx", target_size=[3, 2], image_shape=[224, 224, 3]
+        )
+        feature_map = tf.expand_dims(
+            tf.reshape(tf.range(64), [8, 8, 1]), axis=0
+        )
         rois = tf.reshape(tf.constant([0.0, 0.0, 224, 224]), [1, 1, 4])
         pooled_feature_map = roi_pooler(feature_map, rois)
-        # the maximum value would be at bottom-right at each block, roi sharded into 3x2 blocks
+        # the maximum value would be at bottom-right at each block, roi sharded
+        # into 3x2 blocks
         # | 0, 1, 2, 3          | 4, 5, 6, 7            |
         # | 8, 9, 10, 11(max)   | 12, 13, 14, 15(max)   |
         # --------------------------------------------
         # | 16, 17, 18, 19      | 20, 21, 22, 23        |
         # | 24, 25, 26, 27      | 28, 29, 30, 31        |
         # | 32, 33, 34, 35(max) | 36, 37, 38, 39(max)   |
         # --------------------------------------------
@@ -95,19 +119,24 @@
         # --------------------------------------------
         expected_feature_map = tf.reshape(
             tf.constant([11, 15, 35, 39, 59, 63]), [1, 3, 2, 1]
         )
         self.assertAllClose(expected_feature_map, pooled_feature_map)
 
     def test_roi_quantize_w(self):
-        roi_pooler = ROIPooler("yxyx", target_size=[2, 3], image_shape=[224, 224, 3])
-        feature_map = tf.expand_dims(tf.reshape(tf.range(64), [8, 8, 1]), axis=0)
+        roi_pooler = ROIPooler(
+            "yxyx", target_size=[2, 3], image_shape=[224, 224, 3]
+        )
+        feature_map = tf.expand_dims(
+            tf.reshape(tf.range(64), [8, 8, 1]), axis=0
+        )
         rois = tf.reshape(tf.constant([0.0, 0.0, 224, 224]), [1, 1, 4])
         pooled_feature_map = roi_pooler(feature_map, rois)
-        # the maximum value would be at bottom-right at each block, roi sharded into 2x3 blocks
+        # the maximum value would be at bottom-right at each block, roi sharded
+        # into 2x3 blocks
         # | 0, 1        | 2, 3, 4           | 5, 6, 7           |
         # | 8, 9        | 10, 11, 12        | 13, 14, 15        |
         # | 16, 17      | 18, 19, 20        | 21, 22, 23        |
         # | 24, 25(max) | 26, 27, 28(max)   | 29, 30, 31(max)   |
         # --------------------------------------------
         # | 32, 33      | 34, 35, 36        | 37, 38, 39        |
         # | 40, 41      | 42, 43, 44        | 45, 46, 47        |
@@ -116,16 +145,20 @@
         # --------------------------------------------
         expected_feature_map = tf.reshape(
             tf.constant([25, 28, 31, 57, 60, 63]), [1, 2, 3, 1]
         )
         self.assertAllClose(expected_feature_map, pooled_feature_map)
 
     def test_roi_feature_map_height_smaller_than_roi(self):
-        roi_pooler = ROIPooler("yxyx", target_size=[6, 2], image_shape=[224, 224, 3])
-        feature_map = tf.expand_dims(tf.reshape(tf.range(16), [4, 4, 1]), axis=0)
+        roi_pooler = ROIPooler(
+            "yxyx", target_size=[6, 2], image_shape=[224, 224, 3]
+        )
+        feature_map = tf.expand_dims(
+            tf.reshape(tf.range(16), [4, 4, 1]), axis=0
+        )
         rois = tf.reshape(tf.constant([0.0, 0.0, 224, 224]), [1, 1, 4])
         pooled_feature_map = roi_pooler(feature_map, rois)
         # | 0, 1(max)   | 2, 3(max)     |
         # ------------------repeated----------------------
         # | 4, 5(max)   | 6, 7(max)     |
         # --------------------------------------------
         # | 8, 9(max)   | 10, 11(max)   |
@@ -133,33 +166,44 @@
         # | 12, 13(max) | 14, 15(max)   |
         expected_feature_map = tf.reshape(
             tf.constant([1, 3, 1, 3, 5, 7, 9, 11, 9, 11, 13, 15]), [1, 6, 2, 1]
         )
         self.assertAllClose(expected_feature_map, pooled_feature_map)
 
     def test_roi_feature_map_width_smaller_than_roi(self):
-        roi_pooler = ROIPooler("yxyx", target_size=[2, 6], image_shape=[224, 224, 3])
-        feature_map = tf.expand_dims(tf.reshape(tf.range(16), [4, 4, 1]), axis=0)
+        roi_pooler = ROIPooler(
+            "yxyx", target_size=[2, 6], image_shape=[224, 224, 3]
+        )
+        feature_map = tf.expand_dims(
+            tf.reshape(tf.range(16), [4, 4, 1]), axis=0
+        )
         rois = tf.reshape(tf.constant([0.0, 0.0, 224, 224]), [1, 1, 4])
         pooled_feature_map = roi_pooler(feature_map, rois)
         # | 0       | 1         | 2         | 3         |
         # | 4(max)  | 5(max)    | 6(max)    | 7(max)    |
         # --------------------------------------------
         # | 8       | 9         | 10        | 11        |
         # | 12(max) | 13(max)   | 14(max)   | 15(max)   |
         # --------------------------------------------
         expected_feature_map = tf.reshape(
-            tf.constant([4, 4, 5, 6, 6, 7, 12, 12, 13, 14, 14, 15]), [1, 2, 6, 1]
+            tf.constant([4, 4, 5, 6, 6, 7, 12, 12, 13, 14, 14, 15]),
+            [1, 2, 6, 1],
         )
         self.assertAllClose(expected_feature_map, pooled_feature_map)
 
     def test_roi_empty(self):
-        roi_pooler = ROIPooler("yxyx", target_size=[2, 2], image_shape=[224, 224, 3])
-        feature_map = tf.expand_dims(tf.reshape(tf.range(1, 65), [8, 8, 1]), axis=0)
+        roi_pooler = ROIPooler(
+            "yxyx", target_size=[2, 2], image_shape=[224, 224, 3]
+        )
+        feature_map = tf.expand_dims(
+            tf.reshape(tf.range(1, 65), [8, 8, 1]), axis=0
+        )
         rois = tf.reshape(tf.constant([0.0, 0.0, 0.0, 0.0]), [1, 1, 4])
         pooled_feature_map = roi_pooler(feature_map, rois)
         # all outputs should be top-left pixel
         self.assertAllClose(tf.ones([1, 2, 2, 1]), pooled_feature_map)
 
     def test_invalid_image_shape(self):
         with self.assertRaisesRegex(ValueError, "dynamic shape"):
-            _ = ROIPooler("rel_yxyx", target_size=[2, 2], image_shape=[None, 224, 3])
+            _ = ROIPooler(
+                "rel_yxyx", target_size=[2, 2], image_shape=[None, 224, 3]
+            )
```

## keras_cv/layers/object_detection/roi_sampler.py

```diff
@@ -9,56 +9,58 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 from keras_cv.bounding_box import iou
 from keras_cv.layers.object_detection import box_matcher
 from keras_cv.layers.object_detection import sampling
 from keras_cv.utils import target_gather
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class _ROISampler(tf.keras.layers.Layer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class _ROISampler(keras.layers.Layer):
     """
-    Sample ROIs for loss related calucation.
+    Sample ROIs for loss related calculation.
 
     With proposals (ROIs) and ground truth, it performs the following:
     1) compute IOU similarity matrix
     2) match each proposal to ground truth box based on IOU
     3) samples positive matches and negative matches and return
 
     `append_gt_boxes` augments proposals with ground truth boxes. This is
     useful in 2 stage detection networks during initialization where the
-    1st stage often cannot produce good proposals for 2nd stage. Setting it
-    to True will allow it to generate more reasonable proposals at the begining.
+    1st stage often cannot produce good proposals for 2nd stage. Setting it to
+    True will allow it to generate more reasonable proposals at the beginning.
 
-    `background_class` allow users to set the labels for background proposals. Default
-    is 0, where users need to manually shift the incoming `gt_classes` if its range is
-    [0, num_classes).
+    `background_class` allow users to set the labels for background proposals.
+    Default is 0, where users need to manually shift the incoming `gt_classes`
+    if its range is [0, num_classes).
 
     Args:
       bounding_box_format: The format of bounding boxes to generate. Refer
         [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
         for more details on supported bounding box formats.
-      roi_matcher: a `BoxMatcher` object that matches proposals
-        with ground truth boxes. the positive match must be 1 and negative match must be -1.
+      roi_matcher: a `BoxMatcher` object that matches proposals with ground
+        truth boxes. The positive match must be 1 and negative match must be -1.
         Such assumption is not being validated here.
-      positive_fraction: the positive ratio w.r.t `num_sampled_rois`. Defaults to 0.25.
-      background_class: the background class which is used to map returned the sampled
-        ground truth which is classified as background.
+      positive_fraction: the positive ratio w.r.t `num_sampled_rois`, defaults
+        to 0.25.
+      background_class: the background class which is used to map returned the
+        sampled ground truth which is classified as background.
       num_sampled_rois: the number of sampled proposals per image for
-        further (loss) calculation. Defaults to 256.
+        further (loss) calculation, defaults to 256.
       append_gt_boxes: boolean, whether gt_boxes will be appended to rois
-        before sample the rois. Defaults to True.
-    """
+        before sample the rois, defaults to True.
+    """  # noqa: E501
 
     def __init__(
         self,
         bounding_box_format: str,
         roi_matcher: box_matcher.BoxMatcher,
         positive_fraction: float = 0.25,
         background_class: int = 0,
@@ -71,16 +73,16 @@
         self.roi_matcher = roi_matcher
         self.positive_fraction = positive_fraction
         self.background_class = background_class
         self.num_sampled_rois = num_sampled_rois
         self.append_gt_boxes = append_gt_boxes
         self.built = True
         # for debugging.
-        self._positives = tf.keras.metrics.Mean()
-        self._negatives = tf.keras.metrics.Mean()
+        self._positives = keras.metrics.Mean()
+        self._negatives = keras.metrics.Mean()
 
     def call(
         self,
         rois: tf.Tensor,
         gt_boxes: tf.Tensor,
         gt_classes: tf.Tensor,
     ):
@@ -97,18 +99,21 @@
           sampled_class_weights: [batch_size, num_sampled_rois, 1]
         """
         if self.append_gt_boxes:
             # num_rois += num_gt
             rois = tf.concat([rois, gt_boxes], axis=1)
         num_rois = rois.get_shape().as_list()[1]
         if num_rois is None:
-            raise ValueError(f"`rois` must have static shape, got {rois.get_shape()}")
+            raise ValueError(
+                f"`rois` must have static shape, got {rois.get_shape()}"
+            )
         if num_rois < self.num_sampled_rois:
             raise ValueError(
-                f"num_rois must be less than `num_sampled_rois` ({self.num_sampled_rois}), got {num_rois}"
+                "num_rois must be less than `num_sampled_rois` "
+                f"({self.num_sampled_rois}), got {num_rois}"
             )
         rois = bounding_box.convert_format(
             rois, source=self.bounding_box_format, target="yxyx"
         )
         gt_boxes = bounding_box.convert_format(
             gt_boxes, source=self.bounding_box_format, target="yxyx"
         )
@@ -124,38 +129,46 @@
         self._positives.update_state(
             tf.reduce_sum(tf.cast(positive_matches, tf.float32), axis=-1)
         )
         self._negatives.update_state(
             tf.reduce_sum(tf.cast(negative_matches, tf.float32), axis=-1)
         )
         # [batch_size, num_rois, 1]
-        background_mask = tf.expand_dims(tf.logical_not(positive_matches), axis=-1)
+        background_mask = tf.expand_dims(
+            tf.logical_not(positive_matches), axis=-1
+        )
         # [batch_size, num_rois, 1]
-        matched_gt_classes = target_gather._target_gather(gt_classes, matched_gt_cols)
+        matched_gt_classes = target_gather._target_gather(
+            gt_classes, matched_gt_cols
+        )
         # also set all background matches to `background_class`
         matched_gt_classes = tf.where(
             background_mask,
             tf.cast(
                 self.background_class * tf.ones_like(matched_gt_classes),
                 gt_classes.dtype,
             ),
             matched_gt_classes,
         )
         # [batch_size, num_rois, 4]
-        matched_gt_boxes = target_gather._target_gather(gt_boxes, matched_gt_cols)
+        matched_gt_boxes = target_gather._target_gather(
+            gt_boxes, matched_gt_cols
+        )
         encoded_matched_gt_boxes = bounding_box._encode_box_to_deltas(
             anchors=rois,
             boxes=matched_gt_boxes,
             anchor_format="yxyx",
             box_format="yxyx",
             variance=[0.1, 0.1, 0.2, 0.2],
         )
         # also set all background matches to 0 coordinates
         encoded_matched_gt_boxes = tf.where(
-            background_mask, tf.zeros_like(matched_gt_boxes), encoded_matched_gt_boxes
+            background_mask,
+            tf.zeros_like(matched_gt_boxes),
+            encoded_matched_gt_boxes,
         )
         # [batch_size, num_rois]
         sampled_indicators = sampling.balanced_sample(
             positive_matches,
             negative_matches,
             self.num_sampled_rois,
             self.positive_fraction,
@@ -173,15 +186,16 @@
         # [batch_size, num_sampled_rois, 1]
         sampled_gt_classes = target_gather._target_gather(
             matched_gt_classes, sampled_indices
         )
         # [batch_size, num_sampled_rois, 1]
         # all negative samples will be ignored in regression
         sampled_box_weights = target_gather._target_gather(
-            tf.cast(positive_matches[..., tf.newaxis], gt_boxes.dtype), sampled_indices
+            tf.cast(positive_matches[..., tf.newaxis], gt_boxes.dtype),
+            sampled_indices,
         )
         # [batch_size, num_sampled_rois, 1]
         sampled_indicators = sampled_indicators[..., tf.newaxis]
         sampled_class_weights = tf.cast(sampled_indicators, gt_classes.dtype)
         return (
             sampled_rois,
             sampled_gt_boxes,
```

## keras_cv/layers/object_detection/roi_sampler_test.py

```diff
@@ -25,65 +25,78 @@
             bounding_box_format="xyxy",
             roi_matcher=box_matcher,
             positive_fraction=0.5,
             num_sampled_rois=2,
             append_gt_boxes=False,
         )
         rois = tf.constant(
-            [[0, 0, 5, 5], [2.5, 2.5, 7.5, 7.5], [5, 5, 10, 10], [7.5, 7.5, 12.5, 12.5]]
+            [
+                [0, 0, 5, 5],
+                [2.5, 2.5, 7.5, 7.5],
+                [5, 5, 10, 10],
+                [7.5, 7.5, 12.5, 12.5],
+            ]
         )
         rois = rois[tf.newaxis, ...]
         # the 3rd box will generate 0 IOUs and not sampled.
         gt_boxes = tf.constant(
             [[10, 10, 15, 15], [2.5, 2.5, 7.5, 7.5], [-1, -1, -1, -1]]
         )
         gt_boxes = gt_boxes[tf.newaxis, ...]
         gt_classes = tf.constant([[2, 10, -1]], dtype=tf.int32)
         gt_classes = gt_classes[..., tf.newaxis]
         _, sampled_gt_boxes, _, sampled_gt_classes, _ = roi_sampler(
             rois, gt_boxes, gt_classes
         )
         # given we only choose 1 positive sample, and `append_labesl` is False,
         # only the 2nd ROI is chosen.
-        expected_gt_boxes = tf.constant([[0.0, 0.0, 0, 0.0], [0.0, 0.0, 0, 0.0]])
+        expected_gt_boxes = tf.constant(
+            [[0.0, 0.0, 0, 0.0], [0.0, 0.0, 0, 0.0]]
+        )
         expected_gt_boxes = expected_gt_boxes[tf.newaxis, ...]
         # only the 2nd ROI is chosen, and the negative ROI is mapped to 0.
         expected_gt_classes = tf.constant([[10], [0]], dtype=tf.int32)
         expected_gt_classes = expected_gt_classes[tf.newaxis, ...]
         self.assertAllClose(
             tf.reduce_max(expected_gt_boxes), tf.reduce_max(sampled_gt_boxes)
         )
         self.assertAllClose(
-            tf.reduce_min(expected_gt_classes), tf.reduce_min(sampled_gt_classes)
+            tf.reduce_min(expected_gt_classes),
+            tf.reduce_min(sampled_gt_classes),
         )
 
     def test_roi_sampler_small_threshold(self):
         box_matcher = BoxMatcher(thresholds=[0.1], match_values=[-1, 1])
         roi_sampler = _ROISampler(
             bounding_box_format="xyxy",
             roi_matcher=box_matcher,
             positive_fraction=0.5,
             num_sampled_rois=2,
             append_gt_boxes=False,
         )
         rois = tf.constant(
-            [[0, 0, 5, 5], [2.5, 2.5, 7.5, 7.5], [5, 5, 10, 10], [7.5, 7.5, 12.5, 12.5]]
+            [
+                [0, 0, 5, 5],
+                [2.5, 2.5, 7.5, 7.5],
+                [5, 5, 10, 10],
+                [7.5, 7.5, 12.5, 12.5],
+            ]
         )
         rois = rois[tf.newaxis, ...]
         # the 3rd box will generate 0 IOUs and not sampled.
         gt_boxes = tf.constant(
             [[10, 10, 15, 15], [2.6, 2.6, 7.6, 7.6], [-1, -1, -1, -1]]
         )
         gt_boxes = gt_boxes[tf.newaxis, ...]
         gt_classes = tf.constant([[2, 10, -1]], dtype=tf.int32)
         gt_classes = gt_classes[..., tf.newaxis]
         sampled_rois, sampled_gt_boxes, _, sampled_gt_classes, _ = roi_sampler(
             rois, gt_boxes, gt_classes
         )
-        # given we only choose 1 positive sample, and `append_labesl` is False,
+        # given we only choose 1 positive sample, and `append_label` is False,
         # only the 2nd ROI is chosen. No negative samples exist given we
         # select positive_threshold to be 0.1. (the minimum IOU is 1/7)
         # given num_sampled_rois=2, it selects the 1st ROI as well.
         expected_rois = tf.constant([[5, 5, 10, 10], [0.0, 0.0, 5.0, 5.0]])
         expected_rois = expected_rois[tf.newaxis, ...]
         # all ROIs are matched to the 2nd gt box.
         # the boxes are encoded by dimensions, so the result is
@@ -96,30 +109,37 @@
         # only the 2nd ROI is chosen, and the negative ROI is mapped to 0.
         expected_gt_classes = tf.constant([[10], [10]], dtype=tf.int32)
         expected_gt_classes = expected_gt_classes[tf.newaxis, ...]
         self.assertAllClose(
             tf.reduce_max(expected_rois, 1), tf.reduce_max(sampled_rois, 1)
         )
         self.assertAllClose(
-            tf.reduce_max(expected_gt_boxes, 1), tf.reduce_max(sampled_gt_boxes, 1)
+            tf.reduce_max(expected_gt_boxes, 1),
+            tf.reduce_max(sampled_gt_boxes, 1),
         )
         self.assertAllClose(expected_gt_classes, sampled_gt_classes)
 
     def test_roi_sampler_large_threshold(self):
-        # the 2nd roi and 2nd gt box has IOU of 0.923, setting positive_threshold to 0.95 to ignore it
+        # the 2nd roi and 2nd gt box has IOU of 0.923, setting
+        # positive_threshold to 0.95 to ignore it.
         box_matcher = BoxMatcher(thresholds=[0.95], match_values=[-1, 1])
         roi_sampler = _ROISampler(
             bounding_box_format="xyxy",
             roi_matcher=box_matcher,
             positive_fraction=0.5,
             num_sampled_rois=2,
             append_gt_boxes=False,
         )
         rois = tf.constant(
-            [[0, 0, 5, 5], [2.5, 2.5, 7.5, 7.5], [5, 5, 10, 10], [7.5, 7.5, 12.5, 12.5]]
+            [
+                [0, 0, 5, 5],
+                [2.5, 2.5, 7.5, 7.5],
+                [5, 5, 10, 10],
+                [7.5, 7.5, 12.5, 12.5],
+            ]
         )
         rois = rois[tf.newaxis, ...]
         # the 3rd box will generate 0 IOUs and not sampled.
         gt_boxes = tf.constant(
             [[10, 10, 15, 15], [2.6, 2.6, 7.6, 7.6], [-1, -1, -1, -1]]
         )
         gt_boxes = gt_boxes[tf.newaxis, ...]
@@ -134,59 +154,72 @@
         expected_gt_classes = tf.constant([[0], [0]], dtype=tf.int32)
         expected_gt_classes = expected_gt_classes[tf.newaxis, ...]
         # self.assertAllClose(expected_rois, sampled_rois)
         self.assertAllClose(expected_gt_boxes, sampled_gt_boxes)
         self.assertAllClose(expected_gt_classes, sampled_gt_classes)
 
     def test_roi_sampler_large_threshold_custom_bg_class(self):
-        # the 2nd roi and 2nd gt box has IOU of 0.923, setting positive_threshold to 0.95 to ignore it
+        # the 2nd roi and 2nd gt box has IOU of 0.923, setting
+        # positive_threshold to 0.95 to ignore it.
         box_matcher = BoxMatcher(thresholds=[0.95], match_values=[-1, 1])
         roi_sampler = _ROISampler(
             bounding_box_format="xyxy",
             roi_matcher=box_matcher,
             positive_fraction=0.5,
             background_class=-1,
             num_sampled_rois=2,
             append_gt_boxes=False,
         )
         rois = tf.constant(
-            [[0, 0, 5, 5], [2.5, 2.5, 7.5, 7.5], [5, 5, 10, 10], [7.5, 7.5, 12.5, 12.5]]
+            [
+                [0, 0, 5, 5],
+                [2.5, 2.5, 7.5, 7.5],
+                [5, 5, 10, 10],
+                [7.5, 7.5, 12.5, 12.5],
+            ]
         )
         rois = rois[tf.newaxis, ...]
         # the 3rd box will generate 0 IOUs and not sampled.
         gt_boxes = tf.constant(
             [[10, 10, 15, 15], [2.6, 2.6, 7.6, 7.6], [-1, -1, -1, -1]]
         )
         gt_boxes = gt_boxes[tf.newaxis, ...]
         gt_classes = tf.constant([[2, 10, -1]], dtype=tf.int32)
         gt_classes = gt_classes[..., tf.newaxis]
         _, sampled_gt_boxes, _, sampled_gt_classes, _ = roi_sampler(
             rois, gt_boxes, gt_classes
         )
         # all ROIs are negative matches, so they are mapped to 0.
         expected_gt_boxes = tf.zeros([1, 2, 4], dtype=tf.float32)
-        # only the 2nd ROI is chosen, and the negative ROI is mapped to -1 from customization.
+        # only the 2nd ROI is chosen, and the negative ROI is mapped to -1 from
+        # customization.
         expected_gt_classes = tf.constant([[-1], [-1]], dtype=tf.int32)
         expected_gt_classes = expected_gt_classes[tf.newaxis, ...]
         # self.assertAllClose(expected_rois, sampled_rois)
         self.assertAllClose(expected_gt_boxes, sampled_gt_boxes)
         self.assertAllClose(expected_gt_classes, sampled_gt_classes)
 
     def test_roi_sampler_large_threshold_append_gt_boxes(self):
-        # the 2nd roi and 2nd gt box has IOU of 0.923, setting positive_threshold to 0.95 to ignore it
+        # the 2nd roi and 2nd gt box has IOU of 0.923, setting
+        # positive_threshold to 0.95 to ignore it.
         box_matcher = BoxMatcher(thresholds=[0.95], match_values=[-1, 1])
         roi_sampler = _ROISampler(
             bounding_box_format="xyxy",
             roi_matcher=box_matcher,
             positive_fraction=0.5,
             num_sampled_rois=2,
             append_gt_boxes=True,
         )
         rois = tf.constant(
-            [[0, 0, 5, 5], [2.5, 2.5, 7.5, 7.5], [5, 5, 10, 10], [7.5, 7.5, 12.5, 12.5]]
+            [
+                [0, 0, 5, 5],
+                [2.5, 2.5, 7.5, 7.5],
+                [5, 5, 10, 10],
+                [7.5, 7.5, 12.5, 12.5],
+            ]
         )
         rois = rois[tf.newaxis, ...]
         # the 3rd box will generate 0 IOUs and not sampled.
         gt_boxes = tf.constant(
             [[10, 10, 15, 15], [2.6, 2.6, 7.6, 7.6], [-1, -1, -1, -1]]
         )
         gt_boxes = gt_boxes[tf.newaxis, ...]
@@ -209,15 +242,20 @@
             bounding_box_format="xyxy",
             roi_matcher=box_matcher,
             positive_fraction=0.5,
             num_sampled_rois=200,
             append_gt_boxes=True,
         )
         rois = tf.constant(
-            [[0, 0, 5, 5], [2.5, 2.5, 7.5, 7.5], [5, 5, 10, 10], [7.5, 7.5, 12.5, 12.5]]
+            [
+                [0, 0, 5, 5],
+                [2.5, 2.5, 7.5, 7.5],
+                [5, 5, 10, 10],
+                [7.5, 7.5, 12.5, 12.5],
+            ]
         )
         rois = rois[tf.newaxis, ...]
         # the 3rd box will generate 0 IOUs and not sampled.
         gt_boxes = tf.constant(
             [[10, 10, 15, 15], [2.6, 2.6, 7.6, 7.6], [-1, -1, -1, -1]]
         )
         gt_boxes = gt_boxes[tf.newaxis, ...]
```

## keras_cv/layers/object_detection/rpn_label_encoder.py

```diff
@@ -11,54 +11,59 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Mapping
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 from keras_cv.bounding_box import iou
 from keras_cv.layers.object_detection import box_matcher
 from keras_cv.layers.object_detection import sampling
 from keras_cv.utils import target_gather
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class _RpnLabelEncoder(tf.keras.layers.Layer):
-    """Transforms the raw labels into training targets for region proposal network (RPN).
+@keras.utils.register_keras_serializable(package="keras_cv")
+class _RpnLabelEncoder(keras.layers.Layer):
+    """Transforms the raw labels into training targets for region proposal
+    network (RPN).
 
     # TODO(tanzhenyu): consider unifying with _ROISampler.
     This is different from _ROISampler for a couple of reasons:
-    1) This deals with unbatched input, dict of anchors and potentially ragged labels
-    2) This deals with ground truth boxes, while _ROISampler deals with padded ground truth
-       boxes with value -1 and padded ground truth classes with value -1
+    1) This deals with unbatched input, dict of anchors and potentially ragged
+       labels.
+    2) This deals with ground truth boxes, while _ROISampler deals with padded
+       ground truth boxes with value -1 and padded ground truth classes with
+       value -1.
     3) this returns positive class target as 1, while _ROISampler returns
        positive class target as-is. (All negative class target are 0)
        The final classification loss will use one hot and #num_fg_classes + 1
     4) this returns #num_anchors dense targets, while _ROISampler returns
        #num_sampled_rois dense targets.
     5) this returns all positive box targets, while _ROISampler still samples
        positive box targets, while all negative box targets are also ignored
        in regression loss.
 
     Args:
       anchor_format: The format of bounding boxes for anchors to generate. Refer
-        [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
-        for more details on supported bounding box formats.
-      ground_truth_box_format: The format of bounding boxes for ground truth boxes to generate.
-      positive_threshold: the float threshold to set an anchor to positive match to gt box.
-        values above it are positive matches.
-      negative_threshold: the float threshold to set an anchor to negative match to gt box.
-        values below it are negative matches.
-      samples_per_image: for each image, the number of positive and negative samples
-        to generate.
+        [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/) for more details on supported bounding box
+        formats.
+      ground_truth_box_format: The format of bounding boxes for ground truth
+        boxes to generate.
+      positive_threshold: the float threshold to set an anchor to positive match
+        to gt box. Values above it are positive matches.
+      negative_threshold: the float threshold to set an anchor to negative match
+        to gt box. Values below it are negative matches.
+      samples_per_image: for each image, the number of positive and negative
+        samples to generate.
       positive_fraction: the fraction of positive samples to the total samples.
 
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         anchor_format,
         ground_truth_box_format,
         positive_threshold,
         negative_threshold,
@@ -77,25 +82,25 @@
         self.box_matcher = box_matcher.BoxMatcher(
             thresholds=[negative_threshold, positive_threshold],
             match_values=[-1, -2, 1],
             force_match_for_each_col=False,
         )
         self.box_variance = box_variance
         self.built = True
-        self._positives = tf.keras.metrics.Mean(name="percent_boxes_matched")
+        self._positives = keras.metrics.Mean(name="percent_boxes_matched")
 
     def call(
         self,
         anchors_dict: Mapping[str, tf.Tensor],
         gt_boxes: tf.Tensor,
         gt_classes: tf.Tensor,
     ):
         """
         Args:
-          anchors: dict of [num_anchors, 4] or [batch_size, num_anchors, 4]
+          anchors_dict: dict of [num_anchors, 4] or [batch_size, num_anchors, 4]
             float Tensor for each level.
           gt_boxes: [num_gt, 4] or [batch_size, num_anchors] float Tensor.
           gt_classes: [num_gt, 1] float or integer Tensor.
         Returns:
           box_targets: dict of [num_anchors, 4] or  for each level.
           box_weights: dict of [num_anchors, 1] for each level.
           class_targets: dict of [num_anchors, 1] for each level.
@@ -109,60 +114,74 @@
         anchors = bounding_box.convert_format(
             anchors, source=self.anchor_format, target="yxyx"
         )
         gt_boxes = bounding_box.convert_format(
             gt_boxes, source=self.ground_truth_box_format, target="yxyx"
         )
         # [num_anchors, num_gt] or [batch_size, num_anchors, num_gt]
-        similarity_mat = iou.compute_iou(anchors, gt_boxes, bounding_box_format="yxyx")
+        similarity_mat = iou.compute_iou(
+            anchors, gt_boxes, bounding_box_format="yxyx"
+        )
         # [num_anchors] or [batch_size, num_anchors]
         matched_gt_indices, matched_vals = self.box_matcher(similarity_mat)
         # [num_anchors] or [batch_size, num_anchors]
         positive_matches = tf.math.equal(matched_vals, 1)
-        # currently SyncOnReadVariable does not support `assign_add` in cross-replica.
-        #        self._positives.update_state(
-        #            tf.reduce_sum(tf.cast(positive_matches, tf.float32), axis=-1)
-        #        )
+        # currently SyncOnReadVariable does not support `assign_add` in
+        # cross-replica.
+        #    self._positives.update_state(
+        #        tf.reduce_sum(tf.cast(positive_matches, tf.float32), axis=-1)
+        #    )
 
         negative_matches = tf.math.equal(matched_vals, -1)
         # [num_anchors, 4] or [batch_size, num_anchors, 4]
-        matched_gt_boxes = target_gather._target_gather(gt_boxes, matched_gt_indices)
-        # [num_anchors, 4] or [batch_size, num_anchors, 4], used as `y_true` for regression loss
+        matched_gt_boxes = target_gather._target_gather(
+            gt_boxes, matched_gt_indices
+        )
+        # [num_anchors, 4] or [batch_size, num_anchors, 4], used as `y_true` for
+        # regression loss
         encoded_box_targets = bounding_box._encode_box_to_deltas(
             anchors,
             matched_gt_boxes,
             anchor_format="yxyx",
             box_format="yxyx",
             variance=self.box_variance,
         )
         # [num_anchors, 1] or [batch_size, num_anchors, 1]
-        box_sample_weights = tf.cast(positive_matches[..., tf.newaxis], gt_boxes.dtype)
+        box_sample_weights = tf.cast(
+            positive_matches[..., tf.newaxis], gt_boxes.dtype
+        )
 
         # [num_anchors, 1] or [batch_size, num_anchors, 1]
         positive_mask = tf.expand_dims(positive_matches, axis=-1)
-        # set all negative and ignored matches to 0, and all positive matches to 1
-        # [num_anchors, 1] or [batch_size, num_anchors, 1]
+        # set all negative and ignored matches to 0, and all positive matches to
+        # 1 [num_anchors, 1] or [batch_size, num_anchors, 1]
         positive_classes = tf.ones_like(positive_mask, dtype=gt_classes.dtype)
         negative_classes = tf.zeros_like(positive_mask, dtype=gt_classes.dtype)
         # [num_anchors, 1] or [batch_size, num_anchors, 1]
-        class_targets = tf.where(positive_mask, positive_classes, negative_classes)
+        class_targets = tf.where(
+            positive_mask, positive_classes, negative_classes
+        )
         # [num_anchors] or [batch_size, num_anchors]
         sampled_indicators = sampling.balanced_sample(
             positive_matches,
             negative_matches,
             self.samples_per_image,
             self.positive_fraction,
         )
         # [num_anchors, 1] or [batch_size, num_anchors, 1]
         class_sample_weights = tf.cast(
             sampled_indicators[..., tf.newaxis], gt_classes.dtype
         )
         if pack:
-            encoded_box_targets = self.unpack_targets(encoded_box_targets, anchors_dict)
-            box_sample_weights = self.unpack_targets(box_sample_weights, anchors_dict)
+            encoded_box_targets = self.unpack_targets(
+                encoded_box_targets, anchors_dict
+            )
+            box_sample_weights = self.unpack_targets(
+                box_sample_weights, anchors_dict
+            )
             class_targets = self.unpack_targets(class_targets, anchors_dict)
             class_sample_weights = self.unpack_targets(
                 class_sample_weights, anchors_dict
             )
         return (
             encoded_box_targets,
             box_sample_weights,
@@ -170,22 +189,25 @@
             class_sample_weights,
         )
 
     def unpack_targets(self, targets, anchors_dict):
         target_shape = len(targets.get_shape().as_list())
         if target_shape != 2 and target_shape != 3:
             raise ValueError(
-                f"unpacking targets must be rank 2 or rank 3, got {target_shape}"
+                "unpacking targets must be rank 2 or rank 3, got "
+                f"{target_shape}"
             )
         unpacked_targets = {}
         count = 0
         for level, anchors in anchors_dict.items():
             num_anchors_lvl = anchors.get_shape().as_list()[0]
             if target_shape == 2:
-                unpacked_targets[level] = targets[count : count + num_anchors_lvl, ...]
+                unpacked_targets[level] = targets[
+                    count : count + num_anchors_lvl, ...
+                ]
             else:
                 unpacked_targets[level] = targets[
                     :, count : count + num_anchors_lvl, ...
                 ]
             count += num_anchors_lvl
         return unpacked_targets
```

## keras_cv/layers/object_detection/rpn_label_encoder_test.py

```diff
@@ -24,15 +24,20 @@
             ground_truth_box_format="xyxy",
             positive_threshold=0.7,
             negative_threshold=0.3,
             positive_fraction=0.5,
             samples_per_image=2,
         )
         rois = tf.constant(
-            [[0, 0, 5, 5], [2.5, 2.5, 7.5, 7.5], [5, 5, 10, 10], [7.5, 7.5, 12.5, 12.5]]
+            [
+                [0, 0, 5, 5],
+                [2.5, 2.5, 7.5, 7.5],
+                [5, 5, 10, 10],
+                [7.5, 7.5, 12.5, 12.5],
+            ]
         )
         # the 3rd box will generate 0 IOUs and not sampled.
         gt_boxes = tf.constant([[10, 10, 15, 15], [2.5, 2.5, 7.5, 7.5]])
         gt_classes = tf.constant([2, 10, -1], dtype=tf.int32)
         gt_classes = gt_classes[..., tf.newaxis]
         box_targets, box_weights, cls_targets, cls_weights = rpn_encoder(
             rois, gt_boxes, gt_classes
@@ -91,15 +96,20 @@
             ground_truth_box_format="xyxy",
             positive_threshold=0.7,
             negative_threshold=0.3,
             positive_fraction=0.5,
             samples_per_image=2,
         )
         rois = tf.constant(
-            [[0, 0, 5, 5], [2.5, 2.5, 7.5, 7.5], [5, 5, 10, 10], [7.5, 7.5, 12.5, 12.5]]
+            [
+                [0, 0, 5, 5],
+                [2.5, 2.5, 7.5, 7.5],
+                [5, 5, 10, 10],
+                [7.5, 7.5, 12.5, 12.5],
+            ]
         )
         # the 3rd box will generate 0 IOUs and not sampled.
         gt_boxes = tf.constant([[10, 10, 15, 15], [2.5, 2.5, 7.5, 7.5]])
         gt_classes = tf.constant([2, 10, -1], dtype=tf.int32)
         gt_classes = gt_classes[..., tf.newaxis]
         rois = rois[tf.newaxis, ...]
         gt_boxes = gt_boxes[tf.newaxis, ...]
```

## keras_cv/layers/object_detection/sampling.py

```diff
@@ -39,17 +39,19 @@
         integer Tensor, 1 for indicating the index is sampled, 0 for
         indicating the index is not sampled.
     """
 
     N = positive_matches.get_shape().as_list()[-1]
     if N < num_samples:
         raise ValueError(
-            f"passed in {positive_matches.shape} has less element than {num_samples}"
+            "passed in {positive_matches.shape} has less element than "
+            f"{num_samples}"
         )
-    # random_val = tf.random.uniform(tf.shape(positive_matches), minval=0., maxval=1.)
+    # random_val = tf.random.uniform(tf.shape(positive_matches), minval=0.,
+    # maxval=1.)
     zeros = tf.zeros_like(positive_matches, dtype=tf.float32)
     ones = tf.ones_like(positive_matches, dtype=tf.float32)
     ones_rand = ones + tf.random.uniform(ones.shape, minval=-0.2, maxval=0.2)
     halfs = 0.5 * tf.ones_like(positive_matches, dtype=tf.float32)
     halfs_rand = halfs + tf.random.uniform(halfs.shape, minval=-0.2, maxval=0.2)
     values = zeros
     values = tf.where(positive_matches, ones_rand, values)
@@ -64,15 +66,17 @@
     # setting all selected samples to zeros
     values = tf.where(selected_indicators, zeros, values)
     # setting all excessive positive matches to zeros as well
     values = tf.where(positive_matches, zeros, values)
     num_neg_samples = num_samples - num_pos_samples
     _, negative_indices = tf.math.top_k(values, k=num_neg_samples)
     selected_indices = tf.concat([positive_indices, negative_indices], axis=-1)
-    selected_indicators = tf.reduce_sum(tf.one_hot(selected_indices, depth=N), axis=-2)
+    selected_indicators = tf.reduce_sum(
+        tf.one_hot(selected_indices, depth=N), axis=-2
+    )
     selected_indicators = tf.minimum(
         selected_indicators, tf.ones_like(selected_indicators)
     )
     selected_indicators = tf.where(
         valid_matches, selected_indicators, tf.zeros_like(selected_indicators)
     )
     return selected_indicators
```

## keras_cv/layers/object_detection/sampling_test.py

```diff
@@ -16,15 +16,26 @@
 
 from keras_cv.layers.object_detection.sampling import balanced_sample
 
 
 class BalancedSamplingTest(tf.test.TestCase):
     def test_balanced_sampling(self):
         positive_matches = tf.constant(
-            [True, False, False, False, False, False, False, False, False, False]
+            [
+                True,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+            ]
         )
         negative_matches = tf.constant(
             [False, True, True, True, True, True, True, True, True, True]
         )
         num_samples = 5
         positive_fraction = 0.2
         res = balanced_sample(
@@ -32,52 +43,98 @@
         )
         # The 1st element must be selected, given it's the only one.
         self.assertAllClose(res[0], 1)
 
     def test_balanced_batched_sampling(self):
         positive_matches = tf.constant(
             [
-                [True, False, False, False, False, False, False, False, False, False],
-                [False, False, False, False, False, False, True, False, False, False],
+                [
+                    True,
+                    False,
+                    False,
+                    False,
+                    False,
+                    False,
+                    False,
+                    False,
+                    False,
+                    False,
+                ],
+                [
+                    False,
+                    False,
+                    False,
+                    False,
+                    False,
+                    False,
+                    True,
+                    False,
+                    False,
+                    False,
+                ],
             ]
         )
         negative_matches = tf.constant(
             [
                 [False, True, True, True, True, True, True, True, True, True],
                 [True, True, True, True, True, True, False, True, True, True],
             ]
         )
         num_samples = 5
         positive_fraction = 0.2
         res = balanced_sample(
             positive_matches, negative_matches, num_samples, positive_fraction
         )
-        # the 1st element from the 1st batch must be selected, given it's the only one
+        # the 1st element from the 1st batch must be selected, given it's the
+        # only one
         self.assertAllClose(res[0][0], 1)
-        # the 7th element from the 2nd batch must be selected, given it's the only one
+        # the 7th element from the 2nd batch must be selected, given it's the
+        # only one
         self.assertAllClose(res[1][6], 1)
 
     def test_balanced_sampling_over_positive_fraction(self):
         positive_matches = tf.constant(
-            [True, False, False, False, False, False, False, False, False, False]
+            [
+                True,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+            ]
         )
         negative_matches = tf.constant(
             [False, True, True, True, True, True, True, True, True, True]
         )
         num_samples = 5
         positive_fraction = 0.4
         res = balanced_sample(
             positive_matches, negative_matches, num_samples, positive_fraction
         )
         # only 1 positive sample exists, thus it is chosen
         self.assertAllClose(res[0], 1)
 
     def test_balanced_sampling_under_positive_fraction(self):
         positive_matches = tf.constant(
-            [True, False, False, False, False, False, False, False, False, False]
+            [
+                True,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+            ]
         )
         negative_matches = tf.constant(
             [False, True, True, True, True, True, True, True, True, True]
         )
         num_samples = 5
         positive_fraction = 0.1
         res = balanced_sample(
@@ -85,32 +142,57 @@
         )
         # no positive is chosen
         self.assertAllClose(res[0], 0)
         self.assertAllClose(tf.reduce_sum(res), 5)
 
     def test_balanced_sampling_over_num_samples(self):
         positive_matches = tf.constant(
-            [True, False, False, False, False, False, False, False, False, False]
+            [
+                True,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+            ]
         )
         negative_matches = tf.constant(
             [False, True, True, True, True, True, True, True, True, True]
         )
         # users want to get 20 samples, but only 10 are available
         num_samples = 20
         positive_fraction = 0.1
         with self.assertRaisesRegex(ValueError, "has less element"):
             _ = balanced_sample(
-                positive_matches, negative_matches, num_samples, positive_fraction
+                positive_matches,
+                negative_matches,
+                num_samples,
+                positive_fraction,
             )
 
     def test_balanced_sampling_no_positive(self):
         positive_matches = tf.constant(
-            [False, False, False, False, False, False, False, False, False, False]
+            [
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+                False,
+            ]
         )
-        # the rest are neither positive nor negative, but ignord matches
+        # the rest are neither positive nor negative, but ignored matches
         negative_matches = tf.constant(
             [False, False, True, False, False, True, False, False, True, False]
         )
         num_samples = 5
         positive_fraction = 0.5
         res = balanced_sample(
             positive_matches, negative_matches, num_samples, positive_fraction
```

## keras_cv/layers/object_detection_3d/heatmap_decoder.py

```diff
@@ -13,47 +13,49 @@
 # limitations under the License.
 
 from typing import Sequence
 from typing import Tuple
 
 import numpy as np
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.object_detection_3d import voxel_utils
 
 
 def decode_bin_heading(predictions: tf.Tensor, num_bin: int) -> tf.Tensor:
     """Decode bin heading.
 
     Computes the box heading (orientation) by decoding the bin predictions. The
     predictions should contain bin classification scores (first num_bin scores)
     and corresponding bin residuals (the following num_bin scores).
 
     Args:
-      predictions: Prediction scores tensor with size [N, num_bin*2] predictions =
-        [:, bin_1, bin_2, ..., bin_k, res_1, res_2, ..., res_k], where k is the
-        number of bins and N is the number of boxes
+      predictions: Prediction scores tensor with size [N, num_bin*2]
+        predictions = [:, bin_1, bin_2, ..., bin_k, res_1, res_2, ..., res_k],
+        where k is the number of bins and N is the number of boxes.
       num_bin: A constant showing the number of bins used in heading bin loss.
 
     Returns:
-      heading: Decoded heading tensor with size [N] in which heading values are in
-        the [-pi, pi] range.
+      heading: Decoded heading tensor with size [N] in which heading values are
+        in the [-pi, pi] range.
 
     Raises:
       ValueError: If the rank of `predictions` is not 2 or `predictions` tensor
         does not more than the expected number of dimensions.
     """
     with tf.name_scope("decode_bin_heading"):
         if len(predictions.shape) != 2:
             raise ValueError(
-                f"The rank of the prediction tensor is expected to be 2. Instead "
-                f"it is : {len(predictions.shape)}."
+                "The rank of the prediction tensor is expected to be 2. "
+                f"Instead it is : {len(predictions.shape)}."
             )
 
-        # Get the index of the bin with the maximum score to build a tensor of [N].
+        # Get the index of the bin with the maximum score to build a tensor of
+        # [N].
         bin_idx = tf.math.argmax(
             predictions[:, 0:num_bin], axis=-1, output_type=tf.int32
         )
         bin_idx_float = tf.cast(bin_idx, dtype=predictions.dtype)
         residual_norm = tf.gather(
             predictions[:, num_bin : num_bin * 2],
             bin_idx[:, tf.newaxis],
@@ -62,15 +64,16 @@
         )[:, 0]
 
         # Divide 2pi into equal sized bins to compute the angle per class/bin.
         angle_per_class = (2 * np.pi) / num_bin
         residual_angle = residual_norm * (angle_per_class / 2)
 
         # bin_center is computed using the bin_idx and angle_per class,
-        # (e.g., 0, 30, 60, 90, 120, ..., 270, 300, 330). Then residual is added.
+        # (e.g., 0, 30, 60, 90, 120, ..., 270, 300, 330). Then residual is
+        # added.
         heading = tf.math.floormod(
             bin_idx_float * angle_per_class + residual_angle, 2 * np.pi
         )
         heading_mask = heading > np.pi
         heading = tf.where(heading_mask, heading - 2 * np.pi, heading)
     return heading
 
@@ -92,23 +95,23 @@
         lwh = size_res_norm * list(anchor_size) + list(anchor_size)
 
         loc = tf.stack(delta, axis=-1)
         box = tf.concat([loc, lwh, heading[:, tf.newaxis]], axis=-1)
         return box
 
 
-class HeatmapDecoder(tf.keras.layers.Layer):
-    """A Keras layer that decodes predictions of an 3d object detection model.
+class HeatmapDecoder(keras.layers.Layer):
+    """A Keras layer that decodes predictions of a 3d object detection model.
 
     Arg:
-      class_id: the integer index for a parcitular class.
+      class_id: the integer index for a particular class.
       num_head_bin: number of bin classes divided by [-2pi, 2pi].
       anchor_size: the size of anchor at each xyz dimension.
       max_pool_size: the 2d pooling size for heatmap.
-      max_num_box: top number of boxes selectd from heatmap.
+      max_num_box: top number of boxes select from heatmap.
       heatmap_threshold: the threshold to set a heatmap as positive.
       voxel_size: the x, y, z dimension of each voxel.
       spatial_size: the x, y, z boundary of voxels.
     """
 
     def __init__(
         self,
@@ -129,26 +132,30 @@
         self.max_pool_size = max_pool_size
         self.max_num_box = max_num_box
         self.heatmap_threshold = heatmap_threshold
         self.voxel_size = voxel_size
         self.spatial_size = spatial_size
         self.built = True
 
-    def call(self, prediction: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
+    def call(
+        self, prediction: tf.Tensor
+    ) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
         """Accepts raw predictions, and returns decoded boxes.
 
         Args:
             prediction: float Tensor.
         """
         heatmap = tf.nn.softmax(prediction[..., :2])[..., 1:2]
         heatmap_pool = tf.nn.max_pool2d(heatmap, self.max_pool_size, 1, "SAME")
         heatmap_mask = heatmap > self.heatmap_threshold
         heatmap_local_maxima_mask = tf.math.equal(heatmap, heatmap_pool)
         # [B, H, W, 1]
-        heatmap_mask = tf.math.logical_and(heatmap_mask, heatmap_local_maxima_mask)
+        heatmap_mask = tf.math.logical_and(
+            heatmap_mask, heatmap_local_maxima_mask
+        )
         # [B, H, W, 1]
         heatmap = tf.where(heatmap_mask, heatmap, 0)
         # [B, H, W]
         heatmap = tf.squeeze(heatmap, axis=-1)
 
         b, h, w = voxel_utils.combined_static_and_dynamic_shape(heatmap)
         heatmap = tf.reshape(heatmap, [b, h * w])
@@ -162,15 +169,17 @@
         # [B, max_num_box, ?]
         box_prediction = tf.gather(box_prediction, top_index, batch_dims=1)
         # [B, max_num_box]
         box_score = tf.gather(heatmap, top_index, batch_dims=1)
         box_class = tf.ones_like(box_score, dtype=tf.int32) * self.class_id
         # [B*max_num_box, ?]
         f = box_prediction.get_shape().as_list()[-1]
-        box_prediction_reshape = tf.reshape(box_prediction, [b * self.max_num_box, f])
+        box_prediction_reshape = tf.reshape(
+            box_prediction, [b * self.max_num_box, f]
+        )
         # [B*max_num_box, 7]
         box_decoded = decode_bin_box(
             box_prediction_reshape, self.num_head_bin, self.anchor_size
         )
         # [B, max_num_box, 7]
         box_decoded = tf.reshape(box_decoded, [b, self.max_num_box, 7])
         global_xyz = tf.zeros([b, 3], dtype=box_decoded.dtype)
@@ -181,15 +190,17 @@
         ref_xyz = tf.squeeze(ref_xyz, axis=-2)
         f = ref_xyz.get_shape().as_list()[-1]
         ref_xyz = tf.reshape(ref_xyz, [b, h * w, f])
         # [B, max_num_box, 3]
         ref_xyz = tf.gather(ref_xyz, top_index, batch_dims=1)
 
         box_decoded_cxyz = ref_xyz + box_decoded[:, :, :3]
-        box_decoded = tf.concat([box_decoded_cxyz, box_decoded[:, :, 3:]], axis=-1)
+        box_decoded = tf.concat(
+            [box_decoded_cxyz, box_decoded[:, :, 3:]], axis=-1
+        )
         return box_decoded, box_class, box_score
 
     def get_config(self):
         config = {
             "class_id": self.class_id,
             "num_head_bin": self.num_head_bin,
             "anchor_size": self.anchor_size,
```

## keras_cv/layers/object_detection_3d/voxel_utils.py

```diff
@@ -37,30 +37,35 @@
 
     Returns:
       [B, H, W, Z, 3] offset locations for each feature map pixel in global
         coordinate.
     """
     voxel_spatial_size = compute_voxel_spatial_size(spatial_size, voxel_size)
     voxel_coord_meshgrid = np.mgrid[
-        0 : voxel_spatial_size[0], 0 : voxel_spatial_size[1], 0 : voxel_spatial_size[2]
+        0 : voxel_spatial_size[0],
+        0 : voxel_spatial_size[1],
+        0 : voxel_spatial_size[2],
     ]
     voxel_coord = np.concatenate(voxel_coord_meshgrid[..., np.newaxis], axis=-1)
     # [H, W, Z, 3]
     voxel_coord = tf.constant(voxel_coord, dtype=global_xyz.dtype)
     # [3]
     voxel_origin = tf.cast(
         compute_voxel_origin(spatial_size, voxel_size),
         dtype=global_xyz.dtype,
     )
     # [H, W, Z, 3]
     voxel_coord = voxel_coord + voxel_origin
     # [H, W, Z, 3]
     ref = voxel_coord_to_point(voxel_coord, voxel_size, dtype=global_xyz.dtype)
     # [1, H, W, Z, 3] + [B, 1, 1, 1, 3] -> [B, H, W, Z, 3]
-    ref = ref[tf.newaxis, ...] + global_xyz[:, tf.newaxis, tf.newaxis, tf.newaxis, :]
+    ref = (
+        ref[tf.newaxis, ...]
+        + global_xyz[:, tf.newaxis, tf.newaxis, tf.newaxis, :]
+    )
     return ref
 
 
 def compute_voxel_spatial_size(
     spatial_size: Sequence[float], voxel_size: Sequence[float]
 ) -> List[int]:
     """Computes how many voxels in each dimension are needed.
@@ -77,15 +82,17 @@
     voxel_spatial_size_float = [
         spatial_size[2 * i + 1] - spatial_size[2 * i] for i in range(dim)
     ]
     # voxel_dim_x / x_range
     voxel_spatial_size_float = [
         i / j for i, j in zip(voxel_spatial_size_float, voxel_size)
     ]
-    voxel_spatial_size_int = [math.ceil(v - EPSILON) for v in voxel_spatial_size_float]
+    voxel_spatial_size_int = [
+        math.ceil(v - EPSILON) for v in voxel_spatial_size_float
+    ]
 
     return voxel_spatial_size_int
 
 
 def compute_voxel_origin(
     spatial_size: Sequence[float],
     voxel_size: Sequence[float],
@@ -121,18 +128,20 @@
       voxel_size: voxel size.
       dtype: the output dtype.
 
     Returns:
       voxelized coordinates.
     """
     with tf.name_scope("point_to_voxel_coord"):
-        point_voxelized = point_xyz / tf.constant(voxel_size, dtype=point_xyz.dtype)
+        point_voxelized = point_xyz / tf.constant(
+            voxel_size, dtype=point_xyz.dtype
+        )
         assert dtype.is_integer or dtype.is_floating, f"{dtype}"
-        # Note: tf.round casts float to the nearest integer. If the float is 0.5, it
-        # casts it to the nearest even integer.
+        # Note: tf.round casts float to the nearest integer. If the float is
+        # 0.5, it casts it to the nearest even integer.
         point_voxelized_round = tf.math.round(point_voxelized)
         if dtype.is_floating:
             assert dtype == point_xyz.dtype, f"{dtype}"
             return point_voxelized_round
         return tf.cast(point_voxelized_round, dtype=dtype)
 
 
@@ -194,15 +203,17 @@
     R * new_loc + L = 0 = > new_loc = -R'*L
     Args:
       rot: [..., 3, 3] rotation matrix.
       loc: [..., 3] location matrix.
     Returns:
       [..., 3] new location matrix.
     """
-    new_loc = -1.0 * tf.linalg.matmul(rot, loc[..., tf.newaxis], transpose_a=True)
+    new_loc = -1.0 * tf.linalg.matmul(
+        rot, loc[..., tf.newaxis], transpose_a=True
+    )
     return tf.squeeze(new_loc, axis=-1)
 
 
 def shape_int_compatible(t: tf.Tensor) -> tf.TensorShape:
     """int32 and int64 compatible tf shape implementation."""
     # tf.shape int32/int64 requires input and output to be on host.
     dtype = t.dtype
@@ -210,15 +221,17 @@
         t = tf.bitcast(t, tf.float32)
     if dtype == tf.int64:
         t = tf.bitcast(t, tf.float64)
 
     return tf.shape(t)
 
 
-def combined_static_and_dynamic_shape(tensor: tf.Tensor) -> List[Union[tf.Tensor, int]]:
+def combined_static_and_dynamic_shape(
+    tensor: tf.Tensor,
+) -> List[Union[tf.Tensor, int]]:
     """Returns a list containing static and dynamic values for the dimensions.
 
     Returns a list of static and dynamic values for shape dimensions. This is
     useful to preserve static shapes when available in reshape operation.
 
     Args:
       tensor: A tensor of any type.
@@ -248,15 +261,16 @@
         ) % (tensor.shape.ndims, expected_rank)
     return tensor
 
 
 def _pad_or_trim_to(x, shape, pad_val=0, pad_after_contents=True):
     """Pad and slice x to the given shape.
 
-    This is branched from Lingvo https://github.com/tensorflow/lingvo/blob/master/lingvo/core/py_utils.py.
+    This is branched from Lingvo
+    https://github.com/tensorflow/lingvo/blob/master/lingvo/core/py_utils.py.
 
     Internal usages for keras_cv libraries only.
 
     Args:
       x: A tensor.
       shape: The shape of the returned tensor.
       pad_val: An int or float used to pad x.
@@ -268,15 +282,17 @@
     Raises:
       ValueError: if shape is a tf.TensorShape and not fully defined.
     """
     if isinstance(shape, (list, tuple)):
         expected_rank = len(shape)
     elif isinstance(shape, tf.TensorShape):
         if not shape.is_fully_defined():
-            raise ValueError("shape %s padding %s must be fully defined." % (shape, x))
+            raise ValueError(
+                "shape %s padding %s must be fully defined." % (shape, x)
+            )
         expected_rank = shape.rank
     else:
         shape = _has_rank(shape, 1)
         expected_rank = tf.size(shape)
     x = _has_rank(x, expected_rank)
 
     pad = shape - tf.minimum(tf.shape(x), shape)
```

## keras_cv/layers/object_detection_3d/voxel_utils_test.py

```diff
@@ -14,26 +14,30 @@
 
 import tensorflow as tf
 
 from keras_cv.layers.object_detection_3d import voxel_utils
 
 
 class PadOrTrimToTest(tf.test.TestCase):
-    """Tests for pad_or_trim_to, branched from https://github.com/tensorflow/lingvo/blob/master/lingvo/core/py_utils_test.py."""
+    """Tests for pad_or_trim_to, branched from
+    https://github.com/tensorflow/lingvo/blob/master/lingvo/core/py_utils_test.py.
+    """
 
     def test_2D_constant_shape_pad(self):
         x = tf.random.normal(shape=(3, 3), seed=123456)
         shape = [4, 6]
         padded_x_right = voxel_utils._pad_or_trim_to(x, shape, pad_val=0)
         padded_x_left = voxel_utils._pad_or_trim_to(
             x, shape, pad_val=0, pad_after_contents=False
         )
         self.assertEqual(padded_x_right.shape.as_list(), [4, 6])
         self.assertEqual(padded_x_left.shape.as_list(), [4, 6])
-        real_x_right, real_x_left = self.evaluate([padded_x_right, padded_x_left])
+        real_x_right, real_x_left = self.evaluate(
+            [padded_x_right, padded_x_left]
+        )
         expected_x_right = [
             [0.38615, 2.975221, -0.852826, 0.0, 0.0, 0.0],
             [-0.571142, -0.432439, 0.413158, 0.0, 0.0, 0.0],
             [0.255314, -0.985647, 1.461641, 0.0, 0.0, 0.0],
             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
         ]
         self.assertAllClose(expected_x_right, real_x_right)
@@ -50,12 +54,14 @@
         shape = [1, 3]
         trimmed_x_right = voxel_utils._pad_or_trim_to(x, shape, pad_val=0)
         trimmed_x_left = voxel_utils._pad_or_trim_to(
             x, shape, pad_val=0, pad_after_contents=False
         )
         self.assertEqual(trimmed_x_right.shape.as_list(), [1, 3])
         self.assertEqual(trimmed_x_left.shape.as_list(), [1, 3])
-        real_x_right, real_x_left = self.evaluate([trimmed_x_right, trimmed_x_left])
+        real_x_right, real_x_left = self.evaluate(
+            [trimmed_x_right, trimmed_x_left]
+        )
         expected_x_right = [[0.38615, 2.975221, -0.852826]]
         self.assertAllClose(expected_x_right, real_x_right)
         expected_x_left = [[0.255314, -0.985647, 1.461641]]
         self.assertAllClose(expected_x_left, real_x_left)
```

## keras_cv/layers/object_detection_3d/voxelization.py

```diff
@@ -13,14 +13,15 @@
 # limitations under the License.
 
 from typing import Sequence
 from typing import Tuple
 
 import numpy as np
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.object_detection_3d import voxel_utils
 
 EPSILON = 1e-4
 VOXEL_FEATURE_MIN = -1000
 
 
@@ -50,20 +51,22 @@
     )
     # [B, N]
     point_voxel_id = tf.math.reduce_sum(point_voxel_xyz_multiplied, axis=-1)
 
     if batch_size == 1:
         return point_voxel_id
 
-    batch_multiplier = tf.range(batch_size, dtype=tf.int32) * voxel_spatial_size_prod[0]
+    batch_multiplier = (
+        tf.range(batch_size, dtype=tf.int32) * voxel_spatial_size_prod[0]
+    )
     batch_multiplier = batch_multiplier[:, tf.newaxis]
     return point_voxel_id + batch_multiplier
 
 
-class PointToVoxel(tf.keras.layers.Layer):
+class PointToVoxel(keras.layers.Layer):
     """Voxelization layer."""
 
     def __init__(
         self,
         voxel_size: Sequence[float],
         spatial_size: Sequence[float],
         **kwargs,
@@ -102,16 +105,16 @@
 
         Args:
           point_xyz: [B, N, dim] point xyz in global coordinate relative to sdc.
           point_mask: [B, N] valid point mask.
 
         Returns:
           point_voxel_feature: [B, N, dim] voxel feature (delta_{x,y,z}).
-          point_voxel_id: [B, N] voxel ID of each point. Invalid voxels have Id's
-            set to 0.
+          point_voxel_id: [B, N] voxel ID of each point. Invalid voxels have
+            Id's set to 0.
           point_voxel_mask: [B, N] validpoint voxel boolean mask.
         """
         # [B, N, dim]
         # convert from point coordinate to voxel index
         point_voxel_xyz_float = voxel_utils.point_to_voxel_coord(
             point_xyz, self._voxel_size, dtype=point_xyz.dtype
         )
@@ -127,73 +130,81 @@
         # get xmin, ymin, zmin
         voxel_origin = voxel_utils.compute_voxel_origin(
             self._spatial_size, self._voxel_size
         )
 
         # [B, N, dim]
         # convert point voxel to positive voxel index
-        point_voxel_xyz = point_voxel_xyz_int - voxel_origin[tf.newaxis, tf.newaxis, :]
+        point_voxel_xyz = (
+            point_voxel_xyz_int - voxel_origin[tf.newaxis, tf.newaxis, :]
+        )
 
         # [B, N]
-        # remove points outside of the voxel boundary
+        # remove points outside the voxel boundary
         point_voxel_mask = tf.logical_and(
             point_voxel_xyz >= 0,
             point_voxel_xyz
-            < tf.constant(self._voxel_spatial_size, dtype=point_voxel_xyz.dtype),
+            < tf.constant(
+                self._voxel_spatial_size, dtype=point_voxel_xyz.dtype
+            ),
         )
         point_voxel_mask = tf.math.reduce_all(point_voxel_mask, axis=-1)
         point_voxel_mask = tf.logical_and(point_voxel_mask, point_mask)
 
         # [B, N]
         point_voxel_mask_int = tf.cast(point_voxel_mask, dtype=tf.int32)
-        # [B, N] for voxel_id, int constant for num_voxels, in the range of [0, B * num_voxels]
+        # [B, N] for voxel_id, int constant for num_voxels, in the range of
+        # [0, B * num_voxels]
         point_voxel_id = compute_point_voxel_id(
             point_voxel_xyz, self._voxel_spatial_size
         )
         # [B, N]
         point_voxel_id = point_voxel_id * point_voxel_mask_int
 
         return point_voxel_feature, point_voxel_id, point_voxel_mask
 
 
-class DynamicVoxelization(tf.keras.layers.Layer):
+class DynamicVoxelization(keras.layers.Layer):
     """Dynamic voxelization and pool layer.
 
     This layer assigns and pools points into voxels,
     then it concatenates with point features and feed into a neural network,
     and max pools all point features inside each voxel.
 
     Args:
-      point_net: a keras Layer that project point feature into another dimension.
+      point_net: a keras Layer that project point feature into another
+        dimension.
       voxel_size: the x, y, z dimension of each voxel.
       spatial_size: the x, y, z boundary of voxels
 
     Returns:
       voxelized feature, a float Tensor.
 
     """
 
     def __init__(
         self,
-        point_net: tf.keras.layers.Layer,
+        point_net: keras.layers.Layer,
         voxel_size: Sequence[float],
         spatial_size: Sequence[float],
         **kwargs,
     ):
         super().__init__(**kwargs)
         self._point_net = point_net
         self._voxelization_layer = PointToVoxel(
             voxel_size=voxel_size, spatial_size=spatial_size
         )
         self._voxel_size = voxel_size
         self._spatial_size = spatial_size
         self._voxel_spatial_size = voxel_utils.compute_voxel_spatial_size(
             spatial_size, self._voxel_size
         )
-        self._voxel_spatial_size_volume = np.prod(self._voxel_spatial_size).item()
+        self._voxel_spatial_size_volume = np.prod(
+            self._voxel_spatial_size
+        ).item()
 
     def call(
         self,
         point_xyz: tf.Tensor,
         point_feature: tf.Tensor,
         point_mask: tf.Tensor,
         training: bool,
@@ -215,17 +226,20 @@
             features. If z_max is 1, z-dim is squeezed.
         """
         (
             point_voxel_feature,
             point_voxel_id,
             point_voxel_mask,
         ) = self._voxelization_layer(point_xyz=point_xyz, point_mask=point_mask)
-        # TODO(tanzhenyu): move compute_point_voxel_id to here, so PointToVoxel layer is more generic.
+        # TODO(tanzhenyu): move compute_point_voxel_id to here, so PointToVoxel
+        #  layer is more generic.
         point_feature = tf.concat([point_feature, point_voxel_feature], axis=-1)
-        batch_size = point_feature.shape.as_list()[0] or tf.shape(point_feature)[0]
+        batch_size = (
+            point_feature.shape.as_list()[0] or tf.shape(point_feature)[0]
+        )
         # [B, N, 1]
         point_mask_float = tf.cast(point_voxel_mask, point_feature.dtype)[
             ..., tf.newaxis
         ]
         # [B, N, dim]
         point_feature = point_feature * point_mask_float
         point_feature = self._point_net(
@@ -234,15 +248,17 @@
         # [B, N, new_dim]
         point_feature = point_feature * point_mask_float
         new_dim = point_feature.shape.as_list()[-1]
         point_feature = tf.reshape(point_feature, [-1, new_dim])
         point_voxel_id = tf.reshape(point_voxel_id, [-1])
         # [B * num_voxels, new_dim]
         voxel_feature = tf.math.unsorted_segment_max(
-            point_feature, point_voxel_id, batch_size * self._voxel_spatial_size_volume
+            point_feature,
+            point_voxel_id,
+            batch_size * self._voxel_spatial_size_volume,
         )
         # unsorted_segment_max sets empty values to -inf(float).
         voxel_feature_valid_mask = voxel_feature > VOXEL_FEATURE_MIN
         voxel_feature = voxel_feature * tf.cast(
             voxel_feature_valid_mask, dtype=voxel_feature.dtype
         )
         out_shape = [batch_size] + self._voxel_spatial_size + [new_dim]
```

## keras_cv/layers/object_detection_3d/voxelization_test.py

```diff
@@ -9,24 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.object_detection_3d.voxelization import DynamicVoxelization
 
 
 class VoxelizationTest(tf.test.TestCase):
     def get_point_net(self):
-        return tf.keras.Sequential(
+        return keras.Sequential(
             [
-                tf.keras.layers.Dense(10),
-                tf.keras.layers.Dense(20),
+                keras.layers.Dense(10),
+                keras.layers.Dense(20),
             ]
         )
 
     def test_voxelization_output_shape_no_z(self):
         layer = DynamicVoxelization(
             point_net=self.get_point_net(),
             voxel_size=[0.1, 0.1, 1000],
@@ -35,20 +36,23 @@
         point_xyz = tf.random.uniform(
             shape=[1, 1000, 3], minval=-5, maxval=5, dtype=tf.float32
         )
         point_feature = tf.random.uniform(
             shape=[1, 1000, 4], minval=-10, maxval=10, dtype=tf.float32
         )
         point_mask = tf.cast(
-            tf.random.uniform(shape=[1, 1000], minval=0, maxval=2, dtype=tf.int32),
+            tf.random.uniform(
+                shape=[1, 1000], minval=0, maxval=2, dtype=tf.int32
+            ),
             tf.bool,
         )
         output = layer(point_xyz, point_feature, point_mask)
         # (20 - (-20)) / 0.1 = 400, (20 - (-20) ) / 1000 = 0.4
-        # the last dimension is replaced with MLP dimension, z dimension is skipped
+        # the last dimension is replaced with MLP dimension, z dimension is
+        # skipped
         self.assertEqual(output.shape, [1, 400, 400, 20])
 
     def test_voxelization_output_shape_with_z(self):
         layer = DynamicVoxelization(
             point_net=self.get_point_net(),
             voxel_size=[0.1, 0.1, 1],
             spatial_size=[-20, 20, -20, 20, -15, 15],
@@ -56,25 +60,28 @@
         point_xyz = tf.random.uniform(
             shape=[1, 1000, 3], minval=-5, maxval=5, dtype=tf.float32
         )
         point_feature = tf.random.uniform(
             shape=[1, 1000, 4], minval=-10, maxval=10, dtype=tf.float32
         )
         point_mask = tf.cast(
-            tf.random.uniform(shape=[1, 1000], minval=0, maxval=2, dtype=tf.int32),
+            tf.random.uniform(
+                shape=[1, 1000], minval=0, maxval=2, dtype=tf.int32
+            ),
             tf.bool,
         )
         output = layer(point_xyz, point_feature, point_mask)
         # (20 - (-20)) / 0.1 = 400, (20 - (-20) ) / 1000 = 0.4
         # (15 - (-15)) / 1 = 30
-        # the last dimension is replaced with MLP dimension, z dimension is skipped
+        # the last dimension is replaced with MLP dimension, z dimension is
+        # skipped
         self.assertEqual(output.shape, [1, 400, 400, 30, 20])
 
     def test_voxelization_numerical(self):
-        point_net = tf.keras.layers.Lambda(lambda x: x)
+        point_net = keras.layers.Lambda(lambda x: x)
         layer = DynamicVoxelization(
             point_net=point_net,
             voxel_size=[1.0, 1.0, 10.0],
             spatial_size=[-5, 5, -5, 5, -2, 2],
         )
         point_xyz = tf.constant(
             [
```

## keras_cv/layers/preprocessing/__init__.py

```diff
@@ -1,67 +1,81 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-# Also export the image KPLs from core keras, so that user can import all the image
-# KPLs from one place.
+# Also export the image KPLs from core keras, so that user can import all the
+# image KPLs from one place.
 
 from tensorflow.keras.layers import CenterCrop
 from tensorflow.keras.layers import RandomHeight
 from tensorflow.keras.layers import RandomWidth
 
 from keras_cv.layers.preprocessing.aug_mix import AugMix
-from keras_cv.layers.preprocessing.augmenter import Augmenter
 from keras_cv.layers.preprocessing.auto_contrast import AutoContrast
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.layers.preprocessing.channel_shuffle import ChannelShuffle
 from keras_cv.layers.preprocessing.cut_mix import CutMix
 from keras_cv.layers.preprocessing.equalization import Equalization
 from keras_cv.layers.preprocessing.fourier_mix import FourierMix
 from keras_cv.layers.preprocessing.grayscale import Grayscale
 from keras_cv.layers.preprocessing.grid_mask import GridMask
+from keras_cv.layers.preprocessing.jittered_resize import JitteredResize
 from keras_cv.layers.preprocessing.maybe_apply import MaybeApply
 from keras_cv.layers.preprocessing.mix_up import MixUp
 from keras_cv.layers.preprocessing.mosaic import Mosaic
 from keras_cv.layers.preprocessing.posterization import Posterization
 from keras_cv.layers.preprocessing.rand_augment import RandAugment
+from keras_cv.layers.preprocessing.random_aspect_ratio import RandomAspectRatio
 from keras_cv.layers.preprocessing.random_augmentation_pipeline import (
     RandomAugmentationPipeline,
 )
 from keras_cv.layers.preprocessing.random_brightness import RandomBrightness
-from keras_cv.layers.preprocessing.random_channel_shift import RandomChannelShift
+from keras_cv.layers.preprocessing.random_channel_shift import (
+    RandomChannelShift,
+)
 from keras_cv.layers.preprocessing.random_choice import RandomChoice
 from keras_cv.layers.preprocessing.random_color_degeneration import (
     RandomColorDegeneration,
 )
 from keras_cv.layers.preprocessing.random_color_jitter import RandomColorJitter
 from keras_cv.layers.preprocessing.random_contrast import RandomContrast
 from keras_cv.layers.preprocessing.random_crop import RandomCrop
-from keras_cv.layers.preprocessing.random_crop_and_resize import RandomCropAndResize
+from keras_cv.layers.preprocessing.random_crop_and_resize import (
+    RandomCropAndResize,
+)
 from keras_cv.layers.preprocessing.random_cutout import RandomCutout
 from keras_cv.layers.preprocessing.random_flip import RandomFlip
-from keras_cv.layers.preprocessing.random_gaussian_blur import RandomGaussianBlur
+from keras_cv.layers.preprocessing.random_gaussian_blur import (
+    RandomGaussianBlur,
+)
 from keras_cv.layers.preprocessing.random_hue import RandomHue
 from keras_cv.layers.preprocessing.random_jpeg_quality import RandomJpegQuality
 from keras_cv.layers.preprocessing.random_rotation import RandomRotation
 from keras_cv.layers.preprocessing.random_saturation import RandomSaturation
 from keras_cv.layers.preprocessing.random_sharpness import RandomSharpness
 from keras_cv.layers.preprocessing.random_shear import RandomShear
 from keras_cv.layers.preprocessing.random_translation import RandomTranslation
 from keras_cv.layers.preprocessing.random_zoom import RandomZoom
-from keras_cv.layers.preprocessing.randomly_zoomed_crop import RandomlyZoomedCrop
-from keras_cv.layers.preprocessing.repeated_augmentation import RepeatedAugmentation
+from keras_cv.layers.preprocessing.randomly_zoomed_crop import (
+    RandomlyZoomedCrop,
+)
+from keras_cv.layers.preprocessing.repeated_augmentation import (
+    RepeatedAugmentation,
+)
 from keras_cv.layers.preprocessing.rescaling import Rescaling
 from keras_cv.layers.preprocessing.resizing import Resizing
 from keras_cv.layers.preprocessing.solarization import Solarization
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
+)
```

## keras_cv/layers/preprocessing/aug_mix.py

```diff
@@ -9,60 +9,64 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import layers
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class AugMix(BaseImageAugmentationLayer):
     """Performs the AugMix data augmentation technique.
 
-    AugMix aims to produce images with variety while preserving the
-    image semantics and local statistics.  During the augmentation process, each image
-    is augmented `num_chains` different ways, each way consisting of `chain_depth`
-    augmentations. Augmentations are sampled from the list: translation, shearing,
-    rotation, posterization, histogram equalization, solarization and auto contrast.
-    The results of each chain are then mixed together with the original
-    image based on random samples from a Dirichlet distribution.
+    AugMix aims to produce images with variety while preserving the image
+    semantics and local statistics. During the augmentation process, each image
+    is augmented `num_chains` different ways, each way consisting of
+    `chain_depth` augmentations. Augmentations are sampled from the list:
+    translation, shearing, rotation, posterization, histogram equalization,
+    solarization and auto contrast. The results of each chain are then mixed
+    together with the original image based on random samples from a Dirichlet
+    distribution.
 
     Args:
         value_range: the range of values the incoming images will have.
             Represented as a two number tuple written (low, high).
             This is typically either `(0, 1)` or `(0, 255)` depending
-            on how your preprocessing pipeline is setup.
-        severity: A tuple of two floats, a single float or a `keras_cv.FactorSampler`.
-            A value is sampled from the provided range.  If a float is passed, the
-            range is interpreted as `(0, severity)`. This value represents the
-            level of strength of augmentations and is in the range [0, 1].
-            Defaults to 0.3.
+            on how your preprocessing pipeline is set up.
+        severity: A tuple of two floats, a single float or a
+            `keras_cv.FactorSampler`. A value is sampled from the provided
+            range. If a float is passed, the range is interpreted as
+            `(0, severity)`. This value represents the level of strength of
+            augmentations and is in the range [0, 1]. Defaults to 0.3.
         num_chains: an integer representing the number of different chains to
-            be mixed. Defaults to 3.
-        chain_depth: an integer or range representing the number of transformations in
-            the chains. Defaults to [1,3].
+            be mixed, defaults to 3.
+        chain_depth: an integer or range representing the number of
+            transformations in the chains. If a range is passed, a random
+            `chain_depth` value sampled from a uniform distribution over the
+            given range is called at the start of the chain. Defaults to [1,3].
         alpha: a float value used as the probability coefficients for the
-            Beta and Dirichlet distributions. Defaults to 1.0.
+            Beta and Dirichlet distributions, defaults to 1.0.
         seed: Integer. Used to create a random seed.
 
     References:
         - [AugMix paper](https://arxiv.org/pdf/1912.02781)
         - [Official Code](https://github.com/google-research/augmix)
-        - [Unoffial TF Code](https://github.com/szacho/augmix-tf)
+        - [Unofficial TF Code](https://github.com/szacho/augmix-tf)
 
     Sample Usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     augmix = keras_cv.layers.AugMix([0, 255])
     augmented_images = augmix(images[:100])
     ```
     """
 
     def __init__(
         self,
@@ -96,24 +100,28 @@
 
         # initialize layers
         self.auto_contrast = layers.AutoContrast(value_range=self.value_range)
         self.equalize = layers.Equalization(value_range=self.value_range)
 
     def _sample_from_dirichlet(self, alpha):
         gamma_sample = tf.random.gamma(
-            shape=(), alpha=alpha, seed=self._random_generator.make_legacy_seed()
+            shape=(),
+            alpha=alpha,
+            seed=self._random_generator.make_legacy_seed(),
+        )
+        return gamma_sample / tf.reduce_sum(
+            gamma_sample, axis=-1, keepdims=True
         )
-        return gamma_sample / tf.reduce_sum(gamma_sample, axis=-1, keepdims=True)
 
     def _sample_from_beta(self, alpha, beta):
         sample_alpha = tf.random.gamma(
-            (), 1.0, beta=alpha, seed=self._random_generator.make_legacy_seed()
+            (), alpha=alpha, seed=self._random_generator.make_legacy_seed()
         )
         sample_beta = tf.random.gamma(
-            (), 1.0, beta=beta, seed=self._random_generator.make_legacy_seed()
+            (), alpha=beta, seed=self._random_generator.make_legacy_seed()
         )
         return sample_alpha / (sample_alpha + sample_beta)
 
     def _sample_depth(self):
         return self._random_generator.random_uniform(
             shape=(),
             minval=self.chain_depth[0],
@@ -155,33 +163,39 @@
             original_range=self.value_range,
             target_range=[0, 255],
         )
 
         bits = tf.cast(self.severity_factor() * 3, tf.int32)
         shift = tf.cast(4 - bits + 1, tf.uint8)
         image = tf.cast(image, tf.uint8)
-        image = tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)
+        image = tf.bitwise.left_shift(
+            tf.bitwise.right_shift(image, shift), shift
+        )
         image = tf.cast(image, self.compute_dtype)
         return preprocessing.transform_value_range(
             images=image,
             original_range=[0, 255],
             target_range=self.value_range,
         )
 
     def _rotate(self, image):
-        angle = tf.expand_dims(tf.cast(self.severity_factor() * 30, tf.float32), axis=0)
+        angle = tf.expand_dims(
+            tf.cast(self.severity_factor() * 30, tf.float32), axis=0
+        )
         shape = tf.cast(tf.shape(image), tf.float32)
 
         return preprocessing.transform(
             tf.expand_dims(image, 0),
             preprocessing.get_rotation_matrix(angle, shape[0], shape[1]),
         )[0]
 
     def _solarize(self, image):
-        threshold = tf.cast(tf.cast(self.severity_factor() * 255, tf.int32), tf.float32)
+        threshold = tf.cast(
+            tf.cast(self.severity_factor() * 255, tf.int32), tf.float32
+        )
 
         image = preprocessing.transform_value_range(
             image, original_range=self.value_range, target_range=(0, 255)
         )
         result = tf.clip_by_value(image, 0, 255)
         result = tf.where(result < threshold, result, 255 - result)
         return preprocessing.transform_value_range(
@@ -197,47 +211,54 @@
         return preprocessing.transform(
             images=tf.expand_dims(image, 0), transforms=transform_x
         )[0]
 
     def _shear_y(self, image):
         y = tf.cast(self.severity_factor() * 0.3, tf.float32)
         y *= preprocessing.random_inversion(self._random_generator)
-        transform_x = layers.RandomShear._format_transform(
+        transform_x = self._format_random_shear_transform(
             [1.0, 0.0, 0.0, y, 1.0, 0.0, 0.0, 0.0]
         )
         return preprocessing.transform(
             images=tf.expand_dims(image, 0), transforms=transform_x
         )[0]
 
+    @staticmethod
+    def _format_random_shear_transform(transform):
+        transform = tf.convert_to_tensor(transform, dtype=tf.float32)
+        return transform[tf.newaxis]
+
     def _translate_x(self, image):
         shape = tf.cast(tf.shape(image), tf.float32)
         x = tf.cast(self.severity_factor() * shape[1] / 3, tf.float32)
         x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=0)
         x *= preprocessing.random_inversion(self._random_generator)
         x = tf.cast(x, tf.int32)
 
         translations = tf.cast(
             tf.concat([x, tf.zeros_like(x)], axis=1), dtype=tf.float32
         )
         return preprocessing.transform(
-            tf.expand_dims(image, 0), preprocessing.get_translation_matrix(translations)
+            tf.expand_dims(image, 0),
+            preprocessing.get_translation_matrix(translations),
         )[0]
 
     def _translate_y(self, image):
         shape = tf.cast(tf.shape(image), tf.float32)
         y = tf.cast(self.severity_factor() * shape[0] / 3, tf.float32)
         y = tf.expand_dims(tf.expand_dims(y, axis=0), axis=0)
         y *= preprocessing.random_inversion(self._random_generator)
         y = tf.cast(y, tf.int32)
 
         translations = tf.cast(
             tf.concat([tf.zeros_like(y), y], axis=1), dtype=tf.float32
         )
         return preprocessing.transform(
-            tf.expand_dims(image, 0), preprocessing.get_translation_matrix(translations)
+            tf.expand_dims(image, 0),
+            preprocessing.get_translation_matrix(translations),
         )[0]
 
     def _apply_op(self, image, op_index):
         augmented = image
         augmented = tf.cond(
             op_index == tf.constant([0], dtype=tf.int32),
             lambda: self._auto_contrast(augmented),
```

## keras_cv/layers/preprocessing/auto_contrast.py

```diff
@@ -9,77 +9,91 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class AutoContrast(BaseImageAugmentationLayer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class AutoContrast(VectorizedBaseImageAugmentationLayer):
     """Performs the AutoContrast operation on an image.
 
     Auto contrast stretches the values of an image across the entire available
-    `value_range`.  This makes differences between pixels more obvious.  An example of
-    this is if an image only has values `[0, 1]` out of the range `[0, 255]`, auto
-    contrast will change the `1` values to be `255`.
+    `value_range`. This makes differences between pixels more obvious. An
+    example of this is if an image only has values `[0, 1]` out of the range
+    `[0, 255]`, auto contrast will change the `1` values to be `255`.
 
     Args:
         value_range: the range of values the incoming images will have.
             Represented as a two number tuple written [low, high].
             This is typically either `[0, 1]` or `[0, 255]` depending
-            on how your preprocessing pipeline is setup.
+            on how your preprocessing pipeline is set up.
     """
 
     def __init__(
         self,
         value_range,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.value_range = value_range
 
-    def augment_image(self, image, transformation=None, **kwargs):
-        original_image = image
-        image = preprocessing.transform_value_range(
-            image,
+    def augment_images(self, images, transformations=None, **kwargs):
+        original_images = images
+        images = preprocessing.transform_value_range(
+            images,
             original_range=self.value_range,
             target_range=(0, 255),
             dtype=self.compute_dtype,
         )
 
-        low = tf.reduce_min(tf.reduce_min(image, axis=0), axis=0)
-        high = tf.reduce_max(tf.reduce_max(image, axis=0), axis=0)
+        low = tf.reduce_min(images, axis=(1, 2), keepdims=True)
+        high = tf.reduce_max(images, axis=(1, 2), keepdims=True)
         scale = 255.0 / (high - low)
         offset = -low * scale
 
-        image = image * scale[None, None] + offset[None, None]
-        result = tf.clip_by_value(image, 0.0, 255.0)
+        images = images * scale + offset
+        result = tf.clip_by_value(images, 0.0, 255.0)
         result = preprocessing.transform_value_range(
             result,
             original_range=(0, 255),
             target_range=self.value_range,
             dtype=self.compute_dtype,
         )
         # don't process NaN channels
-        result = tf.where(tf.math.is_nan(result), original_image, result)
+        result = tf.where(tf.math.is_nan(result), original_images, result)
         return result
 
     def augment_bounding_boxes(self, bounding_boxes, **kwargs):
         return bounding_boxes
 
-    def augment_label(self, label, transformation=None, **kwargs):
-        return label
+    def augment_labels(self, labels, transformations=None, **kwargs):
+        return labels
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
+
+    def augment_keypoints(self, keypoints, transformations, **kwargs):
+        return keypoints
+
+    def augment_targets(self, targets, transformations, **kwargs):
+        return targets
+
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        return self.augment_images(
+            image, transformations=transformation, **kwargs
+        )
 
     def get_config(self):
         config = super().get_config()
         config.update({"value_range": self.value_range})
         return config
```

## keras_cv/layers/preprocessing/auto_contrast_test.py

```diff
@@ -40,15 +40,16 @@
         ys = layer(img)
 
         self.assertTrue(tf.math.reduce_any(ys[0] == 0.0))
         self.assertTrue(tf.math.reduce_any(ys[0] == 255.0))
 
     def test_auto_contrast_different_values_per_channel(self):
         img = tf.constant(
-            [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], dtype=tf.float32
+            [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]],
+            dtype=tf.float32,
         )
         img = tf.expand_dims(img, axis=0)
 
         layer = preprocessing.AutoContrast(value_range=(0, 255))
         ys = layer(img)
 
         self.assertTrue(tf.math.reduce_any(ys[0, ..., 0] == 0.0))
```

## keras_cv/layers/preprocessing/base_image_augmentation_layer.py

```diff
@@ -1,83 +1,93 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 from keras_cv.utils import preprocessing
 
 # In order to support both unbatched and batched inputs, the horizontal
-# and verticle axis is reverse indexed
+# and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 IMAGES = "images"
 LABELS = "labels"
 TARGETS = "targets"
 BOUNDING_BOXES = "bounding_boxes"
 KEYPOINTS = "keypoints"
 SEGMENTATION_MASKS = "segmentation_masks"
 IS_DICT = "is_dict"
 USE_TARGETS = "use_targets"
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class BaseImageAugmentationLayer(tf.keras.__internal__.layers.BaseRandomLayer):
-    """Abstract base layer for image augmentaion.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class BaseImageAugmentationLayer(keras.__internal__.layers.BaseRandomLayer):
+    """Abstract base layer for image augmentation.
 
     This layer contains base functionalities for preprocessing layers which
-    augment image related data, eg. image and in future, label and bounding
-    boxes.  The subclasses could avoid making certain mistakes and reduce code
+    augment image related data, e.g. image and in the future, label and bounding
+    boxes. The subclasses could avoid making certain mistakes and reduce code
     duplications.
 
     This layer requires you to implement one method: `augment_image()`, which
     augments one single image during the training. There are a few additional
     methods that you can implement for added functionality on the layer:
 
     `augment_label()`, which handles label augmentation if the layer supports
     that.
 
     `augment_bounding_boxes()`, which handles the bounding box augmentation, if
     the layer supports that.
 
     `get_random_transformation()`, which should produce a random transformation
-    setting. The tranformation object, which could be any type, will be passed
-    to `augment_image`, `augment_label` and `augment_bounding_boxes`, to
-    coodinate the randomness behavior, eg, in the RandomFlip layer, the image
-    and bounding_boxes should be changed in the same way.
-
-    The `call()` method support two formats of inputs:
-    1. Single image tensor with 3D (HWC) or 4D (NHWC) format.
-    2. A dict of tensors with stable keys. The supported keys are:
-      `"images"`, `"labels"` and `"bounding_boxes"` at the moment. We might add
-      more keys in future when we support more types of augmentation.
-
-    The output of the `call()` will be in two formats, which will be the same
-    structure as the inputs.
-
-    The `call()` will handle the logic detecting the training/inference mode,
-    unpack the inputs, forward to the correct function, and pack the output back
-    to the same structure as the inputs.
-
-    By default the `call()` method leverages the `tf.vectorized_map()` function.
-    Auto-vectorization can be disabled by setting `self.auto_vectorize = False`
-    in your `__init__()` method.  When disabled, `call()` instead relies
-    on `tf.map_fn()`. For example:
+    setting. The transformation object, which could be of any type, will be
+    passed to `augment_image`, `augment_label` and `augment_bounding_boxes`, to
+    coordinate the randomness behaviour, e.g., in the RandomFlip layer, the
+    image and bounding_boxes should be changed in the same way.
+
+    The `call()` method supports two formats of inputs:
+    1. A single image tensor with shape (height, width, channels) or
+       (batch_size, height, width, channels)
+    1. A dict of tensors with any of the following keys (note that `"images"`
+       must be present):
+       * `"images"` - Image Tensor with shape (height, width, channels) or
+        (batch_size, height, width, channels)
+       * `"labels"` - One-hot encoded classification labels Tensor with shape
+        (num_classes) or (batch_size, num_classes)
+       * `"bounding_boxes"` - A dictionary with keys:
+         * `"boxes"` - Tensor with shape (num_boxes, 4) or (batch_size,
+          num_boxes, 4)
+         * `"classes"` - Tensor of class labels for boxes with shape (num_boxes,
+          num_classes) or (batch_size, num_boxes, num_classes).
+       Any other keys included in this dictionary will be ignored and unmodified
+       by an augmentation layer.
+
+    The output of the `call()` will be the same structure as the inputs.
+
+    The `call()` will unpack the inputs, forward to the correct function, and
+    pack the output back to the same structure as the inputs.
+
+    By default, the `call()` method leverages the `tf.vectorized_map()`
+    function. Auto-vectorization can be disabled by setting
+    `self.auto_vectorize = False` in your `__init__()` method. When disabled,
+    `call()` instead relies on `tf.map_fn()`. For example:
 
     ```python
     class SubclassLayer(keras_cv.BaseImageAugmentationLayer):
       def __init__(self):
         super().__init__()
         self.auto_vectorize = False
     ```
@@ -93,17 +103,17 @@
 
       def augment_image(self, image, transformation):
         random_factor = tf.random.uniform([], self._factor[0], self._factor[1])
         mean = tf.math.reduced_mean(inputs, axis=-1, keep_dim=True)
         return (inputs - mean) * random_factor + mean
     ```
 
-    Note that since the randomness is also a common functionnality, this layer
-    also includes a tf.keras.backend.RandomGenerator, which can be used to
-    produce the random numbers.  The random number generator is stored in the
+    Note that since the randomness is also a common functionality, this layer
+    also includes a keras.backend.RandomGenerator, which can be used to
+    produce the random numbers. The random number generator is stored in the
     `self._random_generator` attribute.
     """
 
     def __init__(self, seed=None, **kwargs):
         super().__init__(seed=seed, **kwargs)
 
     @property
@@ -124,17 +134,17 @@
     def force_output_dense_images(self, force_output_dense_images):
         self._force_output_dense_images = force_output_dense_images
 
     @property
     def auto_vectorize(self):
         """Control whether automatic vectorization occurs.
 
-        By default the `call()` method leverages the `tf.vectorized_map()`
-        function.  Auto-vectorization can be disabled by setting
-        `self.auto_vectorize = False` in your `__init__()` method.  When
+        By default, the `call()` method leverages the `tf.vectorized_map()`
+        function. Auto-vectorization can be disabled by setting
+        `self.auto_vectorize = False` in your `__init__()` method. When
         disabled, `call()` instead relies on `tf.map_fn()`. For example:
 
         ```python
         class SubclassLayer(BaseImageAugmentationLayer):
           def __init__(self):
             super().__init__()
             self.auto_vectorize = False
@@ -143,23 +153,27 @@
         return getattr(self, "_auto_vectorize", True)
 
     @auto_vectorize.setter
     def auto_vectorize(self, auto_vectorize):
         self._auto_vectorize = auto_vectorize
 
     def compute_image_signature(self, images):
-        """Computes the output image signature for the `augment_image()` function.
+        """Computes the output image signature for the `augment_image()`
+        function.
 
-        Must be overridden to return tensors with different shapes than the input
-        images.  By default returns either a `tf.RaggedTensorSpec` matching the input
-        image spec, or a `tf.TensorSpec` matching the input image spec.
+        Must be overridden to return tensors with different shapes than the
+        input images. By default, returns either a `tf.RaggedTensorSpec`
+        matching the input image spec, or a `tf.TensorSpec` matching the input
+        image spec.
         """
         if self.force_output_dense_images:
             return tf.TensorSpec(images.shape[1:], self.compute_dtype)
-        if self.force_output_ragged_images or isinstance(images, tf.RaggedTensor):
+        if self.force_output_ragged_images or isinstance(
+            images, tf.RaggedTensor
+        ):
             ragged_spec = tf.RaggedTensorSpec(
                 shape=images.shape[1:],
                 ragged_rank=1,
                 dtype=self.compute_dtype,
             )
             return ragged_spec
         return tf.TensorSpec(images.shape[1:], self.compute_dtype)
@@ -168,15 +182,17 @@
     def _compute_bounding_box_signature(self, bounding_boxes):
         return {
             "boxes": tf.RaggedTensorSpec(
                 shape=[None, 4],
                 ragged_rank=1,
                 dtype=self.compute_dtype,
             ),
-            "classes": tf.RaggedTensorSpec(shape=[None], dtype=self.compute_dtype),
+            "classes": tf.RaggedTensorSpec(
+                shape=[None], dtype=self.compute_dtype
+            ),
         }
 
     # TODO(lukewood): promote to user facing API if needed
     def _compute_keypoints_signature(self, keypoints):
         if isinstance(keypoints, tf.RaggedTensor):
             ragged_spec = tf.RaggedTensorSpec(
                 shape=keypoints.shape[1:],
@@ -191,27 +207,29 @@
         )
 
     # TODO(lukewood): promote to user facing API if needed
     def _compute_target_signature(self, targets):
         return tf.TensorSpec(targets.shape[1:], self.compute_dtype)
 
     def _compute_output_signature(self, inputs):
-        fn_output_signature = {IMAGES: self.compute_image_signature(inputs[IMAGES])}
+        fn_output_signature = {
+            IMAGES: self.compute_image_signature(inputs[IMAGES])
+        }
         bounding_boxes = inputs.get(BOUNDING_BOXES, None)
 
         if bounding_boxes is not None:
-            fn_output_signature[BOUNDING_BOXES] = self._compute_bounding_box_signature(
-                bounding_boxes
-            )
+            fn_output_signature[
+                BOUNDING_BOXES
+            ] = self._compute_bounding_box_signature(bounding_boxes)
 
         segmentation_masks = inputs.get(SEGMENTATION_MASKS, None)
         if segmentation_masks is not None:
-            fn_output_signature[SEGMENTATION_MASKS] = self.compute_image_signature(
-                segmentation_masks
-            )
+            fn_output_signature[
+                SEGMENTATION_MASKS
+            ] = self.compute_image_signature(segmentation_masks)
 
         keypoints = inputs.get(KEYPOINTS, None)
         if keypoints is not None:
             fn_output_signature[KEYPOINTS] = self._compute_keypoints_signature(
                 keypoints
             )
 
@@ -228,115 +246,125 @@
         if BOUNDING_BOXES in inputs:
             return True
         if KEYPOINTS in inputs:
             return True
         return False
 
     def _map_fn(self, func, inputs):
-        """Returns either tf.map_fn or tf.vectorized_map based on the provided inputs.
+        """Returns either tf.map_fn or tf.vectorized_map based on the provided
+        inputs.
 
         Args:
             inputs: dictionary of inputs provided to map_fn.
         """
         if self._any_ragged(inputs) or self.force_output_ragged_images:
             return tf.map_fn(
-                func, inputs, fn_output_signature=self._compute_output_signature(inputs)
+                func,
+                inputs,
+                fn_output_signature=self._compute_output_signature(inputs),
             )
         if self.auto_vectorize:
             return tf.vectorized_map(func, inputs)
         return tf.map_fn(func, inputs)
 
     def augment_image(self, image, transformation, **kwargs):
         """Augment a single image during training.
 
         Args:
           image: 3D image input tensor to the layer. Forwarded from
             `layer.call()`.
           transformation: The transformation object produced by
             `get_random_transformation`. Used to coordinate the randomness
-            between image, label, bounding box, keypoints, and segmentation mask.
+            between image, label, bounding box, keypoints, and segmentation
+            mask.
 
         Returns:
           output 3D tensor, which will be forward to `layer.call()`.
         """
         raise NotImplementedError()
 
     def augment_label(self, label, transformation, **kwargs):
         """Augment a single label during training.
 
         Args:
           label: 1D label to the layer. Forwarded from `layer.call()`.
           transformation: The transformation object produced by
             `get_random_transformation`. Used to coordinate the randomness
-            between image, label, bounding box, keypoints, and segmentation mask.
+            between image, label, bounding box, keypoints, and segmentation
+            mask.
 
         Returns:
           output 1D tensor, which will be forward to `layer.call()`.
         """
         raise NotImplementedError()
 
     def augment_target(self, target, transformation, **kwargs):
         """Augment a single target during training.
 
         Args:
           target: 1D label to the layer. Forwarded from `layer.call()`.
           transformation: The transformation object produced by
             `get_random_transformation`. Used to coordinate the randomness
-            between image, label, bounding box, keypoints, and segmentation mask.
+            between image, label, bounding box, keypoints, and segmentation
+            mask.
 
         Returns:
           output 1D tensor, which will be forward to `layer.call()`.
         """
         return self.augment_label(target, transformation)
 
     def augment_bounding_boxes(self, bounding_boxes, transformation, **kwargs):
         """Augment bounding boxes for one image during training.
 
         Args:
-          image: 3D image input tensor to the layer. Forwarded from
-            `layer.call()`.
           bounding_boxes: 2D bounding boxes to the layer. Forwarded from
             `call()`.
           transformation: The transformation object produced by
             `get_random_transformation`. Used to coordinate the randomness
-            between image, label, bounding box, keypoints, and segmentation mask.
+            between image, label, bounding box, keypoints, and segmentation
+            mask.
 
         Returns:
           output 2D tensor, which will be forward to `layer.call()`.
         """
         raise NotImplementedError()
 
     def augment_keypoints(self, keypoints, transformation, **kwargs):
         """Augment keypoints for one image during training.
 
         Args:
           keypoints: 2D keypoints input tensor to the layer. Forwarded from
             `layer.call()`.
           transformation: The transformation object produced by
             `get_random_transformation`. Used to coordinate the randomness
-            between image, label, bounding box, keypoints, and segmentation mask.
+            between image, label, bounding box, keypoints, and segmentation
+            mask.
 
         Returns:
           output 2D tensor, which will be forward to `layer.call()`.
         """
         raise NotImplementedError()
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         """Augment a single image's segmentation mask during training.
 
         Args:
           segmentation_mask: 3D segmentation mask input tensor to the layer.
-            This should generally have the shape [H, W, 1], or in some cases [H, W, C] for multilabeled data.
-            Forwarded from `layer.call()`.
+            This should generally have the shape [H, W, 1], or in some cases
+            [H, W, C] for multilabeled data. Forwarded from `layer.call()`.
           transformation: The transformation object produced by
             `get_random_transformation`. Used to coordinate the randomness
-            between image, label, bounding box, keypoints, and segmentation mask.
+            between image, label, bounding box, keypoints, and segmentation
+            mask.
 
         Returns:
-          output 3D tensor containing the augmented segmentation mask, which will be forward to `layer.call()`.
+          output 3D tensor containing the augmented segmentation mask, which
+          will be forward to `layer.call()`.
         """
         raise NotImplementedError()
 
     def get_random_transformation(
         self,
         image=None,
         label=None,
@@ -348,54 +376,51 @@
 
         This is used to produce same randomness between
         image/label/bounding_box.
 
         Args:
           image: 3D image tensor from inputs.
           label: optional 1D label tensor from inputs.
-          bounding_box: optional 2D bounding boxes tensor from inputs.
+          bounding_boxes: optional 2D bounding boxes tensor from inputs.
           segmentation_mask: optional 3D segmentation mask tensor from inputs.
 
         Returns:
           Any type of object, which will be forwarded to `augment_image`,
           `augment_label` and `augment_bounding_box` as the `transformation`
           parameter.
         """
         return None
 
-    def call(self, inputs, training=True):
+    def call(self, inputs):
         inputs = self._ensure_inputs_are_compute_dtype(inputs)
-        if training:
-            inputs, metadata = self._format_inputs(inputs)
-            images = inputs[IMAGES]
-            if images.shape.rank == 3:
-                return self._format_output(self._augment(inputs), metadata)
-            elif images.shape.rank == 4:
-                return self._format_output(self._batch_augment(inputs), metadata)
-            else:
-                raise ValueError(
-                    "Image augmentation layers are expecting inputs to be "
-                    "rank 3 (HWC) or 4D (NHWC) tensors. Got shape: "
-                    f"{images.shape}"
-                )
+        inputs, metadata = self._format_inputs(inputs)
+        images = inputs[IMAGES]
+        if images.shape.rank == 3:
+            return self._format_output(self._augment(inputs), metadata)
+        elif images.shape.rank == 4:
+            return self._format_output(self._batch_augment(inputs), metadata)
         else:
-            return inputs
+            raise ValueError(
+                "Image augmentation layers are expecting inputs to be "
+                "rank 3 (HWC) or 4D (NHWC) tensors. Got shape: "
+                f"{images.shape}"
+            )
 
     def _augment(self, inputs):
         raw_image = inputs.get(IMAGES, None)
         image = raw_image
         label = inputs.get(LABELS, None)
         bounding_boxes = inputs.get(BOUNDING_BOXES, None)
         keypoints = inputs.get(KEYPOINTS, None)
         segmentation_mask = inputs.get(SEGMENTATION_MASKS, None)
 
         image_ragged = isinstance(image, tf.RaggedTensor)
-        # At this point, the tensor is not actually ragged as we have mapped over the
-        # batch axis.  This call is required to make `tf.shape()` behave as users
-        # subclassing the layer expect.
+        # At this point, the tensor is not actually ragged as we have mapped
+        # over the batch axis. This call is required to make `tf.shape()` behave
+        # as users subclassing the layer expect.
         if image_ragged:
             image = image.to_tensor()
 
         transformation = self.get_random_transformation(
             image=image,
             label=label,
             bounding_boxes=bounding_boxes,
@@ -468,37 +493,41 @@
             # single image input tensor
             metadata[IS_DICT] = False
             inputs = {IMAGES: inputs}
             return inputs, metadata
 
         if not isinstance(inputs, dict):
             raise ValueError(
-                f"Expect the inputs to be image tensor or dict. Got inputs={inputs}"
+                "Expect the inputs to be image tensor or dict. Got "
+                f"inputs={inputs}"
             )
 
         if BOUNDING_BOXES in inputs:
-            inputs[BOUNDING_BOXES] = self._format_bounding_boxes(inputs[BOUNDING_BOXES])
+            inputs[BOUNDING_BOXES] = self._format_bounding_boxes(
+                inputs[BOUNDING_BOXES]
+            )
 
         if isinstance(inputs, dict) and TARGETS in inputs:
             # TODO(scottzhu): Check if it only contains the valid keys
             inputs[LABELS] = inputs[TARGETS]
             del inputs[TARGETS]
             metadata[USE_TARGETS] = True
             return inputs, metadata
 
         return inputs, metadata
 
     def _format_bounding_boxes(self, bounding_boxes):
-        # We can't catch the case where this is None, sometimes RaggedTensor drops this
-        # dimension
+        # We can't catch the case where this is None, sometimes RaggedTensor
+        # drops this dimension
         if "classes" not in bounding_boxes:
             raise ValueError(
-                "Bounding boxes are missing class_id. If you would like to pad the "
-                "bounding boxes with class_id, use: "
-                "`bounding_boxes['classes'] = tf.ones_like(bounding_boxes['boxes'])`."
+                "Bounding boxes are missing class_id. If you would like to pad "
+                "the bounding boxes with class_id, use: "
+                "`bounding_boxes['classes'] = "
+                "tf.ones_like(bounding_boxes['boxes'])`."
             )
         return bounding_boxes
 
     def _format_output(self, output, metadata):
         if not metadata[IS_DICT]:
             return output[IMAGES]
         elif metadata[USE_TARGETS]:
@@ -513,16 +542,11 @@
                 self.compute_dtype,
             )
         inputs[IMAGES] = preprocessing.ensure_tensor(
             inputs[IMAGES],
             self.compute_dtype,
         )
         if BOUNDING_BOXES in inputs:
-            inputs[BOUNDING_BOXES]["boxes"] = preprocessing.ensure_tensor(
-                inputs[BOUNDING_BOXES]["boxes"],
-                self.compute_dtype,
-            )
-            inputs[BOUNDING_BOXES]["classes"] = preprocessing.ensure_tensor(
-                inputs[BOUNDING_BOXES]["classes"],
-                self.compute_dtype,
+            inputs[BOUNDING_BOXES] = bounding_box.ensure_tensor(
+                inputs[BOUNDING_BOXES], dtype=self.compute_dtype
             )
         return inputs
```

## keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py

```diff
@@ -44,15 +44,17 @@
             "boxes": bounding_boxes["boxes"] + transformation,
             "classes": bounding_boxes["classes"] + transformation,
         }
 
     def augment_keypoints(self, keypoints, transformation, **kwargs):
         return keypoints + transformation
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return segmentation_mask + transformation
 
 
 class VectorizeDisabledLayer(BaseImageAugmentationLayer):
     def __init__(self, **kwargs):
         self.auto_vectorize = False
         super().__init__(**kwargs)
@@ -74,15 +76,17 @@
         self.assertIsInstance(output, dict)
 
     def test_augment_casts_dtypes(self):
         add_layer = RandomAddLayer(fixed_value=2.0)
         images = tf.ones((2, 8, 8, 3), dtype="uint8")
         output = add_layer(images)
 
-        self.assertAllClose(tf.ones((2, 8, 8, 3), dtype="float32") * 3.0, output)
+        self.assertAllClose(
+            tf.ones((2, 8, 8, 3), dtype="float32") * 3.0, output
+        )
 
     def test_augment_batch_images(self):
         add_layer = RandomAddLayer()
         images = np.random.random(size=(2, 8, 8, 3)).astype("float32")
         output = add_layer(images)
 
         diff = output - images
@@ -164,15 +168,17 @@
                     "classes": bounding_boxes["classes"] + 2.0,
                 }
             ),
             "keypoints": keypoints + 2.0,
             "segmentation_masks": segmentation_mask + 2.0,
         }
 
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
 
         self.assertAllClose(output["images"], expected_output["images"])
         self.assertAllClose(output["keypoints"], expected_output["keypoints"])
         self.assertAllClose(
             output["bounding_boxes"]["boxes"],
             expected_output["bounding_boxes"]["boxes"],
         )
@@ -188,33 +194,39 @@
         add_layer = RandomAddLayer()
         images = np.random.random(size=(2, 8, 8, 3)).astype("float32")
         bounding_boxes = {
             "boxes": np.random.random(size=(2, 3, 4)).astype("float32"),
             "classes": np.random.random(size=(2, 3)).astype("float32"),
         }
         keypoints = np.random.random(size=(2, 3, 5, 2)).astype("float32")
-        segmentation_masks = np.random.random(size=(2, 8, 8, 1)).astype("float32")
+        segmentation_masks = np.random.random(size=(2, 8, 8, 1)).astype(
+            "float32"
+        )
 
         output = add_layer(
             {
                 "images": images,
                 "bounding_boxes": bounding_boxes,
                 "keypoints": keypoints,
                 "segmentation_masks": segmentation_masks,
             }
         )
 
         bounding_boxes_diff = (
             output["bounding_boxes"]["boxes"] - bounding_boxes["boxes"]
         )
         keypoints_diff = output["keypoints"] - keypoints
-        segmentation_mask_diff = output["segmentation_masks"] - segmentation_masks
+        segmentation_mask_diff = (
+            output["segmentation_masks"] - segmentation_masks
+        )
         self.assertNotAllClose(bounding_boxes_diff[0], bounding_boxes_diff[1])
         self.assertNotAllClose(keypoints_diff[0], keypoints_diff[1])
-        self.assertNotAllClose(segmentation_mask_diff[0], segmentation_mask_diff[1])
+        self.assertNotAllClose(
+            segmentation_mask_diff[0], segmentation_mask_diff[1]
+        )
 
         @tf.function
         def in_tf_function(inputs):
             return add_layer(inputs)
 
         output = in_tf_function(
             {
@@ -225,28 +237,34 @@
             }
         )
 
         bounding_boxes_diff = (
             output["bounding_boxes"]["boxes"] - bounding_boxes["boxes"]
         )
         keypoints_diff = output["keypoints"] - keypoints
-        segmentation_mask_diff = output["segmentation_masks"] - segmentation_masks
+        segmentation_mask_diff = (
+            output["segmentation_masks"] - segmentation_masks
+        )
         self.assertNotAllClose(bounding_boxes_diff[0], bounding_boxes_diff[1])
         self.assertNotAllClose(keypoints_diff[0], keypoints_diff[1])
-        self.assertNotAllClose(segmentation_mask_diff[0], segmentation_mask_diff[1])
+        self.assertNotAllClose(
+            segmentation_mask_diff[0], segmentation_mask_diff[1]
+        )
 
     def test_augment_all_data_in_tf_function(self):
         add_layer = RandomAddLayer()
         images = np.random.random(size=(2, 8, 8, 3)).astype("float32")
         bounding_boxes = bounding_boxes = {
             "boxes": np.random.random(size=(2, 3, 4)).astype("float32"),
             "classes": np.random.random(size=(2, 3)).astype("float32"),
         }
         keypoints = np.random.random(size=(2, 5, 2)).astype("float32")
-        segmentation_masks = np.random.random(size=(2, 8, 8, 1)).astype("float32")
+        segmentation_masks = np.random.random(size=(2, 8, 8, 1)).astype(
+            "float32"
+        )
 
         @tf.function
         def in_tf_function(inputs):
             return add_layer(inputs)
 
         output = in_tf_function(
             {
@@ -257,11 +275,15 @@
             }
         )
 
         bounding_boxes_diff = (
             output["bounding_boxes"]["boxes"] - bounding_boxes["boxes"]
         )
         keypoints_diff = output["keypoints"] - keypoints
-        segmentation_mask_diff = output["segmentation_masks"] - segmentation_masks
+        segmentation_mask_diff = (
+            output["segmentation_masks"] - segmentation_masks
+        )
         self.assertNotAllClose(bounding_boxes_diff[0], bounding_boxes_diff[1])
         self.assertNotAllClose(keypoints_diff[0], keypoints_diff[1])
-        self.assertNotAllClose(segmentation_mask_diff[0], segmentation_mask_diff[1])
+        self.assertNotAllClose(
+            segmentation_mask_diff[0], segmentation_mask_diff[1]
+        )
```

## keras_cv/layers/preprocessing/channel_shuffle.py

```diff
@@ -1,95 +1,126 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class ChannelShuffle(BaseImageAugmentationLayer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class ChannelShuffle(VectorizedBaseImageAugmentationLayer):
     """Shuffle channels of an input image.
 
     Input shape:
-        The expected images should be [0-255] pixel ranges.
         3D (unbatched) or 4D (batched) tensor with shape:
         `(..., height, width, channels)`, in `"channels_last"` format
-
     Output shape:
         3D (unbatched) or 4D (batched) tensor with shape:
         `(..., height, width, channels)`, in `"channels_last"` format
 
     Args:
-        groups: Number of groups to divide the input channels. Default 3.
+        groups: Number of groups to divide the input channels, defaults to 3.
         seed: Integer. Used to create a random seed.
 
-    Call arguments:
-        inputs: Tensor representing images of shape
-            `(batch_size, width, height, channels)`, with dtype tf.float32 / tf.uint8,
-            ` or (width, height, channels)`, with dtype tf.float32 / tf.uint8
-        training: A boolean argument that determines whether the call should be run
-            in inference mode or training mode. Default: True.
-
     Usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
-    channel_shuffle = keras_cv.layers.ChannelShuffle()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
+    channel_shuffle = ChannelShuffle(groups=3)
     augmented_images = channel_shuffle(images)
     ```
     """
 
     def __init__(self, groups=3, seed=None, **kwargs):
         super().__init__(seed=seed, **kwargs)
         self.groups = groups
         self.seed = seed
 
-    def augment_image(self, image, transformation=None, **kwargs):
-        shape = tf.shape(image)
-        height, width = shape[0], shape[1]
-        num_channels = image.shape[2]
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        # get batched shuffled indices
+        # for example: batch_size=2; self.group=5
+        # indices = [
+        #     [0, 2, 3, 4, 1],
+        #     [4, 1, 0, 2, 3]
+        # ]
+        indices_distribution = self._random_generator.random_uniform(
+            (batch_size, self.groups)
+        )
+        indices = tf.argsort(indices_distribution, axis=-1)
+        return indices
+
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        # self.augment_images must have
+        # 4D images (batch_size, height, width, channel)
+        # 2D transformations (batch_size, groups)
+        image = tf.expand_dims(image, axis=0)
+        transformation = tf.expand_dims(transformation, axis=0)
+        image = self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
+        return tf.squeeze(image, axis=0)
+
+    def augment_images(self, images, transformations, **kwargs):
+        batch_size = tf.shape(images)[0]
+        height, width = images.shape[1], images.shape[2]
+        num_channels = images.shape[3]
+        indices = transformations
+
+        # append batch indexes next to shuffled indices
+        batch_indexs = tf.repeat(tf.range(batch_size), self.groups)
+        batch_indexs = tf.reshape(batch_indexs, (batch_size, self.groups))
+        indices = tf.stack([batch_indexs, indices], axis=-1)
 
         if not num_channels % self.groups == 0:
             raise ValueError(
                 "The number of input channels should be "
                 "divisible by the number of groups."
                 f"Received: channels={num_channels}, groups={self.groups}"
             )
 
         channels_per_group = num_channels // self.groups
-        image = tf.reshape(image, [height, width, self.groups, channels_per_group])
-        image = tf.transpose(image, perm=[2, 0, 1, 3])
-        image = tf.random.shuffle(image, seed=self.seed)
-        image = tf.transpose(image, perm=[1, 2, 3, 0])
-        image = tf.reshape(image, [height, width, num_channels])
 
-        return image
+        images = tf.reshape(
+            images, [batch_size, height, width, self.groups, channels_per_group]
+        )
+        images = tf.transpose(images, perm=[0, 3, 1, 2, 4])
+        images = tf.gather_nd(images, indices=indices)
+        images = tf.transpose(images, perm=[0, 2, 3, 4, 1])
+        images = tf.reshape(images, [batch_size, height, width, num_channels])
+
+        return images
+
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
+
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
 
-    def augment_bounding_boxes(self, bounding_boxes, **kwargs):
+    def augment_bounding_boxes(self, bounding_boxes, transformations, **kwargs):
         return bounding_boxes
 
-    def augment_label(self, label, transformation=None, **kwargs):
-        return label
-
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
-
     def get_config(self):
-        config = super().get_config()
-        config.update({"groups": self.groups, "seed": self.seed})
-        return config
-
-    def compute_output_shape(self, input_shape):
-        return input_shape
+        config = {
+            "groups": self.groups,
+            "seed": self.seed,
+        }
+        base_config = super().get_config()
+        return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/channel_shuffle_test.py

```diff
@@ -1,21 +1,21 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
+import numpy as np
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing.channel_shuffle import ChannelShuffle
 
 
 class ChannelShuffleTest(tf.test.TestCase):
     def test_return_shapes(self):
@@ -65,15 +65,17 @@
         layer = ChannelShuffle(groups=1)
         xs = layer(xs, training=True)
         self.assertTrue(tf.math.reduce_any(xs[0] == 2.0))
         self.assertTrue(tf.math.reduce_any(xs[1] == 1.0))
 
     def test_in_tf_function(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0),
+            tf.stack(
+                [2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0
+            ),
             dtype=tf.float32,
         )
 
         layer = ChannelShuffle(groups=1)
 
         @tf.function
         def augment(x):
@@ -88,7 +90,34 @@
             tf.ones((512, 512, 1)),
             dtype=tf.float32,
         )
 
         layer = ChannelShuffle(groups=1)
         xs = layer(xs, training=True)
         self.assertTrue(tf.math.reduce_any(xs == 1.0))
+
+    def test_channel_shuffle_on_batched_images_independently(self):
+        image = tf.random.uniform((100, 100, 3))
+        batched_images = tf.stack((image, image), axis=0)
+        layer = ChannelShuffle(groups=3)
+
+        results = layer(batched_images)
+
+        self.assertNotAllClose(results[0], results[1])
+
+    def test_config_with_custom_name(self):
+        layer = ChannelShuffle(name="image_preproc")
+        config = layer.get_config()
+        layer_1 = ChannelShuffle.from_config(config)
+        self.assertEqual(layer_1.name, layer.name)
+
+    def test_output_dtypes(self):
+        inputs = np.array([[[1], [2]], [[3], [4]]], dtype="float64")
+        layer = ChannelShuffle(groups=1)
+        self.assertAllEqual(layer(inputs).dtype, "float32")
+        layer = ChannelShuffle(groups=1, dtype="uint8")
+        self.assertAllEqual(layer(inputs).dtype, "uint8")
+
+    def test_config(self):
+        layer = ChannelShuffle(groups=5)
+        config = layer.get_config()
+        self.assertEqual(config["groups"], 5)
```

## keras_cv/layers/preprocessing/cut_mix.py

```diff
@@ -7,38 +7,40 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import fill_utils
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class CutMix(BaseImageAugmentationLayer):
     """CutMix implements the CutMix data augmentation technique.
 
     Args:
-        alpha: Float between 0 and 1.  Inverse scale parameter for the gamma
-            distribution.  This controls the shape of the distribution from which the
-            smoothing values are sampled.  Defaults 1.0, which is a recommended value
-            when training an imagenet1k classification model.
+        alpha: Float between 0 and 1. Inverse scale parameter for the gamma
+            distribution. This controls the shape of the distribution from which
+            the smoothing values are sampled. Defaults to 1.0, which is a
+            recommended value when training an imagenet1k classification model.
         seed: Integer. Used to create a random seed.
     References:
        - [CutMix paper]( https://arxiv.org/abs/1905.04899).
 
     Sample usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     labels = tf.one_hot(labels.squeeze(), 10)
 
     cutmix = keras_cv.layers.preprocessing.cut_mix.CutMix(10)
     output = cutmix({"images": images[:32], "labels": labels[:32]})
     # output == {'images': updated_images, 'labels': updated_labels}
     ```
     """
@@ -46,18 +48,18 @@
     def __init__(self, alpha=1.0, seed=None, **kwargs):
         super().__init__(seed=seed, **kwargs)
         self.alpha = alpha
         self.seed = seed
 
     def _sample_from_beta(self, alpha, beta, shape):
         sample_alpha = tf.random.gamma(
-            shape, 1.0, beta=alpha, seed=self._random_generator.make_legacy_seed()
+            shape, alpha=alpha, seed=self._random_generator.make_legacy_seed()
         )
         sample_beta = tf.random.gamma(
-            shape, 1.0, beta=beta, seed=self._random_generator.make_legacy_seed()
+            shape, alpha=beta, seed=self._random_generator.make_legacy_seed()
         )
         return sample_alpha / (sample_alpha + sample_beta)
 
     def _batch_augment(self, inputs):
         self._validate_inputs(inputs)
         images = inputs.get("images", None)
         labels = inputs.get("labels", None)
@@ -71,38 +73,42 @@
         images, labels = self._update_labels(*self._cutmix(images, labels))
         inputs["images"] = images
         inputs["labels"] = labels
         return inputs
 
     def _augment(self, inputs):
         raise ValueError(
-            "CutMix received a single image to `call`.  The layer relies on "
+            "CutMix received a single image to `call`. The layer relies on "
             "combining multiple examples, and as such will not behave as "
-            "expected.  Please call the layer with 2 or more samples."
+            "expected. Please call the layer with 2 or more samples."
         )
 
     def _cutmix(self, images, labels):
         """Apply cutmix."""
         input_shape = tf.shape(images)
         batch_size, image_height, image_width = (
             input_shape[0],
             input_shape[1],
             input_shape[2],
         )
 
-        permutation_order = tf.random.shuffle(tf.range(0, batch_size), seed=self.seed)
-        lambda_sample = self._sample_from_beta(self.alpha, self.alpha, (batch_size,))
+        permutation_order = tf.random.shuffle(
+            tf.range(0, batch_size), seed=self.seed
+        )
+        lambda_sample = self._sample_from_beta(
+            self.alpha, self.alpha, (batch_size,)
+        )
 
         ratio = tf.math.sqrt(1 - lambda_sample)
 
         cut_height = tf.cast(
             ratio * tf.cast(image_height, dtype=tf.float32), dtype=tf.int32
         )
         cut_width = tf.cast(
-            ratio * tf.cast(image_height, dtype=tf.float32), dtype=tf.int32
+            ratio * tf.cast(image_width, dtype=tf.float32), dtype=tf.int32
         )
 
         random_center_height = tf.random.uniform(
             shape=[batch_size], minval=0, maxval=image_height, dtype=tf.int32
         )
         random_center_width = tf.random.uniform(
             shape=[batch_size], minval=0, maxval=image_width, dtype=tf.int32
```

## keras_cv/layers/preprocessing/cut_mix_test.py

```diff
@@ -11,24 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing.cut_mix import CutMix
 
-classes = 10
+num_classes = 10
 
 
 class CutMixTest(tf.test.TestCase):
     def test_return_shapes(self):
         xs = tf.ones((2, 512, 512, 3))
         # randomly sample labels
         ys = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 2)
         ys = tf.squeeze(ys)
-        ys = tf.one_hot(ys, classes)
+        ys = tf.one_hot(ys, num_classes)
 
         layer = CutMix(seed=1)
         outputs = layer({"images": xs, "labels": ys})
         xs, ys = outputs["images"], outputs["labels"]
 
         self.assertEqual(xs.shape, [2, 512, 512, 3])
         self.assertEqual(ys.shape, [2, 10])
@@ -77,15 +77,17 @@
         self.assertTrue(tf.math.reduce_any(xs[1] == 2.0))
         # No labels should still be close to their original values
         self.assertNotAllClose(ys, 1.0)
         self.assertNotAllClose(ys, 0.0)
 
     def test_in_tf_function(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0),
+            tf.stack(
+                [2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0
+            ),
             tf.float32,
         )
         ys = tf.one_hot(tf.constant([0, 1]), 2)
 
         layer = CutMix(seed=1)
 
         @tf.function
@@ -122,15 +124,17 @@
             _ = layer(inputs)
 
     def test_int_labels(self):
         xs = tf.ones((2, 512, 512, 3))
         ys = tf.one_hot(tf.constant([1, 0]), 2, dtype=tf.int32)
         inputs = {"images": xs, "labels": ys}
         layer = CutMix()
-        with self.assertRaisesRegexp(ValueError, "CutMix received labels with type"):
+        with self.assertRaisesRegexp(
+            ValueError, "CutMix received labels with type"
+        ):
             _ = layer(inputs)
 
     def test_image_input(self):
         xs = tf.ones((2, 512, 512, 3))
         layer = CutMix()
         with self.assertRaisesRegexp(
             ValueError, "CutMix expects 'labels' to be present in its inputs"
```

## keras_cv/layers/preprocessing/equalization.py

```diff
@@ -7,46 +7,48 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class Equalization(BaseImageAugmentationLayer):
     """Equalization performs histogram equalization on a channel-wise basis.
 
     Args:
-        value_range: a tuple or a list of two elements. The first value represents
-            the lower bound for values in passed images, the second represents the
-            upper bound. Images passed to the layer should have values within
-            `value_range`.
-        bins: Integer indicating the number of bins to use in histogram equalization.
-            Should be in the range [0, 256].
+        value_range: a tuple or a list of two elements. The first value
+            represents the lower bound for values in passed images, the second
+            represents the upper bound. Images passed to the layer should have
+            values within `value_range`.
+        bins: Integer indicating the number of bins to use in histogram
+            equalization. Should be in the range [0, 256].
 
     Usage:
     ```python
     equalize = Equalization()
 
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     # Note that images are an int8 Tensor with values in the range [0, 255]
     images = equalize(images)
     ```
 
     Call arguments:
-        images: Tensor of pixels in range [0, 255], in RGB format.  Can be
-            of type float or int.  Should be in NHWC format.
+        images: Tensor of pixels in range [0, 255], in RGB format. Can be
+            of type float or int. Should be in NHWC format.
     """
 
     def __init__(self, value_range, bins=256, **kwargs):
         super().__init__(**kwargs)
         self.bins = bins
         self.value_range = value_range
 
@@ -58,40 +60,41 @@
                 with channels last
             channel_index: channel to equalize
         """
         image = image[..., channel_index]
         # Compute the histogram of the image channel.
         histogram = tf.histogram_fixed_width(image, [0, 255], nbins=self.bins)
 
-        # For the purposes of computing the step, filter out the nonzeros.
-        # Zeroes are replaced by a big number while calculating min to keep shape
-        # constant across input sizes for compatibility with vectorized_map
+        # For the purposes of computing the step, filter out the non-zeros.
+        # Zeroes are replaced by a big number while calculating min to keep
+        # shape constant across input sizes for compatibility with
+        # vectorized_map
 
         big_number = 1410065408
         histogram_without_zeroes = tf.where(
             tf.equal(histogram, 0),
             big_number,
             histogram,
         )
 
-        step = (tf.reduce_sum(histogram) - tf.reduce_min(histogram_without_zeroes)) // (
-            self.bins - 1
-        )
+        step = (
+            tf.reduce_sum(histogram) - tf.reduce_min(histogram_without_zeroes)
+        ) // (self.bins - 1)
 
         def build_mapping(histogram, step):
             # Compute the cumulative sum, shifting by step // 2
             # and then normalization by step.
             lookup_table = (tf.cumsum(histogram) + (step // 2)) // step
             # Shift lookup_table, prepending with 0.
             lookup_table = tf.concat([[0], lookup_table[:-1]], 0)
-            # Clip the counts to be in range.  This is done
+            # Clip the counts to be in range. This is done
             # in the C code for image.point.
             return tf.clip_by_value(lookup_table, 0, 255)
 
-        # If step is zero, return the original image.  Otherwise, build
+        # If step is zero, return the original image. Otherwise, build
         # lookup table from the full histogram and step and then index from it.
         result = tf.cond(
             tf.equal(step, 0),
             lambda: image,
             lambda: tf.gather(build_mapping(histogram, step), image),
         )
 
@@ -116,14 +119,16 @@
 
     def augment_bounding_boxes(self, bounding_boxes, **kwargs):
         return bounding_boxes
 
     def augment_label(self, label, transformation=None, **kwargs):
         return label
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return segmentation_mask
 
     def get_config(self):
         config = super().get_config()
         config.update({"bins": self.bins, "value_range": self.value_range})
         return config
```

## keras_cv/layers/preprocessing/equalization_test.py

```diff
@@ -7,16 +7,18 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
 from absl.testing import parameterized
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.equalization import Equalization
 
 
 class EqualizationTest(tf.test.TestCase, parameterized.TestCase):
     def test_return_shapes(self):
         xs = 255 * tf.ones((2, 512, 512, 3), dtype=tf.int32)
@@ -24,17 +26,17 @@
         xs = layer(xs)
 
         self.assertEqual(xs.shape, [2, 512, 512, 3])
         self.assertAllEqual(xs, 255 * tf.ones((2, 512, 512, 3)))
 
     def test_return_shapes_inside_model(self):
         layer = Equalization(value_range=(0, 255))
-        inp = tf.keras.layers.Input(shape=[512, 512, 5])
+        inp = keras.layers.Input(shape=[512, 512, 5])
         out = layer(inp)
-        model = tf.keras.models.Model(inp, out)
+        model = keras.models.Model(inp, out)
 
         self.assertEqual(model.layers[-1].output_shape, (None, 512, 512, 5))
 
     def test_equalizes_to_all_bins(self):
         xs = tf.random.uniform((2, 512, 512, 3), 0, 255, dtype=tf.float32)
         layer = Equalization(value_range=(0, 255))
         xs = layer(xs)
```

## keras_cv/layers/preprocessing/fourier_mix.py

```diff
@@ -7,65 +7,69 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class FourierMix(BaseImageAugmentationLayer):
     """FourierMix implements the FMix data augmentation technique.
 
     Args:
-        alpha: Float value for beta distribution.  Inverse scale parameter for the gamma
-            distribution.  This controls the shape of the distribution from which the
-            smoothing values are sampled.  Defaults to 0.5, which is a recommended value
-            in the paper.
-        decay_power: A float value representing the decay power.  Defaults to 3, as
-            recommended in the paper.
+        alpha: Float value for beta distribution. Inverse scale parameter for
+            the gamma distribution. This controls the shape of the distribution
+            from which the smoothing values are sampled. Defaults to 0.5, which
+            is a recommended value in the paper.
+        decay_power: A float value representing the decay power, defaults to 3,
+            as recommended in the paper.
         seed: Integer. Used to create a random seed.
     References:
         - [FMix paper](https://arxiv.org/abs/2002.12047).
 
     Sample usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     fourier_mix = keras_cv.layers.preprocessing.FourierMix(0.5)
-    augmented_images, updated_labels = fourier_mix({'images': images, 'labels': labels})
+    augmented_images, updated_labels = fourier_mix(
+        {'images': images, 'labels': labels}
+    )
     # output == {'images': updated_images, 'labels': updated_labels}
     ```
     """
 
     def __init__(self, alpha=0.5, decay_power=3, seed=None, **kwargs):
         super().__init__(seed=seed, **kwargs)
         self.alpha = alpha
         self.decay_power = decay_power
         self.seed = seed
 
     def _sample_from_beta(self, alpha, beta, shape):
         sample_alpha = tf.random.gamma(
-            shape, 1.0, beta=alpha, seed=self._random_generator.make_legacy_seed()
+            shape, alpha=alpha, seed=self._random_generator.make_legacy_seed()
         )
         sample_beta = tf.random.gamma(
-            shape, 1.0, beta=beta, seed=self._random_generator.make_legacy_seed()
+            shape, alpha=beta, seed=self._random_generator.make_legacy_seed()
         )
         return sample_alpha / (sample_alpha + sample_beta)
 
     @staticmethod
     def _fftfreq(signal_size, sample_spacing=1):
-        """This function returns the sample frequencies of a discrete fourier transform.
-        The result array contains the frequency bin centers starting at 0 using the
-        sample spacing.
+        """This function returns the sample frequencies of a discrete fourier
+        transform. The result array contains the frequency bin centers starting
+        at 0 using the sample spacing.
         """
 
         results = tf.concat(
             [
                 tf.range((signal_size - 1) / 2 + 1, dtype=tf.int32),
                 tf.range(-(signal_size // 2), 0, dtype=tf.int32),
             ],
@@ -79,17 +83,20 @@
         fx = FourierMix._fftfreq(w)[: w // 2 + 1 + w % 2]
         fy = FourierMix._fftfreq(h)
         fy = tf.expand_dims(fy, -1)
 
         return tf.math.sqrt(fx * fx + fy * fy)
 
     def _get_spectrum(self, freqs, decay_power, channel, h, w):
-        # Function to apply a low pass filter by decaying its high frequency components.
+        # Function to apply a low pass filter by decaying its high frequency
+        # components.
         scale = tf.ones(1) / tf.cast(
-            tf.math.maximum(freqs, tf.convert_to_tensor([1 / tf.reduce_max([w, h])]))
+            tf.math.maximum(
+                freqs, tf.convert_to_tensor([1 / tf.reduce_max([w, h])])
+            )
             ** decay_power,
             tf.float32,
         )
 
         param_size = tf.concat(
             [tf.constant([channel]), tf.shape(freqs), tf.constant([2])], 0
         )
@@ -112,15 +119,17 @@
         mask = mask / tf.reduce_max(mask)
         return mask
 
     def _binarise_mask(self, mask, lam, in_shape):
         # Create the final mask from the sampled values.
         idx = tf.argsort(tf.reshape(mask, [-1]), direction="DESCENDING")
         mask = tf.reshape(mask, [-1])
-        num = tf.cast(tf.math.round(lam * tf.cast(tf.size(mask), tf.float32)), tf.int32)
+        num = tf.cast(
+            tf.math.round(lam * tf.cast(tf.size(mask), tf.float32)), tf.int32
+        )
 
         updates = tf.concat(
             [
                 tf.ones((num,), tf.float32),
                 tf.zeros((tf.size(mask) - num,), tf.float32),
             ],
             0,
@@ -140,41 +149,51 @@
             raise ValueError(
                 "FourierMix expects inputs in a dictionary with format "
                 '{"images": images, "labels": labels}.'
                 f"Got: inputs = {inputs}"
             )
         images, lambda_sample, permutation_order = self._fourier_mix(images)
         if labels is not None:
-            labels = self._update_labels(labels, lambda_sample, permutation_order)
+            labels = self._update_labels(
+                labels, lambda_sample, permutation_order
+            )
             inputs["labels"] = labels
         inputs["images"] = images
         return inputs
 
     def _augment(self, inputs):
         raise ValueError(
-            "FourierMix received a single image to `call`.  The layer relies on "
+            "FourierMix received a single image to `call`. The layer relies on "
             "combining multiple examples, and as such will not behave as "
-            "expected.  Please call the layer with 2 or more samples."
+            "expected. Please call the layer with 2 or more samples."
         )
 
     def _fourier_mix(self, images):
         shape = tf.shape(images)
-        permutation_order = tf.random.shuffle(tf.range(0, shape[0]), seed=self.seed)
+        permutation_order = tf.random.shuffle(
+            tf.range(0, shape[0]), seed=self.seed
+        )
 
-        lambda_sample = self._sample_from_beta(self.alpha, self.alpha, (shape[0],))
+        lambda_sample = self._sample_from_beta(
+            self.alpha, self.alpha, (shape[0],)
+        )
 
         # generate masks utilizing mapped calls
         masks = tf.map_fn(
-            lambda x: self._sample_mask_from_transform(self.decay_power, shape[1:-1]),
+            lambda x: self._sample_mask_from_transform(
+                self.decay_power, shape[1:-1]
+            ),
             tf.range(shape[0], dtype=tf.float32),
         )
 
         # binarise masks utilizing mapped calls
         masks = tf.map_fn(
-            lambda i: self._binarise_mask(masks[i], lambda_sample[i], shape[1:-1]),
+            lambda i: self._binarise_mask(
+                masks[i], lambda_sample[i], shape[1:-1]
+            ),
             tf.range(shape[0], dtype=tf.int32),
             fn_output_signature=tf.float32,
         )
         masks = tf.expand_dims(masks, -1)
 
         fmix_images = tf.gather(images, permutation_order)
         images = masks * images + (1.0 - masks) * fmix_images
@@ -183,18 +202,22 @@
 
     def _update_labels(self, labels, lambda_sample, permutation_order):
         labels_for_fmix = tf.gather(labels, permutation_order)
 
         # for broadcasting
         batch_size = tf.expand_dims(tf.shape(labels)[0], -1)
         labels_rank = tf.rank(labels)
-        broadcast_shape = tf.concat([batch_size, tf.ones(labels_rank - 1, tf.int32)], 0)
+        broadcast_shape = tf.concat(
+            [batch_size, tf.ones(labels_rank - 1, tf.int32)], 0
+        )
         lambda_sample = tf.reshape(lambda_sample, broadcast_shape)
 
-        labels = lambda_sample * labels + (1.0 - lambda_sample) * labels_for_fmix
+        labels = (
+            lambda_sample * labels + (1.0 - lambda_sample) * labels_for_fmix
+        )
         return labels
 
     def get_config(self):
         config = {
             "alpha": self.alpha,
             "decay_power": self.decay_power,
             "seed": self.seed,
```

## keras_cv/layers/preprocessing/fourier_mix_test.py

```diff
@@ -11,24 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing.fourier_mix import FourierMix
 
-classes = 10
+num_classes = 10
 
 
 class FourierMixTest(tf.test.TestCase):
     def test_return_shapes(self):
         xs = tf.ones((2, 512, 512, 3))
         # randomly sample labels
         ys = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 2)
         ys = tf.squeeze(ys)
-        ys = tf.one_hot(ys, classes)
+        ys = tf.one_hot(ys, num_classes)
 
         layer = FourierMix()
         outputs = layer({"images": xs, "labels": ys})
         xs, ys = (
             outputs["images"],
             outputs["labels"],
         )
@@ -83,19 +83,23 @@
 
         # No labels should still be close to their originals
         self.assertNotAllClose(ys, 1.0)
         self.assertNotAllClose(ys, 0.0)
 
     def test_image_input_only(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0),
+            tf.stack(
+                [2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0
+            ),
             tf.float32,
         )
         layer = FourierMix()
-        with self.assertRaisesRegexp(ValueError, "expects inputs in a dictionary"):
+        with self.assertRaisesRegexp(
+            ValueError, "expects inputs in a dictionary"
+        ):
             _ = layer(xs)
 
     def test_single_image_input(self):
         xs = tf.ones((512, 512, 3))
         ys = tf.one_hot(tf.constant([1]), 2)
         inputs = {"images": xs, "labels": ys}
         layer = FourierMix()
```

## keras_cv/layers/preprocessing/grayscale.py

```diff
@@ -9,23 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class Grayscale(BaseImageAugmentationLayer):
-    """Grayscale is a preprocessing layer that transforms RGB images to Grayscale images.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class Grayscale(VectorizedBaseImageAugmentationLayer):
+    """Grayscale is a preprocessing layer that transforms RGB images to
+    Grayscale images.
     Input images should have values in the range of [0, 255].
 
     Input shape:
         3D (unbatched) or 4D (batched) tensor with shape:
         `(..., height, width, channels)`, in `"channels_last"` format
     Output shape:
         3D (unbatched) or 4D (batched) tensor with shape:
@@ -37,64 +39,65 @@
             (..., height, width, 3) will have the following shapes
             after the `Grayscale` operation:
                  a. (..., height, width, 1) if output_channels = 1
                  b. (..., height, width, 3) if output_channels = 3.
 
     Usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     to_grayscale = keras_cv.layers.preprocessing.Grayscale()
     augmented_images = to_grayscale(images)
     ```
     """
 
     def __init__(self, output_channels=1, **kwargs):
         super().__init__(**kwargs)
         self.output_channels = output_channels
-        # This layer may raise an error when running on GPU using auto_vectorize
-        self.auto_vectorize = False
-
-    def compute_image_signature(self, images):
-        # required because of the `output_channels` argument
-        if isinstance(images, tf.RaggedTensor):
-            ragged_spec = tf.RaggedTensorSpec(
-                shape=images.shape[1:3] + [self.output_channels],
-                ragged_rank=1,
-                dtype=self.compute_dtype,
-            )
-            return ragged_spec
-        return tf.TensorSpec(
-            images.shape[1:3] + [self.output_channels], self.compute_dtype
-        )
+        self._check_input_params(output_channels)
 
     def _check_input_params(self, output_channels):
         if output_channels not in [1, 3]:
             raise ValueError(
                 "Received invalid argument output_channels. "
                 f"output_channels must be in 1 or 3. Got {output_channels}"
             )
         self.output_channels = output_channels
 
-    def augment_image(self, image, transformation=None, **kwargs):
-        grayscale = tf.image.rgb_to_grayscale(image)
+    def compute_ragged_image_signature(self, images):
+        ragged_spec = tf.RaggedTensorSpec(
+            shape=images.shape[1:3] + (self.output_channels,),
+            ragged_rank=1,
+            dtype=self.compute_dtype,
+        )
+        return ragged_spec
+
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        return self.augment_images(
+            image, transformations=transformation, **kwargs
+        )
+
+    def augment_images(self, images, transformations=None, **kwargs):
+        grayscale = tf.image.rgb_to_grayscale(images)
         if self.output_channels == 1:
             return grayscale
         elif self.output_channels == 3:
             return tf.image.grayscale_to_rgb(grayscale)
         else:
             raise ValueError("Unsupported value for `output_channels`.")
 
     def augment_bounding_boxes(self, bounding_boxes, **kwargs):
         return bounding_boxes
 
-    def augment_label(self, label, transformation=None, **kwargs):
-        return label
+    def augment_labels(self, labels, transformations=None, **kwargs):
+        return labels
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
 
     def get_config(self):
         config = {
             "output_channels": self.output_channels,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
```

## keras_cv/layers/preprocessing/grayscale_test.py

```diff
@@ -14,32 +14,32 @@
 import tensorflow as tf
 
 from keras_cv.layers import preprocessing
 
 
 class GrayscaleTest(tf.test.TestCase):
     def test_return_shapes(self):
-        xs = tf.ones((2, 512, 512, 3))
+        xs = tf.ones((2, 52, 24, 3))
 
         layer = preprocessing.Grayscale(
             output_channels=1,
         )
         xs1 = layer(xs, training=True)
 
         layer = preprocessing.Grayscale(
             output_channels=3,
         )
         xs2 = layer(xs, training=True)
 
-        self.assertEqual(xs1.shape, [2, 512, 512, 1])
-        self.assertEqual(xs2.shape, [2, 512, 512, 3])
+        self.assertEqual(xs1.shape, [2, 52, 24, 1])
+        self.assertEqual(xs2.shape, [2, 52, 24, 3])
 
     def test_in_tf_function(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((100, 100, 3)), tf.ones((100, 100, 3))], axis=0),
+            tf.stack([2 * tf.ones((10, 10, 3)), tf.ones((10, 10, 3))], axis=0),
             tf.float32,
         )
 
         # test 1
         layer = preprocessing.Grayscale(
             output_channels=1,
         )
@@ -57,47 +57,47 @@
 
         @tf.function
         def augment(x):
             return layer(x, training=True)
 
         xs2 = augment(xs)
 
-        self.assertEqual(xs1.shape, [2, 100, 100, 1])
-        self.assertEqual(xs2.shape, [2, 100, 100, 3])
+        self.assertEqual(xs1.shape, [2, 10, 10, 1])
+        self.assertEqual(xs2.shape, [2, 10, 10, 3])
 
     def test_non_square_image(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((512, 1024, 3)), tf.ones((512, 1024, 3))], axis=0),
+            tf.stack([2 * tf.ones((52, 24, 3)), tf.ones((52, 24, 3))], axis=0),
             tf.float32,
         )
 
         layer = preprocessing.Grayscale(
             output_channels=1,
         )
         xs1 = layer(xs, training=True)
 
         layer = preprocessing.Grayscale(
             output_channels=3,
         )
         xs2 = layer(xs, training=True)
 
-        self.assertEqual(xs1.shape, [2, 512, 1024, 1])
-        self.assertEqual(xs2.shape, [2, 512, 1024, 3])
+        self.assertEqual(xs1.shape, [2, 52, 24, 1])
+        self.assertEqual(xs2.shape, [2, 52, 24, 3])
 
     def test_in_single_image(self):
         xs = tf.cast(
-            tf.ones((512, 512, 3)),
+            tf.ones((52, 24, 3)),
             dtype=tf.float32,
         )
 
         layer = preprocessing.Grayscale(
             output_channels=1,
         )
         xs1 = layer(xs, training=True)
 
         layer = preprocessing.Grayscale(
             output_channels=3,
         )
         xs2 = layer(xs, training=True)
 
-        self.assertEqual(xs1.shape, [512, 512, 1])
-        self.assertEqual(xs2.shape, [512, 512, 3])
+        self.assertEqual(xs1.shape, [52, 24, 1])
+        self.assertEqual(xs2.shape, [52, 24, 3])
```

## keras_cv/layers/preprocessing/grid_mask.py

```diff
@@ -9,14 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras import layers
 
 from keras_cv import core
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import fill_utils
@@ -29,15 +30,15 @@
     w_diff = masks_shape[1] - width
 
     h_start = tf.cast(h_diff / 2, tf.int32)
     w_start = tf.cast(w_diff / 2, tf.int32)
     return tf.image.crop_to_bounding_box(mask, h_start, w_start, height, width)
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class GridMask(BaseImageAugmentationLayer):
     """GridMask class for grid-mask augmentation.
 
 
     Input shape:
         Int or float tensor with values in the range [0, 255].
         3D (unbatched) or 4D (batched) tensor with shape:
@@ -47,44 +48,47 @@
         `(..., height, width, channels)`, in `"channels_last"` format
 
     Args:
         ratio_factor: A float, tuple of two floats, or `keras_cv.FactorSampler`.
             Ratio determines the ratio from spacings to grid masks.
             Lower values make the grid
             size smaller, and higher values make the grid mask large.
-            Floats should be in the range [0, 1].  0.5 indicates that grid and
+            Floats should be in the range [0, 1]. 0.5 indicates that grid and
             spacing will be of equal size. To always use the same value, pass a
             `keras_cv.ConstantFactorSampler()`.
 
             Defaults to `(0, 0.5)`.
         rotation_factor:
-            The rotation_factor will be used to randomly rotate the grid_mask during
-            training. Default to 0.1, which results in an output rotating by a
-            random amount in the range [-10% * 2pi, 10% * 2pi].
+            The rotation_factor will be used to randomly rotate the grid_mask
+            during training. Default to 0.1, which results in an output rotating
+            by a random amount in the range [-10% * 2pi, 10% * 2pi].
 
             A float represented as fraction of 2 Pi, or a tuple of size 2
             representing lower and upper bound for rotating clockwise and
-            counter-clockwise. A positive values means rotating counter clock-wise,
-            while a negative value means clock-wise. When represented as a single
-            float, this value is used for both the upper and lower bound. For
-            instance, factor=(-0.2, 0.3) results in an output rotation by a random
-            amount in the range [-20% * 2pi, 30% * 2pi]. factor=0.2 results in an
-            output rotating by a random amount in the range [-20% * 2pi, 20% * 2pi].
+            counter-clockwise. A positive values means rotating counter
+            clock-wise, while a negative value means clock-wise. When
+            represented as a single float, this value is used for both the upper
+            and lower bound. For instance, factor=(-0.2, 0.3) results in an
+            output rotation by a random amount in the range [-20% * 2pi,
+            30% * 2pi]. factor=0.2 results in an output rotating by a random
+            amount in the range [-20% * 2pi, 20% * 2pi].
 
         fill_mode: Pixels inside the gridblock are filled according to the given
-            mode (one of `{"constant", "gaussian_noise"}`). Default: "constant".
+            mode (one of `{"constant", "gaussian_noise"}`), defaults to
+            "constant".
             - *constant*: Pixels are filled with the same constant value.
             - *gaussian_noise*: Pixels are filled with random gaussian noise.
-        fill_value: an integer represents of value to be filled inside the gridblock
-            when `fill_mode="constant"`. Valid integer range [0 to 255]
+        fill_value: an integer represents of value to be filled inside the
+            gridblock when `fill_mode="constant"`. Valid integer range
+            [0 to 255]
         seed: Integer. Used to create a random seed.
 
     Usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     random_gridmask = keras_cv.layers.preprocessing.GridMask()
     augmented_images = random_gridmask(images)
     ```
 
     References:
         - [GridMask paper](https://arxiv.org/abs/2001.04086)
     """
@@ -102,15 +106,15 @@
         self.ratio_factor = preprocessing.parse_factor(
             ratio_factor, param_name="ratio_factor"
         )
 
         if isinstance(rotation_factor, core.FactorSampler):
             raise ValueError(
                 "Currently `GridMask.rotation_factor` does not support the "
-                "`FactorSampler` API.  This will be supported in the next Keras "
+                "`FactorSampler` API. This will be supported in the next Keras "
                 "release. For now, please pass a float for the "
                 "`rotation_factor` argument."
             )
 
         self.fill_mode = fill_mode
         self.fill_value = fill_value
         self.rotation_factor = rotation_factor
@@ -131,15 +135,15 @@
             raise ValueError(
                 f"fill_value should be in the range [0, 255]. Got {fill_value}"
             )
 
         if fill_mode not in ["constant", "gaussian_noise", "random"]:
             raise ValueError(
                 '`fill_mode` should be "constant", '
-                f'"gaussian_noise", or "random".  Got `fill_mode`={fill_mode}'
+                f'"gaussian_noise", or "random". Got `fill_mode`={fill_mode}'
             )
 
     def get_random_transformation(
         self, image=None, label=None, bounding_boxes=None, **kwargs
     ):
         ratio = self.ratio_factor()
 
@@ -238,15 +242,17 @@
 
     def augment_bounding_boxes(self, bounding_boxes, **kwargs):
         return bounding_boxes
 
     def augment_label(self, label, transformation=None, **kwargs):
         return label
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return segmentation_mask
 
     def get_config(self):
         config = {
             "ratio_factor": self.ratio_factor,
             "rotation_factor": self.rotation_factor,
             "fill_mode": self.fill_mode,
```

## keras_cv/layers/preprocessing/grid_mask_test.py

```diff
@@ -74,15 +74,17 @@
         self.assertTrue(tf.math.reduce_any(xs[0] == float(fill_value)))
         self.assertTrue(tf.math.reduce_any(xs[0] == 2.0))
         self.assertTrue(tf.math.reduce_any(xs[1] == float(fill_value)))
         self.assertTrue(tf.math.reduce_any(xs[1] == 1.0))
 
     def test_in_tf_function(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0),
+            tf.stack(
+                [2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0
+            ),
             dtype=tf.float32,
         )
 
         fill_value = 255.0
         layer = GridMask(
             ratio_factor=keras_cv.ConstantFactorSampler(0.5),
             rotation_factor=0.5,
@@ -104,11 +106,13 @@
 
     def test_in_single_image(self):
         xs = tf.cast(
             tf.ones((512, 512, 1)),
             dtype=tf.float32,
         )
 
-        layer = GridMask(ratio_factor=(0.5, 0.5), fill_mode="constant", fill_value=0.0)
+        layer = GridMask(
+            ratio_factor=(0.5, 0.5), fill_mode="constant", fill_value=0.0
+        )
         xs = layer(xs, training=True)
         self.assertTrue(tf.math.reduce_any(xs == 0.0))
         self.assertTrue(tf.math.reduce_any(xs == 1.0))
```

## keras_cv/layers/preprocessing/jittered_resize.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,229 +13,287 @@
 # limitations under the License.
 #
 # Some code in this file was inspired & adapted from `tensorflow_models`.
 # Reference:
 # https://github.com/tensorflow/models/blob/master/official/vision/ops/preprocess_ops.py
 
 import tensorflow as tf
+from tensorflow import keras
 
-import keras_cv
-from keras_cv import layers
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv import bounding_box
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
+from keras_cv.utils import preprocessing as preprocessing_utils
 
+H_AXIS = -3
+W_AXIS = -2
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class JitteredResize(BaseImageAugmentationLayer):
+
+@keras.utils.register_keras_serializable(package="keras_cv")
+class JitteredResize(VectorizedBaseImageAugmentationLayer):
     """JitteredResize implements resize with scale distortion.
 
-    JitteredResize takes a three step approach to size-distortion based image
-    augmentation.  This technique is specifically tuned for object detection pipelines.
-    The layer takes an input of images and bounding boxes, both of which may be ragged.
-    It outputs a dense image tensor, ready to feed to a model for training.
-    As such this layer will commonly be the final step in an augmentation
-    pipeline.
+    JitteredResize takes a three-step approach to size-distortion based image
+    augmentation. This technique is specifically tuned for object detection
+    pipelines. The layer takes an input of images and bounding boxes, both of
+    which may be ragged. It outputs a dense image tensor, ready to feed to a
+    model for training. As such this layer will commonly be the final step in an
+    augmentation pipeline.
 
     The augmentation process is as follows:
 
-    The image is first scaled according to a randomly sampled scale factor.  The width
-    and height of the image are then resized according to the sampled scale.  This is
-    done to introduce noise into the local scale of features in the image. A subset of
-    the image is then cropped randomly according to `crop_size`.  This crop is then
-    padded to be `target_size`.  Bounding boxes are translated and scaled according to
-    the random scaling and random cropping.
+    The image is first scaled according to a randomly sampled scale factor. The
+    width and height of the image are then resized according to the sampled
+    scale. This is done to introduce noise into the local scale of features in
+    the image. A subset of the image is then cropped randomly according to
+    `crop_size`. This crop is then padded to be `target_size`. Bounding boxes
+    are translated and scaled according to the random scaling and random
+    cropping.
+
+    Args:
+        target_size: A tuple representing the output size of images.
+        scale_factor: A tuple of two floats or a `keras_cv.FactorSampler`. For
+            each augmented image a value is sampled from the provided range.
+            This factor is used to scale the input image.
+            To replicate the results of the MaskRCNN paper pass `(0.8, 1.25)`.
+        crop_size: (Optional) the size of the image to crop from the scaled
+            image, defaults to `target_size` when not provided.
+        bounding_box_format: The format of bounding boxes of input boxes.
+            Refer to
+            https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box/converters.py
+            for more details on supported bounding box formats.
+        interpolation: String, the interpolation method, defaults to
+            `"bilinear"`. Supports `"bilinear"`, `"nearest"`, `"bicubic"`,
+            `"area"`, `"lanczos3"`, `"lanczos5"`, `"gaussian"`,
+            `"mitchellcubic"`.
+        seed: (Optional) integer to use as the random seed.
 
     Usage:
     ```python
     train_ds = load_object_detection_dataset()
     jittered_resize = layers.JitteredResize(
         target_size=(640, 640),
         scale_factor=(0.8, 1.25),
         bounding_box_format="xywh",
     )
-    train_ds = train_ds.map(jittered_resize, num_parallel_calls=tf.data.AUTOTUNE)
+    train_ds = train_ds.map(
+        jittered_resize, num_parallel_calls=tf.data.AUTOTUNE
+    )
     # images now are (640, 640, 3)
 
     # an example using crop size
     train_ds = load_object_detection_dataset()
     jittered_resize = layers.JitteredResize(
         target_size=(640, 640),
         crop_size=(250, 250),
         scale_factor=(0.8, 1.25),
         bounding_box_format="xywh",
     )
-    train_ds = train_ds.map(jittered_resize, num_parallel_calls=tf.data.AUTOTUNE)
+    train_ds = train_ds.map(
+        jittered_resize, num_parallel_calls=tf.data.AUTOTUNE
+    )
     # images now are (640, 640, 3), but they were resized from a 250x250 crop.
     ```
-
-    Args:
-        target_size: A tuple repesenting the output size of images.
-        scale_factor: A tuple of two floats or a `keras_cv.FactorSampler`. For each
-            augmented image a value is sampled from the provided range.
-            This factor is used to scale the input image.
-            To replicate the results of the MaskRCNN paper pass `(0.8, 1.25)`.
-        crop_size: (Optional) the size of the image to crop from the scaled image.
-            Defaults to `target_size` when not provided.
-        bounding_box_format: The format of bounding boxes of input boxes. Refer
-            to https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box/converters.py
-            for more details on supported bounding box formats.
-        interpolation: String, the interpolation method. Defaults to `"bilinear"`.
-            Supports `"bilinear"`, `"nearest"`, `"bicubic"`, `"area"`, `"lanczos3"`,
-            `"lanczos5"`, `"gaussian"`, `"mitchellcubic"`.
-        seed: (Optional) integer to use as the random seed.
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         target_size,
         scale_factor,
         crop_size=None,
         bounding_box_format=None,
         interpolation="bilinear",
         seed=None,
         **kwargs,
     ):
         super().__init__(**kwargs)
         if not isinstance(target_size, tuple) or len(target_size) != 2:
             raise ValueError(
-                "JitteredResize() expects `target_size` to be "
-                f"a tuple of two integers.  Received `target_size={target_size}`"
+                "JitteredResize() expects `target_size` to be a tuple of two "
+                f"integers. Received `target_size={target_size}`"
             )
 
         crop_size = crop_size or target_size
-        self.interpolation = keras_cv.utils.get_interpolation(interpolation)
-        self.scale_factor = keras_cv.utils.parse_factor(
+        self.interpolation = preprocessing_utils.get_interpolation(
+            interpolation
+        )
+        self.scale_factor = preprocessing_utils.parse_factor(
             scale_factor,
             min_value=0.0,
             max_value=None,
             param_name="scale_factor",
             seed=seed,
         )
         self.crop_size = crop_size
         self.target_size = target_size
-        self._inference_resizing = layers.Resizing(
-            target_size[0],
-            target_size[1],
-            pad_to_aspect_ratio=True,
-            interpolation=interpolation,
-            bounding_box_format=bounding_box_format,
-        )
         self.bounding_box_format = bounding_box_format
         self.seed = seed
+
         self.force_output_dense_images = True
-        self.auto_vectorize = False
 
-    def call(self, inputs, training=True):
-        if training:
-            return super().call(inputs, training)
-        else:
-            inputs = self._ensure_inputs_are_compute_dtype(inputs)
-            inputs, meta_data = self._format_inputs(inputs)
-            output = self._inference_resizing(inputs)
-            return self._format_output(output, meta_data)
+    def compute_ragged_image_signature(self, images):
+        ragged_spec = tf.RaggedTensorSpec(
+            shape=list(self.target_size) + [images.shape[-1]],
+            ragged_rank=1,
+            dtype=self.compute_dtype,
+        )
+        return ragged_spec
 
-    def get_random_transformation(self, image=None, **kwargs):
-        original_image_shape = tf.shape(image)
-        image_shape = tf.cast(original_image_shape[0:2], tf.float32)
+    def get_random_transformation_batch(
+        self, batch_size, images=None, **kwargs
+    ):
+        heights, widths = self._get_image_shape(images)
+        image_shapes = tf.cast(
+            tf.concat((heights, widths), axis=-1), dtype=tf.float32
+        )
 
-        scaled_size = tf.round(image_shape * self.scale_factor())
-        scale = tf.minimum(
-            scaled_size[0] / image_shape[0], scaled_size[1] / image_shape[1]
+        scaled_sizes = tf.round(
+            image_shapes * self.scale_factor(shape=(batch_size, 1))
+        )
+        scales = tf.where(
+            tf.less(
+                scaled_sizes[..., 0] / image_shapes[..., 0],
+                scaled_sizes[..., 1] / image_shapes[..., 1],
+            ),
+            scaled_sizes[..., 0] / image_shapes[..., 0],
+            scaled_sizes[..., 1] / image_shapes[..., 1],
         )
 
-        scaled_size = tf.round(image_shape * scale)
-        image_scale = scaled_size / image_shape
+        scaled_sizes = tf.round(image_shapes * scales[..., tf.newaxis])
+        image_scales = scaled_sizes / image_shapes
 
-        max_offset = scaled_size - self.crop_size
-        max_offset = tf.where(
-            tf.less(max_offset, 0), tf.zeros_like(max_offset), max_offset
+        max_offsets = scaled_sizes - self.crop_size
+        max_offsets = tf.where(
+            tf.less(max_offsets, 0), tf.zeros_like(max_offsets), max_offsets
         )
-        offset = max_offset * tf.random.uniform([2], minval=0, maxval=1)
-        offset = tf.cast(offset, tf.int32)
-
+        offsets = max_offsets * self._random_generator.random_uniform(
+            shape=(batch_size, 2), minval=0, maxval=1, dtype=tf.float32
+        )
+        offsets = tf.cast(offsets, tf.int32)
         return {
-            "original_size": original_image_shape,
-            "image_scale": image_scale,
-            "scaled_size": scaled_size,
-            "offset": offset,
+            "image_scales": image_scales,
+            "scaled_sizes": scaled_sizes,
+            "offsets": offsets,
         }
 
-    def compute_image_signature(self, images):
-        return tf.TensorSpec(
-            shape=list(self.target_size) + [images.shape[-1]],
-            dtype=self.compute_dtype,
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        image = tf.expand_dims(image, axis=0)
+        scaled_sizes = transformation["scaled_sizes"]
+        offsets = transformation["offsets"]
+        transformation = {
+            "scaled_sizes": tf.expand_dims(scaled_sizes, axis=0),
+            "offsets": tf.expand_dims(offsets, axis=0),
+        }
+        image = self.augment_images(
+            images=image, transformations=transformation, **kwargs
         )
+        return tf.squeeze(image, axis=0)
 
-    def augment_image(self, image, transformation, **kwargs):
+    def augment_images(self, images, transformations, **kwargs):
         # unpackage augmentation arguments
-        scaled_size = transformation["scaled_size"]
-        offset = transformation["offset"]
-        target_size = self.target_size
-        crop_size = self.crop_size
-
-        scaled_image = tf.image.resize(
-            image, tf.cast(scaled_size, tf.int32), method=self.interpolation
-        )
-        scaled_image = scaled_image[
-            offset[0] : offset[0] + crop_size[0],
-            offset[1] : offset[1] + crop_size[1],
-            :,
-        ]
-        scaled_image = tf.image.pad_to_bounding_box(
-            scaled_image, 0, 0, target_size[0], target_size[1]
+        scaled_sizes = transformations["scaled_sizes"]
+        offsets = transformations["offsets"]
+        inputs_for_resize_and_crop_single_image = {
+            "images": images,
+            "scaled_sizes": scaled_sizes,
+            "offsets": offsets,
+        }
+        scaled_images = tf.map_fn(
+            self.resize_and_crop_single_image,
+            inputs_for_resize_and_crop_single_image,
+            fn_output_signature=tf.float32,
         )
-        return tf.cast(scaled_image, self.compute_dtype)
+        return tf.cast(scaled_images, self.compute_dtype)
 
-    def augment_bounding_boxes(self, bounding_boxes, transformation, **kwargs):
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
+
+    def augment_bounding_boxes(
+        self, bounding_boxes, transformations, raw_images=None, **kwargs
+    ):
         if self.bounding_box_format is None:
             raise ValueError(
                 "Please provide a `bounding_box_format` when augmenting "
                 "bounding boxes with `JitteredResize()`."
             )
+        if isinstance(bounding_boxes["boxes"], tf.RaggedTensor):
+            bounding_boxes = bounding_box.to_dense(bounding_boxes)
         result = bounding_boxes.copy()
-        image_scale = tf.cast(transformation["image_scale"], self.compute_dtype)
-        offset = tf.cast(transformation["offset"], self.compute_dtype)
-        original_size = transformation["original_size"]
+        image_scales = tf.cast(
+            transformations["image_scales"], self.compute_dtype
+        )
+        offsets = tf.cast(transformations["offsets"], self.compute_dtype)
 
-        bounding_boxes = keras_cv.bounding_box.convert_format(
+        bounding_boxes = bounding_box.convert_format(
             bounding_boxes,
-            image_shape=original_size,
+            images=raw_images,
             source=self.bounding_box_format,
             target="yxyx",
         )
 
         # Adjusts box coordinates based on image_scale and offset.
         yxyx = bounding_boxes["boxes"]
-        yxyx *= tf.tile(tf.expand_dims(image_scale, axis=0), [1, 2])
-        yxyx -= tf.tile(tf.expand_dims(offset, axis=0), [1, 2])
+        yxyx *= tf.tile(image_scales, [1, 2])[..., tf.newaxis, :]
+        yxyx -= tf.tile(offsets, [1, 2])[..., tf.newaxis, :]
 
         result["boxes"] = yxyx
-        result = keras_cv.bounding_box.clip_to_image(
+        result = bounding_box.clip_to_image(
             result,
             image_shape=self.target_size + (3,),
             bounding_box_format="yxyx",
         )
-        result = keras_cv.bounding_box.convert_format(
+        result = bounding_box.convert_format(
             result,
             image_shape=self.target_size + (3,),
             source="yxyx",
             target=self.bounding_box_format,
         )
         return result
 
-    def augment_label(self, label, transformation, **kwargs):
-        return label
+    def _get_image_shape(self, images):
+        if isinstance(images, tf.RaggedTensor):
+            heights = tf.reshape(images.row_lengths(), (-1, 1))
+            widths = tf.reshape(
+                tf.reduce_max(images.row_lengths(axis=2), 1), (-1, 1)
+            )
+        else:
+            batch_size = tf.shape(images)[0]
+            heights = tf.repeat(tf.shape(images)[H_AXIS], repeats=[batch_size])
+            heights = tf.reshape(heights, shape=(-1, 1))
+            widths = tf.repeat(tf.shape(images)[W_AXIS], repeats=[batch_size])
+            widths = tf.reshape(widths, shape=(-1, 1))
+        return tf.cast(heights, dtype=tf.int32), tf.cast(widths, dtype=tf.int32)
+
+    def resize_and_crop_single_image(self, inputs):
+        image = inputs.get("images", None)
+        scaled_size = inputs.get("scaled_sizes", None)
+        offset = inputs.get("offsets", None)
+
+        scaled_image = tf.image.resize(image, tf.cast(scaled_size, tf.int32))
+        scaled_image = scaled_image[
+            offset[0] : offset[0] + self.crop_size[0],
+            offset[1] : offset[1] + self.crop_size[1],
+            :,
+        ]
+        scaled_image = tf.image.pad_to_bounding_box(
+            scaled_image, 0, 0, self.target_size[0], self.target_size[1]
+        )
+        return scaled_image
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "target_size": self.target_size,
                 "scale_factor": self.scale_factor,
                 "crop_size": self.crop_size,
                 "bounding_box_format": self.bounding_box_format,
                 "interpolation": self.interpolation,
                 "seed": self.seed,
             }
         )
         return config
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/jittered_resize_test.py

```diff
@@ -1,24 +1,26 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_cv import bounding_box
+from keras_cv import core
 from keras_cv import layers
 
 
 class JitteredResizeTest(tf.test.TestCase, parameterized.TestCase):
     batch_size = 4
     height = 9
     width = 8
@@ -53,15 +55,17 @@
         layer = layers.JitteredResize(
             target_size=self.target_size,
             scale_factor=(3 / 4, 4 / 3),
             bounding_box_format="rel_xywh",
             seed=self.seed,
         )
         output = layer(input, training=True)
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
         expected_output = {
             "boxes": tf.convert_to_tensor([[0, 0, 1, 1]], dtype=tf.float32),
             "classes": tf.convert_to_tensor([0], dtype=tf.float32),
         }
         self.assertAllClose(
             expected_output["boxes"],
             output["bounding_boxes"]["boxes"],
@@ -87,15 +91,17 @@
         layer = layers.JitteredResize(
             target_size=self.target_size,
             scale_factor=(3 / 4, 4 / 3),
             bounding_box_format="rel_xyxy",
             seed=self.seed,
         )
         output = layer(input, training=True)
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
         expected_output = {
             "classes": tf.convert_to_tensor([[0, 0], [0, 0]], dtype=tf.float32),
             "boxes": tf.convert_to_tensor(
                 [
                     [[0, 0, 1, 1], [0, 0, 1, 1]],
                     [[0, 0, 1, 1], [0, 0, 1, 1]],
                 ],
@@ -156,35 +162,56 @@
             expected_output["boxes"].to_tensor(),
             output["bounding_boxes"]["boxes"].to_tensor(),
         )
         self.assertAllClose(
             expected_output["classes"], output["bounding_boxes"]["classes"]
         )
 
-    def test_augment_inference_mode(self):
-        image = tf.zeros([20, 20, 3])
-        boxes = {
-            "boxes": tf.convert_to_tensor([[0, 0, 1, 1]], dtype=tf.float32),
-            "classes": tf.convert_to_tensor([0], dtype=tf.float32),
-        }
-        input = {"images": image, "bounding_boxes": boxes}
-
+    def test_independence_of_jittered_resize_on_batched_images(self):
+        image = tf.random.uniform((100, 100, 3))
+        batched_images = tf.stack((image, image), axis=0)
         layer = layers.JitteredResize(
             target_size=self.target_size,
             scale_factor=(3 / 4, 4 / 3),
-            bounding_box_format="rel_xywh",
             seed=self.seed,
         )
-        output = layer(input, training=False)
-        expected_output = layer._inference_resizing(output)
-        self.assertAllClose(
-            expected_output["bounding_boxes"]["boxes"],
-            output["bounding_boxes"]["boxes"],
+
+        results = layer(batched_images)
+
+        self.assertNotAllClose(results[0], results[1])
+
+    def test_config_with_custom_name(self):
+        layer = layers.JitteredResize(
+            target_size=self.target_size,
+            scale_factor=(3 / 4, 4 / 3),
+            name="image_preproc",
         )
-        self.assertAllClose(
-            expected_output["bounding_boxes"]["classes"],
-            output["bounding_boxes"]["classes"],
+        config = layer.get_config()
+        layer_1 = layers.JitteredResize.from_config(config)
+        self.assertEqual(layer_1.name, layer.name)
+
+    def test_output_dtypes(self):
+        inputs = np.array([[[1], [2]], [[3], [4]]], dtype="float64")
+        layer = layers.JitteredResize(
+            target_size=self.target_size,
+            scale_factor=(3 / 4, 4 / 3),
         )
-        self.assertAllClose(
-            expected_output["images"],
-            output["images"],
+        self.assertAllEqual(layer(inputs).dtype, "float32")
+        layer = layers.JitteredResize(
+            target_size=self.target_size,
+            scale_factor=(3 / 4, 4 / 3),
+            dtype="uint8",
+        )
+        self.assertAllEqual(layer(inputs).dtype, "uint8")
+
+    def test_config(self):
+        layer = layers.JitteredResize(
+            target_size=self.target_size,
+            scale_factor=(3 / 4, 4 / 3),
+            bounding_box_format="xyxy",
+        )
+        config = layer.get_config()
+        self.assertEqual(config["target_size"], self.target_size)
+        self.assertTrue(
+            isinstance(config["scale_factor"], core.UniformFactorSampler)
         )
+        self.assertEqual(config["bounding_box_format"], "xyxy")
```

## keras_cv/layers/preprocessing/maybe_apply.py

```diff
@@ -7,46 +7,47 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import tensorflow as tf
+
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class MaybeApply(BaseImageAugmentationLayer):
     """Apply provided layer to random elements in a batch.
 
     Args:
-        layer: a keras `Layer` or `BaseImageAugmentationLayer`. This layer will be
-            applied to randomly chosen samples in a batch. Layer should not modify the
-            size of provided inputs.
-        rate: controls the frequency of applying the layer. 1.0 means all elements in
-            a batch will be modified. 0.0 means no elements will be modified.
-            Defaults to 0.5.
-        batchwise: (Optional) bool, whether or not to pass entire batches to the
-            underlying layer.  When set to true, only a single random sample is
+        layer: a keras `Layer` or `BaseImageAugmentationLayer`. This layer will
+            be applied to randomly chosen samples in a batch. Layer should not
+            modify the size of provided inputs.
+        rate: controls the frequency of applying the layer. 1.0 means all
+            elements in a batch will be modified. 0.0 means no elements will be
+            modified. Defaults to 0.5.
+        batchwise: (Optional) bool, whether to pass entire batches to the
+            underlying layer. When set to true, only a single random sample is
             drawn to determine if the batch should be passed to the underlying
-            layer.  This is useful when using `MixUp()`, `CutMix()`, `Mosaic()`,
+            layer. This is useful when using `MixUp()`, `CutMix()`, `Mosaic()`,
             etc.
         auto_vectorize: bool, whether to use tf.vectorized_map or tf.map_fn for
-            batched input. Setting this to True might give better performance but
-            currently doesn't work with XLA. Defaults to False.
+            batched input. Setting this to True might give better performance
+            but currently doesn't work with XLA. Defaults to False.
         seed: integer, controls random behaviour.
 
     Example usage:
     ```
     # Let's declare an example layer that will set all image pixels to zero.
-    zero_out = tf.keras.layers.Lambda(lambda x: {"images": 0 * x["images"]})
+    zero_out = keras.layers.Lambda(lambda x: {"images": 0 * x["images"]})
 
     # Create a small batch of random, single-channel, 2x2 images:
     images = tf.random.stateless_uniform(shape=(5, 2, 2, 1), seed=[0, 1])
     print(images[..., 0])
     # <tf.Tensor: shape=(5, 2, 2), dtype=float32, numpy=
     # array([[[0.08216608, 0.40928006],
     #         [0.39318466, 0.3162533 ]],
@@ -79,15 +80,16 @@
     #
     #        [[0.38977218, 0.80855536],
     #         [0.6040567 , 0.10502195]],
     #
     #        [[0.        , 0.        ],
     #         [0.        , 0.        ]]], dtype=float32)>
 
-    # We can observe that the layer has been randomly applied to 2 out of 5 samples.
+    # We can observe that the layer has been randomly applied to 2 out of 5
+    samples.
     ```
     """
 
     def __init__(
         self,
         layer,
         rate=0.5,
@@ -95,24 +97,28 @@
         auto_vectorize=False,
         seed=None,
         **kwargs,
     ):
         super().__init__(seed=seed, **kwargs)
 
         if not (0 <= rate <= 1.0):
-            raise ValueError(f"rate must be in range [0, 1]. Received rate: {rate}")
+            raise ValueError(
+                f"rate must be in range [0, 1]. Received rate: {rate}"
+            )
 
         self._layer = layer
         self._rate = rate
         self.auto_vectorize = auto_vectorize
         self.batchwise = batchwise
         self.seed = seed
 
     def _should_augment(self):
-        return self._random_generator.random_uniform(shape=()) > 1.0 - self._rate
+        return (
+            self._random_generator.random_uniform(shape=()) > 1.0 - self._rate
+        )
 
     def _batch_augment(self, inputs):
         if self.batchwise:
             # batchwise augmentations
             if self._should_augment():
                 return self._layer(inputs)
             else:
```

## keras_cv/layers/preprocessing/maybe_apply_test.py

```diff
@@ -7,16 +7,18 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
 from absl.testing import parameterized
+from tensorflow import keras
 
 from keras_cv import layers
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.layers.preprocessing.maybe_apply import MaybeApply
 
@@ -103,15 +105,15 @@
 
         outputs = layer({"images": dummy_inputs, "labels": dummy_labels})
 
         self.assertAllEqual(outputs["labels"], tf.zeros_like(dummy_labels))
 
     def test_works_with_native_keras_layers(self):
         dummy_inputs = self.rng.uniform(shape=(32, 224, 224, 3))
-        zero_out = tf.keras.layers.Lambda(lambda x: {"images": 0 * x["images"]})
+        zero_out = keras.layers.Lambda(lambda x: {"images": 0 * x["images"]})
         layer = MaybeApply(rate=1.0, layer=zero_out)
 
         outputs = layer(dummy_inputs)
 
         self.assertAllEqual(outputs, tf.zeros_like(dummy_inputs))
 
     def test_works_with_xla(self):
```

## keras_cv/layers/preprocessing/mix_up.py

```diff
@@ -7,92 +7,102 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class MixUp(BaseImageAugmentationLayer):
     """MixUp implements the MixUp data augmentation technique.
 
     Args:
-        alpha: Float between 0 and 1.  Inverse scale parameter for the gamma
-            distribution.  This controls the shape of the distribution from which the
-            smoothing values are sampled.  Defaults 0.2, which is a recommended value
-            when training an imagenet1k classification model.
+        alpha: Float between 0 and 1. Inverse scale parameter for the gamma
+            distribution. This controls the shape of the distribution from which
+            the smoothing values are sampled. Defaults to 0.2, which is a
+            recommended value when training an imagenet1k classification model.
         seed: Integer. Used to create a random seed.
 
     References:
         - [MixUp paper](https://arxiv.org/abs/1710.09412).
         - [MixUp for Object Detection paper](https://arxiv.org/pdf/1902.04103).
 
     Sample usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     images, labels = images[:10], labels[:10]
     # Labels must be floating-point and one-hot encoded
     labels = tf.cast(tf.one_hot(labels, 10), tf.float32)
     mixup = keras_cv.layers.preprocessing.MixUp(10)
-    augmented_images, updated_labels = mixup({'images': images, 'labels': labels})
+    augmented_images, updated_labels = mixup(
+        {'images': images, 'labels': labels}
+    )
     # output == {'images': updated_images, 'labels': updated_labels}
     ```
     """
 
     def __init__(self, alpha=0.2, seed=None, **kwargs):
         super().__init__(seed=seed, **kwargs)
         self.alpha = alpha
         self.seed = seed
 
     def _sample_from_beta(self, alpha, beta, shape):
         sample_alpha = tf.random.gamma(
-            shape, 1.0, beta=alpha, seed=self._random_generator.make_legacy_seed()
+            shape, alpha=alpha, seed=self._random_generator.make_legacy_seed()
         )
         sample_beta = tf.random.gamma(
-            shape, 1.0, beta=beta, seed=self._random_generator.make_legacy_seed()
+            shape, alpha=beta, seed=self._random_generator.make_legacy_seed()
         )
         return sample_alpha / (sample_alpha + sample_beta)
 
     def _batch_augment(self, inputs):
         self._validate_inputs(inputs)
         images = inputs.get("images", None)
         labels = inputs.get("labels", None)
         bounding_boxes = inputs.get("bounding_boxes", None)
         images, lambda_sample, permutation_order = self._mixup(images)
         if labels is not None:
-            labels = self._update_labels(labels, lambda_sample, permutation_order)
+            labels = self._update_labels(
+                labels, lambda_sample, permutation_order
+            )
             inputs["labels"] = labels
         if bounding_boxes is not None:
             bounding_boxes = self._update_bounding_boxes(
                 bounding_boxes, permutation_order
             )
             inputs["bounding_boxes"] = bounding_boxes
         inputs["images"] = images
         return inputs
 
     def _augment(self, inputs):
         raise ValueError(
-            "MixUp received a single image to `call`.  The layer relies on "
+            "MixUp received a single image to `call`. The layer relies on "
             "combining multiple examples, and as such will not behave as "
-            "expected.  Please call the layer with 2 or more samples."
+            "expected. Please call the layer with 2 or more samples."
         )
 
     def _mixup(self, images):
         batch_size = tf.shape(images)[0]
-        permutation_order = tf.random.shuffle(tf.range(0, batch_size), seed=self.seed)
+        permutation_order = tf.random.shuffle(
+            tf.range(0, batch_size), seed=self.seed
+        )
 
-        lambda_sample = self._sample_from_beta(self.alpha, self.alpha, (batch_size,))
+        lambda_sample = self._sample_from_beta(
+            self.alpha, self.alpha, (batch_size,)
+        )
         lambda_sample = tf.cast(
             tf.reshape(lambda_sample, [-1, 1, 1, 1]), dtype=self.compute_dtype
         )
 
         mixup_images = tf.cast(
             tf.gather(images, permutation_order), dtype=self.compute_dtype
         )
@@ -102,15 +112,17 @@
         return images, tf.squeeze(lambda_sample), permutation_order
 
     def _update_labels(self, labels, lambda_sample, permutation_order):
         labels_for_mixup = tf.gather(labels, permutation_order)
 
         lambda_sample = tf.reshape(lambda_sample, [-1, 1])
 
-        labels = lambda_sample * labels + (1.0 - lambda_sample) * labels_for_mixup
+        labels = (
+            lambda_sample * labels + (1.0 - lambda_sample) * labels_for_mixup
+        )
 
         return labels
 
     def _update_bounding_boxes(self, bounding_boxes, permutation_order):
         boxes, classes = bounding_boxes["boxes"], bounding_boxes["classes"]
         boxes_for_mixup = tf.gather(boxes, permutation_order)
         classes_for_mixup = tf.gather(classes, permutation_order)
```

## keras_cv/layers/preprocessing/mix_up_test.py

```diff
@@ -11,35 +11,39 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing.mix_up import MixUp
 
-classes = 10
+num_classes = 10
 
 
 class MixUpTest(tf.test.TestCase):
     def test_return_shapes(self):
         xs = tf.ones((2, 512, 512, 3))
         # randomly sample labels
         ys_labels = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 2)
         ys_labels = tf.squeeze(ys_labels)
-        ys_labels = tf.one_hot(ys_labels, classes)
+        ys_labels = tf.one_hot(ys_labels, num_classes)
 
         # randomly sample bounding boxes
         ys_bounding_boxes = {
             "boxes": tf.random.uniform((2, 3, 4), 0, 1),
             "classes": tf.random.uniform((2, 3), 0, 1),
         }
 
         layer = MixUp()
         # mixup on labels
         outputs = layer(
-            {"images": xs, "labels": ys_labels, "bounding_boxes": ys_bounding_boxes}
+            {
+                "images": xs,
+                "labels": ys_labels,
+                "bounding_boxes": ys_bounding_boxes,
+            }
         )
         xs, ys_labels, ys_bounding_boxes = (
             outputs["images"],
             outputs["labels"],
             outputs["bounding_boxes"],
         )
 
@@ -95,19 +99,23 @@
 
         # No labels should still be close to their originals
         self.assertNotAllClose(ys, 1.0)
         self.assertNotAllClose(ys, 0.0)
 
     def test_image_input_only(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0),
+            tf.stack(
+                [2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0
+            ),
             tf.float32,
         )
         layer = MixUp()
-        with self.assertRaisesRegexp(ValueError, "expects inputs in a dictionary"):
+        with self.assertRaisesRegexp(
+            ValueError, "expects inputs in a dictionary"
+        ):
             _ = layer(xs)
 
     def test_single_image_input(self):
         xs = tf.ones((512, 512, 3))
         ys = tf.one_hot(tf.constant([1]), 2)
         inputs = {"images": xs, "labels": ys}
         layer = MixUp()
@@ -117,15 +125,17 @@
             _ = layer(inputs)
 
     def test_int_labels(self):
         xs = tf.ones((2, 512, 512, 3))
         ys = tf.one_hot(tf.constant([1, 0]), 2, dtype=tf.int32)
         inputs = {"images": xs, "labels": ys}
         layer = MixUp()
-        with self.assertRaisesRegexp(ValueError, "MixUp received labels with type"):
+        with self.assertRaisesRegexp(
+            ValueError, "MixUp received labels with type"
+        ):
             _ = layer(inputs)
 
     def test_image_input(self):
         xs = tf.ones((2, 512, 512, 3))
         layer = MixUp()
         with self.assertRaisesRegexp(
             ValueError, "MixUp expects inputs in a dictionary with format"
```

## keras_cv/layers/preprocessing/mosaic.py

```diff
@@ -1,302 +1,292 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    BATCHED,
 )
-from keras_cv.utils import preprocessing
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    BOUNDING_BOXES,
+)
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    IMAGES,
+)
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    LABELS,
+)
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
+)
+from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class Mosaic(BaseImageAugmentationLayer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class Mosaic(VectorizedBaseImageAugmentationLayer):
     """Mosaic implements the mosaic data augmentation technique.
 
-    Mosaic data augmentation first takes 4 images from the batch and makes a grid.
-    After that based on the offset, a crop is taken to form the mosaic image. Labels
-    are in the same ratio as the the area of their images in the output image. Bounding
-    boxes are translated according to the position of the 4 images.
+    Mosaic data augmentation first takes 4 images from the batch and makes a
+    grid. After that based on the offset, a crop is taken to form the mosaic
+    image. Labels are in the same ratio as the area of their images in the
+    output image. Bounding boxes are translated according to the position of the
+    4 images.
 
     Args:
-        offset: A tuple of two floats, a single float or `keras_cv.FactorSampler`.
-            `offset` is used to determine the offset of the mosaic center from the
-            top-left corner of the mosaic. If a tuple is used, the x and y coordinates
-            of the mosaic center are sampled between the two values for every image
-            augmented. If a single float is used, a value between `0.0` and the passed
-            float is sampled.  In order to ensure the value is always the same, please
+        offset: A tuple of two floats, a single float or
+            `keras_cv.FactorSampler`. `offset` is used to determine the offset
+            of the mosaic center from the top-left corner of the mosaic. If a
+            tuple is used, the x and y coordinates of the mosaic center are
+            sampled between the two values for every image augmented. If a
+            single float is used, a value between `0.0` and the passed float is
+            sampled. In order to ensure the value is always the same, please
             pass a tuple with two identical floats: `(0.5, 0.5)`. Defaults to
             (0.25, 0.75).
-        bounding_box_format: a case-insensitive string (for example, "xyxy") to be
-            passed if bounding boxes are being augmented by this layer.
-            Each bounding box is defined by at least these 4 values. The inputs
-            may contain additional information such as classes and confidence after
-            these 4 values but these values will be ignored and returned as is. For
-            detailed information on the supported formats, see the
+        bounding_box_format: a case-insensitive string (for example, "xyxy") to
+            be passed if bounding boxes are being augmented by this layer. Each
+            bounding box is defined by at least these 4 values. The inputs may
+            contain additional information such as classes and confidence after
+            these 4 values but these values will be ignored and returned as is.
+            For detailed information on the supported formats, see the
             [KerasCV bounding box documentation](https://keras.io/api/keras_cv/bounding_box/formats/).
-            Defualts to None.
-        seed: Integer. Used to create a random seed.
+            Defaults to None.
+        seed: integer, used to create a random seed.
 
     References:
         - [Yolov4 paper](https://arxiv.org/pdf/2004.10934).
         - [Yolov5 implementation](https://github.com/ultralytics/yolov5).
         - [YoloX implementation](https://github.com/Megvii-BaseDetection/YOLOX)
 
     Sample usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     labels = tf.one_hot(labels,10)
     labels = tf.cast(tf.squeeze(labels), tf.float32)
     mosaic = keras_cv.layers.preprocessing.Mosaic()
     output = mosaic({'images': images, 'labels': labels})
     # output == {'images': updated_images, 'labels': updated_labels}
     ```
-    """
+    """  # noqa: E501
 
     def __init__(
         self, offset=(0.25, 0.75), bounding_box_format=None, seed=None, **kwargs
     ):
         super().__init__(seed=seed, **kwargs)
         self.offset = offset
         self.bounding_box_format = bounding_box_format
-        self.center_sampler = preprocessing.parse_factor(
+        self.center_sampler = preprocessing_utils.parse_factor(
             offset, param_name="offset", seed=seed
         )
         self.seed = seed
 
-    def _batch_augment(self, inputs):
-        self._validate_inputs(inputs)
-        images = inputs.get("images", None)
-        labels = inputs.get("labels", None)
-        bounding_boxes = inputs.get("bounding_boxes", None)
-
-        batch_size = tf.shape(images)[0]
+    def get_random_transformation_batch(self, batch_size, **kwargs):
         # pick 3 indices for every batch to create the mosaic output with.
-        permutation_order = tf.random.uniform(
+        permutation_order = self._random_generator.random_uniform(
             (batch_size, 3),
             minval=0,
             maxval=batch_size,
             dtype=tf.int32,
-            seed=self._random_generator.make_legacy_seed(),
         )
-        # concatenate the batches with permutation order to get all 4 images of the mosaic
+        # concatenate the batches with permutation order to get all 4 images of
+        # the mosaic
         permutation_order = tf.concat(
-            [tf.expand_dims(tf.range(batch_size), axis=-1), permutation_order], axis=-1
+            [tf.expand_dims(tf.range(batch_size), axis=-1), permutation_order],
+            axis=-1,
         )
 
-        input_height, input_width, _ = images.shape[1:]
-
-        mosaic_centers_x = (
-            self.center_sampler(
-                tf.expand_dims(batch_size, axis=0), dtype=self.compute_dtype
-            )
-            * input_width
+        mosaic_centers_x = self.center_sampler(
+            shape=(batch_size,), dtype=self.compute_dtype
         )
-        mosaic_centers_y = (
-            self.center_sampler(
-                shape=tf.expand_dims(batch_size, axis=0), dtype=self.compute_dtype
-            )
-            * input_height
+        mosaic_centers_y = self.center_sampler(
+            shape=(batch_size,), dtype=self.compute_dtype
         )
         mosaic_centers = tf.stack((mosaic_centers_x, mosaic_centers_y), axis=-1)
 
-        # return the mosaics
-        images = tf.vectorized_map(
-            lambda index: self._update_image(
-                images, permutation_order, mosaic_centers, index
-            ),
-            tf.range(batch_size),
-        )
-
-        if labels is not None:
-            labels = tf.vectorized_map(
-                lambda index: self._update_label(
-                    images, labels, permutation_order, mosaic_centers, index
-                ),
-                tf.range(batch_size),
-            )
-            inputs["labels"] = labels
-
-        if bounding_boxes is not None:
-            # values to translate the boxes by in the mosaic image
-            translate_x = tf.stack(
-                [
-                    mosaic_centers_x - input_width,
-                    mosaic_centers_x,
-                    mosaic_centers_x - input_width,
-                    mosaic_centers_x,
-                ],
-                axis=-1,
-            )
-
-            translate_y = tf.stack(
-                [
-                    mosaic_centers_y - input_height,
-                    mosaic_centers_y - input_height,
-                    mosaic_centers_y,
-                    mosaic_centers_y,
-                ],
-                axis=-1,
-            )
-
-            bounding_boxes = bounding_box.to_dense(bounding_boxes)
-            bounding_boxes = tf.map_fn(
-                lambda index: self._update_bounding_box(
-                    images,
-                    bounding_boxes,
-                    permutation_order,
-                    translate_x,
-                    translate_y,
-                    index,
-                ),
-                tf.range(batch_size),
-                fn_output_signature={
-                    "boxes": tf.RaggedTensorSpec(
-                        shape=[None, 4],
-                        ragged_rank=1,
-                        dtype=self.compute_dtype,
-                    ),
-                    "classes": tf.RaggedTensorSpec(
-                        shape=[None], dtype=self.compute_dtype
-                    ),
-                },
-            )
-            bounding_boxes = bounding_box.to_ragged(bounding_boxes)
-            inputs["bounding_boxes"] = bounding_boxes
-        inputs["images"] = images
-        return inputs
+        return {
+            "permutation_order": permutation_order,
+            "mosaic_centers": mosaic_centers,
+        }
 
-    def _augment(self, inputs):
+    def augment_ragged_image(self, image, transformation, **kwargs):
         raise ValueError(
-            "Mosaic received a single image to `call`.  The layer relies on "
-            "combining multiple examples, and as such will not behave as "
-            "expected.  Please call the layer with 4 or more samples."
+            "Mosaic received ragged images to `call`. The layer relies on "
+            "combining multiple examples with same size, and as such will not "
+            "behave as expected. Please call the layer with dense images with "
+            "same size. This is an implementation constraint, not an algorithm "
+            "constraint. If you find this method helpful, please open an issue "
+            "on KerasCV."
         )
 
-    def _update_image(self, images, permutation_order, mosaic_centers, index):
-        # forms mosaic for one image from the batch
+    def augment_images(self, images, transformations, **kwargs):
+        batch_size = tf.shape(images)[0]
         input_height, input_width, _ = images.shape[1:]
-        mosaic_images = tf.gather(images, permutation_order[index])
 
-        top = tf.concat([mosaic_images[0], mosaic_images[1]], axis=1)
-        bottom = tf.concat([mosaic_images[2], mosaic_images[3]], axis=1)
-        output = tf.concat([top, bottom], axis=0)
+        # forms mosaic for one image from the batch
+        permutation_order = transformations["permutation_order"]
+        mosaic_images = tf.gather(images, permutation_order)
+
+        tops = tf.concat([mosaic_images[:, 0], mosaic_images[:, 1]], axis=2)
+        bottoms = tf.concat([mosaic_images[:, 2], mosaic_images[:, 3]], axis=2)
+        outputs = tf.concat([tops, bottoms], axis=1)
 
         # cropping coordinates for the mosaic
-        x1 = (input_width - mosaic_centers[index][0]) / (input_width * 2 - 1)
-        y1 = (input_height - mosaic_centers[index][1]) / (input_height * 2 - 1)
-        x2 = x1 + (input_width) / (input_width * 2 - 1)
-        y2 = y1 + (input_height) / (input_height * 2 - 1)
-
-        # helps avoid retracing caused by slicing, inspired by RRC implementation
-        output = tf.image.crop_and_resize(
-            tf.expand_dims(output, axis=0),
-            [[y1, x1, y2, x2]],
-            [0],
+        mosaic_centers = transformations["mosaic_centers"]
+        mosaic_centers_x = mosaic_centers[..., 0] * input_width
+        mosaic_centers_y = mosaic_centers[..., 1] * input_height
+        x1s = (input_width - mosaic_centers_x) / (input_width * 2 - 1)
+        y1s = (input_height - mosaic_centers_y) / (input_height * 2 - 1)
+        x2s = x1s + (input_width) / (input_width * 2 - 1)
+        y2s = y1s + (input_height) / (input_height * 2 - 1)
+        cropping_boxes = tf.stack([y1s, x1s, y2s, x2s], axis=-1)
+
+        # helps avoid retracing caused by slicing, inspired by RRC
+        # implementation
+        # boxes must be type tf.float32
+        outputs = tf.image.crop_and_resize(
+            outputs,
+            tf.cast(cropping_boxes, tf.float32),
+            tf.range(batch_size),
             [input_height, input_width],
         )
-        # tf.image.crop_and_resize will always output float32, so we need to recast
-        output = tf.cast(output, self.compute_dtype)
-        return tf.squeeze(output)
+        # tf.image.crop_and_resize will always output float32, so we need to
+        # recast tf.image.crop_and_resize outputs
+        # [num_boxes, crop_height, crop_width, depth] since num_boxes is always
+        # one we squeeze axis 0
+        outputs = tf.cast(outputs, self.compute_dtype)
+        return outputs
 
-    def _update_label(self, images, labels, permutation_order, mosaic_centers, index):
-        # updates labels for one output mosaic
+    def augment_labels(self, labels, transformations, images=None, **kwargs):
         input_height, input_width, _ = images.shape[1:]
-        labels_for_mosaic = tf.gather(labels, permutation_order[index])
-        center_x = mosaic_centers[index][0]
-        center_y = mosaic_centers[index][1]
+        # updates labels for one output mosaic
+        permutation_order = transformations["permutation_order"]
+        labels_for_mosaic = tf.gather(labels, permutation_order)
+
+        mosaic_centers = transformations["mosaic_centers"]
+        center_x = mosaic_centers[..., 0] * input_width
+        center_y = mosaic_centers[..., 1] * input_height
 
         area = input_height * input_width
 
         # labels are in the same ratio as the area of the images
         top_left_ratio = (center_x * center_y) / area
         top_right_ratio = ((input_width - center_x) * center_y) / area
         bottom_left_ratio = (center_x * (input_height - center_y)) / area
         bottom_right_ratio = (
             (input_width - center_x) * (input_height - center_y)
         ) / area
-        label = (
-            labels_for_mosaic[0] * top_left_ratio
-            + labels_for_mosaic[1] * top_right_ratio
-            + labels_for_mosaic[2] * bottom_left_ratio
-            + labels_for_mosaic[3] * bottom_right_ratio
+        labels = (
+            labels_for_mosaic[:, 0] * top_left_ratio[:, tf.newaxis]
+            + labels_for_mosaic[:, 1] * top_right_ratio[:, tf.newaxis]
+            + labels_for_mosaic[:, 2] * bottom_left_ratio[:, tf.newaxis]
+            + labels_for_mosaic[:, 3] * bottom_right_ratio[:, tf.newaxis]
         )
-        return label
+        return labels
 
-    def _update_bounding_box(
-        self, images, bounding_boxes, permutation_order, translate_x, translate_y, index
+    def augment_bounding_boxes(
+        self, bounding_boxes, transformations, images=None, **kwargs
     ):
-        # updates bounding_boxes for one output mosaic
+        batch_size = tf.shape(images)[0]
+        input_height, input_width, _ = images.shape[1:]
+        bounding_boxes = bounding_box.to_dense(bounding_boxes)
         bounding_boxes = bounding_box.convert_format(
             bounding_boxes,
             source=self.bounding_box_format,
             target="xyxy",
             images=images,
             dtype=self.compute_dtype,
         )
         boxes, classes = bounding_boxes["boxes"], bounding_boxes["classes"]
 
-        classes_for_mosaic = tf.gather(classes, permutation_order[index])
-        boxes_for_mosaic = tf.gather(boxes, permutation_order[index])
-
-        # stacking translate values such that the shape is (4, 1, 4) or (num_images, broadcast dim, coordinates)
-        translate_values = tf.stack(
+        # values to translate the boxes by in the mosaic image
+        mosaic_centers = transformations["mosaic_centers"]
+        mosaic_centers_x = mosaic_centers[..., 0] * input_width
+        mosaic_centers_y = mosaic_centers[..., 1] * input_height
+        translate_x = tf.stack(
             [
-                translate_x[index],
-                translate_y[index],
-                translate_x[index],
-                translate_y[index],
+                mosaic_centers_x - input_width,
+                mosaic_centers_x,
+                mosaic_centers_x - input_width,
+                mosaic_centers_x,
             ],
             axis=-1,
         )
-        translate_values = tf.expand_dims(translate_values, axis=1)
-        # translating boxes
-        boxes_for_mosaic = boxes_for_mosaic + translate_values
-
-        boxes_for_mosaic = tf.reshape(boxes_for_mosaic, [-1, 4])
-        classes_for_mosaic = tf.reshape(
-            classes_for_mosaic,
+        translate_y = tf.stack(
             [
-                -1,
+                mosaic_centers_y - input_height,
+                mosaic_centers_y - input_height,
+                mosaic_centers_y,
+                mosaic_centers_y,
             ],
+            axis=-1,
         )
+        # updates bounding_boxes for one output mosaic
+        permutation_order = transformations["permutation_order"]
+        classes_for_mosaic = tf.gather(classes, permutation_order)
+        boxes_for_mosaic = tf.gather(boxes, permutation_order)
 
-        boxes_for_mosaic = {"boxes": boxes_for_mosaic, "classes": classes_for_mosaic}
+        # stacking translate values such that the shape is (B, 4, 1, 4) or
+        # (batch_size, num_images, broadcast dim, coordinates)
+        translate_values = tf.stack(
+            [translate_x, translate_y, translate_x, translate_y], axis=-1
+        )
+        translate_values = tf.expand_dims(translate_values, axis=2)
+        # translating boxes
+        boxes_for_mosaic = boxes_for_mosaic + translate_values
+        boxes_for_mosaic = tf.reshape(boxes_for_mosaic, [batch_size, -1, 4])
+        classes_for_mosaic = tf.reshape(classes_for_mosaic, [batch_size, -1])
+        boxes_for_mosaic = {
+            "boxes": boxes_for_mosaic,
+            "classes": classes_for_mosaic,
+        }
         boxes_for_mosaic = bounding_box.clip_to_image(
             boxes_for_mosaic,
             bounding_box_format="xyxy",
-            images=images[index],
+            images=images,
         )
-        boxes_for_mosaic = bounding_box.to_ragged(boxes_for_mosaic)
         boxes_for_mosaic = bounding_box.convert_format(
             boxes_for_mosaic,
             source="xyxy",
             target=self.bounding_box_format,
-            images=images[index],
+            images=images,
             dtype=self.compute_dtype,
         )
         return boxes_for_mosaic
 
+    def _batch_augment(self, inputs):
+        self._validate_inputs(inputs)
+        return super()._batch_augment(inputs)
+
+    def call(self, inputs):
+        _, metadata = self._format_inputs(inputs)
+        if metadata[BATCHED] is not True:
+            raise ValueError(
+                "Mosaic received a single image to `call`. The "
+                "layer relies on combining multiple examples, and as such "
+                "will not behave as expected. Please call the layer with 4 "
+                "or more samples."
+            )
+        return super().call(inputs=inputs)
+
     def _validate_inputs(self, inputs):
-        images = inputs.get("images", None)
-        labels = inputs.get("labels", None)
-        bounding_boxes = inputs.get("bounding_boxes", None)
+        images = inputs.get(IMAGES, None)
+        labels = inputs.get(LABELS, None)
+        bounding_boxes = inputs.get(BOUNDING_BOXES, None)
         if images is None or (labels is None and bounding_boxes is None):
             raise ValueError(
                 "Mosaic expects inputs in a dictionary with format "
                 '{"images": images, "labels": labels}. or'
                 '{"images": images, "bounding_boxes": bounding_boxes}'
                 f"Got: inputs = {inputs}"
             )
@@ -316,7 +306,11 @@
             "offset": self.offset,
             "bounding_box_format": self.bounding_box_format,
             "seed": self.seed,
         }
         base_config = super().get_config()
 
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/mosaic_test.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,34 +11,38 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing.mosaic import Mosaic
 
-classes = 10
+num_classes = 10
 
 
 class MosaicTest(tf.test.TestCase):
     def test_return_shapes(self):
         xs = tf.ones((2, 512, 512, 3))
         # randomly sample labels
         ys_labels = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 2)
         ys_labels = tf.squeeze(ys_labels)
-        ys_labels = tf.one_hot(ys_labels, classes)
+        ys_labels = tf.one_hot(ys_labels, num_classes)
 
         # randomly sample bounding boxes
         ys_bounding_boxes = {
             "boxes": tf.random.uniform((2, 3, 4), 0, 1),
             "classes": tf.random.uniform((2, 3), 0, 1),
         }
         layer = Mosaic(bounding_box_format="xywh")
         # mosaic on labels
         outputs = layer(
-            {"images": xs, "labels": ys_labels, "bounding_boxes": ys_bounding_boxes}
+            {
+                "images": xs,
+                "labels": ys_labels,
+                "bounding_boxes": ys_bounding_boxes,
+            }
         )
         xs, ys_labels, ys_bounding_boxes = (
             outputs["images"],
             outputs["labels"],
             outputs["bounding_boxes"],
         )
 
@@ -67,19 +71,23 @@
         xs, ys = outputs["images"], outputs["labels"]
 
         self.assertEqual(xs.shape, [2, 4, 4, 3])
         self.assertEqual(ys.shape, [2, 2])
 
     def test_image_input_only(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0),
+            tf.stack(
+                [2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0
+            ),
             tf.float32,
         )
         layer = Mosaic()
-        with self.assertRaisesRegexp(ValueError, "expects inputs in a dictionary"):
+        with self.assertRaisesRegexp(
+            ValueError, "expects inputs in a dictionary"
+        ):
             _ = layer(xs)
 
     def test_single_image_input(self):
         xs = tf.ones((512, 512, 3))
         ys = tf.one_hot(tf.constant([1]), 2)
         inputs = {"images": xs, "labels": ys}
         layer = Mosaic()
@@ -89,15 +97,17 @@
             _ = layer(inputs)
 
     def test_int_labels(self):
         xs = tf.ones((2, 512, 512, 3))
         ys = tf.one_hot(tf.constant([1, 0]), 2, dtype=tf.int32)
         inputs = {"images": xs, "labels": ys}
         layer = Mosaic()
-        with self.assertRaisesRegexp(ValueError, "Mosaic received labels with type"):
+        with self.assertRaisesRegexp(
+            ValueError, "Mosaic received labels with type"
+        ):
             _ = layer(inputs)
 
     def test_image_input(self):
         xs = tf.ones((2, 512, 512, 3))
         layer = Mosaic()
         with self.assertRaisesRegexp(
             ValueError, "Mosaic expects inputs in a dictionary with format"
```

## keras_cv/layers/preprocessing/posterization.py

```diff
@@ -7,72 +7,74 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils.preprocessing import transform_value_range
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class Posterization(BaseImageAugmentationLayer):
     """Reduces the number of bits for each color channel.
 
     References:
-    - [AutoAugment: Learning Augmentation Policies from Data](
-        https://arxiv.org/abs/1805.09501
-    )
-    - [RandAugment: Practical automated data augmentation with a reduced search space](
-        https://arxiv.org/abs/1909.13719
-    )
+    - [AutoAugment: Learning Augmentation Policies from Data](https://arxiv.org/abs/1805.09501)
+    - [RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/abs/1909.13719)
 
     Args:
-        value_range: a tuple or a list of two elements. The first value represents
-            the lower bound for values in passed images, the second represents the
-            upper bound. Images passed to the layer should have values within
-            `value_range`. Defaults to `(0, 255)`.
-        bits: integer. The number of bits to keep for each channel. Must be a value
-            between 1-8.
+        value_range: a tuple or a list of two elements. The first value
+            represents the lower bound for values in passed images, the second
+            represents the upper bound. Images passed to the layer should have
+            values within `value_range`. Defaults to `(0, 255)`.
+        bits: integer, the number of bits to keep for each channel. Must be a
+            value between 1-8.
 
      Usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     print(images[0, 0, 0])
     # [59 62 63]
-    # Note that images are Tensors with values in the range [0, 255] and uint8 dtype
+    # Note that images are Tensors with values in the range [0, 255] and uint8
+    dtype
     posterization = Posterization(bits=4, value_range=[0, 255])
     images = posterization(images)
     print(images[0, 0, 0])
     # [48., 48., 48.]
-    # NOTE: the layer will output values in tf.float32, regardless of input dtype.
+    # NOTE: the layer will output values in tf.float32, regardless of input
+        dtype.
     ```
 
      Call arguments:
         inputs: input tensor in two possible formats:
             1. single 3D (HWC) image or 4D (NHWC) batch of images.
             2. A dict of tensors where the images are under `"images"` key.
-    """
+    """  # noqa: E501
 
     def __init__(self, value_range, bits, **kwargs):
         super().__init__(**kwargs)
 
         if not len(value_range) == 2:
             raise ValueError(
                 "value_range must be a sequence of two elements. "
                 f"Received: {value_range}"
             )
 
         if not (0 < bits < 9):
-            raise ValueError(f"Bits value must be between 1-8. Received bits: {bits}.")
+            raise ValueError(
+                f"Bits value must be between 1-8. Received bits: {bits}."
+            )
 
         self._shift = 8 - bits
         self._value_range = value_range
 
     def augment_image(self, image, **kwargs):
         image = transform_value_range(
             images=image,
@@ -90,20 +92,22 @@
             target_range=self._value_range,
             dtype=self.compute_dtype,
         )
 
     def augment_bounding_boxes(self, bounding_boxes, **kwargs):
         return bounding_boxes
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return segmentation_mask
 
     def _batch_augment(self, inputs):
-        # Skip the use of vectorized_map or map_fn as the implementation is already
-        # vectorized
+        # Skip the use of vectorized_map or map_fn as the implementation is
+        # already vectorized
         return self._augment(inputs)
 
     def _posterize(self, image):
         return tf.bitwise.left_shift(
             tf.bitwise.right_shift(image, self._shift), self._shift
         )
```

## keras_cv/layers/preprocessing/posterization_test.py

```diff
@@ -39,20 +39,24 @@
 
         layer = Posterization(bits=bits, value_range=[0, 255])
         output = layer(dummy_input)
 
         self.assertAllEqual(output, expected_output)
 
     def _get_random_bits(self):
-        return int(self.rng.uniform(shape=(), minval=1, maxval=9, dtype=tf.int32))
+        return int(
+            self.rng.uniform(shape=(), minval=1, maxval=9, dtype=tf.int32)
+        )
 
     def test_single_image_rescaled(self):
         bits = self._get_random_bits()
         dummy_input = self.rng.uniform(shape=(224, 224, 3), maxval=1.0)
-        expected_output = self._calc_expected_output(dummy_input * 255, bits=bits) / 255
+        expected_output = (
+            self._calc_expected_output(dummy_input * 255, bits=bits) / 255
+        )
 
         layer = Posterization(bits=bits, value_range=[0, 1])
         output = layer(dummy_input)
 
         self.assertAllClose(output, expected_output)
 
     def test_batched_input(self):
@@ -80,16 +84,16 @@
         apply(dummy_input)
 
     @staticmethod
     def _calc_expected_output(image, bits):
         """Posterization in numpy, based on Albumentations:
 
         The algorithm is basically:
-        1. create a lookup table of all possible input pixel values to pixel values
-            after posterize
+        1. create a lookup table of all possible input pixel values to pixel
+            values after posterize
         2. map each pixel in the input to created lookup table.
 
         Source:
             https://github.com/albumentations-team/albumentations/blob/89a675cbfb2b76f6be90e7049cd5211cb08169a5/albumentations/augmentations/functional.py#L407
         """
         dtype = image.dtype
         image = tf.cast(image, tf.uint8)
```

## keras_cv/layers/preprocessing/ragged_image_test.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -56,94 +56,120 @@
             "contrast_factor": (0.5, 0.9),
             "saturation_factor": (0.5, 0.9),
             "hue_factor": (0.5, 0.9),
             "seed": 1,
         },
     ),
     (
+        "RandomContrast",
+        layers.RandomContrast,
+        {"value_range": (0, 255), "factor": 0.5},
+    ),
+    (
         "RandomGaussianBlur",
         layers.RandomGaussianBlur,
         {"kernel_size": 3, "factor": (0.0, 3.0)},
     ),
     ("RandomFlip", layers.RandomFlip, {"mode": "horizontal"}),
     ("RandomJpegQuality", layers.RandomJpegQuality, {"factor": (75, 100)}),
+    ("RandomRotation", layers.RandomRotation, {"factor": 0.5}),
     ("RandomSaturation", layers.RandomSaturation, {"factor": 0.5}),
     (
         "RandomSharpness",
         layers.RandomSharpness,
         {"factor": 0.5, "value_range": (0, 255)},
     ),
-    ("RandomShear", layers.RandomShear, {"x_factor": 0.3, "x_factor": 0.3}),
+    ("RandomShear", layers.RandomShear, {"x_factor": 0.3, "y_factor": 0.3}),
+    (
+        "RandomTranslation",
+        layers.RandomTranslation,
+        {"height_factor": 0.5, "width_factor": 0.5},
+    ),
+    (
+        "RandomZoom",
+        layers.RandomZoom,
+        {"height_factor": 0.2, "width_factor": 0.5},
+    ),
     ("Solarization", layers.Solarization, {"value_range": (0, 255)}),
+    (
+        "RandomBrightness",
+        layers.RandomBrightness,
+        {"factor": (1, 1), "value_range": (0, 1)},
+    ),
 ]
 
 DENSE_OUTPUT_TEST_CONFIGURATIONS = [
     (
-        "RandomCropAndResize",
-        layers.RandomCropAndResize,
+        "JitteredResize",
+        layers.JitteredResize,
         {
             "target_size": (224, 224),
-            "crop_area_factor": (0.8, 1.0),
-            "aspect_ratio_factor": (3 / 4, 4 / 3),
+            "scale_factor": (0.8, 1.25),
+            "bounding_box_format": "xywh",
         },
     ),
     (
-        "Resizing",
-        layers.Resizing,
-        {
-            "height": 224,
-            "width": 224,
-        },
+        "RandomCrop",
+        layers.RandomCrop,
+        {"height": 2, "width": 2},
     ),
     (
-        "JitteredResize",
-        layers.JitteredResize,
+        "RandomCropAndResize",
+        layers.RandomCropAndResize,
         {
             "target_size": (224, 224),
-            "scale_factor": (0.8, 1.25),
-            "bounding_box_format": "xywh",
+            "crop_area_factor": (0.8, 1.0),
+            "aspect_ratio_factor": (3 / 4, 4 / 3),
         },
     ),
     (
         "RandomlyZoomedCrop",
         layers.RandomlyZoomedCrop,
         {
             "height": 224,
             "width": 224,
             "zoom_factor": (0.8, 1.0),
             "aspect_ratio_factor": (3 / 4, 4 / 3),
         },
     ),
+    (
+        "Resizing",
+        layers.Resizing,
+        {
+            "height": 224,
+            "width": 224,
+        },
+    ),
 ]
 
 RAGGED_OUTPUT_TEST_CONFIGURATIONS = [
     ("RandomAspectRatio", layers.RandomAspectRatio, {"factor": (0.9, 1.1)}),
 ]
 
 
 class RaggedImageTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(*CONSISTENT_OUTPUT_TEST_CONFIGURATIONS)
     def test_preserves_ragged_status(self, layer_cls, init_args):
         layer = layer_cls(**init_args)
         inputs = tf.ragged.stack(
             [
-                tf.ones((512, 512, 3)),
-                tf.ones((600, 300, 3)),
+                tf.ones((5, 5, 3)),
+                tf.ones((8, 8, 3)),
             ]
         )
         outputs = layer(inputs)
         self.assertTrue(isinstance(outputs, tf.RaggedTensor))
 
     @parameterized.named_parameters(*DENSE_OUTPUT_TEST_CONFIGURATIONS)
     def test_converts_ragged_to_dense(self, layer_cls, init_args):
         layer = layer_cls(**init_args)
         inputs = tf.ragged.stack(
             [
-                tf.ones((512, 512, 3)),
-                tf.ones((600, 300, 3)),
+                tf.ones((5, 5, 3)),
+                tf.ones((8, 8, 3)),
             ]
         )
         outputs = layer(inputs)
         self.assertTrue(isinstance(outputs, tf.Tensor))
 
     @parameterized.named_parameters(*RAGGED_OUTPUT_TEST_CONFIGURATIONS)
     def test_dense_to_ragged(self, layer_cls, init_args):
```

## keras_cv/layers/preprocessing/rand_augment.py

```diff
@@ -1,37 +1,38 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import tensorflow as tf
+
+from tensorflow import keras
 
 from keras_cv import core
 from keras_cv.layers import preprocessing as cv_preprocessing
 from keras_cv.layers.preprocessing.random_augmentation_pipeline import (
     RandomAugmentationPipeline,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandAugment(RandomAugmentationPipeline):
     """RandAugment performs the Rand Augment operation on input images.
 
-    This layer can be thought of as an all in one image augmentation layer.  The policy
-    implemented by this layer has been benchmarked extensively and is effective on a
-    wide variety of datasets.
+    This layer can be thought of as an all-in-one image augmentation layer. The
+    policy implemented by this layer has been benchmarked extensively and is
+    effective on a wide variety of datasets.
 
     The policy operates as follows:
 
     For each augmentation in the range `[0, augmentations_per_image]`,
     the policy selects a random operation from a list of operations.
     It then samples a random number and if that number is less than
     `rate` applies it to the given image.
@@ -39,39 +40,41 @@
     References:
         - [RandAugment](https://arxiv.org/abs/1909.13719)
 
     Args:
         value_range: the range of values the incoming images will have.
             Represented as a two number tuple written [low, high].
             This is typically either `[0, 1]` or `[0, 255]` depending
-            on how your preprocessing pipeline is setup.
-        augmentations_per_image: the number of layers to use in the rand augment policy.
-            Defaults to `3`.
-        magnitude: magnitude is the mean of the normal distribution used to sample the
-            magnitude used for each data augmentation.  magnitude should
-            be a float in the range `[0, 1]`.  A magnitude of `0` indicates that the
-            augmentations are as weak as possible (not recommended), while a value of
-            `1.0` implies use of the strongest possible augmentation.  All magnitudes
-            are clipped to the range `[0, 1]` after sampling.  Defaults to `0.5`.
-        magnitude_stddev: the standard deviation to use when drawing values
-            for the perturbations.  Keep in mind magnitude will still be clipped to the
-            range `[0, 1]` after samples are drawn from the normal distribution.
-            Defaults to `0.15`.
-        rate:  the rate at which to apply each augmentation.  This parameter is applied
-            on a per-distortion layer, per image.  Should be in the range `[0, 1]`.
-            To reproduce the original RandAugment paper results, set this to `10/11`.
-            The original `RandAugment` paper includes an Identity transform.  By setting
-            the rate to 10/11 in our implementation, the behavior is identical to
-            sampling an Identity augmentation 10/11th of the time.
-            Defaults to `1.0`.
-        geometric: whether or not to include geometric augmentations.  This should be
-            set to False when performing object detection.  Defaults to True.
+            on how your preprocessing pipeline is set up.
+        augmentations_per_image: the number of layers to use in the rand augment
+            policy, defaults to `3`.
+        magnitude: magnitude is the mean of the normal distribution used to
+            sample the magnitude used for each data augmentation. Magnitude
+            should be a float in the range `[0, 1]`. A magnitude of `0`
+            indicates that the augmentations are as weak as possible (not
+            recommended), while a value of `1.0` implies use of the strongest
+            possible augmentation. All magnitudes are clipped to the range
+            `[0, 1]` after sampling. Defaults to `0.5`.
+        magnitude_stddev: the standard deviation to use when drawing values for
+            the perturbations. Keep in mind magnitude will still be clipped to
+            the range `[0, 1]` after samples are drawn from the normal
+            distribution. Defaults to `0.15`.
+        rate: the rate at which to apply each augmentation. This parameter is
+            applied on a per-distortion layer, per image. Should be in the range
+            `[0, 1]`. To reproduce the original RandAugment paper results, set
+            this to `10/11`. The original `RandAugment` paper includes an
+            Identity transform. By setting the rate to 10/11 in our
+            implementation, the behavior is identical to sampling an Identity
+            augmentation 10/11th of the time. Defaults to `1.0`.
+        geometric: whether to include geometric augmentations. This
+            should be set to False when performing object detection. Defaults to
+            True.
     Usage:
     ```python
-    (x_test, y_test), _ = tf.keras.datasets.cifar10.load_data()
+    (x_test, y_test), _ = keras.datasets.cifar10.load_data()
     rand_augment = keras_cv.layers.RandAugment(
         value_range=(0, 255), augmentations_per_image=3, magnitude=0.5
     )
     x_test = rand_augment(x_test)
     ```
     """
 
@@ -82,29 +85,34 @@
         magnitude=0.5,
         magnitude_stddev=0.15,
         rate=10 / 11,
         geometric=True,
         seed=None,
         **kwargs,
     ):
-        # As an optimization RandAugment makes all internal layers use (0, 255) while
+        # As an optimization RandAugment makes all internal layers use (0, 255)
         # and we handle range transformation at the _augment level.
         if magnitude < 0.0 or magnitude > 1:
             raise ValueError(
-                f"`magnitude` must be in the range [0, 1], got `magnitude={magnitude}`"
+                "`magnitude` must be in the range [0, 1], got "
+                f"`magnitude={magnitude}`"
             )
         if magnitude_stddev < 0.0 or magnitude_stddev > 1:
             raise ValueError(
                 "`magnitude_stddev` must be in the range [0, 1], got "
                 f"`magnitude_stddev={magnitude}`"
             )
 
         super().__init__(
             layers=RandAugment.get_standard_policy(
-                (0, 255), magnitude, magnitude_stddev, geometric=geometric, seed=seed
+                (0, 255),
+                magnitude,
+                magnitude_stddev,
+                geometric=geometric,
+                seed=seed,
             ),
             augmentations_per_image=augmentations_per_image,
             rate=rate,
             **kwargs,
             seed=seed,
         )
         self.magnitude = float(magnitude)
@@ -137,32 +145,40 @@
             **policy["equalize"], value_range=value_range, seed=seed
         )
 
         solarize = cv_preprocessing.Solarization(
             **policy["solarize"], value_range=value_range, seed=seed
         )
 
-        color = cv_preprocessing.RandomColorDegeneration(**policy["color"], seed=seed)
-        contrast = cv_preprocessing.RandomContrast(**policy["contrast"], seed=seed)
+        color = cv_preprocessing.RandomColorDegeneration(
+            **policy["color"], seed=seed
+        )
+        contrast = cv_preprocessing.RandomContrast(
+            **policy["contrast"], value_range=value_range, seed=seed
+        )
         brightness = cv_preprocessing.RandomBrightness(
             **policy["brightness"], value_range=value_range, seed=seed
         )
 
         layers = [
             auto_contrast,
             equalize,
             solarize,
             color,
             contrast,
             brightness,
         ]
 
         if geometric:
-            shear_x = cv_preprocessing.RandomShear(**policy["shear_x"], seed=seed)
-            shear_y = cv_preprocessing.RandomShear(**policy["shear_y"], seed=seed)
+            shear_x = cv_preprocessing.RandomShear(
+                **policy["shear_x"], seed=seed
+            )
+            shear_y = cv_preprocessing.RandomShear(
+                **policy["shear_y"], seed=seed
+            )
             translate_x = cv_preprocessing.RandomTranslation(
                 **policy["translate_x"], seed=seed
             )
             translate_y = cv_preprocessing.RandomTranslation(
                 **policy["translate_y"], seed=seed
             )
             layers += [shear_x, shear_y, translate_x, translate_y]
@@ -192,29 +208,33 @@
 
 def equalize_policy(magnitude, magnitude_stddev):
     return {}
 
 
 def solarize_policy(magnitude, magnitude_stddev):
     # We cap additions at 110, because if we add more than 110 we will be nearly
-    # nullifying the information contained in the image, making the model train on noise
+    # nullifying the information contained in the image, making the model train
+    # on noise
     maximum_addition_value = 110
     addition_factor = core.NormalFactorSampler(
         mean=magnitude * maximum_addition_value,
         stddev=magnitude_stddev * maximum_addition_value,
         min_value=0,
         max_value=maximum_addition_value,
     )
     threshold_factor = core.NormalFactorSampler(
         mean=(255 - (magnitude * 255)),
         stddev=(magnitude_stddev * 255),
         min_value=0,
         max_value=255,
     )
-    return {"addition_factor": addition_factor, "threshold_factor": threshold_factor}
+    return {
+        "addition_factor": addition_factor,
+        "threshold_factor": threshold_factor,
+    }
 
 
 def color_policy(magnitude, magnitude_stddev):
     factor = core.NormalFactorSampler(
         mean=magnitude,
         stddev=magnitude_stddev,
         min_value=0,
```

## keras_cv/layers/preprocessing/rand_augment_test.py

```diff
@@ -35,19 +35,24 @@
     @parameterized.named_parameters(
         ("0_255", 0, 255),
         ("neg_1_1", -1, 1),
         ("0_1", 0, 1),
     )
     def test_runs_with_value_range(self, low, high):
         rand_augment = layers.RandAugment(
-            augmentations_per_image=3, magnitude=0.5, rate=1.0, value_range=(low, high)
+            augmentations_per_image=3,
+            magnitude=0.5,
+            rate=1.0,
+            value_range=(low, high),
         )
         xs = tf.random.uniform((2, 512, 512, 3), low, high, dtype=tf.float32)
         ys = rand_augment(xs)
-        self.assertTrue(tf.math.reduce_all(tf.logical_and(ys >= low, ys <= high)))
+        self.assertTrue(
+            tf.math.reduce_all(tf.logical_and(ys >= low, ys <= high))
+        )
 
     @parameterized.named_parameters(
         ("float32", tf.float32),
         ("int32", tf.int32),
         ("uint8", tf.uint8),
     )
     def test_runs_with_dtype_input(self, dtype):
@@ -71,27 +76,37 @@
         xs = tf.random.uniform((2, 512, 512, 3), lower, upper, dtype=tf.float32)
         ys = rand_augment(xs)
         self.assertLessEqual(tf.math.reduce_max(ys), upper)
         self.assertGreaterEqual(tf.math.reduce_min(ys), lower)
 
     def test_runs_unbatched(self):
         rand_augment = layers.RandAugment(
-            augmentations_per_image=3, magnitude=0.5, rate=1.0, value_range=(0, 255)
+            augmentations_per_image=3,
+            magnitude=0.5,
+            rate=1.0,
+            value_range=(0, 255),
         )
         xs = tf.random.uniform((512, 512, 3), 0, 255, dtype=tf.float32)
         ys = rand_augment(xs)
         self.assertEqual(xs.shape, ys.shape)
 
     def test_runs_no_geo(self):
         rand_augment = layers.RandAugment(
             augmentations_per_image=2,
             magnitude=0.5,
             rate=1.0,
             geometric=False,
             value_range=(0, 255),
         )
         self.assertFalse(
-            any([isinstance(x, layers.RandomTranslation) for x in rand_augment.layers])
+            any(
+                [
+                    isinstance(x, layers.RandomTranslation)
+                    for x in rand_augment.layers
+                ]
+            )
         )
         self.assertFalse(
-            any([isinstance(x, layers.RandomShear) for x in rand_augment.layers])
+            any(
+                [isinstance(x, layers.RandomShear) for x in rand_augment.layers]
+            )
         )
```

## keras_cv/layers/preprocessing/random_aspect_ratio.py

```diff
@@ -7,33 +7,36 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 import keras_cv
 from keras_cv import bounding_box
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomAspectRatio(BaseImageAugmentationLayer):
-    """RandomAspectRatio randomly distorts the aspect ratio of the provided image.
+    """RandomAspectRatio randomly distorts the aspect ratio of the provided
+    image.
 
-    This is done on an element-wise basis, and as a consequence this layer always
-    returns a tf.RaggedTensor.
+    This is done on an element-wise basis, and as a consequence this layer
+    always returns a tf.RaggedTensor.
 
     Args:
-        factor: a range of values in the range `(0, infinity)` that determines the
-            percentage to distort the aspect ratio of each image by.
+        factor: a range of values in the range `(0, infinity)` that determines
+            the percentage to distort the aspect ratio of each image by.
         interpolation: interpolation method used in the `Resize` op.
              Supported values are `"nearest"` and `"bilinear"`.
              Defaults to `"bilinear"`.
     """
 
     def __init__(
         self,
@@ -42,15 +45,19 @@
         bounding_box_format=None,
         seed=None,
         **kwargs
     ):
         super().__init__(**kwargs)
         self.interpolation = keras_cv.utils.get_interpolation(interpolation)
         self.factor = keras_cv.utils.parse_factor(
-            factor, min_value=0.0, max_value=None, seed=seed, param_name="factor"
+            factor,
+            min_value=0.0,
+            max_value=None,
+            seed=seed,
+            param_name="factor",
         )
         self.bounding_box_format = bounding_box_format
         self.seed = seed
         self.auto_vectorize = False
         self.force_output_ragged_images = True
 
     def get_random_transformation(self, **kwargs):
@@ -59,15 +66,17 @@
     def compute_image_signature(self, images):
         return tf.RaggedTensorSpec(
             shape=(None, None, images.shape[-1]),
             ragged_rank=1,
             dtype=self.compute_dtype,
         )
 
-    def augment_bounding_boxes(self, bounding_boxes, transformation, image, **kwargs):
+    def augment_bounding_boxes(
+        self, bounding_boxes, transformation, image, **kwargs
+    ):
         if self.bounding_box_format is None:
             raise ValueError(
                 "Please provide a `bounding_box_format` when augmenting "
                 "bounding boxes with `RandomAspectRatio()`."
             )
         bounding_boxes = bounding_boxes.copy()
         img_shape = tf.shape(image)
@@ -101,15 +110,17 @@
         # images....transformation
         img_shape = tf.cast(tf.shape(image), self.compute_dtype)
         height, width = img_shape[0], img_shape[1]
         height = height / transformation
         width = width * transformation
 
         target_size = tf.cast(tf.stack([height, width]), tf.int32)
-        result = tf.image.resize(image, size=target_size, method=self.interpolation)
+        result = tf.image.resize(
+            image, size=target_size, method=self.interpolation
+        )
         return tf.cast(result, self.compute_dtype)
 
     def augment_label(self, label, transformation, **kwargs):
         return label
 
     def get_config(self):
         config = {
```

## keras_cv/layers/preprocessing/random_aspect_ratio_test.py

```diff
@@ -23,23 +23,14 @@
         input_image_shape = (8, 100, 100, 3)
         image = tf.random.uniform(shape=input_image_shape)
 
         layer = layers.RandomAspectRatio(factor=(0.9, 1.1))
         output = layer(image, training=True)
         self.assertNotEqual(output.shape, image.shape)
 
-    def test_inference_preserves_image(self):
-        # Checks if original and augmented images are different
-        input_image_shape = (8, 100, 100, 3)
-        image = tf.random.uniform(shape=input_image_shape)
-
-        layer = layers.RandomAspectRatio(factor=(0.9, 1.1))
-        output = layer(image, training=False)
-        self.assertAllClose(image, output)
-
     def test_grayscale(self):
         # Checks if original and augmented images are different
         input_image_shape = (8, 100, 100, 1)
         image = tf.random.uniform(shape=input_image_shape, seed=1223)
 
         layer = layers.RandomAspectRatio(factor=(0.9, 1.1))
         output = layer(image, training=True)
@@ -55,10 +46,12 @@
             "classes": tf.ragged.constant([[0, 0], [0]], dtype=tf.float32),
         }
         input = {"images": image, "bounding_boxes": bounding_boxes}
         layer = layers.RandomAspectRatio(
             factor=(0.9, 1.1), bounding_box_format="rel_xywh"
         )
         output = layer(input, training=True)
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
         bounding_boxes = bounding_box.to_dense(bounding_boxes)
         self.assertAllClose(bounding_boxes, output["bounding_boxes"])
```

## keras_cv/layers/preprocessing/random_augmentation_pipeline.py

```diff
@@ -7,65 +7,71 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers import preprocessing
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomAugmentationPipeline(BaseImageAugmentationLayer):
-    """RandomAugmentationPipeline constructs a pipeline based on provided arguments.
+    """RandomAugmentationPipeline constructs a pipeline based on provided
+    arguments.
 
-    The implemented policy does the following: for each inputs provided in `call`(), the
-    policy first inputs a random number, if the number is < rate, the policy then
-    selects a random layer from the provided list of `layers`.  It then calls the
-    `layer()` on the inputs.  This is done `augmentations_per_image` times.
+    The implemented policy does the following: for each input provided in
+    `call`(), the policy first inputs a random number, if the number is < rate,
+    the policy then selects a random layer from the provided list of `layers`.
+    It then calls the `layer()` on the inputs. This is done
+    `augmentations_per_image` times.
 
     This layer can be used to create custom policies resembling `RandAugment` or
     `AutoAugment`.
 
     Usage:
     ```python
     # construct a list of layers
     layers = keras_cv.layers.RandAugment.get_standard_policy(
         value_range=(0, 255), magnitude=0.75, magnitude_stddev=0.3
     )
-    layers = layers[:4]  # slice out some layers you don't want for whatever reason
+    layers = layers[:4]  # slice out some layers you don't want for whatever
+                           reason
     layers = layers + [keras_cv.layers.GridMask()]
 
     # create the pipeline.
     pipeline = keras_cv.layers.RandomAugmentationPipeline(
         layers=layers, augmentations_per_image=3
     )
 
     augmented_images = pipeline(images)
     ```
 
     Args:
-        layers: a list of `keras.Layers`.  These are randomly inputs during
-            augmentation to augment the inputs passed in `call()`.  The layers passed
-            should subclass `BaseImageAugmentationLayer`. Passing `layers=[]`
-            would result in a no-op.
-        augmentations_per_image: the number of layers to apply to each inputs in the
-            `call()` method.
-        rate: the rate at which to apply each augmentation.  This is applied on a per
-            augmentation bases, so if `augmentations_per_image=3` and `rate=0.5`, the
-            odds an image will receive no augmentations is 0.5^3, or 0.5*0.5*0.5.
+        layers: a list of `keras.Layers`. These are randomly inputs during
+            augmentation to augment the inputs passed in `call()`. The layers
+            passed should subclass `BaseImageAugmentationLayer`. Passing
+            `layers=[]` would result in a no-op.
+        augmentations_per_image: the number of layers to apply to each inputs in
+            the `call()` method.
+        rate: the rate at which to apply each augmentation. This is applied on a
+            per augmentation bases, so if `augmentations_per_image=3` and
+            `rate=0.5`, the odds an image will receive no augmentations is
+            0.5^3, or 0.5*0.5*0.5.
         auto_vectorize: whether to use `tf.vectorized_map` or `tf.map_fn` to
-            apply the augmentations.  This offers a significant performance boost, but
-            can only be used if all the layers provided to the `layers` argument
-            support auto vectorization.
+            apply the augmentations. This offers a significant performance
+            boost, but can only be used if all the layers provided to the
+            `layers` argument support auto vectorization.
         seed: Integer. Used to create a random seed.
     """
 
     def __init__(
         self,
         layers,
         augmentations_per_image,
@@ -110,7 +116,16 @@
                 "auto_vectorize": self.auto_vectorize,
                 "rate": self.rate,
                 "layers": self.layers,
                 "seed": self.seed,
             }
         )
         return config
+
+    @classmethod
+    def from_config(cls, config):
+        layers = config.pop("layers", None)
+        if layers:
+            if isinstance(layers[0], dict):
+                layers = keras.utils.deserialize_keras_object(layers)
+            config["layers"] = layers
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py

```diff
@@ -7,36 +7,42 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
 from absl.testing import parameterized
+from tensorflow import keras
 
 from keras_cv import layers
 
 
-class AddOneToInputs(tf.keras.layers.Layer):
+class AddOneToInputs(keras.layers.Layer):
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
     def call(self, inputs):
         result = inputs.copy()
         result["images"] = inputs["images"] + 1
         return result
 
 
 class RandomAugmentationPipelineTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(("1", 1), ("3", 3), ("5", 5))
-    def test_calls_layers_augmentations_per_image_times(self, augmentations_per_image):
+    def test_calls_layers_augmentations_per_image_times(
+        self, augmentations_per_image
+    ):
         layer = AddOneToInputs()
         pipeline = layers.RandomAugmentationPipeline(
-            layers=[layer], augmentations_per_image=augmentations_per_image, rate=1.0
+            layers=[layer],
+            augmentations_per_image=augmentations_per_image,
+            rate=1.0,
         )
         xs = tf.random.uniform((2, 5, 5, 3), 0, 100, dtype=tf.float32)
         os = pipeline(xs)
 
         self.assertAllClose(xs + augmentations_per_image, os)
 
     def test_supports_empty_layers_argument(self):
@@ -65,24 +71,28 @@
 
     @parameterized.named_parameters(("1", 1), ("3", 3), ("5", 5))
     def test_calls_layers_augmentations_per_image_times_single_image(
         self, augmentations_per_image
     ):
         layer = AddOneToInputs()
         pipeline = layers.RandomAugmentationPipeline(
-            layers=[layer], augmentations_per_image=augmentations_per_image, rate=1.0
+            layers=[layer],
+            augmentations_per_image=augmentations_per_image,
+            rate=1.0,
         )
         xs = tf.random.uniform((5, 5, 3), 0, 100, dtype=tf.float32)
         os = pipeline(xs)
 
         self.assertAllClose(xs + augmentations_per_image, os)
 
     @parameterized.named_parameters(("1", 1), ("3", 3), ("5", 5))
     def test_respects_rate(self, augmentations_per_image):
         layer = AddOneToInputs()
         pipeline = layers.RandomAugmentationPipeline(
-            layers=[layer], augmentations_per_image=augmentations_per_image, rate=0.0
+            layers=[layer],
+            augmentations_per_image=augmentations_per_image,
+            rate=0.0,
         )
         xs = tf.random.uniform((2, 5, 5, 3), 0, 100, dtype=tf.float32)
         os = pipeline(xs)
 
         self.assertAllClose(xs, os)
```

## keras_cv/layers/preprocessing/random_brightness.py

```diff
@@ -1,104 +1,128 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
-from keras_cv.utils import preprocessing
+from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomBrightness(BaseImageAugmentationLayer):
-    """A preprocessing layer which randomly adjusts brightness during training.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomBrightness(VectorizedBaseImageAugmentationLayer):
+    """A preprocessing layer which randomly adjusts brightness.
+
     This layer will randomly increase/reduce the brightness for the input RGB
     images.
 
-    At inference time, the output will be identical to the input.
-    Call the layer with `training=True` to adjust the brightness of the input.
-
     Note that different brightness adjustment factors
-    will be apply to each the images in the batch.
+    will be applied to each the images in the batch.
 
     Args:
       factor: Float or a list/tuple of 2 floats between -1.0 and 1.0. The
         factor is used to determine the lower bound and upper bound of the
         brightness adjustment. A float value will be chosen randomly between
         the limits. When -1.0 is chosen, the output image will be black, and
         when 1.0 is chosen, the image will be fully white. When only one float
         is provided, eg, 0.2, then -0.2 will be used for lower bound and 0.2
         will be used for upper bound.
       value_range: Optional list/tuple of 2 floats for the lower and upper limit
-        of the values of the input data. Defaults to [0.0, 255.0]. Can be
+        of the values of the input data, defaults to [0.0, 255.0]. Can be
         changed to e.g. [0.0, 1.0] if the image input has been scaled before
-        this layer.  The brightness adjustment will be scaled to this range, and
+        this layer. The brightness adjustment will be scaled to this range, and
         the output values will be clipped to this range.
       seed: optional integer, for fixed RNG behavior.
     Inputs: 3D (HWC) or 4D (NHWC) tensor, with float or int dtype. Input pixel
       values can be of any range (e.g. `[0., 1.)` or `[0, 255]`)
     Output: 3D (HWC) or 4D (NHWC) tensor with brightness adjusted based on the
       `factor`. By default, the layer will output floats. The output value will
       be clipped to the range `[0, 255]`, the valid range of RGB colors, and
       rescaled based on the `value_range` if needed.
+
+    Usage:
+    ```python
+    (images, labels), _ = keras.datasets.cifar10.load_data()
+    random_brightness = keras_cv.layers.preprocessing.RandomBrightness()
+    augmented_images = random_brightness(images)
     ```
     """
 
     def __init__(self, factor, value_range=(0, 255), seed=None, **kwargs):
         super().__init__(seed=seed, force_generator=True, **kwargs)
         if isinstance(factor, float) or isinstance(factor, int):
             factor = (-factor, factor)
-        self.factor = preprocessing.parse_factor(factor, min_value=-1, max_value=1)
+        self.factor = preprocessing_utils.parse_factor(
+            factor, min_value=-1, max_value=1
+        )
         self.value_range = value_range
         self.seed = seed
 
-    def augment_image(self, image, transformation, **kwargs):
-        return self._brightness_adjust(image, transformation)
-
-    def augment_label(self, label, transformation, **kwargs):
-        return label
-
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
-
-    def augment_bounding_boxes(self, bounding_boxes, transformation=None, **kwargs):
-        return bounding_boxes
-
-    def get_random_transformation(self, **kwargs):
-        rgb_delta_shape = (1, 1, 1)
-        random_rgb_delta = self.factor(shape=rgb_delta_shape)
-        random_rgb_delta = random_rgb_delta * (
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        rgb_delta_shape = (batch_size, 1, 1, 1)
+        random_rgb_deltas = self.factor(shape=rgb_delta_shape)
+        random_rgb_deltas = random_rgb_deltas * (
             self.value_range[1] - self.value_range[0]
         )
-        return random_rgb_delta
+        return random_rgb_deltas
 
-    def _brightness_adjust(self, image, rgb_delta):
-        rank = image.shape.rank
-        if rank != 3:
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        image = tf.expand_dims(image, axis=0)
+        transformation = tf.expand_dims(transformation, axis=0)
+        image = self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
+        return tf.squeeze(image, axis=0)
+
+    def augment_images(self, images, transformations, **kwargs):
+        rank = images.shape.rank
+        if rank != 4:
             raise ValueError(
-                "Expected the input image to be rank 3. Got "
-                f"inputs.shape = {image.shape}"
+                "Expected the input image to be rank 4. Got "
+                f"inputs.shape = {images.shape}"
             )
-        rgb_delta = tf.cast(rgb_delta, image.dtype)
-        image += rgb_delta
-        return tf.clip_by_value(image, self.value_range[0], self.value_range[1])
+        rgb_deltas = tf.cast(transformations, images.dtype)
+        images += rgb_deltas
+        return tf.clip_by_value(
+            images, self.value_range[0], self.value_range[1]
+        )
+
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
+
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
+
+    def augment_bounding_boxes(self, bounding_boxes, transformations, **kwargs):
+        return bounding_boxes
 
     def get_config(self):
         config = {
             "factor": self.factor,
             "value_range": self.value_range,
             "seed": self.seed,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        if isinstance(config["factor"], dict):
+            config["factor"] = keras.utils.deserialize_keras_object(
+                config["factor"]
+            )
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_brightness_test.py

```diff
@@ -40,46 +40,58 @@
     def test_max_brightness(self):
         image_shape = (4, 8, 8, 3)
         image = tf.random.uniform(shape=image_shape) * 255.0
 
         layer = preprocessing.RandomBrightness(factor=(1, 1))
         output = layer(image)
 
-        self.assertAllClose(output, tf.fill((4, 8, 8, 3), 255), atol=1e-5, rtol=1e-5)
+        self.assertAllClose(
+            output, tf.fill((4, 8, 8, 3), 255), atol=1e-5, rtol=1e-5
+        )
 
     def test_max_brightness_rescaled_value_range(self):
         image_shape = (4, 8, 8, 3)
         image = tf.random.uniform(shape=image_shape)
 
-        layer = preprocessing.RandomBrightness(value_range=(0, 1), factor=(1, 1))
+        layer = preprocessing.RandomBrightness(
+            value_range=(0, 1), factor=(1, 1)
+        )
         output = layer(image)
 
-        self.assertAllClose(output, tf.fill((4, 8, 8, 3), 1), atol=1e-5, rtol=1e-5)
+        self.assertAllClose(
+            output, tf.fill((4, 8, 8, 3), 1), atol=1e-5, rtol=1e-5
+        )
 
     def test_zero_brightness(self):
         image_shape = (4, 8, 8, 3)
         image = tf.random.uniform(shape=image_shape) * 255.0
 
         layer = preprocessing.RandomBrightness(factor=(-1, -1))
         output = layer(image)
 
-        self.assertAllClose(output, tf.fill((4, 8, 8, 3), 0), atol=1e-5, rtol=1e-5)
+        self.assertAllClose(
+            output, tf.fill((4, 8, 8, 3), 0), atol=1e-5, rtol=1e-5
+        )
 
     def test_with_unit8(self):
         image_shape = (4, 8, 8, 3)
-        image = tf.cast(tf.random.uniform(shape=image_shape) * 255.0, dtype=tf.uint8)
+        image = tf.cast(
+            tf.random.uniform(shape=image_shape) * 255.0, dtype=tf.uint8
+        )
 
         layer = preprocessing.RandomBrightness(factor=0)
         output = layer(image)
         self.assertAllClose(image, output, atol=1e-5, rtol=1e-5)
 
         layer = preprocessing.RandomBrightness(factor=(0.3, 0.8))
         output = layer(image)
         self.assertNotAllClose(image, output)
 
     def test_config(self):
-        layer = preprocessing.RandomBrightness(value_range=(0, 1), factor=(0.3, 0.8))
+        layer = preprocessing.RandomBrightness(
+            value_range=(0, 1), factor=(0.3, 0.8)
+        )
         config = layer.get_config()
         self.assertTrue(isinstance(config["factor"], core.UniformFactorSampler))
         self.assertEqual(config["factor"].get_config()["lower"], 0.3)
         self.assertEqual(config["factor"].get_config()["upper"], 0.8)
         self.assertEqual(config["value_range"], (0, 1))
```

## keras_cv/layers/preprocessing/random_channel_shift.py

```diff
@@ -9,22 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomChannelShift(BaseImageAugmentationLayer):
     """Randomly shift values for each channel of the input image(s).
 
     The input images should have values in the `[0-255]` or `[0-1]` range.
 
     Input shape:
         3D (unbatched) or 4D (batched) tensor with shape:
@@ -34,28 +35,29 @@
         3D (unbatched) or 4D (batched) tensor with shape:
         `(..., height, width, channels)`, in `channels_last` format.
 
     Args:
         value_range: The range of values the incoming images will have.
             Represented as a two number tuple written [low, high].
             This is typically either `[0, 1]` or `[0, 255]` depending
-            on how your preprocessing pipeline is setup.
+            on how your preprocessing pipeline is set up.
         factor: A scalar value, or tuple/list of two floating values in
             the range `[0.0, 1.0]`. If `factor` is a single value, it will
             interpret as equivalent to the tuple `(0.0, factor)`. The `factor`
-            will sampled between its range for every image to augment.
-        channels: integer, the number of channels to shift.  Defaults to 3 which
-            corresponds to an RGB shift.  In some cases, there may ber more or less
-            channels.
+            will sample between its range for every image to augment.
+        channels: integer, the number of channels to shift, defaults to 3 which
+            corresponds to an RGB shift. In some cases, there may ber more or
+            less channels.
         seed: Integer. Used to create a random seed.
 
     Usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
-    rgb_shift = keras_cv.layers.RandomChannelShift(value_range=(0, 255), factor=0.5)
+    (images, labels), _ = keras.datasets.cifar10.load_data()
+    rgb_shift = keras_cv.layers.RandomChannelShift(value_range=(0, 255),
+        factor=0.5)
     augmented_images = rgb_shift(images)
     ```
     """
 
     def __init__(self, value_range, factor, channels=3, seed=None, **kwargs):
         super().__init__(**kwargs, seed=seed)
         self.seed = seed
@@ -97,15 +99,17 @@
 
     def augment_bounding_boxes(self, bounding_boxes, **kwargs):
         return bounding_boxes
 
     def augment_label(self, label, transformation=None, **kwargs):
         return label
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return segmentation_mask
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "factor": self.factor,
```

## keras_cv/layers/preprocessing/random_channel_shift_test.py

```diff
@@ -8,25 +8,26 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_cv.layers import preprocessing
 
 
 class RandomChannelShiftTest(tf.test.TestCase, parameterized.TestCase):
     def test_return_shapes(self):
         xs = tf.ones((2, 512, 512, 3))
-        layer = preprocessing.RandomChannelShift(factor=1.0, value_range=(0, 255))
+        layer = preprocessing.RandomChannelShift(
+            factor=1.0, value_range=(0, 255)
+        )
 
         xs = layer(xs, training=True)
         self.assertEqual(xs.shape, [2, 512, 512, 3])
 
     def test_non_square_image(self):
         xs = tf.cast(
             tf.stack(
@@ -41,18 +42,22 @@
 
         xs = layer(xs, training=True)
         self.assertFalse(tf.math.reduce_any(xs[0] == 2.0))
         self.assertFalse(tf.math.reduce_any(xs[1] == 1.0))
 
     def test_in_tf_function(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((100, 100, 3)), tf.ones((100, 100, 3))], axis=0),
+            tf.stack(
+                [2 * tf.ones((100, 100, 3)), tf.ones((100, 100, 3))], axis=0
+            ),
             dtype=tf.float32,
         )
-        layer = preprocessing.RandomChannelShift(factor=0.3, value_range=(0, 255))
+        layer = preprocessing.RandomChannelShift(
+            factor=0.3, value_range=(0, 255)
+        )
 
         @tf.function
         def augment(x):
             return layer(x, training=True)
 
         xs = augment(xs)
         self.assertFalse(tf.math.reduce_any(xs[0] == 2.0))
@@ -81,33 +86,31 @@
         self.assertFalse(tf.math.reduce_any(xs == 1.0))
 
     def test_in_single_image(self):
         xs = tf.cast(
             tf.ones((512, 512, 3)),
             dtype=tf.float32,
         )
-        layer = preprocessing.RandomChannelShift(factor=0.4, value_range=(0, 255))
+        layer = preprocessing.RandomChannelShift(
+            factor=0.4, value_range=(0, 255)
+        )
         xs = layer(xs, training=True)
         self.assertFalse(tf.math.reduce_any(xs == 1.0))
 
     def test_config(self):
         layer = preprocessing.RandomChannelShift(
             factor=[0.1, 0.5], value_range=(0, 255), seed=101
         )
         config = layer.get_config()
         self.assertEqual(config["factor"].get_config()["lower"], 0.1)
         self.assertEqual(config["factor"].get_config()["upper"], 0.5)
         self.assertEqual(config["value_range"], (0, 255))
         self.assertEqual(config["channels"], 3)
         self.assertEqual(config["seed"], 101)
 
-        reconstructed_layer = preprocessing.RandomChannelShift.from_config(config)
+        reconstructed_layer = preprocessing.RandomChannelShift.from_config(
+            config
+        )
         self.assertEqual(reconstructed_layer.factor, layer.factor)
         self.assertEqual(reconstructed_layer.value_range, layer.value_range)
         self.assertEqual(reconstructed_layer.seed, layer.seed)
         self.assertEqual(reconstructed_layer.channels, layer.channels)
-
-    def test_inference(self):
-        layer = preprocessing.RandomChannelShift(factor=0.8, value_range=(0, 255))
-        inputs = np.random.randint(0, 255, size=(224, 224, 3))
-        output = layer(inputs, training=False)
-        self.assertAllClose(inputs, output)
```

## keras_cv/layers/preprocessing/random_choice.py

```diff
@@ -7,78 +7,93 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomChoice(BaseImageAugmentationLayer):
     """RandomChoice constructs a pipeline based on provided arguments.
 
-    The implemented policy does the following: for each inputs provided in `call`(), the
-    policy selects a random layer from the provided list of `layers`.  It then calls the
-    `layer()` on the inputs.
+    The implemented policy does the following: for each input provided in
+    `call`(), the policy selects a random layer from the provided list of
+    `layers`. It then calls the `layer()` on the inputs.
 
     Usage:
     ```python
     # construct a list of layers
     layers = keras_cv.layers.RandAugment.get_standard_policy(
         value_range=(0, 255), magnitude=0.75, magnitude_stddev=0.3
     )
-    layers = layers[:4]  # slice out some layers you don't want for whatever reason
+    layers = layers[:4]  # slice out some layers you don't want for whatever
+                           reason
     layers = layers + [keras_cv.layers.GridMask()]
 
     # create the pipeline.
     pipeline = keras_cv.layers.RandomChoice(layers=layers)
 
     augmented_images = pipeline(images)
     ```
 
     Args:
-        layers: a list of `keras.Layers`.  These are randomly inputs during
-            augmentation to augment the inputs passed in `call()`.  The layers passed
-            should subclass `BaseImageAugmentationLayer`.
+        layers: a list of `keras.Layers`. These are randomly inputs during
+            augmentation to augment the inputs passed in `call()`. The layers
+            passed should subclass `BaseImageAugmentationLayer`.
         auto_vectorize: whether to use `tf.vectorized_map` or `tf.map_fn` to
-            apply the augmentations.  This offers a significant performance boost, but
-            can only be used if all the layers provided to the `layers` argument
-            support auto vectorization.
+            apply the augmentations. This offers a significant performance
+            boost, but can only be used if all the layers provided to the
+            `layers` argument support auto vectorization.
+        batchwise: Boolean, whether to pass entire batches to the
+            underlying layer. When set to `True`, each batch is passed to a
+            single layer, instead of each sample to an independent layer. This
+            is useful when using `MixUp()`, `CutMix()`, `Mosaic()`, etc.
+            Defaults to `False`.
         seed: Integer. Used to create a random seed.
     """
 
     def __init__(
         self,
         layers,
         auto_vectorize=False,
+        batchwise=False,
         seed=None,
         **kwargs,
     ):
         super().__init__(**kwargs, seed=seed, force_generator=True)
         self.layers = layers
         self.auto_vectorize = auto_vectorize
+        self.batchwise = batchwise
         self.seed = seed
 
     def _curry_call_layer(self, inputs, layer):
         def call_layer():
             return layer(inputs)
 
         return call_layer
 
+    def _batch_augment(self, inputs):
+        if self.batchwise:
+            return self._augment(inputs)
+        else:
+            return super()._batch_augment(inputs)
+
     def _augment(self, inputs, *args, **kwargs):
         selected_op = self._random_generator.random_uniform(
             (), minval=0, maxval=len(self.layers), dtype=tf.int32
         )
-
         # Warning:
         # Do not replace the currying function with a lambda.
         # Originally we used a lambda, but due to Python's
         # lack of loop level scope this causes unexpected
         # behavior running outside of graph mode.
         #
         # Autograph has an edge case where the behavior of Python for loop
@@ -98,10 +113,11 @@
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "layers": self.layers,
                 "auto_vectorize": self.auto_vectorize,
                 "seed": self.seed,
+                "batchwise": self.batchwise,
             }
         )
         return config
```

## keras_cv/layers/preprocessing/random_choice_test.py

```diff
@@ -7,21 +7,23 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
 from absl.testing import parameterized
+from tensorflow import keras
 
 from keras_cv import layers
 
 
-class AddOneToInputs(tf.keras.layers.Layer):
+class AddOneToInputs(keras.layers.Layer):
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.call_counter = tf.Variable(initial_value=0)
 
     def call(self, inputs):
         result = inputs.copy()
         result["images"] = inputs["images"] + 1
@@ -47,25 +49,49 @@
             return pipeline(xs)
 
         xs = tf.random.uniform((2, 5, 5, 3), 0, 100, dtype=tf.float32)
         os = call_pipeline(xs)
 
         self.assertAllClose(xs + 1, os)
 
+    def test_batchwise(self):
+        layer = AddOneToInputs()
+        pipeline = layers.RandomChoice(layers=[layer], batchwise=True)
+        xs = tf.random.uniform((4, 5, 5, 3), 0, 100, dtype=tf.float32)
+        os = pipeline(xs)
+
+        self.assertAllClose(xs + 1, os)
+        # Ensure the layer is only called once for the entire batch
+        self.assertEqual(layer.call_counter, 1)
+
+    def test_works_with_cutmix_mixup(self):
+        pipeline = layers.RandomChoice(
+            layers=[layers.CutMix(), layers.MixUp()], batchwise=True
+        )
+        xs = {
+            "images": tf.random.uniform((4, 5, 5, 3), 0, 100, dtype=tf.float32),
+            "labels": tf.random.uniform((4, 10), 0, 1, dtype=tf.float32),
+        }
+        pipeline(xs)
+
     def test_calls_layer_augmentation_single_image(self):
         layer = AddOneToInputs()
         pipeline = layers.RandomChoice(layers=[layer])
         xs = tf.random.uniform((5, 5, 3), 0, 100, dtype=tf.float32)
         os = pipeline(xs)
 
         self.assertAllClose(xs + 1, os)
 
     def test_calls_choose_one_layer_augmentation(self):
         batch_size = 10
-        pipeline = layers.RandomChoice(layers=[AddOneToInputs(), AddOneToInputs()])
+        pipeline = layers.RandomChoice(
+            layers=[AddOneToInputs(), AddOneToInputs()]
+        )
         xs = tf.random.uniform((batch_size, 5, 5, 3), 0, 100, dtype=tf.float32)
         os = pipeline(xs)
 
         self.assertAllClose(xs + 1, os)
 
-        total_calls = pipeline.layers[0].call_counter + pipeline.layers[1].call_counter
+        total_calls = (
+            pipeline.layers[0].call_counter + pipeline.layers[1].call_counter
+        )
         self.assertEqual(total_calls, batch_size)
```

## keras_cv/layers/preprocessing/random_color_degeneration.py

```diff
@@ -7,42 +7,44 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomColorDegeneration(BaseImageAugmentationLayer):
     """Randomly performs the color degeneration operation on given images.
 
-    The sharpness operation first converts an image to gray scale, then back to color.
-    It then takes a weighted average between original image and the degenerated image.
-    This makes colors appear more dull.
+    The sharpness operation first converts an image to gray scale, then back to
+    color. It then takes a weighted average between original image and the
+    degenerated image. This makes colors appear more dull.
 
     Args:
         factor: A tuple of two floats, a single float or a
             `keras_cv.FactorSampler`. `factor` controls the extent to which the
-            image sharpness is impacted. `factor=0.0` makes this layer perform a no-op
-            operation, while a value of 1.0 uses the degenerated result entirely.
-            Values between 0 and 1 result in linear interpolation between the original
-            image and the sharpened image.
-            Values should be between `0.0` and `1.0`.  If a tuple is used, a `factor` is
-            sampled between the two values for every image augmented.  If a single float
-            is used, a value between `0.0` and the passed float is sampled.  In order to
-            ensure the value is always the same, please pass a tuple with two identical
-            floats: `(0.5, 0.5)`.
+            image sharpness is impacted. `factor=0.0` makes this layer perform a
+            no-op operation, while a value of 1.0 uses the degenerated result
+            entirely. Values between 0 and 1 result in linear interpolation
+            between the original image and the sharpened image.
+            Values should be between `0.0` and `1.0`. If a tuple is used, a
+            `factor` is sampled between the two values for every image
+            augmented. If a single float is used, a value between `0.0` and the
+            passed float is sampled. In order to ensure the value is always the
+            same, please pass a tuple with two identical floats: `(0.5, 0.5)`.
         seed: Integer. Used to create a random seed.
     """
 
     def __init__(
         self,
         factor,
         seed=None,
@@ -64,14 +66,24 @@
 
     def augment_bounding_boxes(self, bounding_boxes, **kwargs):
         return bounding_boxes
 
     def augment_label(self, label, transformation=None, **kwargs):
         return label
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return segmentation_mask
 
     def get_config(self):
         config = super().get_config()
         config.update({"factor": self.factor, "seed": self.seed})
         return config
+
+    @classmethod
+    def from_config(cls, config):
+        if isinstance(config["factor"], dict):
+            config["factor"] = keras.utils.deserialize_keras_object(
+                config["factor"]
+            )
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_color_degeneration_test.py

```diff
@@ -54,15 +54,16 @@
         layer = preprocessing.RandomColorDegeneration(factor=(0.7, 0.7))
         ys = layer(xs)
 
         # Color degeneration uses standard luma conversion for RGB->Grayscale.
         # The formula for luma is result= 0.2989*r + 0.5870*g + 0.1140*b
         luma_result = 0.2989 + 2 * 0.5870 + 3 * 0.1140
 
-        # with factor=0.7, luma_result should be blended at a 70% rate with the original
+        # with factor=0.7, luma_result should be blended at a 70% rate with the
+        # original
         r_result = luma_result * 0.7 + 1 * 0.3
         g_result = luma_result * 0.7 + 2 * 0.3
         b_result = luma_result * 0.7 + 3 * 0.3
 
         r = ys[..., 0]
         g = ys[..., 1]
         b = ys[..., 2]
```

## keras_cv/layers/preprocessing/random_color_jitter.py

```diff
@@ -1,49 +1,49 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers import preprocessing
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomColorJitter(BaseImageAugmentationLayer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomColorJitter(VectorizedBaseImageAugmentationLayer):
     """RandomColorJitter class randomly apply brightness, contrast, saturation
     and hue image processing operation sequentially and randomly on the
     input. It expects input as RGB image. The expected image should be
     `(0-255)` pixel ranges.
 
     Input shape:
         3D (unbatched) or 4D (batched) tensor with shape:
         `(..., height, width, channels)`, in `channels_last` format
     Output shape:
         3D (unbatched) or 4D (batched) tensor with shape:
         `(..., height, width, channels)`, in `channels_last` format
 
     Args:
-        value_range:  the range of values the incoming images will have.
+        value_range: the range of values the incoming images will have.
             Represented as a two number tuple written [low, high].
             This is typically either `[0, 1]` or `[0, 255]` depending
-            on how your preprocessing pipeline is setup.
+            on how your preprocessing pipeline is set up.
         brightness_factor: Float or a list/tuple of 2 floats between -1.0
             and 1.0. The factor is used to determine the lower bound and
             upper bound of the brightness adjustment. A float value will be
             chosen randomly between the limits. When -1.0 is chosen, the
             output image will be black, and when 1.0 is chosen, the image
             will be fully white. When only one float is provided, eg, 0.2,
             then -0.2 will be used for lower bound and 0.2 will be used for
@@ -57,24 +57,24 @@
             impacted. `factor=0.5` makes this layer perform a no-op operation.
             `factor=0.0` makes the image to be fully grayscale. `factor=1.0`
             makes the image to be fully saturated.
         hue_factor: A tuple of two floats, a single float or
             `keras_cv.FactorSampler`. `factor` controls the extent to which the
             image sharpness is impacted. `factor=0.0` makes this layer perform
             a no-op operation, while a value of 1.0 performs the most aggressive
-            contrast adjustment available.  If a tuple is used, a `factor` is sampled
-            between the two values for every image augmented.  If a single float
-            is used, a value between `0.0` and the passed float is sampled.
-            In order to ensure the value is always the same, please pass a tuple
-            with two identical floats: `(0.5, 0.5)`.
+            contrast adjustment available. If a tuple is used, a `factor` is
+            sampled between the two values for every image augmented. If a
+            single float is used, a value between `0.0` and the passed float is
+            sampled. In order to ensure the value is always the same, please
+            pass a tuple with two identical floats: `(0.5, 0.5)`.
         seed: Integer. Used to create a random seed.
 
     Usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     color_jitter = keras_cv.layers.RandomColorJitter(
             value_range=(0, 255),
             brightness_factor=(-0.2, 0.5),
             contrast_factor=(0.5, 0.9),
             saturation_factor=(0.5, 0.9),
             hue_factor=(0.5, 0.9),
     )
@@ -100,57 +100,68 @@
         self.hue_factor = hue_factor
         self.seed = seed
 
         self.random_brightness = preprocessing.RandomBrightness(
             factor=self.brightness_factor, value_range=(0, 255), seed=self.seed
         )
         self.random_contrast = preprocessing.RandomContrast(
-            factor=self.contrast_factor, seed=self.seed
+            factor=self.contrast_factor, value_range=(0, 255), seed=self.seed
         )
         self.random_saturation = preprocessing.RandomSaturation(
             factor=self.saturation_factor, seed=self.seed
         )
         self.random_hue = preprocessing.RandomHue(
             factor=self.hue_factor, value_range=(0, 255), seed=self.seed
         )
 
-    def augment_image(self, image, transformation=None, **kwargs):
-        image = preprocessing_utils.transform_value_range(
-            image,
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        return self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
+
+    def augment_images(self, images, transformations=None, **kwargs):
+        images = preprocessing_utils.transform_value_range(
+            images,
             original_range=self.value_range,
             target_range=(0, 255),
             dtype=self.compute_dtype,
         )
-        image = self.random_brightness(image)
-        image = self.random_contrast(image)
-        image = self.random_saturation(image)
-        image = self.random_hue(image)
-        image = preprocessing_utils.transform_value_range(
-            image,
+        images = self.random_brightness(images)
+        images = self.random_contrast(images)
+        images = self.random_saturation(images)
+        images = self.random_hue(images)
+        images = preprocessing_utils.transform_value_range(
+            images,
             original_range=(0, 255),
             target_range=self.value_range,
             dtype=self.compute_dtype,
         )
-        return image
+        return images
 
-    def augment_bounding_boxes(self, bounding_boxes, **kwargs):
-        return bounding_boxes
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
 
-    def augment_label(self, label, transformation=None, **kwargs):
-        return label
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
+    def augment_bounding_boxes(self, bounding_boxes, transformations, **kwargs):
+        return bounding_boxes
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "value_range": self.value_range,
                 "brightness_factor": self.brightness_factor,
                 "contrast_factor": self.contrast_factor,
                 "saturation_factor": self.saturation_factor,
                 "hue_factor": self.hue_factor,
                 "seed": self.seed,
             }
         )
         return config
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_color_jitter_test.py

```diff
@@ -8,15 +8,14 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_cv.layers import preprocessing
 
 
 class RandomColorJitterTest(tf.test.TestCase, parameterized.TestCase):
@@ -87,25 +86,20 @@
 
         config = layer.get_config()
         self.assertEqual(config["brightness_factor"], 0.5)
         self.assertEqual(config["contrast_factor"], (0.5, 0.9))
         self.assertEqual(config["saturation_factor"], (0.5, 0.9))
         self.assertEqual(config["hue_factor"], 0.5)
 
-        reconstructed_layer = preprocessing.RandomColorJitter.from_config(config)
-        self.assertEqual(reconstructed_layer.brightness_factor, layer.brightness_factor)
-        self.assertEqual(reconstructed_layer.contrast_factor, layer.contrast_factor)
-        self.assertEqual(reconstructed_layer.saturation_factor, layer.saturation_factor)
-        self.assertEqual(reconstructed_layer.hue_factor, layer.hue_factor)
-
-    # Test 5: Check if inference model is OK.
-    def test_inference(self):
-        layer = preprocessing.RandomColorJitter(
-            value_range=(0, 255),
-            brightness_factor=0.5,
-            contrast_factor=(0.5, 0.9),
-            saturation_factor=(0.5, 0.9),
-            hue_factor=0.5,
+        reconstructed_layer = preprocessing.RandomColorJitter.from_config(
+            config
+        )
+        self.assertEqual(
+            reconstructed_layer.brightness_factor, layer.brightness_factor
         )
-        inputs = np.random.randint(0, 255, size=(224, 224, 3))
-        output = layer(inputs, training=False)
-        self.assertAllClose(inputs, output)
+        self.assertEqual(
+            reconstructed_layer.contrast_factor, layer.contrast_factor
+        )
+        self.assertEqual(
+            reconstructed_layer.saturation_factor, layer.saturation_factor
+        )
+        self.assertEqual(reconstructed_layer.hue_factor, layer.hue_factor)
```

## keras_cv/layers/preprocessing/random_contrast.py

```diff
@@ -1,94 +1,124 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
-from keras_cv.utils import preprocessing
+from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomContrast(BaseImageAugmentationLayer):
-    """RandomContrast randomly adjusts contrast during training.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomContrast(VectorizedBaseImageAugmentationLayer):
+    """RandomContrast randomly adjusts contrast.
 
     This layer will randomly adjust the contrast of an image or images by a
     random factor. Contrast is adjusted independently for each channel of each
-    image during training.
+    image.
 
     For each channel, this layer computes the mean of the image pixels in the
     channel and then adjusts each component `x` of each pixel to
     `(x - mean) * contrast_factor + mean`.
 
     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and
     in integer or floating point dtype. By default, the layer will output
     floats. The output value will be clipped to the range `[0, 255]`, the valid
     range of RGB colors.
 
     Input shape:
-      3D (unbatched) or 4D (batched) tensor with shape:
-      `(..., height, width, channels)`, in `"channels_last"` format.
+        3D (unbatched) or 4D (batched) tensor with shape:
+        `(..., height, width, channels)`, in `"channels_last"` format.
     Output shape:
-      3D (unbatched) or 4D (batched) tensor with shape:
-      `(..., height, width, channels)`, in `"channels_last"` format.
+        3D (unbatched) or 4D (batched) tensor with shape:
+        `(..., height, width, channels)`, in `"channels_last"` format.
 
     Args:
-      factor: a positive float represented as fraction of value, or a tuple of
-        size 2 representing lower and upper bound. When represented as a single
-        float, lower = upper. The contrast factor will be randomly picked
-        between `[1.0 - lower, 1.0 + upper]`. For any pixel x in the channel,
-        the output will be `(x - mean) * factor + mean` where `mean` is the mean
-        value of the channel.
-      seed: Integer. Used to create a random seed.
+        value_range: A tuple or a list of two elements. The first value
+            represents the lower bound for values in passed images, the second
+            represents the upper bound. Images passed to the layer should have
+            values within `value_range`.
+        factor: A positive float represented as fraction of value, or a tuple of
+            size 2 representing lower and upper bound. When represented as a
+            single float, lower = upper. The contrast factor will be randomly
+            picked between `[1.0 - lower, 1.0 + upper]`. For any pixel x in the
+            channel, the output will be `(x - mean) * factor + mean` where
+            `mean` is the mean value of the channel.
+        seed: Integer. Used to create a random seed.
+
+    Usage:
+    ```python
+    (images, labels), _ = keras.datasets.cifar10.load_data()
+    random_contrast = keras_cv.layers.preprocessing.RandomContrast()
+    augmented_images = random_contrast(images)
+    ```
     """
 
-    def __init__(self, factor, seed=None, **kwargs):
+    def __init__(self, value_range, factor, seed=None, **kwargs):
         super().__init__(seed=seed, force_generator=True, **kwargs)
         if isinstance(factor, (tuple, list)):
             min = 1 - factor[0]
             max = 1 + factor[1]
         else:
             min = 1 - factor
             max = 1 + factor
         self.factor_input = factor
-        self.factor = preprocessing.parse_factor((min, max), min_value=-1, max_value=2)
+        self.factor = preprocessing_utils.parse_factor(
+            (min, max), min_value=-1, max_value=2
+        )
+        self.value_range = value_range
         self.seed = seed
 
-    def get_random_transformation(self, **kwargs):
-        return self.factor()
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        return self.factor(shape=(batch_size, 1, 1, 1))
 
-    def augment_image(self, image, transformation, **kwargs):
-        contrast_factor = transformation
-        output = tf.image.adjust_contrast(image, contrast_factor=contrast_factor)
-        output = tf.clip_by_value(output, 0, 255)
-        output.set_shape(image.shape)
-        return output
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        return self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
+
+    def augment_images(self, images, transformations, **kwargs):
+        contrast_factors = tf.cast(transformations, dtype=images.dtype)
+        means = tf.reduce_mean(images, axis=(1, 2), keepdims=True)
+
+        images = (images - means) * contrast_factors + means
+        images = tf.clip_by_value(
+            images, self.value_range[0], self.value_range[1]
+        )
+        return images
+
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
+
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
 
-    def augment_label(self, label, transformation, **kwargs):
-        return label
-
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
-
-    def augment_bounding_boxes(self, bounding_boxes, transformation=None, **kwargs):
+    def augment_bounding_boxes(self, bounding_boxes, transformations, **kwargs):
         return bounding_boxes
 
     def get_config(self):
         config = {
             "factor": self.factor_input,
+            "value_range": self.value_range,
             "seed": self.seed,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_contrast_test.py

```diff
@@ -17,38 +17,47 @@
 
 
 class RandomContrastTest(tf.test.TestCase):
     def test_preserves_output_shape(self):
         image_shape = (4, 8, 8, 3)
         image = tf.random.uniform(shape=image_shape) * 255.0
 
-        layer = preprocessing.RandomContrast(factor=(0.3, 0.8))
+        layer = preprocessing.RandomContrast(
+            value_range=(0, 255), factor=(0.3, 0.8)
+        )
         output = layer(image)
 
         self.assertEqual(image.shape, output.shape)
         self.assertNotAllClose(image, output)
 
     def test_no_adjustment_for_factor_zero(self):
         image_shape = (4, 8, 8, 3)
         image = tf.random.uniform(shape=image_shape) * 255.0
 
-        layer = preprocessing.RandomContrast(factor=0)
+        layer = preprocessing.RandomContrast(value_range=(0, 255), factor=0)
         output = layer(image)
 
         self.assertAllClose(image, output, atol=1e-5, rtol=1e-5)
 
     def test_with_unit8(self):
         image_shape = (4, 8, 8, 3)
-        image = tf.cast(tf.random.uniform(shape=image_shape) * 255.0, dtype=tf.uint8)
+        image = tf.cast(
+            tf.random.uniform(shape=image_shape) * 255.0, dtype=tf.uint8
+        )
 
-        layer = preprocessing.RandomContrast(factor=0)
+        layer = preprocessing.RandomContrast(value_range=(0, 255), factor=0)
         output = layer(image)
         self.assertAllClose(image, output, atol=1e-5, rtol=1e-5)
 
-        layer = preprocessing.RandomContrast(factor=(0.3, 0.8))
+        layer = preprocessing.RandomContrast(
+            value_range=(0, 255), factor=(0.3, 0.8)
+        )
         output = layer(image)
         self.assertNotAllClose(image, output)
 
     def test_config(self):
-        layer = preprocessing.RandomContrast(factor=(0.3, 0.8))
+        layer = preprocessing.RandomContrast(
+            value_range=(0, 255), factor=(0.3, 0.8)
+        )
         config = layer.get_config()
         self.assertEqual(config["factor"], (0.3, 0.8))
+        self.assertEqual(config["value_range"], (0, 255))
```

## keras_cv/layers/preprocessing/random_crop.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,177 +10,290 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
 
 # In order to support both unbatched and batched inputs, the horizontal
 # and verticle axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomCrop(BaseImageAugmentationLayer):
-    """A preprocessing layer which randomly crops images during training.
-    During training, this layer will randomly choose a location to crop images
-    down to a target size. The layer will crop all the images in the same batch
-    to the same cropping location.
-    At inference time, and during training if an input image is smaller than the
-    target size, the input will be resized and cropped so as to return the
-    largest possible window in the image that matches the target aspect ratio.
-    If you need to apply random cropping at inference time, set `training` to
-    True when calling the layer.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomCrop(VectorizedBaseImageAugmentationLayer):
+    """A preprocessing layer which randomly crops images.
+
+    This layer will randomly choose a location to crop images down to a target
+    size.
+
+    If an input image is smaller than the target size, the input will be
+    resized and cropped to return the largest possible window in the image that
+    matches the target aspect ratio.
+
     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and
-    of interger or floating point dtype. By default, the layer will output
-    floats.
+    of interger or floating point dtype.
+
     Input shape:
-      3D (unbatched) or 4D (batched) tensor with shape:
-      `(..., height, width, channels)`, in `"channels_last"` format.
+        3D (unbatched) or 4D (batched) tensor with shape:
+        `(..., height, width, channels)`, in `"channels_last"` format.
+
     Output shape:
-      3D (unbatched) or 4D (batched) tensor with shape:
-      `(..., target_height, target_width, channels)`.
+        3D (unbatched) or 4D (batched) tensor with shape:
+        `(..., target_height, target_width, channels)`.
+
     Args:
-      height: Integer, the height of the output shape.
-      width: Integer, the width of the output shape.
-      seed: Integer. Used to create a random seed.
+        height: Integer, the height of the output shape.
+        width: Integer, the width of the output shape.
+        seed: Integer. Used to create a random seed.
     """
 
-    def __init__(self, height, width, seed=None, bounding_box_format=None, **kwargs):
-        super().__init__(**kwargs, autocast=False, seed=seed, force_generator=True)
+    def __init__(
+        self, height, width, seed=None, bounding_box_format=None, **kwargs
+    ):
+        super().__init__(
+            **kwargs, autocast=False, seed=seed, force_generator=True
+        )
         self.height = height
         self.width = width
-        self.seed = seed
-        self.auto_vectorize = False
         self.bounding_box_format = bounding_box_format
+        self.seed = seed
 
-    def get_random_transformation(self, image=None, **kwargs):
-        image_shape = tf.shape(image)
-        h_diff = image_shape[H_AXIS] - self.height
-        w_diff = image_shape[W_AXIS] - self.width
-        dtype = image_shape.dtype
-        rands = self._random_generator.random_uniform([2], 0, dtype.max, dtype)
-        h_start = rands[0] % (h_diff + 1)
-        w_start = rands[1] % (w_diff + 1)
-        return {"top": h_start, "left": w_start}
-
-    def augment_image(self, image, transformation, **kwargs):
-        image_shape = tf.shape(image)
-        h_diff = image_shape[H_AXIS] - self.height
-        w_diff = image_shape[W_AXIS] - self.width
-        return tf.cond(
-            tf.reduce_all((h_diff >= 0, w_diff >= 0)),
-            lambda: self._crop(image, transformation),
-            lambda: self._resize(image),
-        )
+        self.force_output_dense_images = True
 
-    def compute_image_signature(self, images):
-        return tf.TensorSpec(
+    def compute_ragged_image_signature(self, images):
+        ragged_spec = tf.RaggedTensorSpec(
             shape=(self.height, self.width, images.shape[-1]),
+            ragged_rank=1,
             dtype=self.compute_dtype,
         )
+        return ragged_spec
+
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        tops = self._random_generator.random_uniform(
+            shape=(batch_size, 1), minval=0, maxval=1, dtype=tf.float32
+        )
+        lefts = self._random_generator.random_uniform(
+            shape=(batch_size, 1), minval=0, maxval=1, dtype=tf.float32
+        )
+        return {"tops": tops, "lefts": lefts}
+
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        image = tf.expand_dims(image, axis=0)
+        tops = transformation["tops"]
+        lefts = transformation["lefts"]
+        transformation = {
+            "tops": tf.expand_dims(tops, axis=0),
+            "lefts": tf.expand_dims(lefts, axis=0),
+        }
+        image = self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
+        return tf.squeeze(image, axis=0)
+
+    def augment_images(self, images, transformations, **kwargs):
+        batch_size = tf.shape(images)[0]
+        channel = tf.shape(images)[-1]
+        heights, widths = self._get_image_shape(images)
+        h_diffs = heights - self.height
+        w_diffs = widths - self.width
+        # broadcast
+        h_diffs = (
+            tf.ones(
+                shape=(batch_size, self.height, self.width, channel),
+                dtype=tf.int32,
+            )
+            * h_diffs[:, tf.newaxis, tf.newaxis, :]
+        )
+        w_diffs = (
+            tf.ones(
+                shape=(batch_size, self.height, self.width, channel),
+                dtype=tf.int32,
+            )
+            * w_diffs[:, tf.newaxis, tf.newaxis, :]
+        )
+        return tf.where(
+            tf.math.logical_and(h_diffs >= 0, w_diffs >= 0),
+            self._crop_images(images, transformations),
+            self._resize_images(images),
+        )
+
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
 
     def augment_bounding_boxes(
-        self, bounding_boxes, transformation, image=None, **kwargs
+        self, bounding_boxes, transformations, raw_images=None, **kwargs
     ):
         if self.bounding_box_format is None:
             raise ValueError(
                 "`RandomCrop()` was called with bounding boxes,"
                 "but no `bounding_box_format` was specified in the constructor."
                 "Please specify a bounding box format in the constructor. i.e."
                 "`RandomCrop(bounding_box_format='xyxy')`"
             )
+        if isinstance(bounding_boxes["boxes"], tf.RaggedTensor):
+            bounding_boxes = bounding_box.to_dense(
+                bounding_boxes, default_value=-1
+            )
+        batch_size = tf.shape(raw_images)[0]
+        heights, widths = self._get_image_shape(raw_images)
+
         bounding_boxes = bounding_box.convert_format(
             bounding_boxes,
             source=self.bounding_box_format,
             target="xyxy",
-            images=image,
+            images=raw_images,
+        )
+        h_diffs = heights - self.height
+        w_diffs = widths - self.width
+        # broadcast
+        num_bounding_boxes = tf.shape(bounding_boxes["boxes"])[-2]
+        h_diffs = (
+            tf.ones(
+                shape=(batch_size, num_bounding_boxes, 4),
+                dtype=tf.int32,
+            )
+            * h_diffs[:, tf.newaxis, :]
+        )
+        w_diffs = (
+            tf.ones(
+                shape=(batch_size, num_bounding_boxes, 4),
+                dtype=tf.int32,
+            )
+            * w_diffs[:, tf.newaxis, :]
         )
-        image_shape = tf.shape(image)
-        h_diff = image_shape[H_AXIS] - self.height
-        w_diff = image_shape[W_AXIS] - self.width
-        bounding_boxes = tf.cond(
-            tf.reduce_all((h_diff >= 0, w_diff >= 0)),
-            lambda: self._crop_bounding_boxes(image, bounding_boxes, transformation),
-            lambda: self._resize_bounding_boxes(
-                image,
-                bounding_boxes,
+        boxes = tf.where(
+            tf.math.logical_and(h_diffs >= 0, w_diffs >= 0),
+            self._crop_bounding_boxes(
+                raw_images, bounding_boxes["boxes"], transformations
+            ),
+            self._resize_bounding_boxes(
+                raw_images,
+                bounding_boxes["boxes"],
             ),
         )
+        bounding_boxes["boxes"] = boxes
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes,
             bounding_box_format="xyxy",
-            image_shape=(self.height, self.width, image_shape[-1]),
+            image_shape=(self.height, self.width, None),
         )
         bounding_boxes = bounding_box.convert_format(
             bounding_boxes,
             source="xyxy",
             target=self.bounding_box_format,
             dtype=self.compute_dtype,
-            images=image,
+            image_shape=(self.height, self.width, None),
         )
         return bounding_boxes
 
-    def _crop(self, image, transformation):
-        top = transformation["top"]
-        left = transformation["left"]
-        return tf.image.crop_to_bounding_box(image, top, left, self.height, self.width)
-
-    def _resize(self, image):
-        resizing_layer = tf.keras.layers.Resizing(self.height, self.width)
-        outputs = resizing_layer(image)
-        # smart_resize will always output float32, so we need to re-cast.
-        return tf.cast(outputs, self.compute_dtype)
+    def _get_image_shape(self, images):
+        if isinstance(images, tf.RaggedTensor):
+            heights = tf.reshape(images.row_lengths(), (-1, 1))
+            widths = tf.reshape(
+                tf.reduce_max(images.row_lengths(axis=2), 1), (-1, 1)
+            )
+        else:
+            batch_size = tf.shape(images)[0]
+            heights = tf.repeat(tf.shape(images)[H_AXIS], repeats=[batch_size])
+            heights = tf.reshape(heights, shape=(-1, 1))
+            widths = tf.repeat(tf.shape(images)[W_AXIS], repeats=[batch_size])
+            widths = tf.reshape(widths, shape=(-1, 1))
+        return tf.cast(heights, dtype=tf.int32), tf.cast(widths, dtype=tf.int32)
+
+    def _crop_images(self, images, transformations):
+        batch_size = tf.shape(images)[0]
+        heights, widths = self._get_image_shape(images)
+        heights = tf.cast(heights, dtype=tf.float32)
+        widths = tf.cast(widths, dtype=tf.float32)
+
+        tops = transformations["tops"]
+        lefts = transformations["lefts"]
+        x1s = lefts * (widths - self.width)
+        y1s = tops * (heights - self.height)
+        x2s = x1s + self.width
+        y2s = y1s + self.height
+        # normalize
+        x1s /= widths
+        y1s /= heights
+        x2s /= widths
+        y2s /= heights
+        boxes = tf.concat([y1s, x1s, y2s, x2s], axis=-1)
+
+        images = tf.image.crop_and_resize(
+            images,
+            boxes,
+            tf.range(batch_size),
+            [self.height, self.width],
+            method="nearest",
+        )
+        return tf.cast(images, dtype=self.compute_dtype)
 
-    def augment_label(self, label, transformation, **kwargs):
-        return label
+    def _resize_images(self, images):
+        resizing_layer = keras.layers.Resizing(self.height, self.width)
+        outputs = resizing_layer(images)
+        return tf.cast(outputs, dtype=self.compute_dtype)
+
+    def _crop_bounding_boxes(self, images, boxes, transformation):
+        tops = transformation["tops"]
+        lefts = transformation["lefts"]
+        heights, widths = self._get_image_shape(images)
+        heights = tf.cast(heights, dtype=tf.float32)
+        widths = tf.cast(widths, dtype=tf.float32)
+
+        # compute offsets for xyxy bounding_boxes
+        top_offsets = tf.cast(
+            tf.math.round(tops * (heights - self.height)),
+            dtype=self.compute_dtype,
+        )
+        left_offsets = tf.cast(
+            tf.math.round(lefts * (widths - self.width)),
+            dtype=self.compute_dtype,
+        )
+
+        x1s, y1s, x2s, y2s = tf.split(boxes, 4, axis=-1)
+        x1s -= tf.expand_dims(left_offsets, axis=1)
+        y1s -= tf.expand_dims(top_offsets, axis=1)
+        x2s -= tf.expand_dims(left_offsets, axis=1)
+        y2s -= tf.expand_dims(top_offsets, axis=1)
+        outputs = tf.concat([x1s, y1s, x2s, y2s], axis=-1)
+        return outputs
+
+    def _resize_bounding_boxes(self, images, boxes):
+        heights, widths = self._get_image_shape(images)
+        heights = tf.cast(heights, dtype=tf.float32)
+        widths = tf.cast(widths, dtype=tf.float32)
+        x_scale = tf.cast(self.width / widths, dtype=self.compute_dtype)
+        y_scale = tf.cast(self.height / heights, dtype=self.compute_dtype)
+        x1s, y1s, x2s, y2s = tf.split(boxes, 4, axis=-1)
+        outputs = tf.concat(
+            [
+                x1s * x_scale[:, tf.newaxis, :],
+                y1s * y_scale[:, tf.newaxis, :],
+                x2s * x_scale[:, tf.newaxis, :],
+                y2s * y_scale[:, tf.newaxis, :],
+            ],
+            axis=-1,
+        )
+        return outputs
 
     def get_config(self):
         config = {
             "height": self.height,
             "width": self.width,
             "seed": self.seed,
             "bounding_box_format": self.bounding_box_format,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
 
-    def _crop_bounding_boxes(self, image, bounding_boxes, transformation):
-        top = tf.cast(transformation["top"], dtype=self.compute_dtype)
-        left = tf.cast(transformation["left"], dtype=self.compute_dtype)
-        output = bounding_boxes.copy()
-        x1, y1, x2, y2 = tf.split(bounding_boxes["boxes"], [1, 1, 1, 1], axis=-1)
-        output["boxes"] = tf.concat(
-            [
-                x1 - left,
-                y1 - top,
-                x2 - left,
-                y2 - top,
-            ],
-            axis=-1,
-        )
-        return output
-
-    def _resize_bounding_boxes(self, image, bounding_boxes):
-        output = bounding_boxes.copy()
-        image_shape = tf.shape(image)
-        x_scale = tf.cast(self.width / image_shape[W_AXIS], dtype=self.compute_dtype)
-        y_scale = tf.cast(self.height / image_shape[H_AXIS], dtype=self.compute_dtype)
-        x1, y1, x2, y2 = tf.split(bounding_boxes["boxes"], [1, 1, 1, 1], axis=-1)
-        output["boxes"] = tf.concat(
-            [
-                x1 * x_scale,
-                y1 * y_scale,
-                x2 * x_scale,
-                y2 * y_scale,
-            ],
-            axis=-1,
-        )
-
-        return output
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_crop_and_resize.py

```diff
@@ -9,72 +9,76 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 from keras_cv import core
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomCropAndResize(BaseImageAugmentationLayer):
     """Randomly crops a part of an image and resizes it to provided size.
 
-    This implementation takes an intuitive approach, where we crop the images to a
-    random height and width, and then resize them. To do this, we first sample a
-    random value for area using `crop_area_factor` and a value for aspect ratio using
-    `aspect_ratio_factor`. Further we get the new height and width by
-    dividing and multiplying the old height and width by the random area
+    This implementation takes an intuitive approach, where we crop the images to
+    a random height and width, and then resize them. To do this, we first sample
+    a random value for area using `crop_area_factor` and a value for aspect
+    ratio using `aspect_ratio_factor`. Further we get the new height and width
+    by dividing and multiplying the old height and width by the random area
     respectively. We then sample offsets for height and width and clip them such
-    that the cropped area does not exceed image boundaries. Finally we do the
+    that the cropped area does not exceed image boundaries. Finally, we do the
     actual cropping operation and resize the image to `target_size`.
 
     Args:
-        target_size: A tuple of two integers used as the target size to ultimately crop
-            images to.
+        target_size: A tuple of two integers used as the target size to
+            ultimately crop images to.
         crop_area_factor: A tuple of two floats, ConstantFactorSampler or
-            UniformFactorSampler. The ratio of area of the cropped part to
-            that of original image is sampled using this factor. Represents the
-            lower and upper bounds for the area relative to the original image
-            of the cropped image before resizing it to `target_size`.  For
+            UniformFactorSampler. The ratio of area of the cropped part to that
+            of original image is sampled using this factor. Represents the lower
+            and upper bounds for the area relative to the original image of the
+            cropped image before resizing it to `target_size`. For
             self-supervised pretraining a common value for this parameter is
-            `(0.08, 1.0)`.  For fine tuning and classification a common value for this
-            is `0.8, 1.0`.
+            `(0.08, 1.0)`. For fine tuning and classification a common value for
+            this is `0.8, 1.0`.
         aspect_ratio_factor: A tuple of two floats, ConstantFactorSampler or
             UniformFactorSampler. Aspect ratio means the ratio of width to
-            height of the cropped image. In the context of this layer, the aspect ratio
-            sampled represents a value to distort the aspect ratio by.
-            Represents the lower and upper bound for the aspect ratio of the
-            cropped image before resizing it to `target_size`.  For most tasks, this
-            should be `(3/4, 4/3)`.  To perform a no-op provide the value `(1.0, 1.0)`.
+            height of the cropped image. In the context of this layer, the
+            aspect ratio sampled represents a value to distort the aspect ratio
+            by. Represents the lower and upper bound for the aspect ratio of the
+            cropped image before resizing it to `target_size`. For most tasks,
+            this should be `(3/4, 4/3)`. To perform a no-op provide the value
+            `(1.0, 1.0)`.
         interpolation: (Optional) A string specifying the sampling method for
-            resizing. Defaults to "bilinear".
-        seed: (Optional) Used to create a random seed. Defaults to None.
+            resizing, defaults to "bilinear".
+        seed: (Optional) Used to create a random seed, defaults to None.
     """
 
     def __init__(
         self,
         target_size,
         crop_area_factor,
         aspect_ratio_factor,
         interpolation="bilinear",
         bounding_box_format=None,
         seed=None,
         **kwargs,
     ):
         super().__init__(seed=seed, **kwargs)
 
-        self._check_class_arguments(target_size, crop_area_factor, aspect_ratio_factor)
+        self._check_class_arguments(
+            target_size, crop_area_factor, aspect_ratio_factor
+        )
         self.target_size = target_size
         self.aspect_ratio_factor = preprocessing.parse_factor(
             aspect_ratio_factor,
             min_value=0.0,
             max_value=None,
             param_name="aspect_ratio_factor",
             seed=seed,
@@ -96,15 +100,17 @@
     ):
         crop_area_factor = self.crop_area_factor()
         aspect_ratio = self.aspect_ratio_factor()
 
         new_height = tf.clip_by_value(
             tf.sqrt(crop_area_factor / aspect_ratio), 0.0, 1.0
         )  # to avoid unwanted/unintuitive effects
-        new_width = tf.clip_by_value(tf.sqrt(crop_area_factor * aspect_ratio), 0.0, 1.0)
+        new_width = tf.clip_by_value(
+            tf.sqrt(crop_area_factor * aspect_ratio), 0.0, 1.0
+        )
 
         height_offset = self._random_generator.random_uniform(
             (),
             minval=tf.minimum(0.0, 1.0 - new_height),
             maxval=tf.maximum(0.0, 1.0 - new_height),
             dtype=tf.float32,
         )
@@ -119,32 +125,14 @@
         y1 = height_offset
         y2 = height_offset + new_height
         x1 = width_offset
         x2 = width_offset + new_width
 
         return [[y1, x1, y2, x2]]
 
-    def call(self, inputs, training=True):
-        if training:
-            return super().call(inputs, training)
-        else:
-            inputs = self._ensure_inputs_are_compute_dtype(inputs)
-            inputs, meta_data = self._format_inputs(inputs)
-            output = inputs
-            # self._resize() returns valid results for both batched and
-            # unbatched
-            output["images"] = self._resize(inputs["images"])
-
-            if "segmentation_masks" in inputs:
-                output["segmentation_masks"] = self._resize(
-                    inputs["segmentation_masks"], interpolation="nearest"
-                )
-
-            return self._format_output(output, meta_data)
-
     def compute_image_signature(self, images):
         return tf.TensorSpec(
             shape=(self.target_size[0], self.target_size[1], images.shape[-1]),
             dtype=self.compute_dtype,
         )
 
     def augment_image(self, image, transformation, **kwargs):
@@ -154,15 +142,17 @@
         return target
 
     def _transform_bounding_boxes(bounding_boxes, transformation):
         bounding_boxes = bounding_boxes.copy()
         t_y1, t_x1, t_y2, t_x2 = transformation[0]
         t_dx = t_x2 - t_x1
         t_dy = t_y2 - t_y1
-        x1, y1, x2, y2 = tf.split(bounding_boxes["boxes"], [1, 1, 1, 1], axis=-1)
+        x1, y1, x2, y2 = tf.split(
+            bounding_boxes["boxes"], [1, 1, 1, 1], axis=-1
+        )
         output = tf.concat(
             [
                 (x1 - t_x1) / t_dx,
                 (y1 - t_y1) / t_dy,
                 (x2 - t_x1) / t_dx,
                 (y2 - t_y1) / t_dy,
             ],
@@ -204,15 +194,15 @@
             target=self.bounding_box_format,
             dtype=self.compute_dtype,
             images=image,
         )
         return bounding_boxes
 
     def _resize(self, image, **kwargs):
-        outputs = tf.keras.preprocessing.image.smart_resize(
+        outputs = keras.preprocessing.image.smart_resize(
             image, self.target_size, **kwargs
         )
         # smart_resize will always output float32, so we need to re-cast.
         return tf.cast(outputs, self.compute_dtype)
 
     def _check_class_arguments(
         self, target_size, crop_area_factor, aspect_ratio_factor
@@ -231,31 +221,35 @@
 
         if (
             not isinstance(crop_area_factor, (tuple, list, core.FactorSampler))
             or isinstance(crop_area_factor, float)
             or isinstance(crop_area_factor, int)
         ):
             raise ValueError(
-                "`crop_area_factor` must be tuple of two positive floats less than "
-                "or equal to 1 or keras_cv.core.FactorSampler instance. Received "
-                f"crop_area_factor={crop_area_factor}"
+                "`crop_area_factor` must be tuple of two positive floats less "
+                "than or equal to 1 or keras_cv.core.FactorSampler instance. "
+                f"Received crop_area_factor={crop_area_factor}"
             )
 
         if (
-            not isinstance(aspect_ratio_factor, (tuple, list, core.FactorSampler))
+            not isinstance(
+                aspect_ratio_factor, (tuple, list, core.FactorSampler)
+            )
             or isinstance(aspect_ratio_factor, float)
             or isinstance(aspect_ratio_factor, int)
         ):
             raise ValueError(
                 "`aspect_ratio_factor` must be tuple of two positive floats or "
                 "keras_cv.core.FactorSampler instance. Received "
                 f"aspect_ratio_factor={aspect_ratio_factor}"
             )
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return self._crop_and_resize(
             segmentation_mask, transformation, method="nearest"
         )
 
     def get_config(self):
         config = super().get_config()
         config.update(
@@ -266,14 +260,28 @@
                 "interpolation": self.interpolation,
                 "bounding_box_format": self.bounding_box_format,
                 "seed": self.seed,
             }
         )
         return config
 
+    @classmethod
+    def from_config(cls, config):
+        if isinstance(config["crop_area_factor"], dict):
+            config["crop_area_factor"] = keras.utils.deserialize_keras_object(
+                config["crop_area_factor"]
+            )
+        if isinstance(config["aspect_ratio_factor"], dict):
+            config[
+                "aspect_ratio_factor"
+            ] = keras.utils.deserialize_keras_object(
+                config["aspect_ratio_factor"]
+            )
+        return cls(**config)
+
     def _crop_and_resize(self, image, transformation, method=None):
         image = tf.expand_dims(image, axis=0)
         boxes = transformation
 
         # See bit.ly/tf_crop_resize for more details
         augmented_image = tf.image.crop_and_resize(
             image,  # image shape: [B, H, W, C]
```

## keras_cv/layers/preprocessing/random_crop_and_resize_test.py

```diff
@@ -55,39 +55,25 @@
         output = layer(image, training=True)
 
         input_image_resized = tf.image.resize(image, self.target_size)
 
         self.assertAllEqual(output.shape, (4, 224, 224, 1))
         self.assertNotAllClose(output, input_image_resized)
 
-    def test_preserves_image(self):
-        image_shape = (self.batch_size, self.height, self.width, 3)
-        image = tf.random.uniform(shape=image_shape)
-
-        layer = preprocessing.RandomCropAndResize(
-            target_size=self.target_size,
-            aspect_ratio_factor=(3 / 4, 4 / 3),
-            crop_area_factor=(0.8, 1.0),
-        )
-
-        input_resized = tf.image.resize(image, self.target_size)
-        output = layer(image, training=False)
-
-        self.assertAllClose(output, input_resized)
-
     @parameterized.named_parameters(
         ("Not tuple or list", dict()),
         ("Length not equal to 2", [1, 2, 3]),
         ("Members not int", (2.3, 4.5)),
         ("Single integer", 5),
     )
     def test_target_size_errors(self, target_size):
         with self.assertRaisesRegex(
             ValueError,
-            "`target_size` must be tuple of two integers. Received target_size=(.*)",
+            "`target_size` must be tuple of two integers. "
+            "Received target_size=(.*)",
         ):
             _ = preprocessing.RandomCropAndResize(
                 target_size=target_size,
                 aspect_ratio_factor=(3 / 4, 4 / 3),
                 crop_area_factor=(0.8, 1.0),
             )
 
@@ -96,15 +82,16 @@
         ("Single integer", 5),
         ("Single float", 5.0),
     )
     def test_aspect_ratio_factor_errors(self, aspect_ratio_factor):
         with self.assertRaisesRegex(
             ValueError,
             "`aspect_ratio_factor` must be tuple of two positive floats or "
-            "keras_cv.core.FactorSampler instance. Received aspect_ratio_factor=(.*)",
+            "keras_cv.core.FactorSampler instance. "
+            "Received aspect_ratio_factor=(.*)",
         ):
             _ = preprocessing.RandomCropAndResize(
                 target_size=(224, 224),
                 aspect_ratio_factor=aspect_ratio_factor,
                 crop_area_factor=(0.8, 1.0),
             )
 
@@ -112,31 +99,31 @@
         ("Not tuple or list", dict()),
         ("Single integer", 5),
         ("Single float", 5.0),
     )
     def test_crop_area_factor_errors(self, crop_area_factor):
         with self.assertRaisesRegex(
             ValueError,
-            "`crop_area_factor` must be tuple of two positive floats less than or "
-            "equal to 1 or keras_cv.core.FactorSampler instance. Received "
-            "crop_area_factor=(.*)",
+            "`crop_area_factor` must be tuple of two positive floats less than "
+            "or equal to 1 or keras_cv.core.FactorSampler instance. "
+            "Received crop_area_factor=(.*)",
         ):
             _ = preprocessing.RandomCropAndResize(
                 target_size=(224, 224),
                 aspect_ratio_factor=(3 / 4, 4 / 3),
                 crop_area_factor=crop_area_factor,
             )
 
     def test_augment_sparse_segmentation_mask(self):
-        classes = 8
+        num_classes = 8
 
         input_image_shape = (1, self.height, self.width, 3)
         mask_shape = (1, self.height, self.width, 1)
         image = tf.random.uniform(shape=input_image_shape, seed=self.seed)
-        mask = np.random.randint(2, size=mask_shape) * (classes - 1)
+        mask = np.random.randint(2, size=mask_shape) * (num_classes - 1)
 
         inputs = {"images": image, "segmentation_masks": mask}
 
         # Crop-only to exactly 1/2 of the size
         layer = preprocessing.RandomCropAndResize(
             target_size=(150, 150),
             aspect_ratio_factor=(1, 1),
@@ -156,22 +143,25 @@
             crop_area_factor=(0.8, 1.0),
             seed=self.seed,
         )
         output = layer(inputs, training=True)
         self.assertAllInSet(output["segmentation_masks"], [0, 7])
 
     def test_augment_one_hot_segmentation_mask(self):
-        classes = 8
+        num_classes = 8
 
         input_image_shape = (1, self.height, self.width, 3)
         mask_shape = (1, self.height, self.width, 1)
         image = tf.random.uniform(shape=input_image_shape, seed=self.seed)
         mask = tf.one_hot(
-            tf.squeeze(np.random.randint(2, size=mask_shape) * (classes - 1), axis=-1),
-            classes,
+            tf.squeeze(
+                np.random.randint(2, size=mask_shape) * (num_classes - 1),
+                axis=-1,
+            ),
+            num_classes,
         )
 
         inputs = {"images": image, "segmentation_masks": mask}
 
         # Crop-only to exactly 1/2 of the size
         layer = preprocessing.RandomCropAndResize(
             target_size=(150, 150),
@@ -201,16 +191,20 @@
         )
         output = layer(input, training=True)
 
         expected_output = {
             "boxes": tf.convert_to_tensor([[0, 0, 1, 1]], dtype=tf.float32),
             "classes": tf.convert_to_tensor([0], dtype=tf.float32),
         }
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
-        self.assertAllClose(expected_output["boxes"], output["bounding_boxes"]["boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
+        self.assertAllClose(
+            expected_output["boxes"], output["bounding_boxes"]["boxes"]
+        )
         self.assertAllClose(
             expected_output["classes"], output["bounding_boxes"]["classes"]
         )
 
     def test_augment_boxes_batched_input(self):
         image = tf.zeros([20, 20, 3])
 
@@ -236,16 +230,20 @@
                 [
                     [[0, 0, 1, 1], [0, 0, 1, 1]],
                     [[0, 0, 1, 1], [0, 0, 1, 1]],
                 ]
             ),
             "classes": tf.convert_to_tensor([[0, 0], [0, 0]]),
         }
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
-        self.assertAllClose(expected_output["boxes"], output["bounding_boxes"]["boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
+        self.assertAllClose(
+            expected_output["boxes"], output["bounding_boxes"]["boxes"]
+        )
         self.assertAllClose(
             expected_output["classes"], output["bounding_boxes"]["classes"]
         )
 
     def test_augment_boxes_ragged(self):
         image = tf.zeros([2, 20, 20, 3])
         boxes = {
@@ -267,12 +265,16 @@
         expected_output = {
             "boxes": tf.ragged.constant(
                 [[[0, 0, 1, 1], [0, 0, 1, 1]], [[0, 0, 1, 1]]], dtype=tf.float32
             ),
             "classes": tf.ragged.constant([[0, 0], [0]]),
         }
         expected_output = bounding_box.to_dense(expected_output)
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
-        self.assertAllClose(expected_output["boxes"], output["bounding_boxes"]["boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
+        self.assertAllClose(
+            expected_output["boxes"], output["bounding_boxes"]["boxes"]
+        )
         self.assertAllClose(
             expected_output["classes"], output["bounding_boxes"]["classes"]
         )
```

## keras_cv/layers/preprocessing/random_crop_test.py

```diff
@@ -1,25 +1,27 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import unittest
 
 import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.random_crop import RandomCrop
 
 
 class RandomCropTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(
         ("random_crop_4_by_6", 4, 6),
@@ -52,29 +54,34 @@
         np.random.seed(1337)
         height, width = 10, 8
         inp = np.random.random((12, 3, 3, 3))
         layer = RandomCrop(height, width)
         actual_output = layer(inp)
         # In this case, output should equal resizing with crop_to_aspect
         # ratio.
-        resizing_layer = tf.keras.layers.Resizing(height, width)
+        resizing_layer = keras.layers.Resizing(height, width)
         expected_output = resizing_layer(inp)
         self.assertAllEqual(expected_output, actual_output)
 
     def test_training_with_mock(self):
         np.random.seed(1337)
+        batch_size = 12
         height, width = 3, 4
         height_offset = np.random.randint(low=0, high=3)
         width_offset = np.random.randint(low=0, high=5)
-        mock_offset = [height_offset, width_offset]
+        # manually compute transformations which shift height_offset and
+        # width_offset respectively
+        tops = tf.ones((batch_size, 1)) * (height_offset / (5 - height))
+        lefts = tf.ones((batch_size, 1)) * (width_offset / (8 - width))
+        transformations = {"tops": tops, "lefts": lefts}
         layer = RandomCrop(height, width)
         with unittest.mock.patch.object(
-            layer._random_generator,
-            "random_uniform",
-            return_value=mock_offset,
+            layer,
+            "get_random_transformation_batch",
+            return_value=transformations,
         ):
             inp = np.random.random((12, 5, 8, 3))
             actual_output = layer(inp, training=True)
             expected_output = inp[
                 :,
                 height_offset : (height_offset + height),
                 width_offset : (width_offset + width),
@@ -86,85 +93,169 @@
         np.random.seed(1337)
         height, width = 8, 16
         inp = np.random.random((12, 8, 16, 3))
         layer = RandomCrop(height, width)
         actual_output = layer(inp, training=False)
         self.assertAllClose(inp, actual_output)
 
-    def test_config_with_custom_name(self):
-        layer = RandomCrop(5, 5, name="image_preproc")
-        config = layer.get_config()
-        layer_1 = RandomCrop.from_config(config)
-        self.assertEqual(layer_1.name, layer.name)
-
     def test_unbatched_image(self):
         np.random.seed(1337)
         inp = np.random.random((16, 16, 3))
-        mock_offset = [2, 2]
+        # manually compute transformations which shift 2 pixels
+        mock_offset = tf.ones(shape=(1, 1), dtype=tf.float32) * 0.25
         layer = RandomCrop(8, 8)
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
             return_value=mock_offset,
         ):
             actual_output = layer(inp, training=True)
             self.assertAllClose(inp[2:10, 2:10, :], actual_output)
 
     def test_batched_input(self):
         np.random.seed(1337)
         inp = np.random.random((20, 16, 16, 3))
-        mock_offset = [2, 2]
+        # manually compute transformations which shift 2 pixels
+        mock_offset = tf.ones(shape=(20, 1), dtype=tf.float32) * 2 / (16 - 8)
         layer = RandomCrop(8, 8)
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
             return_value=mock_offset,
         ):
             actual_output = layer(inp, training=True)
             self.assertAllClose(inp[:, 2:10, 2:10, :], actual_output)
 
-    def test_output_dtypes(self):
-        inputs = np.array([[[1], [2]], [[3], [4]]], dtype="float64")
-        layer = RandomCrop(2, 2)
-        self.assertAllEqual(layer(inputs).dtype, "float32")
-        layer = RandomCrop(2, 2, dtype="uint8")
-        self.assertAllEqual(layer(inputs).dtype, "uint8")
-
-    def test_compute_output_signature(self):
-        inputs = np.random.random((16, 16, 3))
+    def test_compute_ragged_output_signature(self):
+        inputs = tf.ragged.stack(
+            [
+                np.random.random(size=(8, 8, 3)).astype("float32"),
+                np.random.random(size=(16, 8, 3)).astype("float32"),
+            ]
+        )
         layer = RandomCrop(2, 2)
         output = layer(inputs)
-        output_signature = layer.compute_image_signature(inputs).shape
-        self.assertAllEqual(output.shape, output_signature)
+        output_signature = layer.compute_ragged_image_signature(inputs).shape
+        self.assertAllEqual(output.shape[1:], output_signature)
 
     def test_augment_bounding_boxes_crop(self):
-        input_image = np.random.random((512, 512, 3)).astype(np.float32)
+        orig_height, orig_width = 512, 512
+        height, width = 100, 200
+        input_image = np.random.random((orig_height, orig_width, 3)).astype(
+            np.float32
+        )
         bboxes = {
             "boxes": tf.convert_to_tensor([[200, 200, 400, 400]]),
             "classes": tf.convert_to_tensor([1]),
         }
         input = {"images": input_image, "bounding_boxes": bboxes}
-        layer = RandomCrop(height=100, width=200, bounding_box_format="xyxy", seed=10)
         # for top = 300 and left = 305
-        output = layer(input)
-        expected_output = np.asarray(
-            [[0.0, 0.0, 95.0, 100.0]],
-        )
-        self.assertAllClose(
-            expected_output, output["bounding_boxes"]["boxes"].to_tensor(-1)
+        height_offset = 300
+        width_offset = 305
+        tops = tf.ones((1, 1)) * (height_offset / (orig_height - height))
+        lefts = tf.ones((1, 1)) * (width_offset / (orig_width - width))
+        transformations = {"tops": tops, "lefts": lefts}
+        layer = RandomCrop(
+            height=height, width=width, bounding_box_format="xyxy"
         )
+        with unittest.mock.patch.object(
+            layer,
+            "get_random_transformation_batch",
+            return_value=transformations,
+        ):
+            output = layer(input)
+            expected_output = np.asarray(
+                [[0.0, 0.0, 95.0, 100.0]],
+            )
+        self.assertAllClose(expected_output, output["bounding_boxes"]["boxes"])
 
     def test_augment_bounding_boxes_resize(self):
         input_image = np.random.random((256, 256, 3)).astype(np.float32)
         bboxes = {
             "boxes": tf.convert_to_tensor([[100, 100, 200, 200]]),
             "classes": tf.convert_to_tensor([1]),
         }
         input = {"images": input_image, "bounding_boxes": bboxes}
         layer = RandomCrop(height=512, width=512, bounding_box_format="xyxy")
         output = layer(input)
         expected_output = np.asarray(
             [[200.0, 200.0, 400.0, 400.0]],
         )
-        self.assertAllClose(
-            expected_output, output["bounding_boxes"]["boxes"].to_tensor(-1)
+        self.assertAllClose(expected_output, output["bounding_boxes"]["boxes"])
+
+    def test_in_tf_function(self):
+        np.random.seed(1337)
+        inp = np.random.random((20, 16, 16, 3))
+        mock_offset = tf.ones(shape=(20, 1), dtype=tf.float32) * 2 / (16 - 8)
+        layer = RandomCrop(8, 8)
+
+        @tf.function
+        def augment(x):
+            return layer(x, training=True)
+
+        with unittest.mock.patch.object(
+            layer._random_generator,
+            "random_uniform",
+            return_value=mock_offset,
+        ):
+            actual_output = augment(inp)
+            self.assertAllClose(inp[:, 2:10, 2:10, :], actual_output)
+
+    def test_random_crop_on_batched_images_independently(self):
+        image = tf.random.uniform((100, 100, 3))
+        batched_images = tf.stack((image, image), axis=0)
+        layer = RandomCrop(height=25, width=25)
+
+        results = layer(batched_images)
+
+        self.assertNotAllClose(results[0], results[1])
+
+    def test_random_crop_on_batched_ragged_images_and_bounding_boxes(self):
+        images = tf.ragged.constant(
+            [np.ones((8, 8, 3)), np.ones((4, 8, 3))], dtype="float32"
+        )
+        boxes = {
+            "boxes": tf.ragged.stack(
+                [
+                    tf.ones((3, 4), dtype=tf.float32),
+                    tf.ones((3, 4), dtype=tf.float32),
+                ],
+            ),
+            "classes": tf.ragged.stack(
+                [
+                    tf.ones((3,), dtype=tf.float32),
+                    tf.ones((3,), dtype=tf.float32),
+                ],
+            ),
+        }
+        inputs = {"images": images, "bounding_boxes": boxes}
+        layer = RandomCrop(height=2, width=2, bounding_box_format="xyxy")
+
+        results = layer(inputs)
+
+        self.assertTrue(isinstance(results["images"], tf.Tensor))
+        self.assertTrue(
+            isinstance(results["bounding_boxes"]["boxes"], tf.RaggedTensor)
+        )
+        self.assertTrue(
+            isinstance(results["bounding_boxes"]["classes"], tf.RaggedTensor)
         )
+
+    def test_config_with_custom_name(self):
+        layer = RandomCrop(5, 5, name="image_preproc")
+        config = layer.get_config()
+        layer_1 = RandomCrop.from_config(config)
+        self.assertEqual(layer_1.name, layer.name)
+
+    def test_output_dtypes(self):
+        inputs = np.array([[[1], [2]], [[3], [4]]], dtype="float64")
+        layer = RandomCrop(2, 2)
+        self.assertAllEqual(layer(inputs).dtype, "float32")
+        layer = RandomCrop(2, 2, dtype="uint8")
+        self.assertAllEqual(layer(inputs).dtype, "uint8")
+
+    def test_config(self):
+        layer = RandomCrop(height=2, width=3, bounding_box_format="xyxy")
+        config = layer.get_config()
+        self.assertEqual(config["height"], 2)
+        self.assertEqual(config["width"], 3)
+        self.assertEqual(config["bounding_box_format"], "xyxy")
```

## keras_cv/layers/preprocessing/random_cutout.py

```diff
@@ -7,59 +7,61 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import fill_utils
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomCutout(BaseImageAugmentationLayer):
     """Randomly cut out rectangles from images and fill them.
 
     Args:
         height_factor: A tuple of two floats, a single float or a
-            `keras_cv.FactorSampler`.  `height_factor` controls the size of the
-            cutouts. `height_factor=0.0` means the rectangle will be of size 0% of the
-            image height, `height_factor=0.1` means the rectangle will have a size of
-            10% of the image height, and so forth.
-            Values should be between `0.0` and `1.0`.  If a tuple is used, a
-            `height_factor` is sampled between the two values for every image augmented.
-            If a single float is used, a value between `0.0` and the passed float is
-            sampled.  In order to ensure the value is always the same, please pass a
-            tuple with two identical floats: `(0.5, 0.5)`.
+            `keras_cv.FactorSampler`. `height_factor` controls the size of the
+            cutouts. `height_factor=0.0` means the rectangle will be of size 0%
+            of the image height, `height_factor=0.1` means the rectangle will
+            have a size of 10% of the image height, and so forth. Values should
+            be between `0.0` and `1.0`. If a tuple is used, a `height_factor`
+            is sampled between the two values for every image augmented. If a
+            single float is used, a value between `0.0` and the passed float is
+            sampled. In order to ensure the value is always the same, please
+            pass a tuple with two identical floats: `(0.5, 0.5)`.
         width_factor: A tuple of two floats, a single float or a
-            `keras_cv.FactorSampler`.  `width_factor` controls the size of the
-            cutouts. `width_factor=0.0` means the rectangle will be of size 0% of the
-            image height, `width_factor=0.1` means the rectangle will have a size of 10%
-            of the image width, and so forth.
-            Values should be between `0.0` and `1.0`.  If a tuple is used, a
-            `width_factor` is sampled between the two values for every image augmented.
-            If a single float is used, a value between `0.0` and the passed float is
-            sampled.  In order to ensure the value is always the same, please pass a
-            tuple with two identical floats: `(0.5, 0.5)`.
+            `keras_cv.FactorSampler`. `width_factor` controls the size of the
+            cutouts. `width_factor=0.0` means the rectangle will be of size 0%
+            of the image height, `width_factor=0.1` means the rectangle will
+            have a size of 10% of the image width, and so forth.
+            Values should be between `0.0` and `1.0`. If a tuple is used, a
+            `width_factor` is sampled between the two values for every image
+            augmented. If a single float is used, a value between `0.0` and the
+            passed float is sampled. In order to ensure the value is always the
+            same, please pass a tuple with two identical floats: `(0.5, 0.5)`.
         fill_mode: Pixels inside the patches are filled according to the given
             mode (one of `{"constant", "gaussian_noise"}`).
             - *constant*: Pixels are filled with the same constant value.
             - *gaussian_noise*: Pixels are filled with random gaussian noise.
         fill_value: a float represents the value to be filled inside the patches
             when `fill_mode="constant"`.
         seed: Integer. Used to create a random seed.
 
     Sample usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     random_cutout = keras_cv.layers.preprocessing.RandomCutout(0.5, 0.5)
     augmented_images = random_cutout(images)
     ```
     """
 
     def __init__(
         self,
@@ -81,15 +83,15 @@
         self.fill_mode = fill_mode
         self.fill_value = fill_value
         self.seed = seed
 
         if fill_mode not in ["gaussian_noise", "constant"]:
             raise ValueError(
                 '`fill_mode` should be "gaussian_noise" '
-                f'or "constant".  Got `fill_mode`={fill_mode}'
+                f'or "constant". Got `fill_mode`={fill_mode}'
             )
 
     def _parse_bounds(self, factor):
         if isinstance(factor, (tuple, list)):
             return factor[0], factor[1]
         else:
             return type(factor)(0), factor
```

## keras_cv/layers/preprocessing/random_cutout_test.py

```diff
@@ -40,23 +40,27 @@
         self.assertTrue(tf.math.reduce_any(xs[0] == 2.0))
         self.assertTrue(tf.math.reduce_any(xs[1] == fill_value))
         self.assertTrue(tf.math.reduce_any(xs[1] == 1.0))
 
     def test_return_shapes(self):
         xs = tf.ones((2, 512, 512, 3))
 
-        layer = preprocessing.RandomCutout(height_factor=0.5, width_factor=0.5, seed=1)
+        layer = preprocessing.RandomCutout(
+            height_factor=0.5, width_factor=0.5, seed=1
+        )
         xs = layer(xs)
 
         self.assertEqual(xs.shape, [2, 512, 512, 3])
 
     def test_return_shapes_single_element(self):
         xs = tf.ones((512, 512, 3))
 
-        layer = preprocessing.RandomCutout(height_factor=0.5, width_factor=0.5, seed=1)
+        layer = preprocessing.RandomCutout(
+            height_factor=0.5, width_factor=0.5, seed=1
+        )
         xs = layer(xs)
 
         self.assertEqual(xs.shape, [512, 512, 3])
 
     def test_random_cutout_single_float(self):
         self._run_test(0.5, 0.5)
 
@@ -118,15 +122,17 @@
         self.assertTrue(tf.math.reduce_any(xs[0] == fill_value))
         self.assertTrue(tf.math.reduce_any(xs[0] == 2.0))
         self.assertTrue(tf.math.reduce_any(xs[1] == fill_value))
         self.assertTrue(tf.math.reduce_any(xs[1] == 1.0))
 
     def test_in_tf_function(self):
         xs = tf.cast(
-            tf.stack([2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0),
+            tf.stack(
+                [2 * tf.ones((100, 100, 1)), tf.ones((100, 100, 1))], axis=0
+            ),
             tf.float32,
         )
 
         patch_value = 0.0
         layer = preprocessing.RandomCutout(
             height_factor=0.5,
             width_factor=0.5,
```

## keras_cv/layers/preprocessing/random_flip.py

```diff
@@ -1,67 +1,78 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
 
 # In order to support both unbatched and batched inputs, the horizontal
 # and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 # Defining modes for random flipping
 HORIZONTAL = "horizontal"
 VERTICAL = "vertical"
 HORIZONTAL_AND_VERTICAL = "horizontal_and_vertical"
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomFlip(BaseImageAugmentationLayer):
-    """A preprocessing layer which randomly flips images during training.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomFlip(VectorizedBaseImageAugmentationLayer):
+    """A preprocessing layer which randomly flips images.
 
     This layer will flip the images horizontally and or vertically based on the
-    `mode` attribute. During inference time, the output will be identical to
-    input. Call the layer with `training=True` to flip the input.
+    `mode` attribute.
 
     Input shape:
-      3D (unbatched) or 4D (batched) tensor with shape:
-      `(..., height, width, channels)`, in `"channels_last"` format.
+        3D (unbatched) or 4D (batched) tensor with shape:
+        `(..., height, width, channels)`, in `"channels_last"` format.
 
     Output shape:
-      3D (unbatched) or 4D (batched) tensor with shape:
-      `(..., height, width, channels)`, in `"channels_last"` format.
+        3D (unbatched) or 4D (batched) tensor with shape:
+        `(..., height, width, channels)`, in `"channels_last"` format.
 
-    Arguments:
-      mode: String indicating which flip mode to use. Can be `"horizontal"`,
-        `"vertical"`, or `"horizontal_and_vertical"`. Defaults to
-        `"horizontal"`. `"horizontal"` is a left-right flip and `"vertical"` is
-        a top-bottom flip.
-      seed: Integer. Used to create a random seed.
-      bounding_box_format: The format of bounding boxes of input dataset. Refer to
-        https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box/converters.py
-        for more details on supported bounding box formats.
-    """
-
-    def __init__(self, mode=HORIZONTAL, seed=None, bounding_box_format=None, **kwargs):
+    Args:
+        mode: String indicating which flip mode to use. Can be `"horizontal"`,
+            `"vertical"`, or `"horizontal_and_vertical"`, defaults to
+            `"horizontal"`. `"horizontal"` is a left-right flip and
+            `"vertical"` is a top-bottom flip.
+        rate: A float that controls the frequency of flipping. 1.0 indicates
+            that images are always flipped. 0.0 indicates no flipping.
+            Defaults to 0.5.
+        seed: Integer. Used to create a random seed.
+        bounding_box_format: The format of bounding boxes of input dataset.
+            Refer to
+            https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box/converters.py
+            for more details on supported bounding box formats.
+    """  # noqa: E501
+
+    def __init__(
+        self,
+        mode=HORIZONTAL,
+        rate=0.5,
+        seed=None,
+        bounding_box_format=None,
+        **kwargs,
+    ):
         super().__init__(seed=seed, force_generator=True, **kwargs)
         self.mode = mode
         self.seed = seed
         if mode == HORIZONTAL:
             self.horizontal = True
             self.vertical = False
         elif mode == VERTICAL:
@@ -71,130 +82,170 @@
             self.horizontal = True
             self.vertical = True
         else:
             raise ValueError(
                 "RandomFlip layer {name} received an unknown mode="
                 "{arg}".format(name=self.name, arg=mode)
             )
-        self.auto_vectorize = True
         self.bounding_box_format = bounding_box_format
+        if rate < 0.0 or rate > 1.0:
+            raise ValueError(
+                f"`rate` should be inside of range [0, 1]. Got rate={rate}"
+            )
+        self.rate = rate
 
-    def augment_label(self, label, transformation, **kwargs):
-        return label
-
-    def augment_image(self, image, transformation, **kwargs):
-        return RandomFlip._flip_image(image, transformation)
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        flip_horizontals = tf.zeros(shape=(batch_size, 1))
+        flip_verticals = tf.zeros(shape=(batch_size, 1))
 
-    def get_random_transformation(self, **kwargs):
-        flip_horizontal = False
-        flip_vertical = False
         if self.horizontal:
-            flip_horizontal = self._random_generator.random_uniform(shape=[]) > 0.5
+            flip_horizontals = self._random_generator.random_uniform(
+                shape=(batch_size, 1)
+            )
+
         if self.vertical:
-            flip_vertical = self._random_generator.random_uniform(shape=[]) > 0.5
+            flip_verticals = self._random_generator.random_uniform(
+                shape=(batch_size, 1)
+            )
+
         return {
-            "flip_horizontal": tf.cast(flip_horizontal, dtype=tf.bool),
-            "flip_vertical": tf.cast(flip_vertical, dtype=tf.bool),
+            "flip_horizontals": flip_horizontals,
+            "flip_verticals": flip_verticals,
         }
 
-    def _flip_image(image, transformation):
-        flipped_output = tf.cond(
-            transformation["flip_horizontal"],
-            lambda: tf.image.flip_left_right(image),
-            lambda: image,
-        )
-        flipped_output = tf.cond(
-            transformation["flip_vertical"],
-            lambda: tf.image.flip_up_down(flipped_output),
-            lambda: flipped_output,
-        )
-        flipped_output.set_shape(image.shape)
-        return flipped_output
-
-    def _flip_bounding_boxes_horizontal(bounding_boxes):
-        x1, x2, x3, x4 = tf.split(bounding_boxes["boxes"], [1, 1, 1, 1], axis=-1)
-        output = tf.stack(
-            [
-                1 - x3,
-                x2,
-                1 - x1,
-                x4,
-            ],
-            axis=-1,
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        image = tf.expand_dims(image, axis=0)
+        flip_horizontals = transformation["flip_horizontals"]
+        flip_verticals = transformation["flip_verticals"]
+        transformation = {
+            "flip_horizontals": tf.expand_dims(flip_horizontals, axis=0),
+            "flip_verticals": tf.expand_dims(flip_verticals, axis=0),
+        }
+        image = self.augment_images(
+            images=image, transformations=transformation, **kwargs
         )
-        bounding_boxes = bounding_boxes.copy()
-        bounding_boxes["boxes"] = tf.squeeze(output, axis=1)
-        return bounding_boxes
+        return tf.squeeze(image, axis=0)
 
-    def _flip_bounding_boxes_vertical(bounding_boxes):
-        x1, x2, x3, x4 = tf.split(bounding_boxes["boxes"], [1, 1, 1, 1], axis=-1)
-        output = tf.stack(
-            [
-                x1,
-                1 - x4,
-                x3,
-                1 - x2,
-            ],
-            axis=-1,
-        )
-        output = tf.squeeze(output, axis=1)
-        bounding_boxes = bounding_boxes.copy()
-        bounding_boxes["boxes"] = output
-        return bounding_boxes
+    def augment_images(self, images, transformations, **kwargs):
+        return self._flip_images(images, transformations)
+
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
 
     def augment_bounding_boxes(
-        self, bounding_boxes, transformation=None, image=None, **kwargs
+        self, bounding_boxes, transformations=None, raw_images=None, **kwargs
     ):
         if self.bounding_box_format is None:
             raise ValueError(
                 "`RandomFlip()` was called with bounding boxes,"
                 "but no `bounding_box_format` was specified in the constructor."
                 "Please specify a bounding box format in the constructor. i.e."
                 "`RandomFlip(bounding_box_format='xyxy')`"
             )
-        bounding_boxes = bounding_boxes.copy()
+        bounding_boxes = bounding_box.to_dense(bounding_boxes)
         bounding_boxes = bounding_box.convert_format(
             bounding_boxes,
             source=self.bounding_box_format,
             target="rel_xyxy",
-            images=image,
+            images=raw_images,
         )
-        bounding_boxes = tf.cond(
-            transformation["flip_horizontal"],
-            lambda: RandomFlip._flip_bounding_boxes_horizontal(bounding_boxes),
-            lambda: bounding_boxes,
-        )
-        bounding_boxes = tf.cond(
-            transformation["flip_vertical"],
-            lambda: RandomFlip._flip_bounding_boxes_vertical(bounding_boxes),
-            lambda: bounding_boxes,
+        boxes = bounding_boxes["boxes"]
+        batch_size = tf.shape(boxes)[0]
+        max_boxes = tf.shape(boxes)[1]
+        flip_horizontals = transformations["flip_horizontals"]
+        flip_verticals = transformations["flip_verticals"]
+
+        # broadcast
+        flip_horizontals = (
+            tf.ones(shape=(batch_size, max_boxes, 4))
+            * flip_horizontals[:, tf.newaxis, :]
+        )
+        flip_verticals = (
+            tf.ones(shape=(batch_size, max_boxes, 4))
+            * flip_verticals[:, tf.newaxis, :]
+        )
+
+        boxes = tf.where(
+            flip_horizontals > (1.0 - self.rate),
+            self._flip_boxes_horizontal(boxes),
+            boxes,
+        )
+        boxes = tf.where(
+            flip_verticals > (1.0 - self.rate),
+            self._flip_boxes_vertical(boxes),
+            boxes,
         )
+
+        bounding_boxes = bounding_boxes.copy()
+        bounding_boxes["boxes"] = boxes
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes,
             bounding_box_format="rel_xyxy",
-            images=image,
+            images=raw_images,
         )
         bounding_boxes = bounding_box.convert_format(
             bounding_boxes,
             source="rel_xyxy",
             target=self.bounding_box_format,
             dtype=self.compute_dtype,
-            images=image,
+            images=raw_images,
         )
-        return bounding_box.to_ragged(bounding_boxes)
+        return bounding_boxes
 
-    def augment_segmentation_mask(
-        self, segmentation_mask, transformation=None, **kwargs
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations=None, **kwargs
     ):
-        return RandomFlip._flip_image(segmentation_mask, transformation)
+        return self._flip_images(segmentation_masks, transformations)
 
-    def compute_output_shape(self, input_shape):
-        return input_shape
+    def _flip_images(self, images, transformations):
+        batch_size = tf.shape(images)[0]
+        height, width = tf.shape(images)[1], tf.shape(images)[2]
+        channel = tf.shape(images)[3]
+        flip_horizontals = transformations["flip_horizontals"]
+        flip_verticals = transformations["flip_verticals"]
+
+        # broadcast
+        flip_horizontals = (
+            tf.ones(shape=(batch_size, height, width, channel))
+            * flip_horizontals[:, tf.newaxis, tf.newaxis, :]
+        )
+        flip_verticals = (
+            tf.ones(shape=(batch_size, height, width, channel))
+            * flip_verticals[:, tf.newaxis, tf.newaxis, :]
+        )
+
+        flipped_outputs = tf.where(
+            flip_horizontals > (1.0 - self.rate),
+            tf.image.flip_left_right(images),
+            images,
+        )
+        flipped_outputs = tf.where(
+            flip_verticals > (1.0 - self.rate),
+            tf.image.flip_up_down(flipped_outputs),
+            flipped_outputs,
+        )
+        flipped_outputs.set_shape(images.shape)
+        return flipped_outputs
+
+    def _flip_boxes_horizontal(self, boxes):
+        x1, x2, x3, x4 = tf.split(boxes, 4, axis=-1)
+        outputs = tf.concat([1 - x3, x2, 1 - x1, x4], axis=-1)
+        return outputs
+
+    def _flip_boxes_vertical(self, boxes):
+        x1, x2, x3, x4 = tf.split(boxes, 4, axis=-1)
+        outputs = tf.concat([x1, 1 - x4, x3, 1 - x2], axis=-1)
+        return outputs
 
     def get_config(self):
         config = {
             "mode": self.mode,
+            "rate": self.rate,
             "seed": self.seed,
             "bounding_box_format": self.bounding_box_format,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_flip_test.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,219 +14,253 @@
 import unittest
 
 import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_cv import bounding_box
+from keras_cv.layers.preprocessing.random_flip import HORIZONTAL_AND_VERTICAL
 from keras_cv.layers.preprocessing.random_flip import RandomFlip
 
 
 class RandomFlipTest(tf.test.TestCase, parameterized.TestCase):
     def test_horizontal_flip(self):
         np.random.seed(1337)
-        mock_random = [0.6, 0.6]
+        mock_random = tf.convert_to_tensor([[0.6], [0.6]])
         inp = np.random.random((2, 5, 8, 3))
         expected_output = np.flip(inp, axis=2)
         layer = RandomFlip("horizontal")
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
-            side_effect=mock_random,
+            return_value=mock_random,
         ):
-            actual_output = layer(inp, training=True)
+            actual_output = layer(inp)
             self.assertAllClose(expected_output, actual_output)
 
     def test_flip_ragged(self):
-        images = tf.ragged.stack([tf.ones((512, 512, 3)), tf.ones((1002, 512, 3))])
+        images = tf.ragged.stack(
+            [tf.ones((512, 512, 3)), tf.ones((1002, 512, 3))]
+        )
         bounding_boxes = {
             "boxes": tf.ragged.stack([tf.ones((5, 4)), tf.ones((3, 4))]),
             "classes": tf.ragged.stack([tf.ones((5,)), tf.ones((3,))]),
         }
         inputs = {"images": images, "bounding_boxes": bounding_boxes}
         layer = RandomFlip(mode="horizontal", bounding_box_format="xywh")
         _ = layer(inputs)
 
     def test_vertical_flip(self):
         np.random.seed(1337)
-        mock_random = [0.6, 0.6]
+        mock_random = tf.convert_to_tensor([[0.6], [0.6]])
         inp = np.random.random((2, 5, 8, 3))
         expected_output = np.flip(inp, axis=1)
         layer = RandomFlip("vertical")
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
-            side_effect=mock_random,
+            return_value=mock_random,
         ):
-            actual_output = layer(inp, training=True)
+            actual_output = layer(inp)
             self.assertAllClose(expected_output, actual_output)
 
     def test_flip_both(self):
         np.random.seed(1337)
-        mock_random = [0.6, 0.6, 0.6, 0.6]
+        mock_random = tf.convert_to_tensor([[0.6], [0.6]])
         inp = np.random.random((2, 5, 8, 3))
         expected_output = np.flip(inp, axis=2)
         expected_output = np.flip(expected_output, axis=1)
         layer = RandomFlip("horizontal_and_vertical")
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
-            side_effect=mock_random,
+            return_value=mock_random,
+        ):
+            actual_output = layer(inp)
+        self.assertAllClose(expected_output, actual_output)
+
+    def test_random_flip_default(self):
+        input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)
+        expected_output = np.flip(input_images, axis=2)
+        mock_random = tf.convert_to_tensor([[0.6], [0.6]])
+        layer = RandomFlip()
+        with unittest.mock.patch.object(
+            layer._random_generator,
+            "random_uniform",
+            return_value=mock_random,
         ):
-            actual_output = layer(inp, training=True)
+            actual_output = layer(input_images)
             self.assertAllClose(expected_output, actual_output)
 
-    def test_random_flip_inference(self):
+    def test_random_flip_low_rate(self):
         input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)
         expected_output = input_images
-        layer = RandomFlip()
-        actual_output = layer(input_images, training=False)
+        # mock_random > 0.5 but no flipping occurs due to low rate
+        mock_random = tf.convert_to_tensor([[0.6], [0.6]])
+        layer = RandomFlip(rate=0.1)
+        with unittest.mock.patch.object(
+            layer._random_generator,
+            "random_uniform",
+            return_value=mock_random,
+        ):
+            actual_output = layer(input_images)
         self.assertAllClose(expected_output, actual_output)
 
-    def test_random_flip_default(self):
+    def test_random_flip_high_rate(self):
         input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)
         expected_output = np.flip(input_images, axis=2)
-        mock_random = [0.6, 0.6, 0.6, 0.6]
-        layer = RandomFlip()
+        # mock_random is small (0.2) but flipping still occurs due to high rate
+        mock_random = tf.convert_to_tensor([[0.2], [0.2]])
+        layer = RandomFlip(rate=0.9)
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
-            side_effect=mock_random,
+            return_value=mock_random,
         ):
-            actual_output = layer(input_images, training=True)
-            self.assertAllClose(expected_output, actual_output)
+            actual_output = layer(input_images)
+        self.assertAllClose(expected_output, actual_output)
 
     def test_config_with_custom_name(self):
         layer = RandomFlip(name="image_preproc")
         config = layer.get_config()
         layer_1 = RandomFlip.from_config(config)
         self.assertEqual(layer_1.name, layer.name)
 
     def test_random_flip_unbatched_image(self):
         input_image = np.random.random((4, 4, 1)).astype(np.float32)
         expected_output = np.flip(input_image, axis=0)
-        mock_random = [0.6, 0.6, 0.6, 0.6]
+        mock_random = tf.convert_to_tensor([[0.6]])
         layer = RandomFlip("vertical")
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
-            side_effect=mock_random,
+            return_value=mock_random,
         ):
-            actual_output = layer(input_image, training=True)
+            actual_output = layer(input_image)
             self.assertAllClose(expected_output, actual_output)
 
     def test_output_dtypes(self):
         inputs = np.array([[[1], [2]], [[3], [4]]], dtype="float64")
         layer = RandomFlip()
         self.assertAllEqual(layer(inputs).dtype, "float32")
         layer = RandomFlip(dtype="uint8")
         self.assertAllEqual(layer(inputs).dtype, "uint8")
 
     def test_augment_bounding_box_batched_input(self):
         image = tf.zeros([20, 20, 3])
         bounding_boxes = {
             "boxes": tf.convert_to_tensor(
-                [[[0, 0, 10, 10], [4, 4, 12, 12]], [[4, 4, 12, 12], [0, 0, 10, 10]]],
+                [
+                    [[0, 0, 10, 10], [4, 4, 12, 12]],
+                    [[4, 4, 12, 12], [0, 0, 10, 10]],
+                ],
                 dtype=tf.float32,
             ),
             "classes": tf.convert_to_tensor(
                 [
-                    [
-                        0,
-                        0,
-                    ],
+                    [0, 0],
                     [0, 0],
                 ]
             ),
         }
 
         input = {"images": [image, image], "bounding_boxes": bounding_boxes}
-        mock_random = [0.6, 0.6, 0.6, 0.6]
-        layer = RandomFlip("horizontal_and_vertical", bounding_box_format="xyxy")
+        mock_random = tf.convert_to_tensor([[0.6], [0.6]])
+        layer = RandomFlip(
+            "horizontal_and_vertical", bounding_box_format="xyxy"
+        )
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
-            side_effect=mock_random,
+            return_value=mock_random,
         ):
-            output = layer(input, training=True)
+            output = layer(input)
 
         expected_output = {
             "boxes": tf.convert_to_tensor(
                 [
                     [[10, 10, 20, 20], [8, 8, 16, 16]],
                     [[8, 8, 16, 16], [10, 10, 20, 20]],
                 ]
             ),
             "classes": tf.convert_to_tensor(
                 [
-                    [
-                        0,
-                        0,
-                    ],
+                    [0, 0],
                     [0, 0],
                 ]
             ),
         }
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
-        self.assertAllClose(expected_output["boxes"], output["bounding_boxes"]["boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
+        self.assertAllClose(
+            expected_output["boxes"], output["bounding_boxes"]["boxes"]
+        )
         self.assertAllClose(
             expected_output["classes"], output["bounding_boxes"]["classes"]
         )
 
     def test_augment_boxes_ragged(self):
         image = tf.zeros([2, 20, 20, 3])
         bounding_boxes = {
             "boxes": tf.ragged.constant(
-                [[[0, 0, 10, 10], [4, 4, 12, 12]], [[0, 0, 10, 10]]], dtype=tf.float32
+                [[[0, 0, 10, 10], [4, 4, 12, 12]], [[0, 0, 10, 10]]],
+                dtype=tf.float32,
             ),
             "classes": tf.ragged.constant([[0, 0], [0]], dtype=tf.float32),
         }
 
         input = {"images": image, "bounding_boxes": bounding_boxes}
-        mock_random = [0.6, 0.6, 0.6, 0.6]
-        layer = RandomFlip("horizontal_and_vertical", bounding_box_format="xyxy")
+        mock_random = tf.convert_to_tensor([[0.6], [0.6]])
+        layer = RandomFlip(
+            "horizontal_and_vertical", bounding_box_format="xyxy"
+        )
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
-            side_effect=mock_random,
+            return_value=mock_random,
         ):
-            output = layer(input, training=True)
+            output = layer(input)
 
         expected_output = {
             "boxes": tf.ragged.constant(
                 [[[10, 10, 20, 20], [8, 8, 16, 16]], [[10, 10, 20, 20]]],
                 dtype=tf.float32,
             ),
             "classes": tf.ragged.constant([[0, 0], [0]], dtype=tf.float32),
         }
 
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
         expected_output = bounding_box.to_dense(expected_output)
-        self.assertAllClose(expected_output["boxes"], output["bounding_boxes"]["boxes"])
+        self.assertAllClose(
+            expected_output["boxes"], output["bounding_boxes"]["boxes"]
+        )
         self.assertAllClose(
             expected_output["classes"], output["bounding_boxes"]["classes"]
         )
 
     def test_augment_segmentation_mask(self):
         np.random.seed(1337)
         image = np.random.random((1, 20, 20, 3)).astype(np.float32)
         mask = np.random.randint(2, size=(1, 20, 20, 1)).astype(np.float32)
 
         input = {"images": image, "segmentation_masks": mask}
 
         # Flip both vertically and horizontally
-        mock_random = [0.6, 0.6]
+        mock_random = tf.convert_to_tensor([[0.6]])
         layer = RandomFlip("horizontal_and_vertical")
 
         with unittest.mock.patch.object(
             layer._random_generator,
             "random_uniform",
-            side_effect=mock_random,
+            return_value=mock_random,
         ):
-            output = layer(input, training=True)
+            output = layer(input)
 
         expected_mask = np.flip(np.flip(mask, axis=1), axis=2)
 
         self.assertAllClose(expected_mask, output["segmentation_masks"])
 
     def test_ragged_bounding_boxes(self):
         input_image = np.random.random((2, 512, 512, 3)).astype(np.float32)
@@ -240,7 +274,25 @@
             ),
             "classes": tf.ragged.constant([[0, 0], [0]], dtype=tf.float32),
         }
 
         input = {"images": input_image, "bounding_boxes": bounding_boxes}
         layer = RandomFlip(bounding_box_format="xyxy")
         _ = layer(input)
+
+    def test_independence_of_random_flip_on_batched_images(self):
+        image = tf.random.uniform((100, 100, 3))
+        batched_images = tf.stack((image, image), axis=0)
+        seed = 2023
+        layer = RandomFlip(mode=HORIZONTAL_AND_VERTICAL, seed=seed)
+
+        results = layer(batched_images)
+
+        self.assertNotAllClose(results[0], results[1])
+
+    def test_config(self):
+        layer = RandomFlip(
+            mode=HORIZONTAL_AND_VERTICAL, bounding_box_format="xyxy"
+        )
+        config = layer.get_config()
+        self.assertEqual(config["mode"], HORIZONTAL_AND_VERTICAL)
+        self.assertEqual(config["bounding_box_format"], "xyxy")
```

## keras_cv/layers/preprocessing/random_gaussian_blur.py

```diff
@@ -9,36 +9,38 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomGaussianBlur(BaseImageAugmentationLayer):
     """Applies a Gaussian Blur with random strength to an image.
 
     Args:
-        kernel_size: int, 2 element tuple or 2 element list. x and y dimensions for
-            the kernel used. If tuple or list, first element is used for the x dimension
-            and second element is used for y dimension. If int, kernel will be squared.
+        kernel_size: int, 2 element tuple or 2 element list. x and y dimensions
+            for the kernel used. If tuple or list, first element is used for the
+            x dimension and second element is used for y dimension. If int,
+            kernel will be squared.
         factor: A tuple of two floats, a single float or a
             `keras_cv.FactorSampler`. `factor` controls the extent to which the
-            image is blurred.  Mathematically, `factor` represents the `sigma` value in
-            a gaussian blur. `factor=0.0` makes this layer perform a no-op
-            operation, and high values make the blur stronger. In order to
-            ensure the value is always the same, please pass a tuple with two identical
-            floats: `(0.5, 0.5)`.
+            image is blurred. Mathematically, `factor` represents the `sigma`
+            value in a gaussian blur. `factor=0.0` makes this layer perform a
+            no-op operation, and high values make the blur stronger. In order to
+            ensure the value is always the same, please pass a tuple with two
+            identical floats: `(0.5, 0.5)`.
     """
 
     def __init__(self, kernel_size, factor, **kwargs):
         super().__init__(**kwargs)
 
         self.factor = preprocessing.parse_factor(
             factor, min_value=0.0, max_value=None, param_name="factor"
@@ -88,26 +90,31 @@
 
     def augment_bounding_boxes(self, bounding_boxes, **kwargs):
         return bounding_boxes
 
     def augment_label(self, label, transformation=None, **kwargs):
         return label
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return segmentation_mask
 
     @staticmethod
     def get_kernel(factor, filter_size):
-        # We are running this in float32, regardless of layer's self.compute_dtype.
-        # Calculating blur_filter in lower precision will corrupt the final results.
+        # We are running this in float32, regardless of layer's
+        # self.compute_dtype. Calculating blur_filter in lower precision will
+        # corrupt the final results.
         x = tf.cast(
-            tf.range(-filter_size // 2 + 1, filter_size // 2 + 1), dtype=tf.float32
+            tf.range(-filter_size // 2 + 1, filter_size // 2 + 1),
+            dtype=tf.float32,
         )
         blur_filter = tf.exp(
-            -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.cast(factor, dtype=tf.float32), 2.0))
+            -tf.pow(x, 2.0)
+            / (2.0 * tf.pow(tf.cast(factor, dtype=tf.float32), 2.0))
         )
         blur_filter /= tf.reduce_sum(blur_filter)
         return blur_filter
 
     def get_config(self):
         config = super().get_config()
         config.update({"factor": self.factor, "kernel_size": self.kernel_size})
```

## keras_cv/layers/preprocessing/random_gaussian_blur_test.py

```diff
@@ -15,28 +15,32 @@
 import tensorflow as tf
 
 from keras_cv.layers import preprocessing
 
 
 class RandomGaussianBlurTest(tf.test.TestCase):
     def test_return_shapes(self):
-        layer = preprocessing.RandomGaussianBlur(kernel_size=(3, 7), factor=(0, 2))
+        layer = preprocessing.RandomGaussianBlur(
+            kernel_size=(3, 7), factor=(0, 2)
+        )
 
         # RGB
         xs = tf.ones((2, 512, 512, 3))
         xs = layer(xs)
         self.assertEqual(xs.shape, [2, 512, 512, 3])
 
         # greyscale
         xs = tf.ones((2, 512, 512, 1))
         xs = layer(xs)
         self.assertEqual(xs.shape, [2, 512, 512, 1])
 
     def test_in_single_image(self):
-        layer = preprocessing.RandomGaussianBlur(kernel_size=(3, 7), factor=(0, 2))
+        layer = preprocessing.RandomGaussianBlur(
+            kernel_size=(3, 7), factor=(0, 2)
+        )
 
         # RGB
         xs = tf.cast(
             tf.ones((512, 512, 3)),
             dtype=tf.float32,
         )
 
@@ -49,15 +53,17 @@
             dtype=tf.float32,
         )
 
         xs = layer(xs)
         self.assertEqual(xs.shape, [512, 512, 1])
 
     def test_non_square_images(self):
-        layer = preprocessing.RandomGaussianBlur(kernel_size=(3, 7), factor=(0, 2))
+        layer = preprocessing.RandomGaussianBlur(
+            kernel_size=(3, 7), factor=(0, 2)
+        )
 
         # RGB
         xs = tf.ones((2, 256, 512, 3))
         xs = layer(xs)
         self.assertEqual(xs.shape, [2, 256, 512, 3])
 
         # greyscale
@@ -75,15 +81,17 @@
 
         # greyscale
         xs = tf.ones((2, 512, 512, 1))
         xs = layer(xs)
         self.assertEqual(xs.shape, [2, 512, 512, 1])
 
     def test_numerical(self):
-        layer = preprocessing.RandomGaussianBlur(kernel_size=3, factor=(1.0, 1.0))
+        layer = preprocessing.RandomGaussianBlur(
+            kernel_size=3, factor=(1.0, 1.0)
+        )
 
         xs = tf.expand_dims(
             tf.constant([[0, 0, 0], [0, 1, 0], [0, 0, 0]]),
             axis=-1,
         )
 
         xs = tf.expand_dims(xs, axis=0)
```

## keras_cv/layers/preprocessing/random_hue.py

```diff
@@ -1,95 +1,132 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
-from keras_cv.utils import preprocessing
+from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomHue(BaseImageAugmentationLayer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomHue(VectorizedBaseImageAugmentationLayer):
     """Randomly adjusts the hue on given images.
 
     This layer will randomly increase/reduce the hue for the input RGB
-    images. At inference time, the output will be identical to the input.
-    Call the layer with `training=True` to adjust the brightness of the input.
+    images.
 
     The image hue is adjusted by converting the image(s) to HSV and rotating the
     hue channel (H) by delta. The image is then converted back to RGB.
 
     Args:
-        factor: A tuple of two floats, a single float or `keras_cv.FactorSampler`.
-            `factor` controls the extent to which the image hue is impacted.
-            `factor=0.0` makes this layer perform a no-op operation, while a value of
-            1.0 performs the most aggressive contrast adjustment available.  If a tuple
-            is used, a `factor` is sampled between the two values for every image
-            augmented.  If a single float is used, a value between `0.0` and the passed
-            float is sampled.  In order to ensure the value is always the same, please
+        factor: A tuple of two floats, a single float or
+            `keras_cv.FactorSampler`. `factor` controls the extent to which the
+            image hue is impacted. `factor=0.0` makes this layer perform a
+            no-op operation, while a value of 1.0 performs the most aggressive
+            contrast adjustment available. If a tuple is used, a `factor` is
+            sampled between the two values for every image augmented. If a
+            single float is used, a value between `0.0` and the passed float is
+            sampled. In order to ensure the value is always the same, please
             pass a tuple with two identical floats: `(0.5, 0.5)`.
-        value_range:  the range of values the incoming images will have.
-            Represented as a two number tuple written [low, high].
-            This is typically either `[0, 1]` or `[0, 255]` depending
-            on how your preprocessing pipeline is setup.
+        value_range: the range of values the incoming images will have.
+            Represented as a two number tuple written [low, high]. This is
+            typically either `[0, 1]` or `[0, 255]` depending on how your
+            preprocessing pipeline is set up.
         seed: Integer. Used to create a random seed.
 
+    Usage:
+    ```python
+    (images, labels), _ = keras.datasets.cifar10.load_data()
+    random_hue = keras_cv.layers.preprocessing.RandomHue()
+    augmented_images = random_hue(images)
+    ```
     """
 
     def __init__(self, factor, value_range, seed=None, **kwargs):
         super().__init__(seed=seed, **kwargs)
-        self.factor = preprocessing.parse_factor(
+        self.factor = preprocessing_utils.parse_factor(
             factor,
         )
         self.value_range = value_range
         self.seed = seed
 
-    def get_random_transformation(self, **kwargs):
-        invert = preprocessing.random_inversion(self._random_generator)
-        # We must scale self.factor() to the range [-0.5, 0.5].  This is because the
-        # tf.image operation performs rotation on the hue saturation value orientation.
-        # This can be thought of as an angle in the range [-180, 180]
-        return invert * self.factor() * 0.5
-
-    def augment_image(self, image, transformation=None, **kwargs):
-        image = preprocessing.transform_value_range(
-            image, self.value_range, (0, 1), dtype=self.compute_dtype
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        invert = self._random_generator.random_uniform(
+            (batch_size,), 0, 1, tf.float32
+        )
+        invert = tf.where(
+            invert > 0.5, -tf.ones_like(invert), tf.ones_like(invert)
+        )
+        # We must scale self.factor() to the range [-0.5, 0.5]. This is because
+        # the tf.image operation performs rotation on the hue saturation value
+        # orientation. This can be thought of as an angle in the range
+        # [-180, 180]
+        return invert * self.factor(shape=(batch_size,)) * 0.5
+
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        return self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
+
+    def augment_images(self, images, transformations, **kwargs):
+        images = preprocessing_utils.transform_value_range(
+            images, self.value_range, (0, 1), dtype=self.compute_dtype
         )
+        adjust_factors = tf.cast(transformations, images.dtype)
+        # broadcast
+        adjust_factors = adjust_factors[..., tf.newaxis, tf.newaxis]
 
         # tf.image.adjust_hue expects floats to be in range [0, 1]
-        image = tf.image.adjust_hue(image, delta=transformation)
+        images = tf.image.rgb_to_hsv(images)
+        h_channel = images[..., 0] + adjust_factors
+        h_channel = tf.where(h_channel > 1.0, h_channel - 1.0, h_channel)
+        h_channel = tf.where(h_channel < 0.0, h_channel + 1.0, h_channel)
+        images = tf.stack([h_channel, images[..., 1], images[..., 2]], axis=-1)
+        images = tf.image.hsv_to_rgb(images)
         # RandomHue is one of the rare KPLs that needs to clip
-        image = tf.clip_by_value(image, 0, 1)
-        image = preprocessing.transform_value_range(
-            image, (0, 1), self.value_range, dtype=self.compute_dtype
+        images = tf.clip_by_value(images, 0, 1)
+        images = preprocessing_utils.transform_value_range(
+            images, (0, 1), self.value_range, dtype=self.compute_dtype
         )
-        return image
+        return images
 
-    def augment_bounding_boxes(self, bounding_boxes, **kwargs):
-        return bounding_boxes
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
 
-    def augment_label(self, label, transformation=None, **kwargs):
-        return label
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
+    def augment_bounding_boxes(self, bounding_boxes, transformations, **kwargs):
+        return bounding_boxes
 
     def get_config(self):
         config = {
             "factor": self.factor,
             "value_range": self.value_range,
             "seed": self.seed,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        if isinstance(config["factor"], dict):
+            config["factor"] = keras.utils.deserialize_keras_object(
+                config["factor"]
+            )
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_hue_test.py

```diff
@@ -42,28 +42,40 @@
         image = tf.random.uniform(shape=image_shape) * 255.0
 
         layer = preprocessing.RandomHue(factor=(1.0, 1.0), value_range=(0, 255))
         output = layer(image)
 
         channel_max = tf.math.reduce_max(output, axis=-1)
         channel_min = tf.math.reduce_min(output, axis=-1)
-        # Make sure the max and min channel are the same between input and output
-        # In the meantime, and channel will swap between each other.
-        self.assertAllClose(channel_max, tf.math.reduce_max(image, axis=-1))
-        self.assertAllClose(channel_min, tf.math.reduce_min(image, axis=-1))
+        # Make sure the max and min channel are the same between input and
+        # output. In the meantime, and channel will swap between each other.
+        self.assertAllClose(
+            channel_max,
+            tf.math.reduce_max(image, axis=-1),
+            atol=1e-5,
+            rtol=1e-5,
+        )
+        self.assertAllClose(
+            channel_min,
+            tf.math.reduce_min(image, axis=-1),
+            atol=1e-5,
+            rtol=1e-5,
+        )
 
     @parameterized.named_parameters(
         ("025", 0.25), ("05", 0.5), ("075", 0.75), ("100", 1.0)
     )
     def test_adjusts_all_values_for_factor(self, factor):
         image_shape = (4, 8, 8, 3)
         # Value range (0, 100)
         image = tf.random.uniform(shape=image_shape) * 100.0
 
-        layer = preprocessing.RandomHue(factor=(factor, factor), value_range=(0, 255))
+        layer = preprocessing.RandomHue(
+            factor=(factor, factor), value_range=(0, 255)
+        )
         output = layer(image)
         self.assertNotAllClose(image, output, atol=1e-5, rtol=1e-5)
 
     def test_adjustment_for_non_rgb_value_range(self):
         image_shape = (4, 8, 8, 3)
         # Value range (0, 100)
         image = tf.random.uniform(shape=image_shape) * 100.0
@@ -74,15 +86,17 @@
 
         layer = preprocessing.RandomHue(factor=(0.3, 0.8), value_range=(0, 255))
         output = layer(image)
         self.assertNotAllClose(image, output)
 
     def test_with_uint8(self):
         image_shape = (4, 8, 8, 3)
-        image = tf.cast(tf.random.uniform(shape=image_shape) * 255.0, dtype=tf.uint8)
+        image = tf.cast(
+            tf.random.uniform(shape=image_shape) * 255.0, dtype=tf.uint8
+        )
 
         layer = preprocessing.RandomHue(factor=(0.0, 0.0), value_range=(0, 255))
         output = layer(image)
         self.assertAllClose(image, output, atol=1e-5, rtol=1e-5)
 
         layer = preprocessing.RandomHue(factor=(0.3, 0.8), value_range=(0, 255))
         output = layer(image)
```

## keras_cv/layers/preprocessing/random_jpeg_quality.py

```diff
@@ -9,53 +9,59 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomJpegQuality(BaseImageAugmentationLayer):
     """Applies Random Jpeg compression artifacts to an image.
 
-    Performs the jpeg compression algorithm on the image.  This layer can used in order
-    to ensure your model is robust to artifacts introduced by JPEG compresion.
+    Performs the jpeg compression algorithm on the image. This layer can be used
+    in order to ensure your model is robust to artifacts introduced by JPEG
+    compression.
 
     Args:
-        factor: 2 element tuple or 2 element list.  During augmentation, a random number
-        is drawn from the factor distribution.  This value is passed to
+        factor: 2 element tuple or 2 element list. During augmentation, a random
+        number is drawn from the factor distribution. This value is passed to
         `tf.image.adjust_jpeg_quality()`.
         seed: Integer. Used to create a random seed.
 
     Usage:
     ```python
     layer = keras_cv.RandomJpegQuality(factor=(75, 100)))
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     augmented_images = layer(images)
     ```
     """
 
     def __init__(self, factor, seed=None, **kwargs):
         super().__init__(**kwargs)
         if isinstance(factor, (float, int)):
             raise ValueError(
                 "RandomJpegQuality() expects factor to be a 2 element "
                 "tuple, list or a `keras_cv.FactorSampler`. "
                 "RandomJpegQuality() received `factor={factor}`."
             )
         self.seed = seed
         self.factor = preprocessing.parse_factor(
-            factor, min_value=0, max_value=100, param_name="factor", seed=self.seed
+            factor,
+            min_value=0,
+            max_value=100,
+            param_name="factor",
+            seed=self.seed,
         )
 
     def get_random_transformation(self, **kwargs):
         return self.factor(dtype=tf.int32)
 
     def augment_image(self, image, transformation=None, **kwargs):
         jpeg_quality = transformation
@@ -63,14 +69,16 @@
 
     def augment_bounding_boxes(self, bounding_boxes, **kwargs):
         return bounding_boxes
 
     def augment_label(self, label, transformation=None, **kwargs):
         return label
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return segmentation_mask
 
     def get_config(self):
         config = super().get_config()
         config.update({"factor": self.factor, "seed": self.seed})
         return config
```

## keras_cv/layers/preprocessing/random_rotation.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,40 +10,37 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import numpy as np
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
-from keras_cv.utils import preprocessing
+from keras_cv.utils import preprocessing as preprocessing_utils
 
 # In order to support both unbatched and batched inputs, the horizontal
-# and verticle axis is reverse indexed
+# and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomRotation(BaseImageAugmentationLayer):
-    """A preprocessing layer which randomly rotates images during training.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomRotation(VectorizedBaseImageAugmentationLayer):
+    """A preprocessing layer which randomly rotates images.
 
     This layer will apply random rotations to each image, filling empty space
     according to `fill_mode`.
 
-    By default, random rotations are only applied during training.
-    At inference time, the layer does nothing. If you need to apply random
-    rotations at inference time, set `training` to True when calling the layer.
-
     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and
-    of interger or floating point dtype. By default, the layer will output
+    of integer or floating point dtype. By default, the layer will output
     floats.
 
     Input shape:
       3D (unbatched) or 4D (batched) tensor with shape:
       `(..., height, width, channels)`, in `"channels_last"` format
 
     Output shape:
@@ -103,175 +100,191 @@
         else:
             self.lower = -factor
             self.upper = factor
         if self.upper < self.lower:
             raise ValueError(
                 "Factor cannot have negative values, " "got {}".format(factor)
             )
-        preprocessing.check_fill_mode_and_interpolation(fill_mode, interpolation)
+        preprocessing_utils.check_fill_mode_and_interpolation(
+            fill_mode, interpolation
+        )
         self.fill_mode = fill_mode
         self.fill_value = fill_value
         self.interpolation = interpolation
         self.seed = seed
         self.bounding_box_format = bounding_box_format
         self.segmentation_classes = segmentation_classes
 
-    def get_random_transformation(self, **kwargs):
+    def get_random_transformation_batch(self, batch_size, **kwargs):
         min_angle = self.lower * 2.0 * np.pi
         max_angle = self.upper * 2.0 * np.pi
-        angle = self._random_generator.random_uniform(
-            shape=[1], minval=min_angle, maxval=max_angle
+        angles = self._random_generator.random_uniform(
+            shape=[batch_size], minval=min_angle, maxval=max_angle
         )
-        return {"angle": angle}
+        return {"angles": angles}
 
-    def augment_image(self, image, transformation, **kwargs):
-        return self._rotate_image(image, transformation)
-
-    def _rotate_image(self, image, transformation):
-        image = preprocessing.ensure_tensor(image, self.compute_dtype)
-        original_shape = image.shape
-        image = tf.expand_dims(image, 0)
-        image_shape = tf.shape(image)
-        img_hd = tf.cast(image_shape[H_AXIS], tf.float32)
-        img_wd = tf.cast(image_shape[W_AXIS], tf.float32)
-        angle = transformation["angle"]
-        output = preprocessing.transform(
-            image,
-            preprocessing.get_rotation_matrix(angle, img_hd, img_wd),
-            fill_mode=self.fill_mode,
-            fill_value=self.fill_value,
-            interpolation=self.interpolation,
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        image = tf.expand_dims(image, axis=0)
+        transformation = {
+            "angles": tf.expand_dims(transformation["angles"], axis=0),
+        }
+        image = self.augment_images(
+            images=image, transformations=transformation, **kwargs
         )
-        output = tf.squeeze(output, 0)
-        output.set_shape(original_shape)
-        return output
+        return tf.squeeze(image, axis=0)
+
+    def augment_images(self, images, transformations, **kwargs):
+        return self._rotate_images(images, transformations)
+
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
 
     def augment_bounding_boxes(
-        self, bounding_boxes, transformation, image=None, **kwargs
+        self, bounding_boxes, transformations, raw_images=None, **kwargs
     ):
         if self.bounding_box_format is None:
             raise ValueError(
                 "`RandomRotation()` was called with bounding boxes,"
                 "but no `bounding_box_format` was specified in the constructor."
                 "Please specify a bounding box format in the constructor. i.e."
                 "`RandomRotation(bounding_box_format='xyxy')`"
             )
+        bounding_boxes = bounding_box.to_dense(bounding_boxes)
 
         bounding_boxes = bounding_box.convert_format(
             bounding_boxes,
             source=self.bounding_box_format,
             target="xyxy",
-            images=image,
+            images=raw_images,
         )
-        image_shape = tf.shape(image)
+        image_shape = tf.shape(raw_images)
         h = image_shape[H_AXIS]
         w = image_shape[W_AXIS]
 
         # origin coordinates, all the points on the image are rotated around
         # this point
-        origin_x, origin_y = tf.cast(w / 2, dtype=self.compute_dtype), tf.cast(
-            h / 2, dtype=self.compute_dtype
-        )
-        angle = transformation["angle"]
-        angle = -angle
-        # calculate coordinates of all four corners of the bounding box
+        origin_x = tf.cast(w / 2, dtype=self.compute_dtype)
+        origin_y = tf.cast(h / 2, dtype=self.compute_dtype)
+        angles = -transformations["angles"]
+        angles = angles[:, tf.newaxis, tf.newaxis, tf.newaxis]
 
+        # calculate coordinates of all four corners of the bounding box
         boxes = bounding_boxes["boxes"]
-        point = tf.stack(
+        points = tf.stack(
             [
-                tf.stack([boxes[:, 0], boxes[:, 1]], axis=1),
-                tf.stack([boxes[:, 2], boxes[:, 1]], axis=1),
-                tf.stack([boxes[:, 2], boxes[:, 3]], axis=1),
-                tf.stack([boxes[:, 0], boxes[:, 3]], axis=1),
+                tf.stack([boxes[:, :, 0], boxes[:, :, 1]], axis=2),
+                tf.stack([boxes[:, :, 2], boxes[:, :, 1]], axis=2),
+                tf.stack([boxes[:, :, 2], boxes[:, :, 3]], axis=2),
+                tf.stack([boxes[:, :, 0], boxes[:, :, 3]], axis=2),
             ],
-            axis=1,
+            axis=2,
         )
         # point_x : x coordinates of all corners of the bounding box
-        point_x = tf.gather(point, [0], axis=2)
+        point_xs = tf.gather(points, [0], axis=3)
+        point_x_offsets = tf.cast((point_xs - origin_x), dtype=tf.float32)
         # point_y : y cordinates of all corners of the bounding box
-        point_y = tf.gather(point, [1], axis=2)
+        point_ys = tf.gather(points, [1], axis=3)
+        point_y_offsets = tf.cast((point_ys - origin_y), dtype=tf.float32)
         # rotated bounding box coordinates
         # new_x : new position of x coordinates of corners of bounding box
         new_x = (
             origin_x
-            + tf.multiply(
-                tf.cos(angle), tf.cast((point_x - origin_x), dtype=tf.float32)
-            )
-            - tf.multiply(
-                tf.sin(angle), tf.cast((point_y - origin_y), dtype=tf.float32)
-            )
+            + tf.multiply(tf.cos(angles), point_x_offsets)
+            - tf.multiply(tf.sin(angles), point_y_offsets)
         )
         # new_y : new position of y coordinates of corners of bounding box
         new_y = (
             origin_y
-            + tf.multiply(
-                tf.sin(angle), tf.cast((point_x - origin_x), dtype=tf.float32)
-            )
-            + tf.multiply(
-                tf.cos(angle), tf.cast((point_y - origin_y), dtype=tf.float32)
-            )
+            + tf.multiply(tf.sin(angles), point_x_offsets)
+            + tf.multiply(tf.cos(angles), point_y_offsets)
         )
         # rotated bounding box coordinates
-        out = tf.concat([new_x, new_y], axis=2)
+        out = tf.concat([new_x, new_y], axis=3)
         # find readjusted coordinates of bounding box to represent it in corners
         # format
-        min_cordinates = tf.math.reduce_min(out, axis=1)
-        max_cordinates = tf.math.reduce_max(out, axis=1)
-        boxes = tf.concat([min_cordinates, max_cordinates], axis=1)
+        min_cordinates = tf.math.reduce_min(out, axis=2)
+        max_cordinates = tf.math.reduce_max(out, axis=2)
+        boxes = tf.concat([min_cordinates, max_cordinates], axis=2)
 
         bounding_boxes = bounding_boxes.copy()
         bounding_boxes["boxes"] = boxes
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes,
             bounding_box_format="xyxy",
-            images=image,
+            images=raw_images,
         )
-        # cordinates cannot be float values, it is casted to int32
+        # coordinates cannot be float values, it is cast to int32
         bounding_boxes = bounding_box.convert_format(
             bounding_boxes,
             source="xyxy",
             target=self.bounding_box_format,
             dtype=self.compute_dtype,
-            images=image,
+            images=raw_images,
         )
         return bounding_boxes
 
-    def augment_label(self, label, transformation, **kwargs):
-        return label
-
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        # If segmentation_classes is specified, we have a dense segmentation mask.
-        # We therefore one-hot encode before rotation to avoid bad interpolation
-        # during the rotation transformation. We then make the mask sparse
-        # again using tf.argmax.
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        # If segmentation_classes is specified, we have a dense segmentation
+        # mask. We therefore one-hot encode before rotation to avoid bad
+        # interpolation during the rotation transformation. We then make the
+        # mask sparse again using tf.argmax.
         if self.segmentation_classes:
             one_hot_mask = tf.one_hot(
-                tf.squeeze(segmentation_mask, axis=-1), self.segmentation_classes
+                tf.squeeze(segmentation_masks, axis=-1),
+                self.segmentation_classes,
+            )
+            rotated_one_hot_mask = self._rotate_images(
+                one_hot_mask, transformations
             )
-            rotated_one_hot_mask = self._rotate_image(one_hot_mask, transformation)
             rotated_mask = tf.argmax(rotated_one_hot_mask, axis=-1)
             return tf.expand_dims(rotated_mask, axis=-1)
         else:
-            if segmentation_mask.shape[-1] == 1:
+            if segmentation_masks.shape[-1] == 1:
                 raise ValueError(
                     "Segmentation masks must be one-hot encoded, or "
                     "RandomRotate must be initialized with "
                     "`segmentation_classes`. `segmentation_classes` was not "
-                    f"specified, and mask has shape {segmentation_mask.shape}"
+                    f"specified, and mask has shape {segmentation_masks.shape}"
                 )
-            rotated_mask = self._rotate_image(segmentation_mask, transformation)
+            rotated_mask = self._rotate_images(
+                segmentation_masks, transformations
+            )
             # Round because we are in one-hot encoding, and we may have
-            # pixels with ambugious value due to floating point math for rotation.
+            # pixels with ambugious value due to floating point math for
+            # rotation.
             return tf.round(rotated_mask)
 
+    def _rotate_images(self, images, transformations):
+        images = preprocessing_utils.ensure_tensor(images, self.compute_dtype)
+        original_shape = images.shape
+        image_shape = tf.shape(images)
+        img_hd = tf.cast(image_shape[H_AXIS], tf.float32)
+        img_wd = tf.cast(image_shape[W_AXIS], tf.float32)
+        angles = transformations["angles"]
+        outputs = preprocessing_utils.transform(
+            images,
+            preprocessing_utils.get_rotation_matrix(angles, img_hd, img_wd),
+            fill_mode=self.fill_mode,
+            fill_value=self.fill_value,
+            interpolation=self.interpolation,
+        )
+        outputs.set_shape(original_shape)
+        return outputs
+
     def get_config(self):
         config = {
             "factor": self.factor,
             "fill_mode": self.fill_mode,
             "fill_value": self.fill_value,
             "interpolation": self.interpolation,
             "bounding_box_format": self.bounding_box_format,
             "segmentation_classes": self.segmentation_classes,
             "seed": self.seed,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_rotation_test.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -22,20 +22,22 @@
     def test_random_rotation_output_shapes(self):
         input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)
         expected_output = input_images
         layer = RandomRotation(0.5)
         actual_output = layer(input_images, training=True)
         self.assertEqual(expected_output.shape, actual_output.shape)
 
-    def test_random_rotation_inference(self):
-        input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)
-        expected_output = input_images
-        layer = RandomRotation(0.5)
-        actual_output = layer(input_images, training=False)
-        self.assertAllClose(expected_output, actual_output)
+    def test_random_rotation_on_batched_images_independently(self):
+        image = tf.random.uniform((100, 100, 3))
+        batched_images = tf.stack((image, image), axis=0)
+        layer = RandomRotation(factor=0.5)
+
+        results = layer(batched_images)
+
+        self.assertNotAllClose(results[0], results[1])
 
     def test_config_with_custom_name(self):
         layer = RandomRotation(0.5, name="image_preproc")
         config = layer.get_config()
         layer_reconstructed = RandomRotation.from_config(config)
         self.assertEqual(layer_reconstructed.name, layer.name)
 
@@ -64,15 +66,17 @@
             ),
             "classes": tf.convert_to_tensor([1, 2], dtype=tf.float32),
         }
         input = {"images": input_image, "bounding_boxes": bounding_boxes}
         # 180 rotation.
         layer = RandomRotation(factor=(0.5, 0.5), bounding_box_format="xyxy")
         output = layer(input)
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
         expected_bounding_boxes = {
             "boxes": tf.convert_to_tensor(
                 [[112.0, 112.0, 312.0, 312.0], [212.0, 212.0, 412.0, 412.0]],
                 dtype=tf.float32,
             ),
             "classes": tf.convert_to_tensor([1, 2], dtype=tf.float32),
         }
@@ -108,15 +112,18 @@
         }
         input = {"images": input_image, "bounding_boxes": bounding_boxes}
         layer = RandomRotation(factor=(0.5, 0.5), bounding_box_format="xyxy")
         output = layer(input)
         expected_output = {
             "boxes": tf.ragged.constant(
                 [
-                    [[112.0, 112.0, 312.0, 312.0], [212.0, 212.0, 412.0, 412.0]],
+                    [
+                        [112.0, 112.0, 312.0, 312.0],
+                        [212.0, 212.0, 412.0, 412.0],
+                    ],
                     [[112.0, 112.0, 312.0, 312.0]],
                 ],
                 dtype=tf.float32,
             ),
             "classes": tf.ragged.constant(
                 [
                     [
@@ -125,51 +132,63 @@
                     ],
                     [0],
                 ],
                 dtype=tf.float32,
             ),
         }
         expected_output = bounding_box.to_dense(expected_output)
-        output["bounding_boxes"] = bounding_box.to_dense(output["bounding_boxes"])
+        output["bounding_boxes"] = bounding_box.to_dense(
+            output["bounding_boxes"]
+        )
 
-        self.assertAllClose(expected_output["boxes"], output["bounding_boxes"]["boxes"])
         self.assertAllClose(
-            expected_output["classes"], output["bounding_boxes"]["classes"]
+            expected_output["boxes"], output["bounding_boxes"]["boxes"]
+        )
+        self.assertAllClose(
+            expected_output["classes"],
+            output["bounding_boxes"]["classes"],
         )
 
     def test_augment_sparse_segmentation_mask(self):
-        classes = 8
+        num_classes = 8
 
         input_images = np.random.random((2, 20, 20, 3)).astype(np.float32)
         # Masks are all 0s or 8s, to verify that when we rotate we don't do bad
         # mask interpolation to either a 0 or a 7
-        masks = np.random.randint(2, size=(2, 20, 20, 1)) * (classes - 1)
+        masks = np.random.randint(2, size=(2, 20, 20, 1)) * (num_classes - 1)
         inputs = {"images": input_images, "segmentation_masks": masks}
 
-        # Attempting to rotate a sparse mask without specifying classes fails.
+        # Attempting to rotate a sparse mask without specifying num_classes
+        # fails.
         bad_layer = RandomRotation(factor=(0.25, 0.25))
         with self.assertRaisesRegex(ValueError, "masks must be one-hot"):
             outputs = bad_layer(inputs)
 
         # 90 degree rotation.
-        layer = RandomRotation(factor=(0.25, 0.25), segmentation_classes=classes)
+        layer = RandomRotation(
+            factor=(0.25, 0.25), segmentation_classes=num_classes
+        )
         outputs = layer(inputs)
         expected_masks = np.rot90(masks, axes=(1, 2))
         self.assertAllClose(expected_masks, outputs["segmentation_masks"])
 
-        # 45 degree rotation. Only verifies that no interpolation takes place.
-        layer = RandomRotation(factor=(0.125, 0.125), segmentation_classes=classes)
+        # 45-degree rotation. Only verifies that no interpolation takes place.
+        layer = RandomRotation(
+            factor=(0.125, 0.125), segmentation_classes=num_classes
+        )
         outputs = layer(inputs)
         self.assertAllInSet(outputs["segmentation_masks"], [0, 7])
 
     def test_augment_one_hot_segmentation_mask(self):
-        classes = 8
+        num_classes = 8
 
         input_images = np.random.random((2, 20, 20, 3)).astype(np.float32)
-        masks = tf.one_hot(np.random.randint(classes, size=(2, 20, 20)), classes)
+        masks = tf.one_hot(
+            np.random.randint(num_classes, size=(2, 20, 20)), num_classes
+        )
         inputs = {"images": input_images, "segmentation_masks": masks}
 
         # 90 rotation.
         layer = RandomRotation(factor=(0.25, 0.25))
         outputs = layer(inputs)
 
         expected_masks = np.rot90(masks, axes=(1, 2))
```

## keras_cv/layers/preprocessing/random_saturation.py

```diff
@@ -1,89 +1,125 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
-from keras_cv.utils import preprocessing
+from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomSaturation(BaseImageAugmentationLayer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomSaturation(VectorizedBaseImageAugmentationLayer):
     """Randomly adjusts the saturation on given images.
 
     This layer will randomly increase/reduce the saturation for the input RGB
-    images. At inference time, the output will be identical to the input.
-    Call the layer with `training=True` to adjust the saturation of the input.
+    images.
 
     Args:
-        factor: A tuple of two floats, a single float or `keras_cv.FactorSampler`.
-            `factor` controls the extent to which the image saturation is impacted.
-            `factor=0.5` makes this layer perform a no-op operation. `factor=0.0` makes
-            the image to be fully grayscale. `factor=1.0` makes the image to be fully
-            saturated.
-            Values should be between `0.0` and `1.0`. If a tuple is used, a `factor`
-            is sampled between the two values for every image augmented.  If a single
-            float is used, a value between `0.0` and the passed float is sampled.
-            In order to ensure the value is always the same, please pass a tuple with
-            two identical floats: `(0.5, 0.5)`.
+        factor: A tuple of two floats, a single float or
+            `keras_cv.FactorSampler`. `factor` controls the extent to which the
+            image saturation is impacted. `factor=0.5` makes this layer perform
+            a no-op operation. `factor=0.0` makes the image to be fully
+            grayscale. `factor=1.0` makes the image to be fully saturated.
+            Values should be between `0.0` and `1.0`. If a tuple is used, a
+            `factor` is sampled between the two values for every image
+            augmented. If a single float is used, a value between `0.0` and the
+            passed float is sampled. In order to ensure the value is always the
+            same, please pass a tuple with two identical floats: `(0.5, 0.5)`.
         seed: Integer. Used to create a random seed.
+
+    Usage:
+    ```python
+    (images, labels), _ = keras.datasets.cifar10.load_data()
+    random_saturation = keras_cv.layers.preprocessing.RandomSaturation()
+    augmented_images = random_saturation(images)
+    ```
     """
 
     def __init__(self, factor, seed=None, **kwargs):
         super().__init__(seed=seed, **kwargs)
-        self.factor = preprocessing.parse_factor(
+        self.factor = preprocessing_utils.parse_factor(
             factor,
             min_value=0.0,
             max_value=1.0,
         )
         self.seed = seed
 
-    def get_random_transformation(self, **kwargs):
-        return self.factor()
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        return self.factor(shape=(batch_size,))
+
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        return self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
 
-    def augment_image(self, image, transformation=None, **kwargs):
+    def augment_images(self, images, transformations, **kwargs):
         # Convert the factor range from [0, 1] to [0, +inf]. Note that the
-        # tf.image.adjust_saturation is trying to apply the following math formula
-        # `output_saturation = input_saturation * factor`. We use the following
-        # method to the do the mapping.
+        # tf.image.adjust_saturation is trying to apply the following math
+        # formula `output_saturation = input_saturation * factor`. We use the
+        # following method to the do the mapping.
         # `y = x / (1 - x)`.
         # This will ensure:
         #   y = +inf when x = 1 (full saturation)
         #   y = 1 when x = 0.5 (no augmentation)
         #   y = 0 when x = 0 (full gray scale)
 
         # Convert the transformation to tensor in case it is a float. When
-        # transformation is 1.0, then it will result in to divide by zero error, but
-        # it will be handled correctly when it is a one tensor.
-        transformation = tf.convert_to_tensor(transformation)
-        adjust_factor = transformation / (1 - transformation)
-        return tf.image.adjust_saturation(image, saturation_factor=adjust_factor)
-
-    def augment_bounding_boxes(self, bounding_boxes, transformation=None, **kwargs):
+        # transformation is 1.0, then it will result in to divide by zero error,
+        # but it will be handled correctly when it is a one tensor.
+        transformations = tf.convert_to_tensor(transformations)
+        adjust_factors = transformations / (1 - transformations)
+        adjust_factors = tf.cast(adjust_factors, dtype=images.dtype)
+
+        images = tf.image.rgb_to_hsv(images)
+        s_channel = tf.multiply(
+            images[..., 1], adjust_factors[..., tf.newaxis, tf.newaxis]
+        )
+        s_channel = tf.clip_by_value(
+            s_channel, clip_value_min=0.0, clip_value_max=1.0
+        )
+        images = tf.stack([images[..., 0], s_channel, images[..., 2]], axis=-1)
+        images = tf.image.hsv_to_rgb(images)
+        return images
+
+    def augment_bounding_boxes(
+        self, bounding_boxes, transformation=None, **kwargs
+    ):
         return bounding_boxes
 
-    def augment_label(self, label, transformation=None, **kwargs):
-        return label
+    def augment_labels(self, labels, transformations=None, **kwargs):
+        return labels
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
 
     def get_config(self):
         config = {
             "factor": self.factor,
             "seed": self.seed,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        if isinstance(config["factor"], dict):
+            config["factor"] = keras.utils.deserialize_keras_object(
+                config["factor"]
+            )
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_saturation_test.py

```diff
@@ -1,24 +1,113 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import core
 from keras_cv.layers import preprocessing
+from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
+    BaseImageAugmentationLayer,
+)
+from keras_cv.utils import preprocessing as preprocessing_utils
+
+
+class OldRandomSaturation(BaseImageAugmentationLayer):
+    """Randomly adjusts the saturation on given images.
+
+    This layer will randomly increase/reduce the saturation for the input RGB
+    images. At inference time, the output will be identical to the input.
+    Call the layer with `training=True` to adjust the saturation of the input.
+
+    Args:
+        factor: A tuple of two floats, a single float or
+            `keras_cv.FactorSampler`. `factor` controls the extent to which the
+            image saturation is impacted. `factor=0.5` makes this layer perform
+            a no-op operation. `factor=0.0` makes the image to be fully
+            grayscale. `factor=1.0` makes the image to be fully saturated.
+            Values should be between `0.0` and `1.0`. If a tuple is used, a
+            `factor` is sampled between the two values for every image
+            augmented. If a single float is used, a value between `0.0` and the
+            passed float is sampled. In order to ensure the value is always the
+            same, please pass a tuple with two identical floats: `(0.5, 0.5)`.
+        seed: Integer. Used to create a random seed.
+    """
+
+    def __init__(self, factor, seed=None, **kwargs):
+        super().__init__(seed=seed, **kwargs)
+        self.factor = preprocessing_utils.parse_factor(
+            factor,
+            min_value=0.0,
+            max_value=1.0,
+        )
+        self.seed = seed
+
+    def get_random_transformation(self, **kwargs):
+        return self.factor()
+
+    def augment_image(self, image, transformation=None, **kwargs):
+        # Convert the factor range from [0, 1] to [0, +inf]. Note that the
+        # tf.image.adjust_saturation is trying to apply the following math
+        # formula `output_saturation = input_saturation * factor`. We use the
+        # following method to the do the mapping.
+        # `y = x / (1 - x)`.
+        # This will ensure:
+        #   y = +inf when x = 1 (full saturation)
+        #   y = 1 when x = 0.5 (no augmentation)
+        #   y = 0 when x = 0 (full gray scale)
+
+        # Convert the transformation to tensor in case it is a float. When
+        # transformation is 1.0, then it will result in to divide by zero error,
+        # but it will be handled correctly when it is a one tensor.
+        transformation = tf.convert_to_tensor(transformation)
+        adjust_factor = transformation / (1 - transformation)
+        return tf.image.adjust_saturation(
+            image, saturation_factor=adjust_factor
+        )
+
+    def augment_bounding_boxes(
+        self, bounding_boxes, transformation=None, **kwargs
+    ):
+        return bounding_boxes
+
+    def augment_label(self, label, transformation=None, **kwargs):
+        return label
+
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
+        return segmentation_mask
+
+    def get_config(self):
+        config = {
+            "factor": self.factor,
+            "seed": self.seed,
+        }
+        base_config = super().get_config()
+        return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        if isinstance(config["factor"], dict):
+            config["factor"] = keras.utils.deserialize_keras_object(
+                config["factor"]
+            )
+        return cls(**config)
 
 
 class RandomSaturationTest(tf.test.TestCase):
     def test_preserves_output_shape(self):
         image_shape = (4, 8, 8, 3)
         image = tf.random.uniform(shape=image_shape) * 255.0
 
@@ -42,18 +131,20 @@
         image = tf.random.uniform(shape=image_shape) * 255.0
 
         layer = preprocessing.RandomSaturation(factor=(0.0, 0.0))
         output = layer(image)
 
         channel_mean = tf.math.reduce_mean(output, axis=-1)
         channel_values = tf.unstack(output, axis=-1)
-        # Make sure all the pixel has the same value among the channel dim, which is
-        # a fully gray RGB.
+        # Make sure all the pixel has the same value among the channel dim,
+        # which is a fully gray RGB.
         for channel_value in channel_values:
-            self.assertAllClose(channel_mean, channel_value, atol=1e-5, rtol=1e-5)
+            self.assertAllClose(
+                channel_mean, channel_value, atol=1e-5, rtol=1e-5
+            )
 
     def test_adjust_to_full_saturation(self):
         image_shape = (4, 8, 8, 3)
         image = tf.random.uniform(shape=image_shape) * 255.0
 
         layer = preprocessing.RandomSaturation(factor=(1.0, 1.0))
         output = layer(image)
@@ -73,15 +164,17 @@
 
         layer = preprocessing.RandomSaturation(factor=(0.3, 0.8))
         output = layer(image)
         self.assertNotAllClose(image, output)
 
     def test_with_unit8(self):
         image_shape = (4, 8, 8, 3)
-        image = tf.cast(tf.random.uniform(shape=image_shape) * 255.0, dtype=tf.uint8)
+        image = tf.cast(
+            tf.random.uniform(shape=image_shape) * 255.0, dtype=tf.uint8
+        )
 
         layer = preprocessing.RandomSaturation(factor=(0.5, 0.5))
         output = layer(image)
         self.assertAllClose(image, output, atol=1e-5, rtol=1e-5)
 
         layer = preprocessing.RandomSaturation(factor=(0.3, 0.8))
         output = layer(image)
@@ -89,7 +182,33 @@
 
     def test_config(self):
         layer = preprocessing.RandomSaturation(factor=(0.3, 0.8))
         config = layer.get_config()
         self.assertTrue(isinstance(config["factor"], core.UniformFactorSampler))
         self.assertEqual(config["factor"].get_config()["lower"], 0.3)
         self.assertEqual(config["factor"].get_config()["upper"], 0.8)
+
+    def test_correctness_with_tf_adjust_saturation_normalized_range(self):
+        image_shape = (16, 32, 32, 3)
+        fixed_factor = (0.8, 0.8)
+        image = tf.random.uniform(shape=image_shape)
+
+        layer = preprocessing.RandomSaturation(factor=fixed_factor)
+        old_layer = OldRandomSaturation(factor=fixed_factor)
+
+        output = layer(image)
+        old_output = old_layer(image)
+
+        self.assertAllClose(old_output, output, atol=1e-5, rtol=1e-5)
+
+    def test_correctness_with_tf_adjust_saturation_rgb_range(self):
+        image_shape = (16, 32, 32, 3)
+        fixed_factor = (0.8, 0.8)
+        image = tf.random.uniform(shape=image_shape) * 255.0
+
+        layer = preprocessing.RandomSaturation(factor=fixed_factor)
+        old_layer = OldRandomSaturation(factor=fixed_factor)
+
+        output = layer(image)
+        old_output = old_layer(image)
+
+        self.assertAllClose(old_output, output, atol=1e-3, rtol=1e-5)
```

## keras_cv/layers/preprocessing/random_sharpness.py

```diff
@@ -1,136 +1,154 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomSharpness(BaseImageAugmentationLayer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomSharpness(VectorizedBaseImageAugmentationLayer):
     """Randomly performs the sharpness operation on given images.
 
-    The sharpness operation first performs a blur operation, then blends between the
-    original image and the blurred image.  This operation makes the edges of an image
-    less sharp than they were in the original image.
+    The sharpness operation first performs a blur operation, then blends between
+    the original image and the blurred image. This operation makes the edges of
+    an image less sharp than they were in the original image.
 
     References:
         - [PIL](https://pillow.readthedocs.io/en/stable/reference/ImageEnhance.html)
 
     Args:
-        factor: A tuple of two floats, a single float or `keras_cv.FactorSampler`.
-            `factor` controls the extent to which the image sharpness is impacted.
-            `factor=0.0` makes this layer perform a no-op operation, while a value of
-            1.0 uses the sharpened result entirely.  Values between 0 and 1 result in
-            linear interpolation between the original image and the sharpened image.
-            Values should be between `0.0` and `1.0`.  If a tuple is used, a `factor` is
-            sampled between the two values for every image augmented.  If a single float
-            is used, a value between `0.0` and the passed float is sampled.  In order to
-            ensure the value is always the same, please pass a tuple with two identical
-            floats: `(0.5, 0.5)`.
+        factor: A tuple of two floats, a single float or
+            `keras_cv.FactorSampler`. `factor` controls the extent to which the
+            image sharpness is impacted. `factor=0.0` makes this layer perform a
+            no-op operation, while a value of 1.0 uses the sharpened result
+            entirely. Values between 0 and 1 result in linear interpolation
+            between the original image and the sharpened image. Values should be
+            between `0.0` and `1.0`. If a tuple is used, a `factor` is sampled
+            between the two values for every image augmented. If a single float
+            is used, a value between `0.0` and the passed float is sampled. In
+            order to ensure the value is always the same, please pass a tuple
+            with two identical floats: `(0.5, 0.5)`.
         value_range: the range of values the incoming images will have.
             Represented as a two number tuple written [low, high].
             This is typically either `[0, 1]` or `[0, 255]` depending
-            on how your preprocessing pipeline is setup.
-    """
+            on how your preprocessing pipeline is set up.
+    """  # noqa: E501
 
     def __init__(
         self,
         factor,
         value_range,
         seed=None,
         **kwargs,
     ):
         super().__init__(seed=seed, **kwargs)
         self.value_range = value_range
         self.factor = preprocessing.parse_factor(factor)
         self.seed = seed
 
-    def get_random_transformation(self, **kwargs):
-        return self.factor(dtype=self.compute_dtype)
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        return self.factor(
+            shape=(batch_size, 1, 1, 1), dtype=self.compute_dtype
+        )
 
-    def augment_image(self, image, transformation=None, **kwargs):
-        image = preprocessing.transform_value_range(
-            image,
+    def augment_images(self, images, transformations, **kwargs):
+        images = preprocessing.transform_value_range(
+            images,
             original_range=self.value_range,
             target_range=(0, 255),
             dtype=self.compute_dtype,
         )
-        original_image = image
-
-        # Make image 4D for conv operation.
-        image = tf.expand_dims(image, axis=0)
+        original_images = images
 
         # [1 1 1]
         # [1 5 1]
         # [1 1 1]
         # all divided by 13 is the default 3x3 gaussian smoothing kernel.
-        # Correlating or Convolving with this filter is equivalent to performing a
-        # gaussian blur.
+        # Correlating or Convolving with this filter is equivalent to performing
+        # a gaussian blur.
         kernel = (
             tf.constant(
                 [[1, 1, 1], [1, 5, 1], [1, 1, 1]],
                 dtype=self.compute_dtype,
                 shape=[3, 3, 1, 1],
             )
             / 13.0
         )
 
         # Tile across channel dimension.
-        channels = tf.shape(image)[-1]
+        channels = tf.shape(images)[-1]
         kernel = tf.tile(kernel, [1, 1, channels, 1])
         strides = [1, 1, 1, 1]
 
         smoothed_image = tf.nn.depthwise_conv2d(
-            image, kernel, strides, padding="VALID", dilations=[1, 1]
+            images, kernel, strides, padding="VALID", dilations=[1, 1]
         )
         smoothed_image = tf.clip_by_value(smoothed_image, 0.0, 255.0)
-        smoothed_image = tf.squeeze(smoothed_image, axis=0)
 
         # For the borders of the resulting image, fill in the values of the
         # original image.
         mask = tf.ones_like(smoothed_image)
-        padded_mask = tf.pad(mask, [[1, 1], [1, 1], [0, 0]])
-        padded_smoothed_image = tf.pad(smoothed_image, [[1, 1], [1, 1], [0, 0]])
+        padded_mask = tf.pad(mask, [[0, 0], [1, 1], [1, 1], [0, 0]])
+        padded_smoothed_image = tf.pad(
+            smoothed_image, [[0, 0], [1, 1], [1, 1], [0, 0]]
+        )
 
         result = tf.where(
-            tf.equal(padded_mask, 1), padded_smoothed_image, original_image
+            tf.equal(padded_mask, 1), padded_smoothed_image, original_images
         )
         # Blend the final result.
-        result = preprocessing.blend(original_image, result, transformation)
+        result = preprocessing.blend(original_images, result, transformations)
         result = preprocessing.transform_value_range(
             result,
             original_range=(0, 255),
             target_range=self.value_range,
             dtype=self.compute_dtype,
         )
         return result
 
-    def augment_bounding_boxes(self, bounding_boxes, transformation, **kwargs):
+    def augment_bounding_boxes(self, bounding_boxes, transformations, **kwargs):
         return bounding_boxes
 
-    def augment_label(self, label, transformation=None, **kwargs):
-        return label
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
+
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
+
+    def augment_keypoints(self, keypoints, transformations, **kwargs):
+        return keypoints
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        images = tf.expand_dims(image, axis=0)
+        new_transformation = tf.expand_dims(transformation, axis=0)
+        output = self.augment_images(images, new_transformation)
+        return tf.squeeze(output, axis=0)
 
     def get_config(self):
         config = super().get_config()
         config.update(
-            {"factor": self.factor, "value_range": self.value_range, "seed": self.seed}
+            {
+                "factor": self.factor,
+                "value_range": self.value_range,
+                "seed": self.seed,
+            }
         )
         return config
```

## keras_cv/layers/preprocessing/random_sharpness_test.py

```diff
@@ -65,7 +65,16 @@
                 ]
             ),
             axis=-1,
         )
         result = tf.expand_dims(result, axis=0)
 
         self.assertAllClose(ys, result)
+
+    def test_random_sharpness_on_batched_images_independently(self):
+        image = tf.random.uniform((100, 100, 3), minval=0, maxval=255)
+        batched_images = tf.stack((image, image), axis=0)
+        layer = preprocessing.RandomSharpness(value_range=(0, 255), factor=0.9)
+
+        results = layer(batched_images)
+
+        self.assertNotAllClose(results[0], results[1])
```

## keras_cv/layers/preprocessing/random_shear.py

```diff
@@ -7,71 +7,72 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import warnings
 
 import tensorflow as tf
+from tensorflow import keras
 
 import keras_cv
 from keras_cv import bounding_box
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomShear(BaseImageAugmentationLayer):
-    """A preprocessing layer which randomly shears images during training.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomShear(VectorizedBaseImageAugmentationLayer):
+    """A preprocessing layer which randomly shears images.
+
     This layer will apply random shearings to each image, filling empty space
     according to `fill_mode`.
-    By default, random shears are only applied during training.
-    At inference time, the layer does nothing. If you need to apply random
-    shear at inference time, set `training` to True when calling the layer.
+
     Input pixel values can be of any range and any data type.
+
     Input shape:
       3D (unbatched) or 4D (batched) tensor with shape:
       `(..., height, width, channels)`, in `"channels_last"` format
     Output shape:
       3D (unbatched) or 4D (batched) tensor with shape:
       `(..., height, width, channels)`, in `"channels_last"` format
 
     Args:
         x_factor: A tuple of two floats, a single float or a
-            `keras_cv.FactorSampler`. For each augmented image a value is sampled
-            from the provided range. If a float is passed, the range is interpreted as
-            `(0, x_factor)`.  Values represent a percentage of the image to shear over.
-             For example, 0.3 shears pixels up to 30% of the way across the image.
-             All provided values should be positive.  If `None` is passed, no shear
-             occurs on the X axis.
-             Defaults to `None`.
+            `keras_cv.FactorSampler`. For each augmented image a value is
+            sampled from the provided range. If a float is passed, the range is
+            interpreted as `(0, x_factor)`. Values represent a percentage of the
+            image to shear over. For example, 0.3 shears pixels up to 30% of the
+            way across the image. All provided values should be positive. If
+            `None` is passed, no shear occurs on the X axis. Defaults to `None`.
         y_factor: A tuple of two floats, a single float or a
-            `keras_cv.FactorSampler`. For each augmented image a value is sampled
-            from the provided range. If a float is passed, the range is interpreted as
-            `(0, y_factor)`. Values represent a percentage of the image to shear over.
-            For example, 0.3 shears pixels up to 30% of the way across the image.
-            All provided values should be positive.  If `None` is passed, no shear
-            occurs on the Y axis.
-            Defaults to `None`.
-        interpolation: interpolation method used in the `ImageProjectiveTransformV3` op.
-             Supported values are `"nearest"` and `"bilinear"`.
-             Defaults to `"bilinear"`.
-        fill_mode: fill_mode in the `ImageProjectiveTransformV3` op.
-             Supported values are `"reflect"`, `"wrap"`, `"constant"`, and `"nearest"`.
-             Defaults to `"reflect"`.
-        fill_value: fill_value in the `ImageProjectiveTransformV3` op.
-             A `Tensor` of type `float32`. The value to be filled when fill_mode is
-             constant".  Defaults to `0.0`.
-        bounding_box_format: The format of bounding boxes of input dataset. Refer to
-             https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box/converters.py
-             for more details on supported bounding box formats.
+            `keras_cv.FactorSampler`. For each augmented image a value is
+            sampled from the provided range. If a float is passed, the range is
+            interpreted as `(0, y_factor)`. Values represent a percentage of the
+            image to shear over. For example, 0.3 shears pixels up to 30% of the
+            way across the image. All provided values should be positive. If
+            `None` is passed, no shear occurs on the Y axis. Defaults to `None`.
+        interpolation: interpolation method used in the
+            `ImageProjectiveTransformV3` op. Supported values are `"nearest"`
+            and `"bilinear"`, defaults to `"bilinear"`.
+        fill_mode: fill_mode in the `ImageProjectiveTransformV3` op. Supported
+            values are `"reflect"`, `"wrap"`, `"constant"`, and `"nearest"`.
+            Defaults to `"reflect"`.
+        fill_value: fill_value in the `ImageProjectiveTransformV3` op. A
+            `Tensor` of type `float32`. The value to be filled when fill_mode is
+            constant". Defaults to `0.0`.
+        bounding_box_format: The format of bounding boxes of input dataset.
+            Refer to
+            https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box/converters.py
+            for more details on supported bounding box formats.
         seed: Integer. Used to create a random seed.
     """
 
     def __init__(
         self,
         x_factor=None,
         y_factor=None,
@@ -93,111 +94,231 @@
             self.y_factor = preprocessing.parse_factor(
                 y_factor, max_value=None, param_name="y_factor", seed=seed
             )
         else:
             self.y_factor = y_factor
         if x_factor is None and y_factor is None:
             warnings.warn(
-                "RandomShear received both `x_factor=None` and `y_factor=None`.  As a "
-                "result, the layer will perform no augmentation."
+                "RandomShear received both `x_factor=None` and `y_factor=None`."
+                " As a result, the layer will perform no augmentation."
             )
         self.interpolation = interpolation
         self.fill_mode = fill_mode
         self.fill_value = fill_value
         self.seed = seed
         self.bounding_box_format = bounding_box_format
 
-    def get_random_transformation(self, **kwargs):
-        x = self._get_shear_amount(self.x_factor)
-        y = self._get_shear_amount(self.y_factor)
-        return (x, y)
-
-    def _get_shear_amount(self, constraint):
-        if constraint is None:
-            return None
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        transformations = {"shear_x": None, "shear_y": None}
+        if self.x_factor is not None:
+            invert = preprocessing.batch_random_inversion(
+                self._random_generator, batch_size
+            )
+            transformations["shear_x"] = (
+                self.x_factor(shape=(batch_size, 1)) * invert
+            )
+
+        if self.y_factor is not None:
+            invert = preprocessing.batch_random_inversion(
+                self._random_generator, batch_size
+            )
+            transformations["shear_y"] = (
+                self.y_factor(shape=(batch_size, 1)) * invert
+            )
+
+        return transformations
 
-        invert = preprocessing.random_inversion(self._random_generator)
-        return invert * constraint()
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        images = tf.expand_dims(image, axis=0)
+        new_transformation = {"shear_x": None, "shear_y": None}
+        shear_x = transformation["shear_x"]
+        if shear_x is not None:
+            new_transformation["shear_x"] = tf.expand_dims(shear_x, axis=0)
 
-    def augment_image(self, image, transformation=None, **kwargs):
-        image = tf.expand_dims(image, axis=0)
+        shear_y = transformation["shear_y"]
+        if shear_y is not None:
+            new_transformation["shear_y"] = tf.expand_dims(shear_y, axis=0)
 
-        x, y = transformation
+        output = self.augment_images(images, new_transformation)
+        return tf.squeeze(output, axis=0)
+
+    def augment_images(self, images, transformations, **kwargs):
+        x, y = transformations["shear_x"], transformations["shear_y"]
 
         if x is not None:
-            transform_x = RandomShear._format_transform(
-                [1.0, x, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]
-            )
-            image = preprocessing.transform(
-                images=image,
-                transforms=transform_x,
+            transforms_x = self._build_shear_x_transform_matrix(x)
+            images = preprocessing.transform(
+                images=images,
+                transforms=transforms_x,
                 interpolation=self.interpolation,
                 fill_mode=self.fill_mode,
                 fill_value=self.fill_value,
             )
 
         if y is not None:
-            transform_y = RandomShear._format_transform(
-                [1.0, 0.0, 0.0, y, 1.0, 0.0, 0.0, 0.0]
-            )
-            image = preprocessing.transform(
-                images=image,
-                transforms=transform_y,
+            transforms_y = self._build_shear_y_transform_matrix(y)
+            images = preprocessing.transform(
+                images=images,
+                transforms=transforms_y,
                 interpolation=self.interpolation,
                 fill_mode=self.fill_mode,
                 fill_value=self.fill_value,
             )
 
-        return tf.squeeze(image, axis=0)
+        return images
 
-    def augment_label(self, label, transformation=None, **kwargs):
-        return label
+    @staticmethod
+    def _build_shear_x_transform_matrix(shear_x):
+        """Build transform matrix for horizontal shear.
+
+        The transform matrix looks like:
+        (1, x, 0)
+        (0, 1, 0)
+        (0, 0, 1)
+        where the last entry is implicit.
+
+        We flatten the matrix to `[1.0, x, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]` for
+        use with ImageProjectiveTransformV3.
+        """
+        batch_size = tf.shape(shear_x)[0]
+        return tf.concat(
+            values=[
+                tf.ones((batch_size, 1), tf.float32),
+                shear_x,
+                tf.zeros((batch_size, 2), tf.float32),
+                tf.ones((batch_size, 1), tf.float32),
+                tf.zeros((batch_size, 3), tf.float32),
+            ],
+            axis=1,
+        )
+
+    @staticmethod
+    def _build_shear_y_transform_matrix(shear_y):
+        """Build transform matrix for vertical shear.
+
+        The transform matrix looks like:
+        (1, 0, 0)
+        (y, 1, 0)
+        (0, 0, 1)
+        where the last entry is implicit.
+
+        We flatten the matrix to `[1.0, 0.0, 0.0, y, 1.0, 0.0, 0.0, 0.0]` for
+        use ImageProjectiveTransformV3.
+        """
+        batch_size = tf.shape(shear_y)[0]
+        return tf.concat(
+            values=[
+                tf.ones((batch_size, 1), tf.float32),
+                tf.zeros((batch_size, 2), tf.float32),
+                shear_y,
+                tf.ones((batch_size, 1), tf.float32),
+                tf.zeros((batch_size, 3), tf.float32),
+            ],
+            axis=1,
+        )
+
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
 
     def augment_bounding_boxes(
-        self, bounding_boxes, transformation, image=None, **kwargs
+        self, bounding_boxes, transformations, images=None, **kwargs
     ):
+        """Augments bounding boxes after a shear operations.
+
+        The algorithm to update (x,y) point coordinates after shearing, tells us
+        to matrix multiply them with inverted transform matrix. This is:
+        ```
+        # for shear x              # for shear_y
+        (1.0, -shear_x) (x)        (1.0,      0.0) (x)
+        (0.0, 1.0     ) (y)        (-shear_y, 1.0) (y)
+        ```
+        We can simplify this equation: any new coordinate can be calculated by
+        `x = x - (shear_x * y)` and `(y = y - (shear_y * x)`
+
+        Notice that each coordinate has to be calculated twice, e.g. `x1` will
+        be affected differently by y1 (top) and y2 (bottom). Therefore, we
+        calculate both `x1_top` and `x1_bottom` and choose the final x1
+        depending on the sign of the used shear value.
+        """
         if self.bounding_box_format is None:
             raise ValueError(
                 "`RandomShear()` was called with bounding boxes,"
                 "but no `bounding_box_format` was specified in the constructor."
                 "Please specify a bounding box format in the constructor. i.e."
                 "`RandomShear(bounding_box_format='xyxy')`"
             )
+
+        # Edge case: boxes is a tf.RaggedTensor
+        if isinstance(bounding_boxes["boxes"], tf.RaggedTensor):
+            bounding_boxes = bounding_box.to_dense(
+                bounding_boxes, default_value=0
+            )
+
         bounding_boxes = keras_cv.bounding_box.convert_format(
             bounding_boxes,
             source=self.bounding_box_format,
             target="rel_xyxy",
-            images=image,
+            images=images,
             dtype=self.compute_dtype,
         )
-        x, y = transformation
-        extended_boxes = self._convert_to_extended_corners_format(
-            bounding_boxes["boxes"]
+
+        shear_x_amount = transformations["shear_x"]
+        shear_y_amount = transformations["shear_y"]
+        x1, y1, x2, y2 = tf.split(bounding_boxes["boxes"], 4, axis=-1)
+
+        # Squeeze redundant extra dimension as it messes multiplication
+        # [num_batches, num_boxes, 1] -> [num_batches, num_boxes]
+        x1 = tf.squeeze(x1, axis=-1)
+        y1 = tf.squeeze(y1, axis=-1)
+        x2 = tf.squeeze(x2, axis=-1)
+        y2 = tf.squeeze(y2, axis=-1)
+
+        # Apply horizontal shear
+        if shear_x_amount is not None:
+            x1_top = x1 - (shear_x_amount * y1)
+            x1_bottom = x1 - (shear_x_amount * y2)
+            x1 = tf.where(shear_x_amount < 0, x1_top, x1_bottom)
+
+            x2_top = x2 - (shear_x_amount * y1)
+            x2_bottom = x2 - (shear_x_amount * y2)
+            x2 = tf.where(shear_x_amount < 0, x2_bottom, x2_top)
+
+        # Apply vertical shear
+        if shear_y_amount is not None:
+            y1_left = y1 - (shear_y_amount * x1)
+            y1_right = y1 - (shear_y_amount * x2)
+            y1 = tf.where(shear_y_amount > 0, y1_right, y1_left)
+
+            y2_left = y2 - (shear_y_amount * x1)
+            y2_right = y2 - (shear_y_amount * x2)
+            y2 = tf.where(shear_y_amount > 0, y2_left, y2_right)
+
+        # Join the results:
+        boxes = tf.concat(
+            [
+                # Add dummy last axis for concat:
+                # (num_batches, num_boxes) -> (num_batches, num_boxes, 1)
+                x1[..., tf.newaxis],
+                y1[..., tf.newaxis],
+                x2[..., tf.newaxis],
+                y2[..., tf.newaxis],
+            ],
+            axis=-1,
         )
-        if x is not None:
-            extended_boxes = self._apply_horizontal_transformation_to_bounding_box(
-                extended_boxes, x
-            )
-        # apply vertical shear
-        if y is not None:
-            extended_boxes = self._apply_vertical_transformation_to_bounding_box(
-                extended_boxes, y
-            )
 
-        boxes = self._convert_to_four_coordinate(extended_boxes, x, y)
         bounding_boxes = bounding_boxes.copy()
         bounding_boxes["boxes"] = boxes
         bounding_boxes = bounding_box.clip_to_image(
-            bounding_boxes, images=image, bounding_box_format="rel_xyxy"
+            bounding_boxes, images=images, bounding_box_format="rel_xyxy"
         )
         bounding_boxes = keras_cv.bounding_box.convert_format(
             bounding_boxes,
             source="rel_xyxy",
             target=self.bounding_box_format,
-            images=image,
+            images=images,
             dtype=self.compute_dtype,
         )
         return bounding_boxes
 
     def get_config(self):
         config = super().get_config()
         config.update(
@@ -208,107 +329,7 @@
                 "fill_mode": self.fill_mode,
                 "fill_value": self.fill_value,
                 "bounding_box_format": self.bounding_box_format,
                 "seed": self.seed,
             }
         )
         return config
-
-    @staticmethod
-    def _format_transform(transform):
-        transform = tf.convert_to_tensor(transform, dtype=tf.float32)
-        return transform[tf.newaxis]
-
-    @staticmethod
-    def _convert_to_four_coordinate(extended_bboxes, x, y):
-        """convert from extended coordinates to 4 coordinates system"""
-        (
-            top_left_x,
-            top_left_y,
-            bottom_right_x,
-            bottom_right_y,
-            top_right_x,
-            top_right_y,
-            bottom_left_x,
-            bottom_left_y,
-        ) = tf.split(extended_bboxes, 8, axis=1)
-
-        # choose x1,x2 when x>0
-        def positive_case_x():
-            final_x1 = bottom_left_x
-            final_x2 = top_right_x
-            return final_x1, final_x2
-
-        # choose x1,x2 when x<0
-        def negative_case_x():
-            final_x1 = top_left_x
-            final_x2 = bottom_right_x
-            return final_x1, final_x2
-
-        if x is not None:
-            final_x1, final_x2 = tf.cond(
-                tf.less(x, 0), negative_case_x, positive_case_x
-            )
-        else:
-            final_x1, final_x2 = top_left_x, bottom_right_x
-
-        # choose y1,y2 when y > 0
-        def positive_case_y():
-            final_y1 = top_right_y
-            final_y2 = bottom_left_y
-            return final_y1, final_y2
-
-        # choose y1,y2 when y < 0
-        def negative_case_y():
-            final_y1 = top_left_y
-            final_y2 = bottom_right_y
-            return final_y1, final_y2
-
-        if y is not None:
-            final_y1, final_y2 = tf.cond(
-                tf.less(y, 0), negative_case_y, positive_case_y
-            )
-        else:
-            final_y1, final_y2 = top_left_y, bottom_right_y
-        return tf.concat(
-            [final_x1, final_y1, final_x2, final_y2],
-            axis=1,
-        )
-
-    @staticmethod
-    def _apply_horizontal_transformation_to_bounding_box(extended_bounding_boxes, x):
-        # create transformation matrix [1,4]
-        matrix = tf.stack([1.0, -x, 0, 1.0], axis=0)
-        # reshape it to [2,2]
-        matrix = tf.reshape(matrix, (2, 2))
-        # reshape unnormalized bboxes from [N,8] -> [N*4,2]
-        new_bboxes = tf.reshape(extended_bounding_boxes, (-1, 2))
-        # [[1,x`],[y`,1]]*[x,y]->[new_x,new_y]
-        transformed_bboxes = tf.reshape(
-            tf.einsum("ij,kj->ki", matrix, new_bboxes), (-1, 8)
-        )
-        return transformed_bboxes
-
-    @staticmethod
-    def _apply_vertical_transformation_to_bounding_box(extended_bounding_boxes, y):
-        # create transformation matrix [1,4]
-        matrix = tf.stack([1.0, 0, -y, 1.0], axis=0)
-        # reshape it to [2,2]
-        matrix = tf.reshape(matrix, (2, 2))
-        # reshape unnormalized bboxes from [N,8] -> [N*4,2]
-        new_bboxes = tf.reshape(extended_bounding_boxes, (-1, 2))
-        # [[1,x`],[y`,1]]*[x,y]->[new_x,new_y]
-        transformed_bboxes = tf.reshape(
-            tf.einsum("ij,kj->ki", matrix, new_bboxes), (-1, 8)
-        )
-        return transformed_bboxes
-
-    @staticmethod
-    def _convert_to_extended_corners_format(boxes):
-        """splits corner boxes top left,bottom right to 4 corners top left,
-        bottom right,top right and bottom left"""
-        x1, y1, x2, y2 = tf.split(boxes, [1, 1, 1, 1], axis=-1)
-        new_boxes = tf.concat(
-            [x1, y1, x2, y2, x2, y1, x1, y2],
-            axis=-1,
-        )
-        return new_boxes
```

## keras_cv/layers/preprocessing/random_shear_test.py

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 
 from keras_cv import bounding_box
 from keras_cv.layers import preprocessing
 
-classes = 10
+num_classes = 10
 
 
 class RandomShearTest(tf.test.TestCase):
     def test_aggressive_shear_fills_at_least_some_pixels(self):
         img_shape = (50, 50, 3)
         xs = tf.stack(
             [2 * tf.ones(img_shape), tf.ones(img_shape)],
@@ -42,15 +42,15 @@
 
     def test_return_shapes(self):
         """test return dict keys and value pairs"""
         xs = tf.ones((2, 512, 512, 3))
         # randomly sample labels
         ys_labels = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 2)
         ys_labels = tf.squeeze(ys_labels)
-        ys_labels = tf.one_hot(ys_labels, classes)
+        ys_labels = tf.one_hot(ys_labels, num_classes)
 
         # randomly sample bounding boxes
         ys_bounding_boxes = {
             "boxes": tf.ones((2, 3, 4)),
             "classes": tf.random.uniform((2, 3), 0, 1),
         }
 
@@ -59,15 +59,19 @@
             y_factor=(0.1, 0.3),
             seed=0,
             fill_mode="constant",
             bounding_box_format="xywh",
         )
 
         outputs = layer(
-            {"images": xs, "targets": ys_labels, "bounding_boxes": ys_bounding_boxes}
+            {
+                "images": xs,
+                "targets": ys_labels,
+                "bounding_boxes": ys_bounding_boxes,
+            }
         )
         xs, ys_labels, ys_bounding_boxes = (
             outputs["images"],
             outputs["targets"],
             outputs["bounding_boxes"],
         )
         ys_bounding_boxes = bounding_box.to_dense(ys_bounding_boxes)
@@ -85,17 +89,19 @@
             seed=0,
             fill_mode="constant",
         )
         outputs = layer(inputs)
         self.assertEqual(outputs["images"].shape, [512, 512, 3])
 
     def test_area(self):
-        xs = tf.ones((512, 512, 3))
+        xs = tf.ones((1, 512, 512, 3))
         ys = {
-            "boxes": tf.constant([[0.3, 0.4, 0.5, 0.6], [0.9, 0.8, 1.0, 1.0]]),
+            "boxes": tf.constant(
+                [[[0.3, 0.4, 0.5, 0.6], [0.9, 0.8, 1.0, 1.0]]]
+            ),
             "classes": tf.constant([2, 3]),
         }
 
         inputs = {"images": xs, "bounding_boxes": ys}
         layer = preprocessing.RandomShear(
             x_factor=(0.3, 0.7),
             y_factor=(0.4, 0.7),
@@ -105,16 +111,24 @@
         )
         outputs = layer(inputs)
         xs, ys_bounding_boxes = (
             outputs["images"],
             outputs["bounding_boxes"]["boxes"],
         )
         new_area = tf.math.multiply(
-            tf.abs(tf.subtract(ys_bounding_boxes[..., 2], ys_bounding_boxes[..., 0])),
-            tf.abs(tf.subtract(ys_bounding_boxes[..., 3], ys_bounding_boxes[..., 1])),
+            tf.abs(
+                tf.subtract(
+                    ys_bounding_boxes[..., 2], ys_bounding_boxes[..., 0]
+                )
+            ),
+            tf.abs(
+                tf.subtract(
+                    ys_bounding_boxes[..., 3], ys_bounding_boxes[..., 1]
+                )
+            ),
         )
         old_area = tf.math.multiply(
             tf.abs(tf.subtract(ys["boxes"][..., 2], ys["boxes"][..., 0])),
             tf.abs(tf.subtract(ys["boxes"][..., 3], ys["boxes"][..., 1])),
         )
         self.assertTrue(tf.math.reduce_all(new_area > old_area))
 
@@ -143,15 +157,16 @@
         xs = outputs["images"]
 
         # None of the individual values should still be close to 1 or 0
         self.assertNotAllClose(xs, 1.0)
         self.assertNotAllClose(xs, 2.0)
 
     def test_no_augmentation(self):
-        """test for no image and bbox augmenation when x_factor,y_factor is 0,0"""
+        """test for no image and bbox augmentation when x_factor,y_factor is
+        0,0"""
         xs = tf.cast(
             tf.stack(
                 [2 * tf.ones((4, 4, 3)), tf.ones((4, 4, 3))],
                 axis=0,
             ),
             tf.float32,
         )
@@ -185,16 +200,20 @@
                 axis=0,
             ),
             tf.float32,
         )
         ys = tf.cast(
             tf.stack(
                 [
-                    tf.constant([[10.0, 20.0, 40.0, 50.0], [12.0, 22.0, 42.0, 54.0]]),
-                    tf.constant([[10.0, 20.0, 40.0, 50.0], [12.0, 22.0, 42.0, 54.0]]),
+                    tf.constant(
+                        [[10.0, 20.0, 40.0, 50.0], [12.0, 22.0, 42.0, 54.0]]
+                    ),
+                    tf.constant(
+                        [[10.0, 20.0, 40.0, 50.0], [12.0, 22.0, 42.0, 54.0]]
+                    ),
                 ],
                 axis=0,
             ),
             tf.float32,
         )
         ys = bounding_box.add_class_id(ys)
         true_ys = tf.cast(
@@ -219,7 +238,36 @@
         )
         layer = preprocessing.RandomShear(
             x_factor=0.2, y_factor=0.2, bounding_box_format="xyxy", seed=1
         )
         outputs = layer({"images": xs, "bounding_boxes": ys})
         _, output_ys = outputs["images"], outputs["bounding_boxes"].to_tensor()
         self.assertAllClose(true_ys, output_ys, rtol=1e-02, atol=1e-03)
+
+    def test_random_shear_on_batched_images_independently(self):
+        image = tf.random.uniform(shape=(100, 100, 3))
+        input_images = tf.stack([image, image], axis=0)
+
+        layer = preprocessing.RandomShear(x_factor=0.5, y_factor=0.5)
+
+        results = layer(input_images)
+        self.assertNotAllClose(results[0], results[1])
+
+    def test_ragged_bounding_box(self):
+        images = tf.random.uniform((2, 16, 16, 3))
+
+        random_box = tf.constant(
+            [[[0.1, 0.2, 1, 1], [0.4, 0.6, 1, 1]]], dtype=tf.float32
+        )
+        random_box = tf.squeeze(random_box, axis=0)
+        random_box = tf.RaggedTensor.from_row_lengths(random_box, [1, 1])
+        classes = tf.ragged.constant([[0], [0]])
+        bounding_boxes = {"boxes": random_box, "classes": classes}
+        inputs = {"images": images, "bounding_boxes": bounding_boxes}
+
+        layer = preprocessing.RandomShear(
+            x_factor=(0.5, 0.5),
+            y_factor=(0.5, 0.5),
+            bounding_box_format="rel_xywh",
+        )
+
+        layer(inputs)
```

## keras_cv/layers/preprocessing/random_translation.py

```diff
@@ -9,91 +9,45 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from keras import backend
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+import keras_cv
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
-from keras_cv.utils import preprocessing
-
-
-def check_fill_mode_and_interpolation(fill_mode, interpolation):
-    if fill_mode not in {"reflect", "wrap", "constant", "nearest"}:
-        raise NotImplementedError(
-            f"Unknown `fill_mode` {fill_mode}. Only `reflect`, `wrap`, "
-            "`constant` and `nearest` are supported."
-        )
-    if interpolation not in {"nearest", "bilinear"}:
-        raise NotImplementedError(
-            f"Unknown `interpolation` {interpolation}. Only `nearest` and "
-            "`bilinear` are supported."
-        )
-
-
-def get_translation_matrix(translations, name=None):
-    """Returns projective transform(s) for the given translation(s).
-
-    Args:
-      translations: A matrix of 2-element lists representing `[dx, dy]`
-        to translate for each image (for a batch of images).
-      name: The name of the op.
-
-    Returns:
-      A tensor of shape `(num_images, 8)` projective transforms which can be
-        given to `transform`.
-    """
-    with backend.name_scope(name or "translation_matrix"):
-        num_translations = tf.shape(translations)[0]
-        # The translation matrix looks like:
-        #     [[1 0 -dx]
-        #      [0 1 -dy]
-        #      [0 0 1]]
-        # where the last entry is implicit.
-        # Translation matrices are always float32.
-        return tf.concat(
-            values=[
-                tf.ones((num_translations, 1), tf.float32),
-                tf.zeros((num_translations, 1), tf.float32),
-                -translations[:, 0, None],
-                tf.zeros((num_translations, 1), tf.float32),
-                tf.ones((num_translations, 1), tf.float32),
-                -translations[:, 1, None],
-                tf.zeros((num_translations, 2), tf.float32),
-            ],
-            axis=1,
-        )
-
+from keras_cv.utils import preprocessing as preprocessing_utils
 
 H_AXIS = -3
 W_AXIS = -2
 
 
-class RandomTranslation(BaseImageAugmentationLayer):
-    """A preprocessing layer which randomly translates images during training.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomTranslation(VectorizedBaseImageAugmentationLayer):
+    """A preprocessing layer which randomly translates images.
 
-    This layer will apply random translations to each image during training,
-    filling empty space according to `fill_mode`.
+    This layer will apply random translations to each image, filling empty
+    space according to `fill_mode`.
 
     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and
     of integer or floating point dtype. By default, the layer will output
     floats.
 
     Args:
       height_factor: a float represented as fraction of value, or a tuple of
           size 2 representing lower and upper bound for shifting vertically. A
           negative value means shifting image up, while a positive value means
           shifting image down. When represented as a single positive float, this
           value is used for both the upper and lower bound. For instance,
           `height_factor=(-0.2, 0.3)` results in an output shifted by a random
-          amount in the range `[-20%, +30%]`.  `height_factor=0.2` results in an
+          amount in the range `[-20%, +30%]`. `height_factor=0.2` results in an
           output height shifted by a random amount in the range `[-20%, +20%]`.
       width_factor: a float represented as fraction of value, or a tuple of size
           2 representing lower and upper bound for shifting horizontally. A
           negative value means shifting image left, while a positive value means
           shifting image right. When represented as a single positive float,
           this value is used for both the upper and lower bound. For instance,
           `width_factor=(-0.2, 0.3)` results in an output shifted left by 20%,
@@ -112,14 +66,19 @@
           - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by
               the nearest pixel.
       interpolation: Interpolation mode. Supported values: `"nearest"`,
           `"bilinear"`.
       seed: Integer. Used to create a random seed.
       fill_value: a float represents the value to be filled outside the
           boundaries when `fill_mode="constant"`.
+      bounding_box_format: The format of bounding boxes of input dataset.
+          Refer to
+          https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box/converters.py
+          for more details on supported bounding box formats. This is required
+          when augmenting data which includes bounding boxes.
 
     Input shape:
         3D (unbatched) or 4D (batched) tensor with shape:
         `(..., height, width, channels)`,  in `"channels_last"` format.
 
     Output shape:
         3D (unbatched) or 4D (batched) tensor with shape:
@@ -130,14 +89,15 @@
         self,
         height_factor,
         width_factor,
         fill_mode="reflect",
         interpolation="bilinear",
         seed=None,
         fill_value=0.0,
+        bounding_box_format=None,
         **kwargs,
     ):
         super().__init__(seed=seed, force_generator=True, **kwargs)
         self.height_factor = height_factor
         if isinstance(height_factor, (tuple, list)):
             self.height_lower = height_factor[0]
             self.height_upper = height_factor[1]
@@ -169,85 +129,138 @@
             )
         if abs(self.width_lower) > 1.0 or abs(self.width_upper) > 1.0:
             raise ValueError(
                 "`width_factor` must have values between [-1, 1], "
                 f"got {width_factor}"
             )
 
-        check_fill_mode_and_interpolation(fill_mode, interpolation)
+        preprocessing_utils.check_fill_mode_and_interpolation(
+            fill_mode, interpolation
+        )
 
         self.fill_mode = fill_mode
         self.fill_value = fill_value
         self.interpolation = interpolation
         self.seed = seed
+        self.bounding_box_format = bounding_box_format
 
-    def augment_image(self, image, transformation, **kwargs):
-        """Translated inputs with random ops."""
-        # The transform op only accepts rank 4 inputs, so if we have an
-        # unbatched image, we need to temporarily expand dims to a batch.
-        original_shape = image.shape
-        inputs = tf.expand_dims(image, 0)
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        height_translations = self._random_generator.random_uniform(
+            shape=[batch_size, 1],
+            minval=self.height_lower,
+            maxval=self.height_upper,
+            dtype=tf.float32,
+        )
+        width_translations = self._random_generator.random_uniform(
+            shape=[batch_size, 1],
+            minval=self.width_lower,
+            maxval=self.width_upper,
+            dtype=tf.float32,
+        )
+        return {
+            "height_translations": height_translations,
+            "width_translations": width_translations,
+        }
+
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        image = tf.expand_dims(image, axis=0)
+        height_translations = transformation["height_translations"]
+        width_translations = transformation["width_translations"]
+        transformation = {
+            "height_translations": tf.expand_dims(height_translations, axis=0),
+            "width_translations": tf.expand_dims(width_translations, axis=0),
+        }
+        image = self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
+        return tf.squeeze(image, axis=0)
 
-        inputs_shape = tf.shape(inputs)
+    def augment_images(self, images, transformations, **kwargs):
+        """Translated inputs with random ops."""
+        original_shape = images.shape
+        inputs_shape = tf.shape(images)
         img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)
         img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)
-        height_translation = transformation["height_translation"]
-        width_translation = transformation["width_translation"]
-        height_translation = height_translation * img_hd
-        width_translation = width_translation * img_wd
+        height_translations = transformations["height_translations"]
+        width_translations = transformations["width_translations"]
+        height_translations = height_translations * img_hd
+        width_translations = width_translations * img_wd
         translations = tf.cast(
-            tf.concat([width_translation, height_translation], axis=1),
+            tf.concat([width_translations, height_translations], axis=1),
             dtype=tf.float32,
         )
-        output = preprocessing.transform(
-            inputs,
-            get_translation_matrix(translations),
+        output = preprocessing_utils.transform(
+            images,
+            preprocessing_utils.get_translation_matrix(translations),
             interpolation=self.interpolation,
             fill_mode=self.fill_mode,
             fill_value=self.fill_value,
         )
-
-        output = tf.squeeze(output, 0)
         output.set_shape(original_shape)
         return output
 
-    def get_random_transformation(self, image=None, **kwargs):
-        batch_size = 1
-        height_translation = self._random_generator.random_uniform(
-            shape=[batch_size, 1],
-            minval=self.height_lower,
-            maxval=self.height_upper,
-            dtype=tf.float32,
-        )
-        width_translation = self._random_generator.random_uniform(
-            shape=[batch_size, 1],
-            minval=self.width_lower,
-            maxval=self.width_upper,
-            dtype=tf.float32,
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
+
+    def augment_bounding_boxes(
+        self, bounding_boxes, transformations, images=None, **kwargs
+    ):
+        if self.bounding_box_format is None:
+            raise ValueError(
+                "`RandomTranslation()` was called with bounding boxes,"
+                "but no `bounding_box_format` was specified in the constructor."
+                "Please specify a bounding box format in the constructor. i.e."
+                "`RandomTranslation(bounding_box_format='xyxy')`"
+            )
+        bounding_boxes = keras_cv.bounding_box.convert_format(
+            bounding_boxes,
+            source=self.bounding_box_format,
+            target="rel_xyxy",
+            images=images,
+            dtype=self.compute_dtype,
         )
-        return {
-            "height_translation": height_translation,
-            "width_translation": width_translation,
-        }
 
-    def _batch_augment(self, inputs):
-        # Change to vectorized_map for better performance, as well as work
-        # around issue for different tensorspec between inputs and outputs.
-        return tf.vectorized_map(self._augment, inputs)
+        boxes = bounding_boxes["boxes"]
+        x1, y1, x2, y2 = tf.split(boxes, [1, 1, 1, 1], axis=-1)
+        x1 += tf.expand_dims(transformations["width_translations"], axis=1)
+        x2 += tf.expand_dims(transformations["width_translations"], axis=1)
+        y1 += tf.expand_dims(transformations["height_translations"], axis=1)
+        y2 += tf.expand_dims(transformations["height_translations"], axis=1)
+
+        bounding_boxes["boxes"] = tf.concat([x1, y1, x2, y2], axis=-1)
+        bounding_boxes = keras_cv.bounding_box.to_dense(bounding_boxes)
+
+        bounding_boxes = keras_cv.bounding_box.clip_to_image(
+            bounding_boxes,
+            bounding_box_format="rel_xyxy",
+            images=images,
+        )
+        bounding_boxes = keras_cv.bounding_box.to_ragged(bounding_boxes)
 
-    def augment_label(self, label, transformation, **kwargs):
-        return label
+        bounding_boxes = keras_cv.bounding_box.convert_format(
+            bounding_boxes,
+            source="rel_xyxy",
+            target=self.bounding_box_format,
+            images=images,
+            dtype=self.compute_dtype,
+        )
+        return bounding_boxes
 
     def compute_output_shape(self, input_shape):
         return input_shape
 
     def get_config(self):
         config = {
             "height_factor": self.height_factor,
             "width_factor": self.width_factor,
             "fill_mode": self.fill_mode,
             "fill_value": self.fill_value,
             "interpolation": self.interpolation,
             "seed": self.seed,
+            "bounding_box_format": self.bounding_box_format,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_translation_test.py

```diff
@@ -18,15 +18,17 @@
 
 from keras_cv.layers import preprocessing
 
 
 class RandomTranslationTest(tf.test.TestCase, parameterized.TestCase):
     def test_random_translation_up_numeric_reflect(self):
         for dtype in (np.int64, np.float32):
-            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
+            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(
+                dtype
+            )
             # Shifting by -.2 * 5 = 1 pixel.
             layer = preprocessing.RandomTranslation(
                 height_factor=(-0.2, -0.2), width_factor=0.0
             )
             output_image = layer(input_image)
             expected_output = np.asarray(
                 [
@@ -38,15 +40,17 @@
                 ]
             ).astype(dtype)
             expected_output = np.reshape(expected_output, (1, 5, 5, 1))
             self.assertAllEqual(expected_output, output_image)
 
     def test_random_translation_up_numeric_constant(self):
         for dtype in (np.int64, np.float32):
-            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
+            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(
+                dtype
+            )
             # Shifting by -.2 * 5 = 1 pixel.
             layer = preprocessing.RandomTranslation(
                 height_factor=(-0.2, -0.2),
                 width_factor=0.0,
                 fill_mode="constant",
             )
             output_image = layer(input_image)
@@ -60,15 +64,17 @@
                 ]
             ).astype(dtype)
             expected_output = np.reshape(expected_output, (1, 5, 5, 1))
             self.assertAllEqual(expected_output, output_image)
 
     def test_random_translation_down_numeric_reflect(self):
         for dtype in (np.int64, np.float32):
-            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
+            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(
+                dtype
+            )
             # Shifting by .2 * 5 = 1 pixel.
             layer = preprocessing.RandomTranslation(
                 height_factor=(0.2, 0.2), width_factor=0.0
             )
             output_image = layer(input_image)
             expected_output = np.asarray(
                 [
@@ -80,15 +86,17 @@
                 ]
             ).astype(dtype)
             expected_output = np.reshape(expected_output, (1, 5, 5, 1))
             self.assertAllEqual(expected_output, output_image)
 
     def test_random_translation_asymmetric_size_numeric_reflect(self):
         for dtype in (np.int64, np.float32):
-            input_image = np.reshape(np.arange(0, 16), (1, 8, 2, 1)).astype(dtype)
+            input_image = np.reshape(np.arange(0, 16), (1, 8, 2, 1)).astype(
+                dtype
+            )
             # Shifting by .5 * 8 = 1 pixel.
             layer = preprocessing.RandomTranslation(
                 height_factor=(0.5, 0.5), width_factor=0.0
             )
             output_image = layer(input_image)
             # pyformat: disable
             expected_output = np.asarray(
@@ -105,15 +113,17 @@
             ).astype(dtype)
             # pyformat: enable
             expected_output = np.reshape(expected_output, (1, 8, 2, 1))
             self.assertAllEqual(expected_output, output_image)
 
     def test_random_translation_down_numeric_constant(self):
         for dtype in (np.int64, np.float32):
-            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
+            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(
+                dtype
+            )
             # Shifting by -.2 * 5 = 1 pixel.
             layer = preprocessing.RandomTranslation(
                 height_factor=(0.2, 0.2),
                 width_factor=0.0,
                 fill_mode="constant",
             )
             output_image = layer(input_image)
@@ -127,15 +137,17 @@
                 ]
             ).astype(dtype)
             expected_output = np.reshape(expected_output, (1, 5, 5, 1))
             self.assertAllEqual(expected_output, output_image)
 
     def test_random_translation_left_numeric_reflect(self):
         for dtype in (np.int64, np.float32):
-            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
+            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(
+                dtype
+            )
             # Shifting by .2 * 5 = 1 pixel.
             layer = preprocessing.RandomTranslation(
                 height_factor=0.0, width_factor=(-0.2, -0.2)
             )
             output_image = layer(input_image)
             expected_output = np.asarray(
                 [
@@ -147,15 +159,17 @@
                 ]
             ).astype(dtype)
             expected_output = np.reshape(expected_output, (1, 5, 5, 1))
             self.assertAllEqual(expected_output, output_image)
 
     def test_random_translation_left_numeric_constant(self):
         for dtype in (np.int64, np.float32):
-            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
+            input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(
+                dtype
+            )
             # Shifting by -.2 * 5 = 1 pixel.
             layer = preprocessing.RandomTranslation(
                 height_factor=0.0,
                 width_factor=(-0.2, -0.2),
                 fill_mode="constant",
             )
             output_image = layer(input_image)
@@ -167,20 +181,24 @@
                     [16, 17, 18, 19, 0],
                     [21, 22, 23, 24, 0],
                 ]
             ).astype(dtype)
             expected_output = np.reshape(expected_output, (1, 5, 5, 1))
             self.assertAllEqual(expected_output, output_image)
 
-    def test_random_translation_inference(self):
-        input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)
-        expected_output = input_images
-        layer = preprocessing.RandomTranslation(0.5, 0.5)
-        actual_output = layer(input_images, training=False)
-        self.assertAllClose(expected_output, actual_output)
+    def test_random_translation_on_batched_images_independently(self):
+        image = tf.random.uniform(shape=(100, 100, 3))
+        input_images = tf.stack([image, image], axis=0)
+
+        layer = preprocessing.RandomTranslation(
+            height_factor=0.5, width_factor=0.5
+        )
+
+        results = layer(input_images)
+        self.assertNotAllClose(results[0], results[1])
 
     def test_config_with_custom_name(self):
         layer = preprocessing.RandomTranslation(0.5, 0.6, name="image_preproc")
         config = layer.get_config()
         layer_1 = preprocessing.RandomTranslation.from_config(config)
         self.assertEqual(layer_1.name, layer.name)
```

## keras_cv/layers/preprocessing/random_zoom.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,29 +11,30 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import tensorflow as tf
 from keras import backend
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
-from keras_cv.utils import preprocessing
+from keras_cv.utils import preprocessing as preprocessing_utils
 
 # In order to support both unbatched and batched inputs, the horizontal
-# and verticle axis is reverse indexed
+# and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomZoom(BaseImageAugmentationLayer):
-    """A preprocessing layer which randomly zooms images during training.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomZoom(VectorizedBaseImageAugmentationLayer):
+    """A preprocessing layer which randomly zooms images.
 
     This layer will randomly zoom in or out on each axis of an image
     independently, filling empty space according to `fill_mode`.
 
     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and
     of integer or floating point dtype. By default, the layer will output
     floats.
@@ -125,73 +126,76 @@
 
             if self.width_lower < -1.0 or self.width_upper < -1.0:
                 raise ValueError(
                     "`width_factor` must have values larger than -1, "
                     f"got {width_factor}"
                 )
 
-        preprocessing.check_fill_mode_and_interpolation(fill_mode, interpolation)
+        preprocessing_utils.check_fill_mode_and_interpolation(
+            fill_mode, interpolation
+        )
 
         self.fill_mode = fill_mode
         self.fill_value = fill_value
         self.interpolation = interpolation
         self.seed = seed
 
-    def get_random_transformation(self, image=None, **kwargs):
-        height_zoom = self._random_generator.random_uniform(
-            shape=[1, 1],
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        height_zooms = self._random_generator.random_uniform(
+            shape=[batch_size, 1],
             minval=1.0 + self.height_lower,
             maxval=1.0 + self.height_upper,
         )
         if self.width_factor is not None:
-            width_zoom = self._random_generator.random_uniform(
-                shape=[1, 1],
+            width_zooms = self._random_generator.random_uniform(
+                shape=[batch_size, 1],
                 minval=1.0 + self.width_lower,
                 maxval=1.0 + self.width_upper,
             )
         else:
-            width_zoom = height_zoom
+            width_zooms = height_zooms
+
+        return {"height_zooms": height_zooms, "width_zooms": width_zooms}
 
-        return {"height_zoom": height_zoom, "width_zoom": width_zoom}
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        image = tf.expand_dims(image, axis=0)
+        width_zooms = transformation["width_zooms"]
+        height_zooms = transformation["height_zooms"]
+        transformation = {
+            "height_zooms": tf.expand_dims(height_zooms, axis=0),
+            "width_zooms": tf.expand_dims(width_zooms, axis=0),
+        }
+        image = self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
+        return tf.squeeze(image, axis=0)
 
-    def augment_image(self, image, transformation, **kwargs):
-        image = preprocessing.ensure_tensor(image, self.compute_dtype)
-        original_shape = image.shape
-        image = tf.expand_dims(image, 0)
-        image_shape = tf.shape(image)
+    def augment_images(self, images, transformations, **kwargs):
+        images = preprocessing_utils.ensure_tensor(images, self.compute_dtype)
+        original_shape = images.shape
+        image_shape = tf.shape(images)
         img_hd = tf.cast(image_shape[H_AXIS], tf.float32)
         img_wd = tf.cast(image_shape[W_AXIS], tf.float32)
-        width_zoom = transformation["width_zoom"]
-        height_zoom = transformation["height_zoom"]
-        zooms = tf.cast(tf.concat([width_zoom, height_zoom], axis=1), dtype=tf.float32)
-        output = preprocessing.transform(
-            image,
+        width_zooms = transformations["width_zooms"]
+        height_zooms = transformations["height_zooms"]
+        zooms = tf.cast(
+            tf.concat([width_zooms, height_zooms], axis=1), dtype=tf.float32
+        )
+        outputs = preprocessing_utils.transform(
+            images,
             self.get_zoom_matrix(zooms, img_hd, img_wd),
             fill_mode=self.fill_mode,
             fill_value=self.fill_value,
             interpolation=self.interpolation,
         )
-        output = tf.squeeze(output, 0)
-        output.set_shape(original_shape)
-        return output
-
-    def augment_label(self, label, transformation, **kwargs):
-        return label
+        outputs.set_shape(original_shape)
+        return outputs
 
-    def get_config(self):
-        config = {
-            "height_factor": self.height_factor,
-            "width_factor": self.width_factor,
-            "fill_mode": self.fill_mode,
-            "fill_value": self.fill_value,
-            "interpolation": self.interpolation,
-            "seed": self.seed,
-        }
-        base_config = super().get_config()
-        return dict(list(base_config.items()) + list(config.items()))
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
 
     def get_zoom_matrix(self, zooms, image_height, image_width, name=None):
         """Returns projective transform(s) for the given zoom(s).
 
         Args:
         zooms: A matrix of 2-element lists representing `[zx, zy]` to zoom for
             each image (for a batch of images).
@@ -226,7 +230,23 @@
                     tf.zeros((num_zooms, 1), tf.float32),
                     zooms[:, 1, None],
                     y_offset,
                     tf.zeros((num_zooms, 2), tf.float32),
                 ],
                 axis=1,
             )
+
+    def get_config(self):
+        config = {
+            "height_factor": self.height_factor,
+            "width_factor": self.width_factor,
+            "fill_mode": self.fill_mode,
+            "fill_value": self.fill_value,
+            "interpolation": self.interpolation,
+            "seed": self.seed,
+        }
+        base_config = super().get_config()
+        return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
```

## keras_cv/layers/preprocessing/random_zoom_test.py

```diff
@@ -48,15 +48,17 @@
             ),
         )
         self.assertAllEqual(expected_output.shape, actual_output.shape)
 
     def test_random_zoom_in_numeric(self):
         for dtype in (np.int64, np.float32):
             input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
-            layer = RandomZoom((-0.5, -0.5), (-0.5, -0.5), interpolation="nearest")
+            layer = RandomZoom(
+                (-0.5, -0.5), (-0.5, -0.5), interpolation="nearest"
+            )
             output_image = layer(np.expand_dims(input_image, axis=0))
             expected_output = np.asarray(
                 [
                     [6, 7, 7, 8, 8],
                     [11, 12, 12, 13, 13],
                     [11, 12, 12, 13, 13],
                     [16, 17, 17, 18, 18],
@@ -103,20 +105,24 @@
                     [0, 21, 22, 24, 0],
                     [0, 0, 0, 0, 0],
                 ]
             ).astype(dtype)
             expected_output = np.reshape(expected_output, (1, 5, 5, 1))
             self.assertAllEqual(expected_output, output_image)
 
-    def test_random_zoom_inference(self):
-        input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)
-        expected_output = input_images
-        layer = RandomZoom(0.5, 0.5)
-        actual_output = layer(input_images, training=False)
-        self.assertAllClose(expected_output, actual_output)
+    def test_random_zoom_on_batched_images_independently(self):
+        image = tf.random.uniform(shape=(100, 100, 3))
+        input_images = tf.stack([image, image], axis=0)
+
+        layer = RandomZoom(
+            height_factor=(-0.4, -0.5), width_factor=(-0.2, -0.3)
+        )
+
+        results = layer(input_images)
+        self.assertNotAllClose(results[0], results[1])
 
     def test_config_with_custom_name(self):
         layer = RandomZoom(0.5, 0.6, name="image_preproc")
         config = layer.get_config()
         layer_1 = RandomZoom.from_config(config)
         self.assertEqual(layer_1.name, layer.name)
```

## keras_cv/layers/preprocessing/randomly_zoomed_crop.py

```diff
@@ -1,57 +1,64 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from keras import backend
+from tensorflow import keras
 
 from keras_cv import core
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
-from keras_cv.utils import preprocessing
+from keras_cv.utils import preprocessing as preprocessing_utils
 
+H_AXIS = -3
+W_AXIS = -2
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class RandomlyZoomedCrop(BaseImageAugmentationLayer):
+
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RandomlyZoomedCrop(VectorizedBaseImageAugmentationLayer):
     """Randomly crops a part of an image and zooms it by a provided amount size.
 
     This implementation takes a distortion-oriented approach, which means the
     amount of distortion in the image is proportional to the `zoom_factor`
     argument. To do this, we first sample a random value for `zoom_factor` and
     `aspect_ratio_factor`. Further we deduce a `crop_size` which abides by the
-    calculated aspect ratio. Finally we do the actual cropping operation and
+    calculated aspect ratio. Finally, we do the actual cropping operation and
     resize the image to `(height, width)`.
 
     Args:
         height: The height of the output shape.
         width: The width of the output shape.
         zoom_factor: A tuple of two floats, ConstantFactorSampler or
-            UniformFactorSampler. Represents the area relative to the original image
-            of the cropped image before resizing it to `(height, width)`.
+            UniformFactorSampler. Represents the area relative to the original
+            image of the cropped image before resizing it to `(height, width)`.
         aspect_ratio_factor: A tuple of two floats, ConstantFactorSampler or
             UniformFactorSampler. Aspect ratio means the ratio of width to
-            height of the cropped image. In the context of this layer, the aspect ratio
-            sampled represents a value to distort the aspect ratio by.
+            height of the cropped image. In the context of this layer, the
+            aspect ratio sampled represents a value to distort the aspect ratio
+            by.
             Represents the lower and upper bound for the aspect ratio of the
-            cropped image before resizing it to `(height, width)`.  For most tasks, this
-            should be `(3/4, 4/3)`.  To perform a no-op provide the value `(1.0, 1.0)`.
+            cropped image before resizing it to `(height, width)`. For most
+            tasks, this should be `(3/4, 4/3)`. To perform a no-op provide the
+            value `(1.0, 1.0)`.
         interpolation: (Optional) A string specifying the sampling method for
-            resizing. Defaults to "bilinear".
-        seed: (Optional) Used to create a random seed. Defaults to None.
+            resizing, defaults to "bilinear".
+        seed: (Optional) Used to create a random seed, defaults to None.
     """
 
     def __init__(
         self,
         height,
         width,
         zoom_factor,
@@ -59,162 +66,207 @@
         interpolation="bilinear",
         seed=None,
         **kwargs,
     ):
         super().__init__(seed=seed, **kwargs)
         self.height = height
         self.width = width
-        self.aspect_ratio_factor = preprocessing.parse_factor(
+        self.aspect_ratio_factor = preprocessing_utils.parse_factor(
             aspect_ratio_factor,
             min_value=0.0,
             max_value=None,
             param_name="aspect_ratio_factor",
             seed=seed,
         )
-        self.zoom_factor = preprocessing.parse_factor(
+        self.zoom_factor = preprocessing_utils.parse_factor(
             zoom_factor,
             min_value=0.0,
             max_value=None,
             param_name="zoom_factor",
             seed=seed,
         )
 
-        self._check_class_arguments(height, width, zoom_factor, aspect_ratio_factor)
+        self._check_class_arguments(
+            height, width, zoom_factor, aspect_ratio_factor
+        )
         self.force_output_dense_images = True
         self.interpolation = interpolation
         self.seed = seed
 
-    def get_random_transformation(
-        self, image=None, label=None, bounding_box=None, **kwargs
+    def _check_class_arguments(
+        self, height, width, zoom_factor, aspect_ratio_factor
     ):
-        zoom_factor = self.zoom_factor()
-        aspect_ratio = self.aspect_ratio_factor()
-
-        original_height = tf.cast(tf.shape(image)[-3], tf.float32)
-        original_width = tf.cast(tf.shape(image)[-2], tf.float32)
-
-        crop_size = (
-            tf.round(self.height / zoom_factor),
-            tf.round(self.width / zoom_factor),
-        )
-
-        new_height = crop_size[0] / tf.sqrt(aspect_ratio)
-
-        new_width = crop_size[1] * tf.sqrt(aspect_ratio)
-
-        height_offset = self._random_generator.random_uniform(
-            (),
-            minval=tf.minimum(0.0, original_height - new_height),
-            maxval=tf.maximum(0.0, original_height - new_height),
-            dtype=tf.float32,
-        )
-
-        width_offset = self._random_generator.random_uniform(
-            (),
-            minval=tf.minimum(0.0, original_width - new_width),
-            maxval=tf.maximum(0.0, original_width - new_width),
-            dtype=tf.float32,
-        )
-
-        new_height = new_height / original_height
-        new_width = new_width / original_width
-
-        height_offset = height_offset / original_height
-        width_offset = width_offset / original_width
-
-        return (new_height, new_width, height_offset, width_offset)
-
-    def call(self, inputs, training=True):
-        if training:
-            return super().call(inputs, training)
-        else:
-            inputs = self._ensure_inputs_are_compute_dtype(inputs)
-            inputs, meta_data = self._format_inputs(inputs)
-            output = inputs
-            # self._resize() returns valid results for both batched and
-            # unbatched
-            output["images"] = self._resize(inputs["images"])
-
-            return self._format_output(output, meta_data)
-
-    def augment_image(self, image, transformation, **kwargs):
-        image_shape = tf.shape(image)
-
-        height = tf.cast(image_shape[-3], tf.float32)
-        width = tf.cast(image_shape[-2], tf.float32)
-
-        image = tf.expand_dims(image, axis=0)
-        new_height, new_width, height_offset, width_offset = transformation
-
-        transform = RandomlyZoomedCrop._format_transform(
-            [
-                new_width,
-                0.0,
-                width_offset * width,
-                0.0,
-                new_height,
-                height_offset * height,
-                0.0,
-                0.0,
-            ]
-        )
-
-        image = preprocessing.transform(
-            images=image,
-            transforms=transform,
-            output_shape=(self.height, self.width),
-            interpolation=self.interpolation,
-            fill_mode="reflect",
-        )
-
-        return tf.squeeze(image, axis=0)
-
-    @staticmethod
-    def _format_transform(transform):
-        transform = tf.convert_to_tensor(transform, dtype=tf.float32)
-        return transform[tf.newaxis]
-
-    def _resize(self, image):
-        outputs = tf.keras.preprocessing.image.smart_resize(
-            image, (self.height, self.width)
-        )
-        # smart_resize will always output float32, so we need to re-cast.
-        return tf.cast(outputs, self.compute_dtype)
-
-    def _check_class_arguments(self, height, width, zoom_factor, aspect_ratio_factor):
         if not isinstance(height, int):
-            raise ValueError("`height` must be an integer. Received height={height}")
+            raise ValueError(
+                f"`height` must be an integer. Received height={height}"
+            )
 
         if not isinstance(width, int):
-            raise ValueError("`width` must be an integer. Received width={width}")
+            raise ValueError(
+                f"`width` must be an integer. Received width={width}"
+            )
 
         if (
             not isinstance(zoom_factor, (tuple, list, core.FactorSampler))
             or isinstance(zoom_factor, float)
             or isinstance(zoom_factor, int)
         ):
             raise ValueError(
                 "`zoom_factor` must be tuple of two positive floats"
                 " or keras_cv.core.FactorSampler instance. Received "
                 f"zoom_factor={zoom_factor}"
             )
 
         if (
-            not isinstance(aspect_ratio_factor, (tuple, list, core.FactorSampler))
+            not isinstance(
+                aspect_ratio_factor, (tuple, list, core.FactorSampler)
+            )
             or isinstance(aspect_ratio_factor, float)
             or isinstance(aspect_ratio_factor, int)
         ):
             raise ValueError(
                 "`aspect_ratio_factor` must be tuple of two positive floats or "
                 "keras_cv.core.FactorSampler instance. Received "
                 f"aspect_ratio_factor={aspect_ratio_factor}"
             )
 
-    def augment_target(self, augment_target, **kwargs):
-        return augment_target
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        zoom_factors = self.zoom_factor(shape=(batch_size, 1))
+        aspect_ratios = self.aspect_ratio_factor(shape=(batch_size,))
+
+        heights = tf.cast(
+            tf.tile([self.height], multiples=(batch_size,)), tf.float32
+        )
+        widths = tf.cast(
+            tf.tile([self.width], multiples=(batch_size,)), tf.float32
+        )
+        crop_size = (
+            tf.round(heights / zoom_factors),
+            tf.round(widths / zoom_factors),
+        )
+
+        new_heights = crop_size[0] / tf.sqrt(aspect_ratios)
+        new_widths = crop_size[1] * tf.sqrt(aspect_ratios)
+
+        height_offsets = self._random_generator.random_uniform(
+            shape=(batch_size, 1), maxval=1.0, dtype=tf.float32
+        )
+        width_offsets = self._random_generator.random_uniform(
+            shape=(batch_size, 1), maxval=1.0, dtype=tf.float32
+        )
+
+        return {
+            "new_heights": new_heights,
+            "new_widths": new_widths,
+            "height_offsets": height_offsets,
+            "width_offsets": width_offsets,
+        }
+
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        image = tf.expand_dims(image, axis=0)
+        new_heights = transformation["new_heights"]
+        new_widths = transformation["new_widths"]
+        height_offsets = transformation["height_offsets"]
+        width_offsets = transformation["width_offsets"]
+        transformation = {
+            "new_heights": tf.expand_dims(new_heights, axis=0),
+            "new_widths": tf.expand_dims(new_widths, axis=0),
+            "height_offsets": tf.expand_dims(height_offsets, axis=0),
+            "width_offsets": tf.expand_dims(width_offsets, axis=0),
+        }
+        image = self.augment_images(
+            images=image, transformations=transformation, **kwargs
+        )
+        return tf.squeeze(image, axis=0)
+
+    def augment_images(self, images, transformations, **kwargs):
+        image_shape = tf.shape(images)
+        image_height = tf.cast(image_shape[H_AXIS], tf.float32)
+        image_width = tf.cast(image_shape[W_AXIS], tf.float32)
+
+        new_widths = transformations["new_widths"]
+        new_heights = transformations["new_heights"]
+        width_offsets = transformations["width_offsets"]
+        height_offsets = transformations["height_offsets"]
+
+        zooms = tf.concat(
+            [new_widths / image_width, new_heights / image_height], axis=1
+        )
+        offsets = tf.concat([width_offsets, height_offsets], axis=1)
+        transforms = self.get_zoomed_crop_matrix(
+            zooms, offsets, image_height, image_width
+        )
+
+        images = preprocessing_utils.transform(
+            images=images,
+            transforms=transforms,
+            output_shape=(self.height, self.width),
+            interpolation=self.interpolation,
+            fill_mode="reflect",
+        )
+        return images
+
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
+
+    def get_zoomed_crop_matrix(
+        self, zooms, offsets, image_height, image_width, name=None
+    ):
+        """Returns projective transform(s) for the given zoom(s) and offset(s).
+
+        Args:
+        zooms: A matrix of 2-element lists representing `[zx, zy]` to zoom for
+            each image (for a batch of images).
+        offsets: A matrix of 2-element lists representing `[ox, oy]` to offset
+            for each image (for a batch of images).
+        image_height: Height of the image(s) to be transformed.
+        image_width: Width of the image(s) to be transformed.
+        name: The name of the op.
+
+        Returns:
+        A tensor of shape `(num_images, 8)`. Projective transforms which can be
+            given to operation `image_projective_transform_v3`.
+            If one row of transforms is
+            `[a0, a1, a2, b0, b1, b2, c0, c1]`, then it maps the *output* point
+            `(x, y)` to a transformed *input* point
+            `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,
+            where `k = c0 x + c1 y + 1`.
+        """
+        with backend.name_scope(name or "zoomed_crop_matrix"):
+            batch_size = tf.shape(zooms)[0]
+            new_widths = zooms[:, 0] * image_width
+            new_heights = zooms[:, 1] * image_height
+            image_heights = tf.cast(
+                tf.tile([image_height], multiples=(batch_size,)), tf.float32
+            )
+            image_widths = tf.cast(
+                tf.tile([image_width], multiples=(batch_size,)), tf.float32
+            )
+            width_offsets = offsets[:, 0] * (image_widths - new_widths)
+            height_offsets = offsets[:, 1] * (image_heights - new_heights)
+            return tf.concat(
+                values=[
+                    zooms[:, 0, tf.newaxis],
+                    tf.zeros((batch_size, 1), tf.float32),
+                    width_offsets[:, tf.newaxis],
+                    tf.zeros((batch_size, 1), tf.float32),
+                    zooms[:, 1, tf.newaxis],
+                    height_offsets[:, tf.newaxis],
+                    tf.zeros((batch_size, 2), tf.float32),
+                ],
+                axis=1,
+            )
+
+    def _resize(self, images, **kwargs):
+        resizing_layer = keras.layers.Resizing(
+            self.height, self.width, **kwargs
+        )
+        outputs = resizing_layer(images)
+        # smart_resize will always output float32, so we need to re-cast.
+        return tf.cast(outputs, self.compute_dtype)
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "height": self.height,
                 "width": self.width,
@@ -222,23 +274,20 @@
                 "aspect_ratio_factor": self.aspect_ratio_factor,
                 "interpolation": self.interpolation,
                 "seed": self.seed,
             }
         )
         return config
 
-    def _crop_and_resize(self, image, transformation, method=None):
-        image = tf.expand_dims(image, axis=0)
-        boxes = transformation
-
-        # See bit.ly/tf_crop_resize for more details
-        augmented_image = tf.image.crop_and_resize(
-            image,  # image shape: [B, H, W, C]
-            boxes,  # boxes: (1, 4) in this case; represents area
-            # to be cropped from the original image
-            [0],  # box_indices: maps boxes to images along batch axis
-            # [0] since there is only one image
-            (self.height, self.width),  # output size
-            method=method or self.interpolation,
-        )
-
-        return tf.squeeze(augmented_image, axis=0)
+    @classmethod
+    def from_config(cls, config):
+        if isinstance(config["zoom_factor"], dict):
+            config["zoom_factor"] = keras.utils.deserialize_keras_object(
+                config["zoom_factor"]
+            )
+        if isinstance(config["aspect_ratio_factor"], dict):
+            config[
+                "aspect_ratio_factor"
+            ] = keras.utils.deserialize_keras_object(
+                config["aspect_ratio_factor"]
+            )
+        return cls(**config)
```

## keras_cv/layers/preprocessing/randomly_zoomed_crop_test.py

```diff
@@ -1,23 +1,25 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_cv import core
 from keras_cv.layers import preprocessing
 
 
 class RandomlyZoomedCropTest(tf.test.TestCase, parameterized.TestCase):
     height, width = 300, 300
     batch_size = 4
     target_size = (224, 224)
@@ -55,30 +57,14 @@
         output = layer(image, training=True)
 
         input_image_resized = tf.image.resize(image, self.target_size)
 
         self.assertAllEqual(output.shape, (4, 224, 224, 1))
         self.assertNotAllClose(output, input_image_resized)
 
-    def test_preserves_image(self):
-        image_shape = (self.batch_size, self.height, self.width, 3)
-        image = tf.random.uniform(shape=image_shape)
-
-        layer = preprocessing.RandomlyZoomedCrop(
-            height=self.target_size[0],
-            width=self.target_size[1],
-            aspect_ratio_factor=(3 / 4, 4 / 3),
-            zoom_factor=(0.8, 1.0),
-        )
-
-        input_resized = tf.image.resize(image, self.target_size)
-        output = layer(image, training=False)
-
-        self.assertAllClose(output, input_resized)
-
     @parameterized.named_parameters(
         ("Not tuple or list", dict()),
         ("Length not equal to 2", [1, 2, 3]),
         ("Members not int", (2.3, 4.5)),
         ("Single float", 1.5),
     )
     def test_height_errors(self, height):
@@ -115,15 +101,16 @@
         ("Single integer", 5),
         ("Single float", 5.0),
     )
     def test_aspect_ratio_factor_errors(self, aspect_ratio_factor):
         with self.assertRaisesRegex(
             ValueError,
             "`aspect_ratio_factor` must be tuple of two positive floats or "
-            "keras_cv.core.FactorSampler instance. Received aspect_ratio_factor=(.*)",
+            "keras_cv.core.FactorSampler instance. Received "
+            "aspect_ratio_factor=(.*)",
         ):
             _ = preprocessing.RandomlyZoomedCrop(
                 height=self.target_size[0],
                 width=self.target_size[1],
                 aspect_ratio_factor=aspect_ratio_factor,
                 zoom_factor=(0.8, 1.0),
             )
@@ -141,7 +128,75 @@
         ):
             _ = preprocessing.RandomlyZoomedCrop(
                 height=self.target_size[0],
                 width=self.target_size[1],
                 aspect_ratio_factor=(3 / 4, 4 / 3),
                 zoom_factor=zoom_factor,
             )
+
+    def test_randomly_zoomed_crop_on_batched_images_independently(self):
+        image = tf.random.uniform(shape=(100, 100, 3))
+        input_images = tf.stack([image, image], axis=0)
+
+        layer = preprocessing.RandomlyZoomedCrop(
+            height=self.target_size[0],
+            width=self.target_size[1],
+            aspect_ratio_factor=(3 / 4, 4 / 3),
+            zoom_factor=(0.8, 1.0),
+            seed=self.seed,
+        )
+
+        results = layer(input_images)
+        self.assertNotAllClose(results[0], results[1])
+
+    def test_config_with_custom_name(self):
+        layer = preprocessing.RandomlyZoomedCrop(
+            height=self.target_size[0],
+            width=self.target_size[1],
+            aspect_ratio_factor=(3 / 4, 4 / 3),
+            zoom_factor=(0.8, 1.0),
+            name="image_preproc",
+        )
+        config = layer.get_config()
+        layer_1 = preprocessing.RandomlyZoomedCrop.from_config(config)
+        self.assertEqual(layer_1.name, layer.name)
+
+    def test_output_dtypes(self):
+        inputs = np.array([[[1], [2]], [[3], [4]]], dtype="float64")
+        layer = preprocessing.RandomlyZoomedCrop(
+            height=self.target_size[0],
+            width=self.target_size[1],
+            aspect_ratio_factor=(3 / 4, 4 / 3),
+            zoom_factor=(0.8, 1.0),
+        )
+        self.assertAllEqual(layer(inputs).dtype, "float32")
+        layer = preprocessing.RandomlyZoomedCrop(
+            height=self.target_size[0],
+            width=self.target_size[1],
+            aspect_ratio_factor=(3 / 4, 4 / 3),
+            zoom_factor=(0.8, 1.0),
+            dtype="uint8",
+        )
+        self.assertAllEqual(layer(inputs).dtype, "uint8")
+
+    def test_config(self):
+        layer = preprocessing.RandomlyZoomedCrop(
+            height=self.target_size[0],
+            width=self.target_size[1],
+            aspect_ratio_factor=(3 / 4, 4 / 3),
+            zoom_factor=(0.8, 1.0),
+        )
+        config = layer.get_config()
+        self.assertTrue(
+            isinstance(config["aspect_ratio_factor"], core.UniformFactorSampler)
+        )
+        self.assertTrue(
+            isinstance(config["zoom_factor"], core.UniformFactorSampler)
+        )
+        self.assertEqual(
+            config["aspect_ratio_factor"].get_config()["lower"], 3 / 4
+        )
+        self.assertEqual(
+            config["aspect_ratio_factor"].get_config()["upper"], 4 / 3
+        )
+        self.assertEqual(config["zoom_factor"].get_config()["lower"], 0.8)
+        self.assertEqual(config["zoom_factor"].get_config()["upper"], 1.0)
```

## keras_cv/layers/preprocessing/repeated_augmentation.py

```diff
@@ -7,37 +7,40 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RepeatedAugmentation(BaseImageAugmentationLayer):
     """RepeatedAugmentation augments each image in a batch multiple times.
 
-    This technique exists to emulate the behavior of stochastic gradient descent within
-    the context of mini-batch gradient descent.  When training large vision models,
-    choosing a large batch size can introduce too much noise into aggregated gradients
-    causing the overall batch's gradients to be less effective than gradients produced
-    using smaller gradients.  RepeatedAugmentation handles this by re-using the same
-    image multiple times within a batch creating correlated samples.
+    This technique exists to emulate the behavior of stochastic gradient descent
+    within the context of mini-batch gradient descent. When training large
+    vision models, choosing a large batch size can introduce too much noise into
+    aggregated gradients causing the overall batch's gradients to be less
+    effective than gradients produced using smaller gradients.
+    RepeatedAugmentation handles this by re-using the same image multiple times
+    within a batch creating correlated samples.
 
     This layer increases your batch size by a factor of `len(augmenters)`.
 
     Args:
         augmenters: the augmenters to use to augment the image
-        shuffle: whether or not to shuffle the result.  Essential when using an
+        shuffle: whether to shuffle the result. Essential when using an
             asynchronous distribution strategy such as ParameterServerStrategy.
 
     Usage:
 
     List of identical augmenters:
     ```python
     repeated_augment = cv_layers.RepeatedAugmentation(
@@ -63,51 +66,64 @@
         "images": tf.ones((8, 512, 512, 3)),
         "labels": tf.ones((8,)),
     }
     outputs = repeated_augment(inputs)
     ```
 
     References:
-    - [DEIT implementaton](https://github.com/facebookresearch/deit/blob/ee8893c8063f6937fec7096e47ba324c206e22b9/samplers.py#L8)
+    - [DEIT implementation](https://github.com/facebookresearch/deit/blob/ee8893c8063f6937fec7096e47ba324c206e22b9/samplers.py#L8)
     - [Original publication](https://openaccess.thecvf.com/content_CVPR_2020/papers/Hoffer_Augment_Your_Batch_Improving_Generalization_Through_Instance_Repetition_CVPR_2020_paper.pdf)
 
-    """
+    """  # noqa: E501
 
     def __init__(self, augmenters, shuffle=True, **kwargs):
         super().__init__(**kwargs)
         self.augmenters = augmenters
         self.shuffle = shuffle
 
     def _batch_augment(self, inputs):
         if "bounding_boxes" in inputs:
             raise ValueError(
-                "RepeatedAugmentation() does not yet support bounding box labels."
+                "RepeatedAugmentation() does not yet support bounding box "
+                "labels."
             )
 
         augmenter_outputs = [augmenter(inputs) for augmenter in self.augmenters]
 
         outputs = {}
         for k in inputs.keys():
-            outputs[k] = tf.concat([output[k] for output in augmenter_outputs], axis=0)
+            outputs[k] = tf.concat(
+                [output[k] for output in augmenter_outputs], axis=0
+            )
 
         if not self.shuffle:
             return outputs
         return self.shuffle_outputs(outputs)
 
     def shuffle_outputs(self, result):
-        indices = tf.range(start=0, limit=tf.shape(result["images"])[0], dtype=tf.int32)
+        indices = tf.range(
+            start=0, limit=tf.shape(result["images"])[0], dtype=tf.int32
+        )
         indices = tf.random.shuffle(indices)
         for key in result:
             result[key] = tf.gather(result[key], indices)
         return result
 
     def _augment(self, inputs):
         raise ValueError(
-            "RepeatedAugmentation() only works in batched mode.  If "
+            "RepeatedAugmentation() only works in batched mode. If "
             "you would like to create batches from a single image, use "
             "`x = tf.expand_dims(x, axis=0)` on your input images and labels."
         )
 
     def get_config(self):
         config = super().get_config()
         config.update({"augmenters": self.augmenters, "shuffle": self.shuffle})
         return config
+
+    @classmethod
+    def from_config(cls, config):
+        if config["augmenters"] and isinstance(config["augmenters"][0], dict):
+            config["augmenters"] = keras.utils.deserialize_keras_object(
+                config["augmenters"]
+            )
+        return cls(**config)
```

## keras_cv/layers/preprocessing/rescaling.py

```diff
@@ -9,43 +9,43 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 # In order to support both unbatched and batched inputs, the horizontal
 # and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class Rescaling(BaseImageAugmentationLayer):
     """A preprocessing layer which rescales input values to a new range.
 
     This layer rescales every value of an input (often an image) by multiplying
     by `scale` and adding `offset`.
 
     For instance:
 
     1. To rescale an input in the ``[0, 255]`` range
     to be in the `[0, 1]` range, you would pass `scale=1./255`.
 
     2. To rescale an input in the ``[0, 255]`` range to be in the `[-1, 1]`
     range, you would pass `scale=1./127.5, offset=-1`.
 
-    The rescaling is applied both during training and inference. Inputs can be
-    of integer or floating point dtype, and by default the layer will output
-    floats.
+    Inputs can be of integer or floating point dtype, and by default the layer
+    will output floats.
 
     Input shape:
       Arbitrary.
 
     Output shape:
       Same as input.
 
@@ -64,18 +64,22 @@
         scale = tf.cast(self.scale, dtype)
         offset = tf.cast(self.offset, dtype)
         return tf.cast(image, dtype) * scale + offset
 
     def augment_label(self, label, transformation, **kwargs):
         return label
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
+    def augment_segmentation_mask(
+        self, segmentation_mask, transformation, **kwargs
+    ):
         return segmentation_mask
 
-    def augment_bounding_boxes(self, bounding_boxes, transformation=None, **kwargs):
+    def augment_bounding_boxes(
+        self, bounding_boxes, transformation=None, **kwargs
+    ):
         return bounding_boxes
 
     def get_config(self):
         config = {
             "scale": self.scale,
             "offset": self.offset,
         }
```

## keras_cv/layers/preprocessing/resizing.py

```diff
@@ -9,14 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 import keras_cv.utils
 from keras_cv import bounding_box
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
@@ -27,44 +28,46 @@
 
 
 class Resizing(BaseImageAugmentationLayer):
     """A preprocessing layer which resizes images.
 
     This layer resizes an image input to a target height and width. The input
     should be a 4D (batched) or 3D (unbatched) tensor in `"channels_last"`
-    format.  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0,
+    format. Input pixel values can be of any range (e.g. `[0., 1.)` or `[0,
     255]`) and of integer or floating point dtype. By default, the layer will
     output floats.
 
     This layer can be called on tf.RaggedTensor batches of input images of
     distinct sizes, and will resize the outputs to dense tensors of uniform
     size.
 
     For an overview and full list of preprocessing layers, see the preprocessing
     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).
 
     Args:
         height: Integer, the height of the output shape.
         width: Integer, the width of the output shape.
-        interpolation: String, the interpolation method. Defaults to `"bilinear"`.
-            Supports `"bilinear"`, `"nearest"`, `"bicubic"`, `"area"`, `"lanczos3"`,
-            `"lanczos5"`, `"gaussian"`, `"mitchellcubic"`.
-        crop_to_aspect_ratio: If True, resize the images without aspect
-            ratio distortion. When the original aspect ratio differs from the target
-            aspect ratio, the output image will be cropped so as to return the
-            largest possible window in the image (of size `(height, width)`) that
-            matches the target aspect ratio. By default
+        interpolation: String, the interpolation method, defaults to
+            `"bilinear"`. Supports `"bilinear"`, `"nearest"`, `"bicubic"`,
+            `"area"`, `"lanczos3"`, `"lanczos5"`, `"gaussian"`,
+            `"mitchellcubic"`.
+        crop_to_aspect_ratio: If True, resize the images without aspect ratio
+            distortion. When the original aspect ratio differs from the target
+            aspect ratio, the output image will be cropped to return the largest
+            possible window in the image (of size `(height, width)`) that
+            matches the target aspect ratio. By default,
             (`crop_to_aspect_ratio=False`), aspect ratio may not be preserved.
-        pad_to_aspect_ratio: If True, resize the images without aspect
-            ratio distortion. When the original aspect ratio differs from the target
-            aspect ratio, the output image will be padded so as to return the
-            largest possible resize of the image (of size `(height, width)`) that
-            matches the target aspect ratio. By default
+        pad_to_aspect_ratio: If True, resize the images without aspect ratio
+            distortion. When the original aspect ratio differs from the target
+            aspect ratio, the output image will be padded to return the largest
+            possible resize of the image (of size `(height, width)`) that
+            matches the target aspect ratio. By default,
             (`pad_to_aspect_ratio=False`), aspect ratio may not be preserved.
-        bounding_box_format: The format of bounding boxes of input dataset. Refer to
+        bounding_box_format: The format of bounding boxes of input dataset.
+            Refer to
             https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box/converters.py
             for more details on supported bounding box formats.
     """
 
     def __init__(
         self,
         height,
@@ -76,28 +79,30 @@
         **kwargs,
     ):
         self.height = height
         self.width = width
         self.interpolation = interpolation
         self.crop_to_aspect_ratio = crop_to_aspect_ratio
         self.pad_to_aspect_ratio = pad_to_aspect_ratio
-        self._interpolation_method = keras_cv.utils.get_interpolation(interpolation)
+        self._interpolation_method = keras_cv.utils.get_interpolation(
+            interpolation
+        )
         self.bounding_box_format = bounding_box_format
         self.force_output_dense_images = True
 
         if pad_to_aspect_ratio and crop_to_aspect_ratio:
             raise ValueError(
                 "`Resizing()` expects at most one of `crop_to_aspect_ratio` or "
                 "`pad_to_aspect_ratio` to be True."
             )
 
         if not pad_to_aspect_ratio and bounding_box_format:
             raise ValueError(
                 "Resizing() only supports bounding boxes when in "
-                "`pad_to_aspect_ratio=True` mode.  "
+                "`pad_to_aspect_ratio=True` mode. "
                 "Please pass `pad_to_aspect_ratio=True`"
                 "when processing bounding boxes with `Resizing()`"
             )
         super().__init__(**kwargs)
 
     def compute_image_signature(self, images):
         return tf.TensorSpec(
@@ -114,15 +119,17 @@
             inputs["images"] = images
 
         if bounding_boxes is not None:
             bounding_boxes = bounding_boxes.copy()
             bounding_boxes["classes"] = tf.expand_dims(
                 bounding_boxes["classes"], axis=0
             )
-            bounding_boxes["boxes"] = tf.expand_dims(bounding_boxes["boxes"], axis=0)
+            bounding_boxes["boxes"] = tf.expand_dims(
+                bounding_boxes["boxes"], axis=0
+            )
             inputs["bounding_boxes"] = bounding_boxes
 
         outputs = self._batch_augment(inputs)
 
         if images is not None:
             images = tf.squeeze(outputs["images"], axis=0)
             inputs["images"] = images
@@ -138,15 +145,17 @@
 
         return inputs
 
     def _resize_with_distortion(self, inputs):
         images = inputs.get("images", None)
 
         size = [self.height, self.width]
-        images = tf.image.resize(images, size=size, method=self._interpolation_method)
+        images = tf.image.resize(
+            images, size=size, method=self._interpolation_method
+        )
         images = tf.cast(images, self.compute_dtype)
 
         inputs["images"] = images
         return inputs
 
     def _resize_with_pad(self, inputs):
         def resize_single_with_pad_to_aspect(x):
@@ -185,15 +194,17 @@
             if bounding_boxes is not None:
                 bounding_boxes = keras_cv.bounding_box.convert_format(
                     bounding_boxes,
                     images=image,
                     source="rel_xyxy",
                     target="xyxy",
                 )
-            image = tf.image.pad_to_bounding_box(image, 0, 0, self.height, self.width)
+            image = tf.image.pad_to_bounding_box(
+                image, 0, 0, self.height, self.width
+            )
             if bounding_boxes is not None:
                 bounding_boxes = keras_cv.bounding_box.clip_to_image(
                     bounding_boxes, images=image, bounding_box_format="xyxy"
                 )
                 bounding_boxes = keras_cv.bounding_box.convert_format(
                     bounding_boxes,
                     images=image,
@@ -226,32 +237,32 @@
 
     def _resize_with_crop(self, inputs):
         images = inputs.get("images", None)
         bounding_boxes = inputs.get("bounding_boxes", None)
         if bounding_boxes is not None:
             raise ValueError(
                 "Resizing(crop_to_aspect_ratio=True) does not support "
-                "bounding box inputs.  Please use `pad_to_aspect_ratio=True` when "
-                "processing bounding boxes with Resizing()."
+                "bounding box inputs. Please use `pad_to_aspect_ratio=True` "
+                "when processing bounding boxes with Resizing()."
             )
         inputs["images"] = images
         size = [self.height, self.width]
 
         # tf.image.resize will always output float32 and operate more
         # efficiently on float32 unless interpolation is nearest, in which case
-        # ouput type matches input type.
+        # output type matches input type.
         if self.interpolation == "nearest":
             input_dtype = self.compute_dtype
         else:
             input_dtype = tf.float32
 
         def resize_with_crop_to_aspect(x):
             if isinstance(x, tf.RaggedTensor):
                 x = x.to_tensor()
-            return tf.keras.preprocessing.image.smart_resize(
+            return keras.preprocessing.image.smart_resize(
                 x, size=size, interpolation=self._interpolation_method
             )
 
         if isinstance(images, tf.RaggedTensor):
             size_as_shape = tf.TensorShape(size)
             shape = size_as_shape + images.shape[-1:]
             spec = tf.TensorSpec(shape, input_dtype)
@@ -260,30 +271,14 @@
             )
         else:
             images = resize_with_crop_to_aspect(images)
 
         inputs["images"] = images
         return inputs
 
-    def call(self, inputs, training=True):
-        inputs = self._ensure_inputs_are_compute_dtype(inputs)
-        inputs, metadata = self._format_inputs(inputs)
-        self._check_inputs(inputs)
-        images = inputs["images"]
-        if images.shape.rank == 3:
-            return self._format_output(self._augment(inputs), metadata)
-        elif images.shape.rank == 4:
-            return self._format_output(self._batch_augment(inputs), metadata)
-        else:
-            raise ValueError(
-                "Image augmentation layers are expecting inputs to be "
-                "rank 3 (HWC) or 4D (NHWC) tensors. Got shape: "
-                f"{images.shape}"
-            )
-
     def _check_inputs(self, inputs):
         for key in inputs:
             if key not in supported_keys:
                 raise ValueError(
                     "Resizing() currently only supports keys "
                     f"[{', '.join(supported_keys)}]. "
                     f"Key `{key}` found in inputs to `Resizing()`. "
@@ -291,16 +286,17 @@
 
     def _batch_augment(self, inputs):
         if (
             inputs.get("bounding_boxes", None) is not None
             and self.bounding_box_format is None
         ):
             raise ValueError(
-                "Resizing requires `bounding_box_format` to be set "
-                "when augmenting bounding boxes, but `self.bounding_box_format=None`."
+                "Resizing requires `bounding_box_format` to be set when "
+                "augmenting bounding boxes, but "
+                "`self.bounding_box_format=None`."
             )
 
         if self.crop_to_aspect_ratio:
             return self._resize_with_crop(inputs)
         if self.pad_to_aspect_ratio:
             return self._resize_with_pad(inputs)
         return self._resize_with_distortion(inputs)
```

## keras_cv/layers/preprocessing/resizing_test.py

```diff
@@ -65,27 +65,35 @@
         ),
     )
     def test_up_sampling(self, kwargs, expected_height, expected_width):
         self._run_output_shape_test(kwargs, expected_height, expected_width)
 
     def test_down_sampling_numeric(self):
         for dtype in (np.int64, np.float32):
-            input_image = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype(dtype)
-            layer = cv_layers.Resizing(height=2, width=2, interpolation="nearest")
+            input_image = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype(
+                dtype
+            )
+            layer = cv_layers.Resizing(
+                height=2, width=2, interpolation="nearest"
+            )
             output_image = layer(input_image)
             # pyformat: disable
             expected_output = np.asarray([[5, 7], [13, 15]]).astype(dtype)
             # pyformat: enable
             expected_output = np.reshape(expected_output, (1, 2, 2, 1))
             self.assertAllEqual(expected_output, output_image)
 
     def test_up_sampling_numeric(self):
         for dtype in (np.int64, np.float32):
-            input_image = np.reshape(np.arange(0, 4), (1, 2, 2, 1)).astype(dtype)
-            layer = cv_layers.Resizing(height=4, width=4, interpolation="nearest")
+            input_image = np.reshape(np.arange(0, 4), (1, 2, 2, 1)).astype(
+                dtype
+            )
+            layer = cv_layers.Resizing(
+                height=4, width=4, interpolation="nearest"
+            )
             output_image = layer(input_image)
             # pyformat: disable
             expected_output = np.asarray(
                 [[0, 0, 1, 1], [0, 0, 1, 1], [2, 2, 3, 3], [2, 2, 3, 3]]
             ).astype(dtype)
             # pyformat: enable
             expected_output = np.reshape(expected_output, (1, 4, 4, 1))
@@ -104,15 +112,17 @@
     def test_config_with_custom_name(self):
         layer = cv_layers.Resizing(5, 5, name="image_preproc")
         config = layer.get_config()
         layer_1 = cv_layers.Resizing.from_config(config)
         self.assertEqual(layer_1.name, layer.name)
 
     def test_crop_to_aspect_ratio(self):
-        input_image = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype("float32")
+        input_image = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype(
+            "float32"
+        )
         layer = cv_layers.Resizing(4, 2, crop_to_aspect_ratio=True)
         output_image = layer(input_image)
         expected_output = np.asarray(
             [
                 [1, 2],
                 [5, 6],
                 [9, 10],
@@ -162,23 +172,14 @@
             [[[1.0], [1.0]], [[1.0], [1.0]]],
             [[[1.0], [1.0]], [[1.0], [1.0]]],
         ]
         self.assertIsInstance(outputs, tf.Tensor)
         self.assertNotIsInstance(outputs, tf.RaggedTensor)
         self.assertAllEqual(expected_output, outputs)
 
-    def test_raises_with_segmap(self):
-        inputs = {
-            "images": np.array([[[1], [2]], [[3], [4]]], dtype="float64"),
-            "segmentation_map": np.array([[[1], [2]], [[3], [4]]], dtype="float64"),
-        }
-        layer = cv_layers.Resizing(2, 2)
-        with self.assertRaises(ValueError):
-            layer(inputs)
-
     def test_output_dtypes(self):
         inputs = np.array([[[1], [2]], [[3], [4]]], dtype="float64")
         layer = cv_layers.Resizing(2, 2)
         self.assertAllEqual(layer(inputs).dtype, "float32")
         layer = cv_layers.Resizing(2, 2, dtype="uint8")
         self.assertAllEqual(layer(inputs).dtype, "uint8")
 
@@ -298,8 +299,10 @@
         outputs = layer(inputs)
         self.assertListEqual(
             [4, 16, 16, 3],
             outputs["images"].shape.as_list(),
         )
 
         self.assertAllEqual(outputs["images"][1][:, :8, :], tf.ones((16, 8, 3)))
-        self.assertAllEqual(outputs["images"][1][:, -8:, :], tf.zeros((16, 8, 3)))
+        self.assertAllEqual(
+            outputs["images"][1][:, -8:, :], tf.zeros((16, 8, 3))
+        )
```

## keras_cv/layers/preprocessing/solarization.py

```diff
@@ -7,61 +7,64 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 
-from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
-    BaseImageAugmentationLayer,
+from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
+    VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class Solarization(BaseImageAugmentationLayer):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class Solarization(VectorizedBaseImageAugmentationLayer):
     """Applies (max_value - pixel + min_value) for each pixel in the image.
 
-    When created without `threshold` parameter, the layer performs solarization to
-    all values. When created with specified `threshold` the layer only augments
-    pixels that are above the `threshold` value
+    When created without `threshold` parameter, the layer performs solarization
+    to all values. When created with specified `threshold` the layer only
+    augments pixels that are above the `threshold` value
 
     Reference:
     - [AutoAugment: Learning Augmentation Policies from Data](
         https://arxiv.org/abs/1805.09501
     )
     - [RandAugment](https://arxiv.org/pdf/1909.13719.pdf)
 
     Args:
-        value_range: a tuple or a list of two elements. The first value represents
-            the lower bound for values in passed images, the second represents the
-            upper bound. Images passed to the layer should have values within
-            `value_range`.
+        value_range: a tuple or a list of two elements. The first value
+            represents the lower bound for values in passed images, the second
+            represents the upper bound. Images passed to the layer should have
+            values within `value_range`.
         addition_factor: (Optional)  A tuple of two floats, a single float or a
-            `keras_cv.FactorSampler`. For each augmented image a value is sampled
-            from the provided range. If a float is passed, the range is interpreted as
-            `(0, addition_factor)`. If specified, this value is added to each pixel
-            before solarization and thresholding.  The addition value should be scaled
-            according to the value range (0, 255). Defaults to 0.0.
-        threshold_factor: (Optional)  A tuple of two floats, a single float or a
-            `keras_cv.FactorSampler`. For each augmented image a value is sampled
-            from the provided range. If a float is passed, the range is interpreted as
-            `(0, threshold_factor)`. If specified, only pixel values above this
-            threshold will be solarized.
+            `keras_cv.FactorSampler`. For each augmented image a value is
+            sampled from the provided range. If a float is passed, the range is
+            interpreted as `(0, addition_factor)`. If specified, this value is
+            added to each pixel before solarization and thresholding. The
+            addition value should be scaled according to the value range
+            (0, 255), defaults to 0.0.
+        threshold_factor: (Optional)  A tuple of two floats, a single float or
+            a `keras_cv.FactorSampler`. For each augmented image a value is
+            sampled from the provided range. If a float is passed, the range is
+            interpreted as `(0, threshold_factor)`. If specified, only pixel
+            values above this threshold will be solarized.
         seed: Integer. Used to create a random seed.
 
     Usage:
     ```python
-    (images, labels), _ = tf.keras.datasets.cifar10.load_data()
+    (images, labels), _ = keras.datasets.cifar10.load_data()
     print(images[0, 0, 0])
     # [59 62 63]
     # Note that images are Tensor with values in the range [0, 255]
-    solarization = Solarization()
+    solarization = Solarization(value_range=(0, 255))
     images = solarization(images)
     print(images[0, 0, 0])
     # [196, 193, 192]
     ```
 
     Call arguments:
         images: Tensor of type int or float, with pixels in
@@ -76,57 +79,88 @@
         threshold_factor=0.0,
         seed=None,
         **kwargs
     ):
         super().__init__(seed=seed, **kwargs)
         self.seed = seed
         self.addition_factor = preprocessing.parse_factor(
-            addition_factor, max_value=255, seed=seed, param_name="addition_factor"
+            addition_factor,
+            max_value=255,
+            seed=seed,
+            param_name="addition_factor",
         )
         self.threshold_factor = preprocessing.parse_factor(
-            threshold_factor, max_value=255, seed=seed, param_name="threshold_factor"
+            threshold_factor,
+            max_value=255,
+            seed=seed,
+            param_name="threshold_factor",
         )
         self.value_range = value_range
 
-    def get_random_transformation(self, **kwargs):
-        return (
-            self.addition_factor(dtype=self.compute_dtype),
-            self.threshold_factor(dtype=self.compute_dtype),
-        )
+    def get_random_transformation_batch(self, batch_size, **kwargs):
+        return {
+            "additions": self.addition_factor(
+                shape=(batch_size, 1, 1, 1), dtype=self.compute_dtype
+            ),
+            "thresholds": self.threshold_factor(
+                shape=(batch_size, 1, 1, 1), dtype=self.compute_dtype
+            ),
+        }
+
+    def augment_ragged_image(self, image, transformation, **kwargs):
+        return self.augment_images(image, transformation)
 
-    def augment_image(self, image, transformation=None, **kwargs):
-        (addition, threshold) = transformation
-        image = preprocessing.transform_value_range(
-            image,
+    def augment_images(self, images, transformations, **kwargs):
+        thresholds = transformations["thresholds"]
+        additions = transformations["additions"]
+        images = preprocessing.transform_value_range(
+            images,
             original_range=self.value_range,
             target_range=(0, 255),
             dtype=self.compute_dtype,
         )
-        result = image + addition
-        result = tf.clip_by_value(result, 0, 255)
-        result = tf.where(result < threshold, result, 255 - result)
-        result = preprocessing.transform_value_range(
-            result,
+        results = images + additions
+        results = tf.clip_by_value(results, 0, 255)
+        results = tf.where(results < thresholds, results, 255 - results)
+        results = preprocessing.transform_value_range(
+            results,
             original_range=(0, 255),
             target_range=self.value_range,
             dtype=self.compute_dtype,
         )
-        return result
+        return results
 
-    def augment_bounding_boxes(self, bounding_boxes, **kwargs):
+    def augment_bounding_boxes(self, bounding_boxes, transformations, **kwargs):
         return bounding_boxes
 
-    def augment_label(self, label, transformation=None, **kwargs):
-        return label
+    def augment_labels(self, labels, transformations, **kwargs):
+        return labels
 
-    def augment_segmentation_mask(self, segmentation_mask, transformation, **kwargs):
-        return segmentation_mask
+    def augment_keypoints(self, keypoints, transformations, **kwargs):
+        return keypoints
+
+    def augment_segmentation_masks(
+        self, segmentation_masks, transformations, **kwargs
+    ):
+        return segmentation_masks
 
     def get_config(self):
         config = {
             "threshold_factor": self.threshold_factor,
             "addition_factor": self.addition_factor,
             "value_range": self.value_range,
             "seed": self.seed,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        if isinstance(config["threshold_factor"], dict):
+            config["threshold_factor"] = keras.utils.deserialize_keras_object(
+                config["threshold_factor"]
+            )
+        if isinstance(config["addition_factor"], dict):
+            config["addition_factor"] = keras.utils.deserialize_keras_object(
+                config["addition_factor"]
+            )
+        return cls(**config)
```

## keras_cv/layers/preprocessing/solarization_test.py

```diff
@@ -36,31 +36,37 @@
         )
 
     @parameterized.named_parameters(
         ("0_245", 0, 245),
         ("255_0", 255, 0),
     )
     def test_solarization_with_addition(self, input_value, output_value):
-        solarization = Solarization(addition_factor=(10.0, 10.0), value_range=(0, 255))
+        solarization = Solarization(
+            addition_factor=(10.0, 10.0), value_range=(0, 255)
+        )
         self._test_input_output(
             layer=solarization,
             input_value=input_value,
             expected_value=output_value,
             dtype=tf.float32,
         )
 
     @parameterized.named_parameters(
         ("0_0", 0, 0),
         ("64_64", 64, 64),
         ("127_127", 127, 127),
         ("191_64", 191, 64),
         ("255_0", 255, 0),
     )
-    def test_only_values_above_threshold_are_solarized(self, input_value, output_value):
-        solarization = Solarization(threshold_factor=(128, 128), value_range=(0, 255))
+    def test_only_values_above_threshold_are_solarized(
+        self, input_value, output_value
+    ):
+        solarization = Solarization(
+            threshold_factor=(128, 128), value_range=(0, 255)
+        )
 
         self._test_input_output(
             layer=solarization,
             input_value=input_value,
             expected_value=output_value,
             dtype=tf.uint8,
         )
@@ -75,7 +81,18 @@
             0,
             255,
         )
 
         output = layer(input)
 
         self.assertAllClose(output, expected_output)
+
+    def test_random_augmentation_applied_per_sample(self):
+        image = tf.random.uniform((16, 16, 3), minval=0, maxval=255)
+        images = tf.stack([image, image])
+        layer = Solarization(
+            value_range=(0, 255), threshold_factor=127, addition_factor=127
+        )
+
+        outputs = layer(images)
+
+        self.assertNotAllEqual(outputs[0], outputs[1])
```

## keras_cv/layers/preprocessing/with_labels_test.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -83,28 +83,39 @@
             "brightness_factor": (-0.2, 0.5),
             "contrast_factor": (0.5, 0.9),
             "saturation_factor": (0.5, 0.9),
             "hue_factor": (0.5, 0.9),
             "seed": 1,
         },
     ),
-    ("RandomContrast", layers.RandomContrast, {"factor": 0.5}),
+    (
+        "RandomContrast",
+        layers.RandomContrast,
+        {"value_range": (0, 255), "factor": 0.5},
+    ),
+    ("RandomFlip", layers.RandomFlip, {"mode": "horizontal"}),
     (
         "RandomGaussianBlur",
         layers.RandomGaussianBlur,
         {"kernel_size": 3, "factor": (0.0, 3.0)},
     ),
     ("RandomJpegQuality", layers.RandomJpegQuality, {"factor": (75, 100)}),
+    ("RandomRotation", layers.RandomRotation, {"factor": 0.5}),
     ("RandomSaturation", layers.RandomSaturation, {"factor": 0.5}),
     (
         "RandomSharpness",
         layers.RandomSharpness,
         {"factor": 0.5, "value_range": (0, 255)},
     ),
     ("RandomShear", layers.RandomShear, {"x_factor": 0.3, "x_factor": 0.3}),
+    (
+        "RandomTranslation",
+        layers.RandomTranslation,
+        {"height_factor": 0.5, "width_factor": 0.5},
+    ),
     ("Solarization", layers.Solarization, {"value_range": (0, 255)}),
     (
         "RandomZoom",
         layers.RandomZoom,
         {"height_factor": 0.2, "width_factor": 0.5},
     ),
     (
```

## keras_cv/layers/preprocessing/with_mixed_precision_test.py

```diff
@@ -1,22 +1,24 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
 from absl.testing import parameterized
+from tensorflow import keras
 
 from keras_cv import layers
 
 TEST_CONFIGURATIONS = [
     ("AutoContrast", layers.AutoContrast, {"value_range": (0, 255)}),
     ("ChannelShuffle", layers.ChannelShuffle, {}),
     ("Equalization", layers.Equalization, {"value_range": (0, 255)}),
@@ -53,14 +55,15 @@
         {"factor": 0.5},
     ),
     (
         "RandomCutout",
         layers.RandomCutout,
         {"height_factor": 0.2, "width_factor": 0.2},
     ),
+    ("RandomFlip", layers.RandomFlip, {"mode": "horizontal"}),
     (
         "RandomHue",
         layers.RandomHue,
         {"factor": 0.5, "value_range": (0, 255)},
     ),
     (
         "RandomTranslation",
@@ -81,19 +84,25 @@
             "contrast_factor": (0.5, 0.9),
             "saturation_factor": (0.5, 0.9),
             "hue_factor": (0.5, 0.9),
             "seed": 1,
         },
     ),
     (
+        "RandomContrast",
+        layers.RandomContrast,
+        {"value_range": (0, 255), "factor": 0.5},
+    ),
+    (
         "RandomGaussianBlur",
         layers.RandomGaussianBlur,
         {"kernel_size": 3, "factor": (0.0, 3.0)},
     ),
     ("RandomJpegQuality", layers.RandomJpegQuality, {"factor": (75, 100)}),
+    ("RandomRotation", layers.RandomRotation, {"factor": 0.5}),
     ("RandomSaturation", layers.RandomSaturation, {"factor": 0.5}),
     (
         "RandomSharpness",
         layers.RandomSharpness,
         {"factor": 0.5, "value_range": (0, 255)},
     ),
     ("RandomAspectRatio", layers.RandomAspectRatio, {"factor": (0.9, 1.1)}),
@@ -139,41 +148,40 @@
     ),
 ]
 
 NO_CPU_FP16_KERNEL_LAYERS = [
     layers.RandomSaturation,
     layers.RandomColorJitter,
     layers.RandomHue,
-    layers.RandomContrast,
 ]
 
 
 class WithMixedPrecisionTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(*TEST_CONFIGURATIONS)
     def test_can_run_in_mixed_precision(self, layer_cls, init_args):
         if not tf.config.list_physical_devices("GPU"):
             if layer_cls in NO_CPU_FP16_KERNEL_LAYERS:
                 self.skipTest(
-                    "There is currently no float16 CPU kernel registered for operations"
-                    " `tf.image.adjust_saturation`, and `tf.image.adjust_hue`. "
-                    "Skipping."
+                    "There is currently no float16 CPU kernel registered for "
+                    "operations `tf.image.adjust_saturation`, and "
+                    "`tf.image.adjust_hue`. Skipping."
                 )
 
-        tf.keras.mixed_precision.set_global_policy("mixed_float16")
+        keras.mixed_precision.set_global_policy("mixed_float16")
 
         img = tf.random.uniform(
             shape=(3, 512, 512, 3), minval=0, maxval=255, dtype=tf.float32
         )
         labels = tf.ones((3,), dtype=tf.float32)
         inputs = {"images": img, "labels": labels}
 
         layer = layer_cls(**init_args)
         layer(inputs)
 
     @classmethod
     def tearDownClass(cls) -> None:
         # Do not affect other tests
-        tf.keras.mixed_precision.set_global_policy("float32")
+        keras.mixed_precision.set_global_policy("float32")
 
 
 if __name__ == "__main__":
     tf.test.main()
```

## keras_cv/layers/preprocessing/with_segmentation_masks_test.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -51,67 +51,86 @@
             "brightness_factor": (-0.2, 0.5),
             "contrast_factor": (0.5, 0.9),
             "saturation_factor": (0.5, 0.9),
             "hue_factor": (0.5, 0.9),
             "seed": 1,
         },
     ),
-    ("RandomContrast", preprocessing.RandomContrast, {"factor": 0.5}),
+    (
+        "RandomContrast",
+        preprocessing.RandomContrast,
+        {"value_range": (0, 255), "factor": 0.5},
+    ),
     (
         "RandomGaussianBlur",
         preprocessing.RandomGaussianBlur,
         {"kernel_size": 3, "factor": (0.0, 3.0)},
     ),
-    ("RandomJpegQuality", preprocessing.RandomJpegQuality, {"factor": (75, 100)}),
+    (
+        "RandomJpegQuality",
+        preprocessing.RandomJpegQuality,
+        {"factor": (75, 100)},
+    ),
+    (
+        "RandomRotation",
+        preprocessing.RandomRotation,
+        {"factor": 0.0, "segmentation_classes": 10},
+    ),
     ("RandomSaturation", preprocessing.RandomSaturation, {"factor": 0.5}),
     (
         "RandomSharpness",
         preprocessing.RandomSharpness,
         {"factor": 0.5, "value_range": (0, 255)},
     ),
     ("Solarization", preprocessing.Solarization, {"value_range": (0, 255)}),
 ]
 
 
 class WithSegmentationMasksTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(*TEST_CONFIGURATIONS)
     def test_can_run_with_segmentation_masks(self, layer_cls, init_args):
-        classes = 10
+        num_classes = 10
         layer = layer_cls(**init_args)
 
         img = tf.random.uniform(
             shape=(3, 512, 512, 3), minval=0, maxval=1, dtype=tf.float32
         )
         segmentation_masks = tf.random.uniform(
-            shape=(3, 512, 512, 1), minval=0, maxval=classes, dtype=tf.int32
+            shape=(3, 512, 512, 1), minval=0, maxval=num_classes, dtype=tf.int32
         )
 
         inputs = {"images": img, "segmentation_masks": segmentation_masks}
         outputs = layer(inputs)
 
         self.assertIn("segmentation_masks", outputs)
         # This currently asserts that all layers are no-ops.
         # When preprocessing layers are updated to mutate segmentation masks,
         # this condition should only be asserted for no-op layers.
-        self.assertAllClose(inputs["segmentation_masks"], outputs["segmentation_masks"])
+        self.assertAllClose(
+            inputs["segmentation_masks"], outputs["segmentation_masks"]
+        )
 
     # This has to be a separate test case to exclude CutMix and MixUp
     # (which are not yet supported for segmentation mask augmentation)
     @parameterized.named_parameters(*TEST_CONFIGURATIONS)
-    def test_can_run_with_segmentation_mask_single_image(self, layer_cls, init_args):
-        classes = 10
+    def test_can_run_with_segmentation_mask_single_image(
+        self, layer_cls, init_args
+    ):
+        num_classes = 10
         layer = layer_cls(**init_args)
         img = tf.random.uniform(
             shape=(512, 512, 3), minval=0, maxval=1, dtype=tf.float32
         )
         segmentation_mask = tf.random.uniform(
-            shape=(512, 512, 1), minval=0, maxval=classes, dtype=tf.int32
+            shape=(512, 512, 1), minval=0, maxval=num_classes, dtype=tf.int32
         )
 
         inputs = {"images": img, "segmentation_masks": segmentation_mask}
         outputs = layer(inputs)
 
         self.assertIn("segmentation_masks", outputs)
         # This currently asserts that all layers are no-ops.
         # When preprocessing layers are updated to mutate segmentation masks,
         # this condition should only be asserted for no-op layers.
-        self.assertAllClose(inputs["segmentation_masks"], outputs["segmentation_masks"])
+        self.assertAllClose(
+            inputs["segmentation_masks"], outputs["segmentation_masks"]
+        )
```

## keras_cv/layers/preprocessing_3d/__init__.py

```diff
@@ -14,23 +14,27 @@
 
 from keras_cv.layers.preprocessing_3d.base_augmentation_layer_3d import (
     BaseAugmentationLayer3D,
 )
 from keras_cv.layers.preprocessing_3d.frustum_random_dropping_points import (
     FrustumRandomDroppingPoints,
 )
-from keras_cv.layers.preprocessing_3d.frustum_random_point_feature_noise import (
+from keras_cv.layers.preprocessing_3d.frustum_random_point_feature_noise import (  # noqa: E501
     FrustumRandomPointFeatureNoise,
 )
 from keras_cv.layers.preprocessing_3d.global_random_dropping_points import (
     GlobalRandomDroppingPoints,
 )
 from keras_cv.layers.preprocessing_3d.global_random_flip import GlobalRandomFlip
-from keras_cv.layers.preprocessing_3d.global_random_rotation import GlobalRandomRotation
-from keras_cv.layers.preprocessing_3d.global_random_scaling import GlobalRandomScaling
+from keras_cv.layers.preprocessing_3d.global_random_rotation import (
+    GlobalRandomRotation,
+)
+from keras_cv.layers.preprocessing_3d.global_random_scaling import (
+    GlobalRandomScaling,
+)
 from keras_cv.layers.preprocessing_3d.global_random_translation import (
     GlobalRandomTranslation,
 )
 from keras_cv.layers.preprocessing_3d.group_points_by_bounding_boxes import (
     GroupPointsByBoundingBoxes,
 )
 from keras_cv.layers.preprocessing_3d.random_copy_paste import RandomCopyPaste
```

## keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py

```diff
@@ -9,65 +9,66 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 POINT_CLOUDS = "point_clouds"
 BOUNDING_BOXES = "bounding_boxes"
 OBJECT_POINT_CLOUDS = "object_point_clouds"
 OBJECT_BOUNDING_BOXES = "object_bounding_boxes"
 ADDITIONAL_POINT_CLOUDS = "additional_point_clouds"
 ADDITIONAL_BOUNDING_BOXES = "additional_bounding_boxes"
 BOX_LABEL_INDEX = 7
 POINTCLOUD_LABEL_INDEX = 3
 POINTCLOUD_FEATURE_INDEX = 4
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class BaseAugmentationLayer3D(tf.keras.__internal__.layers.BaseRandomLayer):
-    """Abstract base layer for data augmentaion for 3D preception.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class BaseAugmentationLayer3D(keras.__internal__.layers.BaseRandomLayer):
+    """Abstract base layer for data augmentation for 3D perception.
 
     This layer contains base functionalities for preprocessing layers which
-    augment 3D preception related data, eg. point_clouds and in future, images.
-    The subclasses could avoid making certain mistakes and reduce code
+    augment 3D perception related data, e.g. point_clouds and in the future,
+    images. The subclasses could avoid making certain mistakes and reduce code
     duplications.
 
     This layer requires you to implement one method: `augment_point_clouds()`,
-    which augments one or a sequence of point clouds during the training. There are a few
-    additional methods that you can implement for added functionality on the
-    layer:
+    which augments one or a sequence of point clouds during the training. There
+    are a few additional methods that you can implement for added functionality
+    on the layer:
 
     `augment_bounding_boxes()`, which handles the bounding box augmentation, if
     the layer supports that.
 
     `get_random_transformation()`, which should produce a random transformation
-    setting. The tranformation object, which could be any type, will be passed
-    to `augment_point_clouds` and `augment_bounding_boxes`, to
-    coodinate the randomness behavior, eg, in the RotateZ layer, the point_clouds
-    and bounding_boxes should be changed in the same way.
+    setting. The transformation object, which could be any type, will be passed
+    to `augment_point_clouds` and `augment_bounding_boxes`, to coordinate the
+    randomness behavior, eg, in the RotateZ layer, the point_clouds and
+    bounding_boxes should be changed in the same way.
 
     The `call()` method support two formats of inputs:
     1. A dict of tensors with stable keys. The supported keys are:
       `"point_clouds"` and `"bounding_boxes"` at the moment. We might add
       more keys in future when we support more types of augmentation.
 
     The output of the `call()` will be in two formats, which will be the same
     structure as the inputs.
 
     The `call()` will handle the logic detecting the training/inference mode,
     unpack the inputs, forward to the correct function, and pack the output back
     to the same structure as the inputs.
 
-    By default the `call()` method leverages the `tf.vectorized_map()` function.
-    Auto-vectorization can be disabled by setting `self.auto_vectorize = False`
-    in your `__init__()` method.  When disabled, `call()` instead relies
-    on `tf.map_fn()`. For example:
+    By default, the `call()` method leverages the `tf.vectorized_map()`
+    function. Auto-vectorization can be disabled by setting
+    `self.auto_vectorize = False` in your `__init__()` method. When disabled,
+    `call()` instead relies on `tf.map_fn()`. For example:
 
     ```python
     class SubclassLayer(keras_cv.BaseImageAugmentationLayer):
       def __init__(self):
         super().__init__()
         self.auto_vectorize = False
     ```
@@ -85,31 +86,31 @@
         pose = transformation['pos']
         # Rotate points.
         pointcloud_xyz = geometry.CoordinateTransform(pointcloud[..., :3], pose)
         pointcloud = tf.concat([pointcloud_xyz, pointcloud[..., 3:]], axis=-1)
         return pointcloud, boxes
     ```
 
-    Note that since the randomness is also a common functionnality, this layer
-    also includes a tf.keras.backend.RandomGenerator, which can be used to
-    produce the random numbers.  The random number generator is stored in the
+    Note that since the randomness is also a common functionality, this layer
+    also includes a keras.backend.RandomGenerator, which can be used to
+    produce the random numbers. The random number generator is stored in the
     `self._random_generator` attribute.
     """
 
     def __init__(self, seed=None, **kwargs):
         super().__init__(seed=seed, **kwargs)
         self.auto_vectorize = False
 
     @property
     def auto_vectorize(self):
         """Control whether automatic vectorization occurs.
 
-        By default the `call()` method leverages the `tf.vectorized_map()`
-        function.  Auto-vectorization can be disabled by setting
-        `self.auto_vectorize = False` in your `__init__()` method.  When
+        By default, the `call()` method leverages the `tf.vectorized_map()`
+        function. Auto-vectorization can be disabled by setting
+        `self.auto_vectorize = False` in your `__init__()` method. When
         disabled, `call()` instead relies on `tf.map_fn()`. For example:
 
         ```python
         class SubclassLayer(BaseImageAugmentationLayer):
           def __init__(self):
             super().__init__()
             self.auto_vectorize = False
@@ -151,39 +152,38 @@
         """Produce random transformation config for one single input.
 
         This is used to produce same randomness between
         image/label/bounding_box.
 
         Args:
           point_clouds: 3D point clouds tensor from inputs.
-          bounding_box: 3D bounding boxes tensor from inputs.
+          bounding_boxes: 3D bounding boxes tensor from inputs.
 
         Returns:
           Any type of object, which will be forwarded to `augment_point_clouds`,
           and `augment_bounding_box` as the `transformation` parameter.
         """
         return None
 
-    def call(self, inputs, training=True):
-        if training:
-            point_clouds = inputs[POINT_CLOUDS]
-            bounding_boxes = inputs[BOUNDING_BOXES]
-            if point_clouds.shape.rank == 3 and bounding_boxes.shape.rank == 3:
-                return self._augment(inputs)
-            elif point_clouds.shape.rank == 4 and bounding_boxes.shape.rank == 4:
-                return self._batch_augment(inputs)
-            else:
-                raise ValueError(
-                    "Point clouds augmentation layers are expecting inputs point clouds and bounding boxes to "
-                    "be rank 3D (Frame, Point, Feature) or 4D (Batch, Frame, Point, Feature) tensors. Got shape: {} and {}".format(
-                        point_clouds.shape, bounding_boxes.shape
-                    )
-                )
+    def call(self, inputs):
+        point_clouds = inputs[POINT_CLOUDS]
+        bounding_boxes = inputs[BOUNDING_BOXES]
+        if point_clouds.shape.rank == 3 and bounding_boxes.shape.rank == 3:
+            return self._augment(inputs)
+        elif point_clouds.shape.rank == 4 and bounding_boxes.shape.rank == 4:
+            return self._batch_augment(inputs)
         else:
-            return inputs
+            raise ValueError(
+                "Point clouds augmentation layers are expecting inputs "
+                "point clouds and bounding boxes to be rank 3D (Frame, Point, "
+                "Feature) or 4D (Batch, Frame, Point, Feature) tensors. Got "
+                "shape: {} and {}".format(
+                    point_clouds.shape, bounding_boxes.shape
+                )
+            )
 
     def _augment(self, inputs):
         point_clouds = inputs.get(POINT_CLOUDS, None)
         bounding_boxes = inputs.get(BOUNDING_BOXES, None)
         transformation = self.get_random_transformation(
             point_clouds=point_clouds,
             bounding_boxes=bounding_boxes,
```

## keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py

```diff
@@ -32,30 +32,34 @@
         random_y = self._random_generator.random_normal(
             (), mean=0.0, stddev=self._translate_noise[1]
         )
         random_z = self._random_generator.random_normal(
             (), mean=0.0, stddev=self._translate_noise[2]
         )
 
-        return {"pose": tf.stack([random_x, random_y, random_z, 0, 0, 0], axis=0)}
+        return {
+            "pose": tf.stack([random_x, random_y, random_z, 0, 0, 0], axis=0)
+        }
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, transformation, **kwargs
     ):
         point_clouds_xyz = point_clouds[..., :3]
         point_clouds_xyz += transformation["pose"][:3]
         bounding_boxes_xyz = bounding_boxes[..., :3]
         bounding_boxes_xyz += transformation["pose"][:3]
         return (
             tf.concat([point_clouds_xyz, point_clouds[..., 3:]], axis=-1),
             tf.concat([bounding_boxes_xyz, bounding_boxes[..., 3:]], axis=-1),
         )
 
 
-class VectorizeDisabledLayer(base_augmentation_layer_3d.BaseAugmentationLayer3D):
+class VectorizeDisabledLayer(
+    base_augmentation_layer_3d.BaseAugmentationLayer3D
+):
     def __init__(self, **kwargs):
         self.auto_vectorize = False
         super().__init__(**kwargs)
 
 
 class BaseImageAugmentationLayerTest(tf.test.TestCase):
     def test_auto_vectorize_disabled(self):
```

## keras_cv/layers/preprocessing_3d/frustum_random_dropping_points.py

```diff
@@ -9,32 +9,36 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
+from keras_cv import point_cloud
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.ops import point_cloud
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 POINTCLOUD_LABEL_INDEX = base_augmentation_layer_3d.POINTCLOUD_LABEL_INDEX
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class FrustumRandomDroppingPoints(base_augmentation_layer_3d.BaseAugmentationLayer3D):
-    """A preprocessing layer which randomly drops point within a randomly generated frustum during training.
-
-    This layer will randomly select a point from the point cloud as the center of a frustum then generate a frustum based
-    on r_distance, theta_width, and phi_width. Points inside the selected frustum are randomly dropped (setting all features to zero)
-    based on drop_rate.
-    The point_clouds tensor shape must be specific and cannot be dynamic.
-    During inference time, the output will be identical to input. Call the layer with `training=True` to drop the input points.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class FrustumRandomDroppingPoints(
+    base_augmentation_layer_3d.BaseAugmentationLayer3D
+):
+    """A preprocessing layer which randomly drops point within a randomly
+    generated frustum during training.
+
+    This layer will randomly select a point from the point cloud as the center
+    of a frustum then generate a frustum based on r_distance, theta_width, and
+    phi_width. Points inside the selected frustum are randomly dropped
+    (setting all features to zero) based on drop_rate. The point_clouds tensor
+    shape must be specific and cannot be dynamic.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features].
@@ -43,48 +47,72 @@
     Output shape:
       A dictionary of Tensors with the same shape as input Tensors.
 
     Arguments:
       r_distance: A float scalar sets the starting distance of a frustum.
       theta_width: A float scalar sets the theta width of a frustum.
       phi_width: A float scalar sets the phi width of a frustum.
-      drop_rate: A float scalar sets the probability threshold for dropping the points.
+      drop_rate: A float scalar sets the probability threshold for dropping the
+        points.
+      exclude_classes: An optional int scalar or a list of ints. Points with the
+        specified class(es) will not be dropped.
+
     """
 
-    def __init__(self, r_distance, theta_width, phi_width, drop_rate=None, **kwargs):
+    def __init__(
+        self,
+        r_distance,
+        theta_width,
+        phi_width,
+        drop_rate=None,
+        exclude_classes=None,
+        **kwargs,
+    ):
         super().__init__(**kwargs)
 
+        if not isinstance(exclude_classes, (tuple, list)):
+            exclude_classes = [exclude_classes]
+
         if r_distance < 0:
-            raise ValueError(f"r_distance must be >=0, but got r_distance={r_distance}")
+            raise ValueError(
+                f"r_distance must be >=0, but got r_distance={r_distance}"
+            )
         if theta_width < 0:
             raise ValueError(
                 f"theta_width must be >=0, but got theta_width={theta_width}"
             )
         if phi_width < 0:
-            raise ValueError(f"phi_width must be >=0, but got phi_width={phi_width}")
+            raise ValueError(
+                f"phi_width must be >=0, but got phi_width={phi_width}"
+            )
         drop_rate = drop_rate if drop_rate else 0.0
         if drop_rate > 1:
-            raise ValueError(f"drop_rate must be <=1, but got drop_rate={drop_rate}")
+            raise ValueError(
+                f"drop_rate must be <=1, but got drop_rate={drop_rate}"
+            )
 
         self._r_distance = r_distance
         self._theta_width = theta_width
         self._phi_width = phi_width
         keep_probability = 1 - drop_rate
         self._keep_probability = keep_probability
+        self._exclude_classes = exclude_classes
 
     def get_config(self):
         return {
             "r_distance": self._r_distance,
             "theta_width": self._theta_width,
             "phi_width": self._phi_width,
             "drop_rate": 1 - self._keep_probability,
+            "exclude_classes": self._exclude_classes,
         }
 
     def get_random_transformation(self, point_clouds, **kwargs):
-        # Randomly select a point from the first frame as the center of the frustum.
+        # Randomly select a point from the first frame as the center of the
+        # frustum.
         valid_points = point_clouds[0, :, POINTCLOUD_LABEL_INDEX] > 0
         num_valid_points = tf.math.reduce_sum(tf.cast(valid_points, tf.int32))
         randomly_select_point_index = tf.random.uniform(
             (), minval=0, maxval=num_valid_points, dtype=tf.int32
         )
         randomly_select_frustum_center = tf.boolean_mask(
             point_clouds[0], valid_points, axis=0
@@ -113,9 +141,18 @@
         random_point_mask = tf.where(~frustum_mask, True, random_point_mask)
         return {"point_mask": random_point_mask}
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, transformation, **kwargs
     ):
         point_mask = transformation["point_mask"]
+
+        # Do not drop points that are protected by setting the corresponding
+        # point_mask = 1.0.
+        protected_points = tf.zeros_like(point_clouds[0, :, -1], dtype=tf.bool)
+        for excluded_class in self._exclude_classes:
+            protected_points |= point_clouds[0, :, -1] == excluded_class
+        point_mask = tf.where(
+            protected_points[tf.newaxis, :, tf.newaxis], True, point_mask
+        )
         point_clouds = tf.where(point_mask, point_clouds, 0.0)
         return (point_clouds, bounding_boxes)
```

## keras_cv/layers/preprocessing_3d/frustum_random_dropping_points_test.py

```diff
@@ -40,15 +40,17 @@
         )
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertAllClose(inputs, outputs)
 
-    def test_not_augment_drop_rate1_frustum_empty_point_clouds_and_bounding_boxes(self):
+    def test_not_augment_drop_rate1_frustum_empty_point_clouds_and_bounding_boxes(  # noqa: E501
+        self,
+    ):
         add_layer = FrustumRandomDroppingPoints(
             r_distance=10, theta_width=0, phi_width=0, drop_rate=1.0
         )
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
@@ -60,14 +62,59 @@
         )
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertAllClose(inputs[POINT_CLOUDS] * 0.0, outputs[POINT_CLOUDS])
 
+    def test_exclude_all_points(self):
+        add_layer = FrustumRandomDroppingPoints(
+            r_distance=0,
+            theta_width=np.pi,
+            phi_width=np.pi,
+            drop_rate=1.0,
+            exclude_classes=1,
+        )
+        point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
+        exclude_classes = np.ones(shape=(2, 50, 1)).astype("float32")
+        point_clouds = np.concatenate([point_clouds, exclude_classes], axis=-1)
+
+        bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
+        inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
+        outputs = add_layer(inputs)
+        self.assertAllClose(inputs, outputs)
+
+    def test_exclude_the_first_half_points(self):
+        add_layer = FrustumRandomDroppingPoints(
+            r_distance=0,
+            theta_width=np.pi,
+            phi_width=np.pi,
+            drop_rate=1.0,
+            exclude_classes=[1, 2],
+        )
+        point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
+        class_1 = np.ones(shape=(2, 10, 1)).astype("float32")
+        class_2 = np.ones(shape=(2, 15, 1)).astype("float32") * 2
+        classes = np.concatenate(
+            [class_1, class_2, np.zeros(shape=(2, 25, 1)).astype("float32")],
+            axis=1,
+        )
+        point_clouds = np.concatenate([point_clouds, classes], axis=-1)
+
+        bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
+        inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
+        outputs = add_layer(inputs)
+        self.assertAllClose(
+            inputs[POINT_CLOUDS][:, 25:, :] * 0.0,
+            outputs[POINT_CLOUDS][:, 25:, :],
+        )
+        self.assertAllClose(
+            inputs[POINT_CLOUDS][:, :25, :], outputs[POINT_CLOUDS][:, :25, :]
+        )
+
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = FrustumRandomDroppingPoints(
             r_distance=0, theta_width=1, phi_width=1, drop_rate=0.5
         )
         point_clouds = np.random.random(size=(3, 2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(3, 2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
```

## keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise.py

```diff
@@ -9,35 +9,40 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
+from keras_cv import point_cloud
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.ops import point_cloud
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 POINTCLOUD_LABEL_INDEX = base_augmentation_layer_3d.POINTCLOUD_LABEL_INDEX
 POINTCLOUD_FEATURE_INDEX = base_augmentation_layer_3d.POINTCLOUD_FEATURE_INDEX
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class FrustumRandomPointFeatureNoise(
     base_augmentation_layer_3d.BaseAugmentationLayer3D
 ):
-    """A preprocessing layer which randomly add noise to point features within a randomly generated frustum during training.
+    """A preprocessing layer which randomly add noise to point features within a
+    randomly generated frustum during training.
 
-    This layer will randomly select a point from the point cloud as the center of a frustum then generate a frustum based
-    on r_distance, theta_width, and phi_width. Uniformly sampled features noise from [1-max_noise_level, 1+max_noise_level] will be multiplied
-    to points inside the selected frustum. Here, we perturbe point features other than (x, y, z, class).
-    The point_clouds tensor shape must be specific and cannot be dynamic.
-    During inference time, the output will be identical to input. Call the layer with `training=True` to add noise to the input points.
+    This layer will randomly select a point from the point cloud as the center
+    of a frustum then generate a frustum based on r_distance, theta_width, and
+    phi_width. Uniformly sampled features noise from [1-max_noise_level,
+    1+max_noise_level] will be multiplied to points inside the selected frustum.
+    Here, we perturbe point features other than (x, y, z, class). The
+    point_clouds tensor shape must be specific and cannot be dynamic. During
+    inference time, the output will be identical to input. Call the layer with
+    `training=True` to add noise to the input points.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 4 features are [x, y, z, class, additional features].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features]. Boxes are expected
@@ -48,58 +53,77 @@
     Output shape:
       A dictionary of Tensors with the same shape as input Tensors.
 
     Arguments:
       r_distance: A float scalar sets the starting distance of a frustum.
       theta_width: A float scalar sets the theta width of a frustum.
       phi_width: A float scalar sets the phi width of a frustum.
-      max_noise_level: A float scalar sets the sampled feature noise range [1-max_noise_level, 1+max_noise_level].
+      max_noise_level: A float scalar sets the sampled feature noise range
+        [1-max_noise_level, 1+max_noise_level].
+      exclude_classes: An optional int scalar or a list of ints. Points with the
+        specified class(es) will not be modified.
 
     """
 
     def __init__(
-        self, r_distance, theta_width, phi_width, max_noise_level=None, **kwargs
+        self,
+        r_distance,
+        theta_width,
+        phi_width,
+        max_noise_level=None,
+        exclude_classes=None,
+        **kwargs
     ):
         super().__init__(**kwargs)
 
+        if not isinstance(exclude_classes, (tuple, list)):
+            exclude_classes = [exclude_classes]
+
         if r_distance < 0:
             raise ValueError("r_distance must be >=0.")
         if theta_width < 0:
             raise ValueError("theta_width must be >=0.")
         if phi_width < 0:
             raise ValueError("phi_width must be >=0.")
         max_noise_level = max_noise_level if max_noise_level else 0.0
         if max_noise_level < 0 or max_noise_level > 1:
             raise ValueError("max_noise_level must be >=0 and <=1.")
 
         self._r_distance = r_distance
         self._theta_width = theta_width
         self._phi_width = phi_width
         self._max_noise_level = max_noise_level
+        self._exclude_classes = exclude_classes
 
     def get_config(self):
         return {
             "r_distance": self._r_distance,
             "theta_width": self._theta_width,
             "phi_width": self._phi_width,
             "max_noise_level": self._max_noise_level,
+            "exclude_classes": self._exclude_classes,
         }
 
     def get_random_transformation(self, point_clouds, **kwargs):
-        # Randomly select a point from the first frame as the center of the frustum.
+        # Randomly select a point from the first frame as the center of the
+        # frustum.
         valid_points = point_clouds[0, :, POINTCLOUD_LABEL_INDEX] > 0
         num_valid_points = tf.math.reduce_sum(tf.cast(valid_points, tf.int32))
         randomly_select_point_index = tf.random.uniform(
             (), minval=0, maxval=num_valid_points, dtype=tf.int32
         )
         randomly_select_frustum_center = tf.boolean_mask(
             point_clouds[0], valid_points, axis=0
         )[randomly_select_point_index, :POINTCLOUD_LABEL_INDEX]
 
-        num_frames, num_points, num_features = point_clouds.get_shape().as_list()
+        (
+            num_frames,
+            num_points,
+            num_features,
+        ) = point_clouds.get_shape().as_list()
         frustum_mask = []
         for f in range(num_frames):
             frustum_mask.append(
                 point_cloud.within_a_frustum(
                     point_clouds[f],
                     randomly_select_frustum_center,
                     self._r_distance,
@@ -118,15 +142,29 @@
                 tf.ones([num_frames, num_points, POINTCLOUD_FEATURE_INDEX]),
                 feature_noise,
             ],
             axis=-1,
         )
         # Do add feature noise outside the frustum mask.
         random_point_noise = tf.where(~frustum_mask, 1.0, noise)
+        random_point_noise = tf.cast(
+            random_point_noise, dtype=self.compute_dtype
+        )
         return {"point_noise": random_point_noise}
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, transformation, **kwargs
     ):
         point_noise = transformation["point_noise"]
+
+        # Do not add noise to points that are protected by setting the
+        # corresponding point_noise = 1.0.
+        protected_points = tf.zeros_like(point_clouds[..., -1], dtype=tf.bool)
+        for excluded_class in self._exclude_classes:
+            protected_points |= point_clouds[..., -1] == excluded_class
+
+        no_noise = tf.ones_like(point_noise, point_noise.dtype)
+        point_noise = tf.where(
+            protected_points[:, :, tf.newaxis], no_noise, point_noise
+        )
         point_clouds *= point_noise
         return (point_clouds, bounding_boxes)
```

## keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise_test.py

```diff
@@ -7,19 +7,21 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import numpy as np
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.frustum_random_point_feature_noise import (
+from keras_cv.layers.preprocessing_3d.frustum_random_point_feature_noise import (  # noqa: E501
     FrustumRandomPointFeatureNoise,
 )
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 POINTCLOUD_LABEL_INDEX = base_augmentation_layer_3d.POINTCLOUD_LABEL_INDEX
 
@@ -38,17 +40,20 @@
         self.assertAllClose(inputs[BOUNDING_BOXES], outputs[BOUNDING_BOXES])
         self.assertAllClose(
             inputs[POINT_CLOUDS][:, :, :POINTCLOUD_LABEL_INDEX],
             outputs[POINT_CLOUDS][:, :, :POINTCLOUD_LABEL_INDEX],
         )
 
     def test_augment_specific_point_clouds_and_bounding_boxes(self):
-        tf.keras.utils.set_random_seed(2)
+        keras.utils.set_random_seed(2)
         add_layer = FrustumRandomPointFeatureNoise(
-            r_distance=10, theta_width=np.pi, phi_width=1.5 * np.pi, max_noise_level=0.5
+            r_distance=10,
+            theta_width=np.pi,
+            phi_width=1.5 * np.pi,
+            max_noise_level=0.5,
         )
         point_clouds = np.array(
             [
                 [
                     [0, 1, 2, 3, 4, 5],
                     [10, 1, 2, 3, 4, 2],
                     [100, 100, 2, 3, 4, 1],
@@ -75,22 +80,26 @@
                     [100, 100, 2, 3, 4, 1],
                     [-20, -20, 21, 1, 0, 1.6563809],
                 ],
             ]
         ).astype("float32")
         self.assertAllClose(inputs[BOUNDING_BOXES], outputs[BOUNDING_BOXES])
         # [-20, -20, 21, 1, 0, 2] is randomly selected as the frustum center.
-        # [0, 1, 2, 3, 4, 5] and [10, 1, 2, 3, 4, 2] are not changed due to less than r_distance.
-        # [100, 100, 2, 3, 4, 1] is not changed due to outside phi_width.
+        # [0, 1, 2, 3, 4, 5] and [10, 1, 2, 3, 4, 2] are not changed due to less
+        # than r_distance. [100, 100, 2, 3, 4, 1] is not changed due to outside
+        # phi_width.
         self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
 
     def test_augment_only_one_valid_point_point_clouds_and_bounding_boxes(self):
-        tf.keras.utils.set_random_seed(2)
+        keras.utils.set_random_seed(2)
         add_layer = FrustumRandomPointFeatureNoise(
-            r_distance=10, theta_width=np.pi, phi_width=1.5 * np.pi, max_noise_level=0.5
+            r_distance=10,
+            theta_width=np.pi,
+            phi_width=1.5 * np.pi,
+            max_noise_level=0.5,
         )
         point_clouds = np.array(
             [
                 [
                     [0, 0, 0, 0, 0, 0],
                     [0, 0, 0, 0, 0, 0],
                     [100, 100, 2, 3, 4, 1],
@@ -116,39 +125,83 @@
                     [0, 0, 0, 0, 0, 0],
                     [100, 100, 2, 3, 3.192014, 0.618371],
                     [0, 0, 0, 0, 0, 0],
                 ],
             ]
         ).astype("float32")
         self.assertAllClose(inputs[BOUNDING_BOXES], outputs[BOUNDING_BOXES])
-        # [100, 100, 2, 3, 4, 1] is selected as the frustum center because it is the only valid point.
+        # [100, 100, 2, 3, 4, 1] is selected as the frustum center because it is
+        # the only valid point.
         self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
 
     def test_not_augment_max_noise_level0_point_clouds_and_bounding_boxes(self):
         add_layer = FrustumRandomPointFeatureNoise(
             r_distance=0, theta_width=1, phi_width=1, max_noise_level=0.0
         )
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertAllClose(inputs, outputs)
 
-    def test_not_augment_max_noise_level1_frustum_empty_point_clouds_and_bounding_boxes(
+    def test_not_augment_max_noise_level1_frustum_empty_point_clouds_and_bounding_boxes(  # noqa: E501
         self,
     ):
         add_layer = FrustumRandomPointFeatureNoise(
             r_distance=10, theta_width=0, phi_width=0, max_noise_level=1.0
         )
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertAllClose(inputs, outputs)
 
+    def test_exclude_all_points(self):
+        add_layer = FrustumRandomPointFeatureNoise(
+            r_distance=0,
+            theta_width=1,
+            phi_width=1,
+            max_noise_level=1.0,
+            exclude_classes=1,
+        )
+        point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
+        exclude_classes = np.ones(shape=(2, 50, 1)).astype("float32")
+        point_clouds = np.concatenate([point_clouds, exclude_classes], axis=-1)
+
+        bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
+        inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
+        outputs = add_layer(inputs)
+        self.assertAllClose(inputs, outputs)
+
+    def test_exclude_the_first_half_points(self):
+        add_layer = FrustumRandomPointFeatureNoise(
+            r_distance=0,
+            theta_width=10,
+            phi_width=10,
+            max_noise_level=1.0,
+            exclude_classes=[1, 2],
+        )
+        point_clouds = np.random.random(size=(2, 10, 10)).astype("float32")
+        class_1 = np.ones(shape=(2, 2, 1)).astype("float32")
+        class_2 = np.ones(shape=(2, 3, 1)).astype("float32") * 2
+        classes = np.concatenate(
+            [class_1, class_2, np.zeros(shape=(2, 5, 1)).astype("float32")],
+            axis=1,
+        )
+        point_clouds = np.concatenate([point_clouds, classes], axis=-1)
+        bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
+        inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
+        outputs = add_layer(inputs)
+        self.assertAllClose(
+            inputs[POINT_CLOUDS][:, :5, :], outputs[POINT_CLOUDS][:, :5, :]
+        )
+        self.assertNotAllClose(
+            inputs[POINT_CLOUDS][:, 5:, :], outputs[POINT_CLOUDS][:, 5:, :]
+        )
+
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = FrustumRandomPointFeatureNoise(
             r_distance=0, theta_width=1, phi_width=1, max_noise_level=0.5
         )
         point_clouds = np.random.random(size=(3, 2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(3, 2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
```

## keras_cv/layers/preprocessing_3d/global_random_dropping_points.py

```diff
@@ -9,27 +9,29 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class GlobalRandomDroppingPoints(base_augmentation_layer_3d.BaseAugmentationLayer3D):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class GlobalRandomDroppingPoints(
+    base_augmentation_layer_3d.BaseAugmentationLayer3D
+):
     """A preprocessing layer which randomly drops point during training.
 
     This layer will randomly drop points based on keep_probability.
-    During inference time, the output will be identical to input. Call the layer with `training=True` to drop the input points.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features]. Boxes are expected
@@ -37,28 +39,39 @@
         https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box_3d/formats.py
         for more details on supported bounding box formats.
 
     Output shape:
       A dictionary of Tensors with the same shape as input Tensors.
 
     Arguments:
-      drop_rate: A float scalar sets the probability threshold for dropping the points.
+      drop_rate: A float scalar sets the probability threshold for dropping the
+        points.
+      exclude_classes: An optional int scalar or a list of ints. Points with the
+        specified class(es) will not be dropped.
+
     """
 
-    def __init__(self, drop_rate=None, **kwargs):
+    def __init__(self, drop_rate=None, exclude_classes=None, **kwargs):
         super().__init__(**kwargs)
         drop_rate = drop_rate if drop_rate else 0.0
 
+        if not isinstance(exclude_classes, (tuple, list)):
+            exclude_classes = [exclude_classes]
+
         if drop_rate > 1:
             raise ValueError("drop_rate must be <=1.")
         keep_probability = 1 - drop_rate
         self._keep_probability = keep_probability
+        self._exclude_classes = exclude_classes
 
     def get_config(self):
-        return {"drop_rate": 1 - self._keep_probability}
+        return {
+            "drop_rate": 1 - self._keep_probability,
+            "exclude_classes": self._exclude_classes,
+        }
 
     def get_random_transformation(self, point_clouds, **kwargs):
         num_points = point_clouds.get_shape().as_list()[-2]
         # Generate mask along point dimension.
         random_point_mask = (
             self._random_generator.random_uniform(
                 [1, num_points, 1], minval=0.0, maxval=1
@@ -68,9 +81,19 @@
 
         return {"point_mask": random_point_mask}
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, transformation, **kwargs
     ):
         point_mask = transformation["point_mask"]
+
+        # Do not add noise to points that are protected by setting the
+        # corresponding point_noise = 1.0.
+        protected_points = tf.zeros_like(point_clouds[0, :, -1], dtype=tf.bool)
+        for excluded_class in self._exclude_classes:
+            protected_points |= point_clouds[0, :, -1] == excluded_class
+
+        point_mask = tf.where(
+            protected_points[tf.newaxis, :, tf.newaxis], True, point_mask
+        )
         point_clouds = tf.where(point_mask, point_clouds, 0.0)
         return (point_clouds, bounding_boxes)
```

## keras_cv/layers/preprocessing_3d/global_random_dropping_points_test.py

```diff
@@ -38,16 +38,16 @@
 
         point_clouds = np.random.random(size=(1, 50, 2)).astype("float32")
         point_clouds = np.concatenate([point_clouds, point_clouds], axis=0)
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertNotAllClose(inputs, outputs)
-        # The augmented point clouds in the first frame should be the same as the
-        # augmented point clouds in the second frame.
+        # The augmented point clouds in the first frame should be the same as
+        # the augmented point clouds in the second frame.
         self.assertAllClose(outputs[POINT_CLOUDS][0], outputs[POINT_CLOUDS][1])
 
     def test_not_augment_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomDroppingPoints(drop_rate=0.0)
 
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
@@ -60,14 +60,49 @@
 
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertAllClose(inputs[POINT_CLOUDS] * 0.0, outputs[POINT_CLOUDS])
 
+    def test_exclude_all_points(self):
+        add_layer = GlobalRandomDroppingPoints(drop_rate=1.0, exclude_classes=1)
+        point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
+        exclude_classes = np.ones(shape=(2, 50, 1)).astype("float32")
+        point_clouds = np.concatenate([point_clouds, exclude_classes], axis=-1)
+
+        bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
+        inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
+        outputs = add_layer(inputs)
+        self.assertAllClose(inputs, outputs)
+
+    def test_exclude_the_first_half_points(self):
+        add_layer = GlobalRandomDroppingPoints(
+            drop_rate=1.0, exclude_classes=[1, 2]
+        )
+        point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
+        class_1 = np.ones(shape=(2, 10, 1)).astype("float32")
+        class_2 = np.ones(shape=(2, 15, 1)).astype("float32") * 2
+        classes = np.concatenate(
+            [class_1, class_2, np.zeros(shape=(2, 25, 1)).astype("float32")],
+            axis=1,
+        )
+        point_clouds = np.concatenate([point_clouds, classes], axis=-1)
+
+        bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
+        inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
+        outputs = add_layer(inputs)
+        self.assertAllClose(
+            inputs[POINT_CLOUDS][:, 25:, :] * 0.0,
+            outputs[POINT_CLOUDS][:, 25:, :],
+        )
+        self.assertAllClose(
+            inputs[POINT_CLOUDS][:, :25, :], outputs[POINT_CLOUDS][:, :25, :]
+        )
+
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomDroppingPoints(drop_rate=0.5)
 
         point_clouds = np.random.random(size=(3, 2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(3, 2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
```

## keras_cv/layers/preprocessing_3d/global_random_flip.py

```diff
@@ -9,30 +9,31 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.ops.point_cloud import wrap_angle_radians
+from keras_cv.point_cloud import wrap_angle_radians
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class GlobalRandomFlip(base_augmentation_layer_3d.BaseAugmentationLayer3D):
-    """A preprocessing layer which flips point clouds and bounding boxes with respect to the specified axis during training.
+    """A preprocessing layer which flips point clouds and bounding boxes with
+    respect to the specified axis during training.
 
     This layer will flip the whole scene with respect to the specified axes.
     Note that this layer currently only supports flipping over the Y axis.
-    During inference time, the output will be identical to input. Call the layer with `training=True` to flip the input.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features]. Boxes are expected
@@ -40,24 +41,25 @@
         https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box_3d/formats.py
         for more details on supported bounding box formats.
 
     Output shape:
       A dictionary of Tensors with the same shape as input Tensors.
 
     Args:
-      flip_x: Whether or not to flip over the X axis. Defaults to False.
-      flip_y: Whether or not to flip over the Y axis. Defaults to True.
-      flip_z: Whether or not to flip over the Z axis. Defaults to False.
+      flip_x: whether to flip over the X axis, defaults to False.
+      flip_y: whether to flip over the Y axis, defaults to True.
+      flip_z: whether to flip over the Z axis, defaults to False.
     """
 
     def __init__(self, flip_x=False, flip_y=True, flip_z=False, **kwargs):
         if flip_x or flip_z:
             raise ValueError(
                 "GlobalRandomFlip currently only supports flipping over the Y "
-                f"axis. Received flip_x={flip_x}, flip_y={flip_y}, flip_z={flip_z}."
+                f"axis. Received flip_x={flip_x}, flip_y={flip_y}, "
+                f"flip_z={flip_z}."
             )
 
         if not (flip_x or flip_y or flip_z):
             raise ValueError("GlobalRandomFlip must flip over at least 1 axis.")
         self.flip_x = flip_x
         self.flip_y = flip_y
         self.flip_z = flip_z
@@ -65,15 +67,16 @@
         super().__init__(**kwargs)
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, transformation, **kwargs
     ):
         point_clouds_y = -point_clouds[..., 1:2]
         point_clouds = tf.concat(
-            [point_clouds[..., 0:1], point_clouds_y, point_clouds[..., 2:]], axis=-1
+            [point_clouds[..., 0:1], point_clouds_y, point_clouds[..., 2:]],
+            axis=-1,
         )
         # Flip boxes.
         bounding_boxes_y = -bounding_boxes[
             ..., CENTER_XYZ_DXDYDZ_PHI.Y : CENTER_XYZ_DXDYDZ_PHI.Y + 1
         ]
         bounding_boxes_xyz = tf.concat(
             [
```

## keras_cv/layers/preprocessing_3d/global_random_flip_test.py

```diff
@@ -28,41 +28,49 @@
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertNotAllClose(inputs, outputs)
 
     def test_augment_specific_random_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomFlip()
-        point_clouds = np.array([[[1, 1, 2, 3, 4, 5, 6, 7, 8, 9]] * 2] * 2).astype(
+        point_clouds = np.array(
+            [[[1, 1, 2, 3, 4, 5, 6, 7, 8, 9]] * 2] * 2
+        ).astype("float32")
+        bounding_boxes = np.array([[[1, 1, 2, 3, 4, 5, 1]] * 2] * 2).astype(
             "float32"
         )
-        bounding_boxes = np.array([[[1, 1, 2, 3, 4, 5, 1]] * 2] * 2).astype("float32")
 
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         flipped_point_clouds = np.array(
             [[[1, -1, 2, 3, 4, 5, 6, 7, 8, 9]] * 2] * 2
         ).astype("float32")
-        flipped_bounding_boxes = np.array([[[1, -1, 2, 3, 4, 5, -1]] * 2] * 2).astype(
-            "float32"
-        )
+        flipped_bounding_boxes = np.array(
+            [[[1, -1, 2, 3, 4, 5, -1]] * 2] * 2
+        ).astype("float32")
         self.assertAllClose(outputs[POINT_CLOUDS], flipped_point_clouds)
         self.assertAllClose(outputs[BOUNDING_BOXES], flipped_bounding_boxes)
 
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomFlip()
         point_clouds = np.random.random(size=(3, 2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(3, 2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertNotAllClose(inputs, outputs)
 
     def test_noop_raises_error(self):
-        with self.assertRaisesRegexp(ValueError, "must flip over at least 1 axis"):
+        with self.assertRaisesRegexp(
+            ValueError, "must flip over at least 1 axis"
+        ):
             _ = GlobalRandomFlip(flip_x=False, flip_y=False, flip_z=False)
 
     def test_flip_x_or_z_raises_error(self):
-        with self.assertRaisesRegexp(ValueError, "only supports flipping over the Y"):
+        with self.assertRaisesRegexp(
+            ValueError, "only supports flipping over the Y"
+        ):
             _ = GlobalRandomFlip(flip_x=True, flip_y=False, flip_z=False)
 
-        with self.assertRaisesRegexp(ValueError, "only supports flipping over the Y"):
+        with self.assertRaisesRegexp(
+            ValueError, "only supports flipping over the Y"
+        ):
             _ = GlobalRandomFlip(flip_x=False, flip_y=False, flip_z=True)
```

## keras_cv/layers/preprocessing_3d/global_random_rotation.py

```diff
@@ -9,32 +9,35 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.ops.point_cloud import coordinate_transform
-from keras_cv.ops.point_cloud import wrap_angle_radians
+from keras_cv.point_cloud import coordinate_transform
+from keras_cv.point_cloud import wrap_angle_radians
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class GlobalRandomRotation(base_augmentation_layer_3d.BaseAugmentationLayer3D):
-    """A preprocessing layer which randomly rotates point clouds and bounding boxes along
-    X, Y and Z axes during training.
+    """A preprocessing layer which randomly rotates point clouds and bounding
+    boxes along X, Y and Z axes during training.
 
-    This layer will randomly rotate the whole scene along the X, Y and Z axes based on a randomly sampled
-    rotation angle between [-max_rotation_angle, max_rotation_angle] (in radians) following a uniform distribution.
-    During inference time, the output will be identical to input. Call the layer with `training=True` to rotate the input.
+    This layer will randomly rotate the whole scene along the X, Y and Z axes
+    based on a randomly sampled rotation angle between [-max_rotation_angle,
+    max_rotation_angle] (in radians) following a uniform distribution. During
+    inference time, the output will be identical to input. Call the layer with
+    `training=True` to rotate the input.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features]. Boxes are expected
@@ -42,31 +45,40 @@
         https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box_3d/formats.py
         for more details on supported bounding box formats.
 
     Output shape:
       A dictionary of Tensors with the same shape as input Tensors.
 
     Arguments:
-      max_rotation_angle_x: A float scalar sets the maximum rotation angle (in radians) along X axis.
-      max_rotation_angle_y: A float scalar sets the maximum rotation angle (in radians) along Y axis.
-      max_rotation_angle_z: A float scalar sets the maximum rotation angle (in radians) along Z axis.
+      max_rotation_angle_x: A float scalar sets the maximum rotation angle (in
+        radians) along X axis.
+      max_rotation_angle_y: A float scalar sets the maximum rotation angle (in
+        radians) along Y axis.
+      max_rotation_angle_z: A float scalar sets the maximum rotation angle (in
+        radians) along Z axis.
 
     """
 
     def __init__(
         self,
         max_rotation_angle_x=None,
         max_rotation_angle_y=None,
         max_rotation_angle_z=None,
         **kwargs
     ):
         super().__init__(**kwargs)
-        max_rotation_angle_x = max_rotation_angle_x if max_rotation_angle_x else 0.0
-        max_rotation_angle_y = max_rotation_angle_y if max_rotation_angle_y else 0.0
-        max_rotation_angle_z = max_rotation_angle_z if max_rotation_angle_z else 0.0
+        max_rotation_angle_x = (
+            max_rotation_angle_x if max_rotation_angle_x else 0.0
+        )
+        max_rotation_angle_y = (
+            max_rotation_angle_y if max_rotation_angle_y else 0.0
+        )
+        max_rotation_angle_z = (
+            max_rotation_angle_z if max_rotation_angle_z else 0.0
+        )
 
         if max_rotation_angle_x < 0:
             raise ValueError("max_rotation_angle_x must be >=0.")
         if max_rotation_angle_y < 0:
             raise ValueError("max_rotation_angle_y must be >=0.")
         if max_rotation_angle_z < 0:
             raise ValueError("max_rotation_angle_z must be >=0.")
@@ -79,41 +91,61 @@
             "max_rotation_angle_x": self._max_rotation_angle_x,
             "max_rotation_angle_y": self._max_rotation_angle_y,
             "max_rotation_angle_z": self._max_rotation_angle_z,
         }
 
     def get_random_transformation(self, **kwargs):
         random_rotation_x = self._random_generator.random_uniform(
-            (), minval=-self._max_rotation_angle_x, maxval=self._max_rotation_angle_x
+            (),
+            minval=-self._max_rotation_angle_x,
+            maxval=self._max_rotation_angle_x,
+            dtype=self.compute_dtype,
         )
         random_rotation_y = self._random_generator.random_uniform(
-            (), minval=-self._max_rotation_angle_y, maxval=self._max_rotation_angle_y
+            (),
+            minval=-self._max_rotation_angle_y,
+            maxval=self._max_rotation_angle_y,
+            dtype=self.compute_dtype,
         )
         random_rotation_z = self._random_generator.random_uniform(
-            (), minval=-self._max_rotation_angle_z, maxval=self._max_rotation_angle_z
+            (),
+            minval=-self._max_rotation_angle_z,
+            maxval=self._max_rotation_angle_z,
+            dtype=self.compute_dtype,
         )
         return {
             "pose": tf.stack(
-                [0, 0, 0, random_rotation_z, random_rotation_x, random_rotation_y],
+                [
+                    0,
+                    0,
+                    0,
+                    random_rotation_z,
+                    random_rotation_x,
+                    random_rotation_y,
+                ],
                 axis=0,
             )
         }
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, transformation, **kwargs
     ):
         pose = transformation["pose"]
         point_clouds_xyz = coordinate_transform(point_clouds[..., :3], pose)
-        point_clouds = tf.concat([point_clouds_xyz, point_clouds[..., 3:]], axis=-1)
+        point_clouds = tf.concat(
+            [point_clouds_xyz, point_clouds[..., 3:]], axis=-1
+        )
 
         bounding_boxes_xyz = coordinate_transform(
             bounding_boxes[..., : CENTER_XYZ_DXDYDZ_PHI.Z + 1], pose
         )
         bounding_boxes_heading = wrap_angle_radians(
-            tf.expand_dims(bounding_boxes[..., CENTER_XYZ_DXDYDZ_PHI.PHI], axis=-1)
+            tf.expand_dims(
+                bounding_boxes[..., CENTER_XYZ_DXDYDZ_PHI.PHI], axis=-1
+            )
             - pose[3]
         )
         bounding_boxes = tf.concat(
             [
                 bounding_boxes_xyz,
                 bounding_boxes[
                     ..., CENTER_XYZ_DXDYDZ_PHI.DX : CENTER_XYZ_DXDYDZ_PHI.DZ + 1
```

## keras_cv/layers/preprocessing_3d/global_random_rotation_test.py

```diff
@@ -11,53 +11,63 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import numpy as np
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.global_random_rotation import GlobalRandomRotation
+from keras_cv.layers.preprocessing_3d.global_random_rotation import (
+    GlobalRandomRotation,
+)
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 
 
 class GlobalRandomRotationTest(tf.test.TestCase):
     def test_augment_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomRotation(
-            max_rotation_angle_x=1.0, max_rotation_angle_y=1.0, max_rotation_angle_z=1.0
+            max_rotation_angle_x=1.0,
+            max_rotation_angle_y=1.0,
+            max_rotation_angle_z=1.0,
         )
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertNotAllClose(inputs, outputs)
 
     def test_not_augment_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomRotation(
-            max_rotation_angle_x=0.0, max_rotation_angle_y=0.0, max_rotation_angle_z=0.0
+            max_rotation_angle_x=0.0,
+            max_rotation_angle_y=0.0,
+            max_rotation_angle_z=0.0,
         )
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertAllClose(inputs, outputs)
 
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomRotation(
-            max_rotation_angle_x=1.0, max_rotation_angle_y=1.0, max_rotation_angle_z=1.0
+            max_rotation_angle_x=1.0,
+            max_rotation_angle_y=1.0,
+            max_rotation_angle_z=1.0,
         )
         point_clouds = np.random.random(size=(3, 2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(3, 2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertNotAllClose(inputs, outputs)
 
     def test_not_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomRotation(
-            max_rotation_angle_x=0.0, max_rotation_angle_y=0.0, max_rotation_angle_z=0.0
+            max_rotation_angle_x=0.0,
+            max_rotation_angle_y=0.0,
+            max_rotation_angle_z=0.0,
         )
         point_clouds = np.random.random(size=(3, 2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(3, 2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertAllClose(inputs, outputs)
```

## keras_cv/layers/preprocessing_3d/global_random_scaling.py

```diff
@@ -9,48 +9,52 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class GlobalRandomScaling(base_augmentation_layer_3d.BaseAugmentationLayer3D):
-    """A preprocessing layer which randomly scales point clouds and bounding boxes along
-    X, Y, and Z axes during training.
+    """A preprocessing layer which randomly scales point clouds and bounding
+    boxes along X, Y, and Z axes during training.
 
-    This layer will randomly scale the whole scene along the  X, Y, and Z axes based on a randomly sampled
-    scaling factor between [min_scaling_factor, max_scaling_factor] following a uniform distribution.
-    During inference time, the output will be identical to input. Call the layer with `training=True` to scale the input.
+    This layer will randomly scale the whole scene along the  X, Y, and Z axes
+    based on a randomly sampled scaling factor between [min_scaling_factor,
+    max_scaling_factor] following a uniform distribution.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
-      bounding_boxes:  3D (multi frames) float32 Tensor with shape
+      bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features]. Boxes are expected
         to follow the CENTER_XYZ_DXDYDZ_PHI format. Refer to
         https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box_3d/formats.py
         for more details on supported bounding box formats.
 
     Output shape:
       A dictionary of Tensors with the same shape as input Tensors.
 
     Arguments:
-      x_factor: A tuple of float scalars or a float scalar sets the minimum and maximum scaling factors for the X axis.
-      y_factor: A tuple of float scalars or a float scalar sets the minimum and maximum scaling factors for the Y axis.
-      z_factor: A tuple of float scalars or a float scalar sets the minimum and maximum scaling factors for the Z axis.
+      x_factor: A tuple of float scalars or a float scalar sets the minimum and
+        maximum scaling factors for the X axis.
+      y_factor: A tuple of float scalars or a float scalar sets the minimum and
+        maximum scaling factors for the Y axis.
+      z_factor: A tuple of float scalars or a float scalar sets the minimum and
+        maximum scaling factors for the Z axis.
     """
 
     def __init__(
         self,
         x_factor=None,
         y_factor=None,
         z_factor=None,
@@ -100,19 +104,21 @@
             or min_y_factor > max_y_factor
             or min_z_factor > max_z_factor
         ):
             raise ValueError("min_factor must be less than max_factor.")
         if preserve_aspect_ratio:
             if min_x_factor != min_y_factor or min_y_factor != min_z_factor:
                 raise ValueError(
-                    "min_factor must be the same when preserve_aspect_ratio is true."
+                    "min_factor must be the same when preserve_aspect_ratio is "
+                    "true."
                 )
             if max_x_factor != max_y_factor or max_y_factor != max_z_factor:
                 raise ValueError(
-                    "max_factor must be the same when preserve_aspect_ratio is true."
+                    "max_factor must be the same when preserve_aspect_ratio is "
+                    "true."
                 )
 
         self._min_x_factor = min_x_factor
         self._max_x_factor = max_x_factor
         self._min_y_factor = min_y_factor
         self._max_y_factor = max_y_factor
         self._min_z_factor = min_z_factor
@@ -134,21 +140,30 @@
                 self._max_z_factor,
             ),
             "preserve_aspect_ratio": self._preserve_aspect_ratio,
         }
 
     def get_random_transformation(self, **kwargs):
         random_scaling_x = self._random_generator.random_uniform(
-            (), minval=self._min_x_factor, maxval=self._max_x_factor
+            (),
+            minval=self._min_x_factor,
+            maxval=self._max_x_factor,
+            dtype=self.compute_dtype,
         )
         random_scaling_y = self._random_generator.random_uniform(
-            (), minval=self._min_y_factor, maxval=self._max_y_factor
+            (),
+            minval=self._min_y_factor,
+            maxval=self._max_y_factor,
+            dtype=self.compute_dtype,
         )
         random_scaling_z = self._random_generator.random_uniform(
-            (), minval=self._min_z_factor, maxval=self._max_z_factor
+            (),
+            minval=self._min_z_factor,
+            maxval=self._max_z_factor,
+            dtype=self.compute_dtype,
         )
         if not self._preserve_aspect_ratio:
             return {
                 "scale": tf.stack(
                     [random_scaling_x, random_scaling_y, random_scaling_z]
                 )
             }
@@ -160,15 +175,17 @@
             }
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, transformation, **kwargs
     ):
         scale = transformation["scale"][tf.newaxis, tf.newaxis, :]
         point_clouds_xyz = point_clouds[..., :3] * scale
-        point_clouds = tf.concat([point_clouds_xyz, point_clouds[..., 3:]], axis=-1)
+        point_clouds = tf.concat(
+            [point_clouds_xyz, point_clouds[..., 3:]], axis=-1
+        )
 
         bounding_boxes_xyzdxdydz = bounding_boxes[
             ..., : CENTER_XYZ_DXDYDZ_PHI.DZ + 1
         ] * tf.concat([scale] * 2, axis=-1)
         bounding_boxes = tf.concat(
             [
                 bounding_boxes_xyzdxdydz,
```

## keras_cv/layers/preprocessing_3d/global_random_scaling_test.py

```diff
@@ -11,15 +11,17 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import numpy as np
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.global_random_scaling import GlobalRandomScaling
+from keras_cv.layers.preprocessing_3d.global_random_scaling import (
+    GlobalRandomScaling,
+)
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 
 
 class GlobalScalingTest(tf.test.TestCase):
     def test_augment_point_clouds_and_bounding_boxes(self):
@@ -61,26 +63,28 @@
 
     def test_2x_scaling_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomScaling(
             x_factor=(2.0, 2.0),
             y_factor=(2.0, 2.0),
             z_factor=(2.0, 2.0),
         )
-        point_clouds = np.array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] * 2] * 2).astype(
+        point_clouds = np.array(
+            [[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] * 2] * 2
+        ).astype("float32")
+        bounding_boxes = np.array([[[0, 1, 2, 3, 4, 5, 6]] * 2] * 2).astype(
             "float32"
         )
-        bounding_boxes = np.array([[[0, 1, 2, 3, 4, 5, 6]] * 2] * 2).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         scaled_point_clouds = np.array(
             [[[0, 2, 4, 3, 4, 5, 6, 7, 8, 9]] * 2] * 2
         ).astype("float32")
-        scaled_bounding_boxes = np.array([[[0, 2, 4, 6, 8, 10, 6]] * 2] * 2).astype(
-            "float32"
-        )
+        scaled_bounding_boxes = np.array(
+            [[[0, 2, 4, 6, 8, 10, 6]] * 2] * 2
+        ).astype("float32")
         self.assertAllClose(outputs[POINT_CLOUDS], scaled_point_clouds)
         self.assertAllClose(outputs[BOUNDING_BOXES], scaled_bounding_boxes)
 
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = GlobalRandomScaling(
             x_factor=(0.5, 1.5),
             y_factor=(0.5, 1.5),
```

## keras_cv/layers/preprocessing_3d/global_random_translation.py

```diff
@@ -9,31 +9,35 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.ops.point_cloud import coordinate_transform
+from keras_cv.point_cloud import coordinate_transform
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class GlobalRandomTranslation(base_augmentation_layer_3d.BaseAugmentationLayer3D):
-    """A preprocessing layer which randomly translates point clouds and bounding boxes along
-    X, Y, and Z axes during training.
-
-    This layer will randomly translate the whole scene along the X, Y,and Z axes based on three randomly sampled
-    translation factors following three normal distributions centered at 0 with standard deviation  [x_stddev, y_stddev, z_stddev].
-    During inference time, the output will be identical to input. Call the layer with `training=True` to translate the input.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class GlobalRandomTranslation(
+    base_augmentation_layer_3d.BaseAugmentationLayer3D
+):
+    """A preprocessing layer which randomly translates point clouds and bounding
+    boxes along X, Y, and Z axes during training.
+
+    This layer will randomly translate the whole scene along the X, Y,and Z axes
+    based on three randomly sampled translation factors following three normal
+    distributions centered at 0 with standard deviation [x_stddev, y_stddev,
+    z_stddev].
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features]. Boxes are expected
@@ -41,17 +45,20 @@
         https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box_3d/formats.py
         for more details on supported bounding box formats.
 
     Output shape:
       A dictionary of Tensors with the same shape as input Tensors.
 
     Arguments:
-      x_stddev: A float scalar sets the translation noise standard deviation along the X axis.
-      y_stddev: A float scalar sets the translation noise standard deviation along the Y axis.
-      z_stddev: A float scalar sets the translation noise standard deviation along the Z axis.
+      x_stddev: A float scalar sets the translation noise standard deviation
+        along the X axis.
+      y_stddev: A float scalar sets the translation noise standard deviation
+        along the Y axis.
+      z_stddev: A float scalar sets the translation noise standard deviation
+        along the Z axis.
     """
 
     def __init__(self, x_stddev=None, y_stddev=None, z_stddev=None, **kwargs):
         super().__init__(**kwargs)
         x_stddev = x_stddev if x_stddev else 0.0
         y_stddev = y_stddev if y_stddev else 0.0
         z_stddev = z_stddev if z_stddev else 0.0
@@ -67,45 +74,53 @@
             "x_stddev": self._x_stddev,
             "y_stddev": self._x_stddev,
             "z_stddev": self._z_stddev,
         }
 
     def get_random_transformation(self, **kwargs):
         random_x_translation = self._random_generator.random_normal(
-            (), mean=0.0, stddev=self._x_stddev
+            (), mean=0.0, stddev=self._x_stddev, dtype=self.compute_dtype
         )
         random_y_translation = self._random_generator.random_normal(
-            (), mean=0.0, stddev=self._y_stddev
+            (), mean=0.0, stddev=self._y_stddev, dtype=self.compute_dtype
         )
         random_z_translation = self._random_generator.random_normal(
-            (), mean=0.0, stddev=self._z_stddev
+            (),
+            mean=0.0,
+            stddev=self._z_stddev,
+            dtype=self.compute_dtype,
         )
         return {
             "pose": tf.stack(
                 [
                     random_x_translation,
                     random_y_translation,
                     random_z_translation,
-                    0,
-                    0,
-                    0,
+                    0.0,
+                    0.0,
+                    0.0,
                 ],
                 axis=0,
             )
         }
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, transformation, **kwargs
     ):
         pose = transformation["pose"]
         point_clouds_xyz = coordinate_transform(point_clouds[..., :3], pose)
-        point_clouds = tf.concat([point_clouds_xyz, point_clouds[..., 3:]], axis=-1)
+        point_clouds = tf.concat(
+            [point_clouds_xyz, point_clouds[..., 3:]], axis=-1
+        )
 
         bounding_boxes_xyz = coordinate_transform(
             bounding_boxes[..., : CENTER_XYZ_DXDYDZ_PHI.Z + 1], pose
         )
         bounding_boxes = tf.concat(
-            [bounding_boxes_xyz, bounding_boxes[..., CENTER_XYZ_DXDYDZ_PHI.DX :]],
+            [
+                bounding_boxes_xyz,
+                bounding_boxes[..., CENTER_XYZ_DXDYDZ_PHI.DX :],
+            ],
             axis=-1,
         )
 
         return (point_clouds, bounding_boxes)
```

## keras_cv/layers/preprocessing_3d/global_random_translation_test.py

```diff
@@ -21,37 +21,45 @@
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 
 
 class GlobalRandomTranslationTest(tf.test.TestCase):
     def test_augment_point_clouds_and_bounding_boxes(self):
-        add_layer = GlobalRandomTranslation(x_stddev=1.0, y_stddev=1.0, z_stddev=1.0)
+        add_layer = GlobalRandomTranslation(
+            x_stddev=1.0, y_stddev=1.0, z_stddev=1.0
+        )
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertNotAllClose(inputs, outputs)
 
     def test_not_augment_point_clouds_and_bounding_boxes(self):
-        add_layer = GlobalRandomTranslation(x_stddev=0.0, y_stddev=0.0, z_stddev=0.0)
+        add_layer = GlobalRandomTranslation(
+            x_stddev=0.0, y_stddev=0.0, z_stddev=0.0
+        )
         point_clouds = np.random.random(size=(2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertAllClose(inputs, outputs)
 
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
-        add_layer = GlobalRandomTranslation(x_stddev=1.0, y_stddev=1.0, z_stddev=1.0)
+        add_layer = GlobalRandomTranslation(
+            x_stddev=1.0, y_stddev=1.0, z_stddev=1.0
+        )
         point_clouds = np.random.random(size=(3, 2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(3, 2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertNotAllClose(inputs, outputs)
 
     def test_not_augment_batch_point_clouds_and_bounding_boxes(self):
-        add_layer = GlobalRandomTranslation(x_stddev=0.0, y_stddev=0.0, z_stddev=0.0)
+        add_layer = GlobalRandomTranslation(
+            x_stddev=0.0, y_stddev=0.0, z_stddev=0.0
+        )
         point_clouds = np.random.random(size=(3, 2, 50, 10)).astype("float32")
         bounding_boxes = np.random.random(size=(3, 2, 10, 7)).astype("float32")
         inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         outputs = add_layer(inputs)
         self.assertAllClose(inputs, outputs)
```

## keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes.py

```diff
@@ -9,55 +9,66 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.ops.point_cloud import group_points_by_boxes
-from keras_cv.ops.point_cloud import is_within_box3d
+from keras_cv.point_cloud import group_points_by_boxes
+from keras_cv.point_cloud import is_within_box3d
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 OBJECT_POINT_CLOUDS = base_augmentation_layer_3d.OBJECT_POINT_CLOUDS
 OBJECT_BOUNDING_BOXES = base_augmentation_layer_3d.OBJECT_BOUNDING_BOXES
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class GroupPointsByBoundingBoxes(base_augmentation_layer_3d.BaseAugmentationLayer3D):
-    """A preprocessing layer which groups point clouds based on bounding boxes during training.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class GroupPointsByBoundingBoxes(
+    base_augmentation_layer_3d.BaseAugmentationLayer3D
+):
+    """A preprocessing layer which groups point clouds based on bounding boxes
+    during training.
 
-    This layer will group point clouds based on bounding boxes and generate OBJECT_POINT_CLOUDS and OBJECT_BOUNDING_BOXES tensors.
-    During inference time, the output will be identical to input. Call the layer with `training=True` to group point clouds based on bounding boxes.
+    This layer will group point clouds based on bounding boxes and generate
+    OBJECT_POINT_CLOUDS and OBJECT_BOUNDING_BOXES tensors.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features]. Boxes are expected
         to follow the CENTER_XYZ_DXDYDZ_PHI format. Refer to
         https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box_3d/formats.py
 
     Output shape:
-      A dictionary of Tensors with the same shape as input Tensors and two additional items for
-        OBJECT_POINT_CLOUDS (shape [num of frames, num of valid boxes, max num of points, num of point features])
-        and OBJECT_BOUNDING_BOXES (shape [num of frames, num of valid boxes, num of box features]).
+      A dictionary of Tensors with the same shape as input Tensors and two
+      additional items for OBJECT_POINT_CLOUDS (shape [num of frames, num of
+      valid boxes, max num of points, num of point features]) and
+      OBJECT_BOUNDING_BOXES (shape [num of frames, num of valid boxes, num of
+      box features]).
 
     Arguments:
       label_index: An optional int scalar sets the target object index.
-        Bounding boxes and corresponding point clouds with box class == label_index will be saved as OBJECT_BOUNDING_BOXES and OBJECT_POINT_CLOUDS.
-        If label index is None, all valid bounding boxes (box class !=0) are used.
-      min_points_per_bounding_boxes: A int scalar sets the min number of points in a bounding box.
-        If a bounding box contains less than min_points_per_bounding_boxes, the bounding box is filtered out.
-      max_points_per_bounding_boxes: A int scalar sets the max number of points in a bounding box.
-        All the object point clouds will be padded or trimmed to the same shape, where the number of points dimension is max_points_per_bounding_boxes.
+        Bounding boxes and corresponding point clouds with box class ==
+        label_index will be saved as OBJECT_BOUNDING_BOXES and
+        OBJECT_POINT_CLOUDS. If label index is None, all valid bounding boxes
+        (box class !=0) are used.
+      min_points_per_bounding_boxes: A int scalar sets the min number of points
+        in a bounding box. If a bounding box contains less than
+        min_points_per_bounding_boxes, the bounding box is filtered out.
+      max_points_per_bounding_boxes: A int scalar sets the max number of points
+        in a bounding box. All the object point clouds will be padded or trimmed
+        to the same shape, where the number of points dimension is
+        max_points_per_bounding_boxes.
     """
 
     def __init__(
         self,
         label_index=None,
         min_points_per_bounding_boxes=0,
         max_points_per_bounding_boxes=2000,
@@ -69,35 +80,37 @@
             raise ValueError("label_index must be >=0 or None.")
         if min_points_per_bounding_boxes < 0:
             raise ValueError("min_points_per_bounding_boxes must be >=0.")
         if max_points_per_bounding_boxes < 0:
             raise ValueError("max_points_per_bounding_boxes must be >=0.")
         if min_points_per_bounding_boxes > max_points_per_bounding_boxes:
             raise ValueError(
-                "max_paste_bounding_boxes must be >= min_points_per_bounding_boxes."
+                "max_paste_bounding_boxes must be >= "
+                "min_points_per_bounding_boxes."
             )
 
         self._label_index = label_index
         self._min_points_per_bounding_boxes = min_points_per_bounding_boxes
         self._max_points_per_bounding_boxes = max_points_per_bounding_boxes
         self._auto_vectorize = False
 
     def get_config(self):
         return {
             "label_index": self._label_index,
-            "min_points_per_bounding_boxes": self._min_points_per_bounding_boxes,
-            "max_points_per_bounding_boxes": self._max_points_per_bounding_boxes,
+            "min_points_per_bounding_boxes": self._min_points_per_bounding_boxes,  # noqa: E501
+            "max_points_per_bounding_boxes": self._max_points_per_bounding_boxes,  # noqa: E501
         }
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, **kwargs
     ):
         if self._label_index:
             bounding_boxes_mask = (
-                bounding_boxes[0, :, CENTER_XYZ_DXDYDZ_PHI.CLASS] == self._label_index
+                bounding_boxes[0, :, CENTER_XYZ_DXDYDZ_PHI.CLASS]
+                == self._label_index
             )
             object_bounding_boxes = tf.boolean_mask(
                 bounding_boxes, bounding_boxes_mask, axis=1
             )
         else:
             bounding_boxes_mask = (
                 bounding_boxes[0, :, CENTER_XYZ_DXDYDZ_PHI.CLASS] > 0.0
@@ -108,35 +121,40 @@
 
         points_in_bounding_boxes = is_within_box3d(
             point_clouds[:, :, :3], object_bounding_boxes[:, :, :7]
         )
         # Filter bounding boxes using the current frame.
         # [num_boxes]
         min_points_filter = (
-            tf.reduce_sum(tf.cast(points_in_bounding_boxes[0], dtype=tf.int32), axis=0)
+            tf.reduce_sum(
+                tf.cast(points_in_bounding_boxes[0], dtype=tf.int32), axis=0
+            )
             >= self._min_points_per_bounding_boxes
         )
 
         object_bounding_boxes = tf.boolean_mask(
             object_bounding_boxes, min_points_filter, axis=1
         )
 
         points_in_bounding_boxes = tf.boolean_mask(
             points_in_bounding_boxes, min_points_filter, axis=2
         )
         # [num of frames, num of boxes, num of points].
-        points_in_bounding_boxes = tf.transpose(points_in_bounding_boxes, [0, 2, 1])
+        points_in_bounding_boxes = tf.transpose(
+            points_in_bounding_boxes, [0, 2, 1]
+        )
         points_in_bounding_boxes = tf.cast(points_in_bounding_boxes, tf.int32)
         sort_valid_index = tf.argsort(
             points_in_bounding_boxes, axis=-1, direction="DESCENDING"
         )
         sort_valid_mask = tf.gather(
             points_in_bounding_boxes, sort_valid_index, axis=2, batch_dims=2
         )[:, :, : self._max_points_per_bounding_boxes]
-        # [num of frames, num of boxes, self._max_points_per_bounding_boxes, num of point features].
+        # [num of frames, num of boxes, self._max_points_per_bounding_boxes, num
+        # of point features].
         object_point_clouds = point_clouds[:, tf.newaxis, :, :]
         num_valid_bounding_boxes = tf.shape(object_bounding_boxes)[1]
         object_point_clouds = tf.tile(
             object_point_clouds, [1, num_valid_bounding_boxes, 1, 1]
         )
         object_point_clouds = tf.gather(
             object_point_clouds, sort_valid_index, axis=2, batch_dims=2
@@ -152,15 +170,16 @@
         )
 
     def augment_point_clouds_bounding_boxes_v2(
         self, point_clouds, bounding_boxes, **kwargs
     ):
         if self._label_index:
             bounding_boxes_mask = (
-                bounding_boxes[0, :, CENTER_XYZ_DXDYDZ_PHI.CLASS] == self._label_index
+                bounding_boxes[0, :, CENTER_XYZ_DXDYDZ_PHI.CLASS]
+                == self._label_index
             )
             object_bounding_boxes = tf.boolean_mask(
                 bounding_boxes, bounding_boxes_mask, axis=1
             )
         else:
             bounding_boxes_mask = (
                 bounding_boxes[0, :, CENTER_XYZ_DXDYDZ_PHI.CLASS] > 0.0
@@ -185,15 +204,16 @@
             object_bounding_boxes, min_points_filter
         )
         # [frames, num_valid_boxes, ragged_points]
         points_in_bounding_boxes = tf.ragged.boolean_mask(
             points_in_bounding_boxes, min_points_filter
         )
         # point_clouds: [frames, num_points, point_feature]
-        # object_point_clouds: [frames, num_valid_boxes, ragged_points, point_feature]
+        # object_point_clouds: [frames, num_valid_boxes, ragged_points,
+        #   point_feature]
         object_point_clouds = tf.gather(
             point_clouds, points_in_bounding_boxes, axis=1, batch_dims=1
         )
 
         return (object_point_clouds, object_bounding_boxes)
 
     def _augment(self, inputs):
@@ -218,44 +238,45 @@
             {
                 OBJECT_POINT_CLOUDS: object_point_clouds,
                 OBJECT_BOUNDING_BOXES: object_bounding_boxes,
             }
         )
         return result
 
-    def call(self, inputs, training=True):
-        if training:
-            point_clouds = inputs[POINT_CLOUDS]
-            bounding_boxes = inputs[BOUNDING_BOXES]
-            if point_clouds.shape.rank == 3 and bounding_boxes.shape.rank == 3:
-                return self._augment(inputs)
-            elif point_clouds.shape.rank == 4 and bounding_boxes.shape.rank == 4:
-                batch = point_clouds.get_shape().as_list()[0]
-                object_point_clouds_list = []
-                object_bounding_boxes_list = []
-                for i in range(batch):
-                    (
-                        object_point_clouds,
-                        object_bounding_boxes,
-                    ) = self.augment_point_clouds_bounding_boxes(
-                        inputs[POINT_CLOUDS][i], inputs[BOUNDING_BOXES][i]
-                    )
-                    object_point_clouds_list += [object_point_clouds]
-                    object_bounding_boxes_list += [object_bounding_boxes]
-                # object_point_clouds shape [num of frames, num of valid boxes, max num of points, num of point features].
-                inputs[OBJECT_POINT_CLOUDS] = tf.concat(
-                    object_point_clouds_list, axis=-3
-                )
-                # object_bounding_boxes shape [num of frames, num of valid boxes, num of box features].
-                inputs[OBJECT_BOUNDING_BOXES] = tf.concat(
-                    object_bounding_boxes_list, axis=-2
-                )
-                return inputs
-            else:
-                raise ValueError(
-                    "Point clouds augmentation layers are expecting inputs point clouds and bounding boxes to "
-                    "be rank 3D (Frame, Point, Feature) or 4D (Batch, Frame, Point, Feature) tensors. Got shape: {} and {}".format(
-                        point_clouds.shape, bounding_boxes.shape
-                    )
+    def call(self, inputs):
+        point_clouds = inputs[POINT_CLOUDS]
+        bounding_boxes = inputs[BOUNDING_BOXES]
+        if point_clouds.shape.rank == 3 and bounding_boxes.shape.rank == 3:
+            return self._augment(inputs)
+        elif point_clouds.shape.rank == 4 and bounding_boxes.shape.rank == 4:
+            batch = point_clouds.get_shape().as_list()[0]
+            object_point_clouds_list = []
+            object_bounding_boxes_list = []
+            for i in range(batch):
+                (
+                    object_point_clouds,
+                    object_bounding_boxes,
+                ) = self.augment_point_clouds_bounding_boxes(
+                    inputs[POINT_CLOUDS][i], inputs[BOUNDING_BOXES][i]
                 )
-        else:
+                object_point_clouds_list += [object_point_clouds]
+                object_bounding_boxes_list += [object_bounding_boxes]
+            # object_point_clouds shape [num of frames, num of valid boxes,
+            # max num of points, num of point features].
+            inputs[OBJECT_POINT_CLOUDS] = tf.concat(
+                object_point_clouds_list, axis=-3
+            )
+            # object_bounding_boxes shape [num of frames, num of valid
+            # boxes, num of box features].
+            inputs[OBJECT_BOUNDING_BOXES] = tf.concat(
+                object_bounding_boxes_list, axis=-2
+            )
             return inputs
+        else:
+            raise ValueError(
+                "Point clouds augmentation layers are expecting inputs "
+                "point clouds and bounding boxes to be rank 3D (Frame, "
+                "Point, Feature) or 4D (Batch, Frame, Point, Feature) "
+                "tensors. Got shape: {} and {}".format(
+                    point_clouds.shape, bounding_boxes.shape
+                )
+            )
```

## keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes_test.py

```diff
@@ -32,15 +32,22 @@
     def test_augment_point_clouds_and_bounding_boxes(self):
         add_layer = GroupPointsByBoundingBoxes(
             label_index=1,
             min_points_per_bounding_boxes=1,
             max_points_per_bounding_boxes=2,
         )
         point_clouds = np.array(
-            [[[0, 1, 2, 3, 4], [10, 1, 2, 3, 4], [0, -1, 2, 3, 4], [100, 100, 2, 3, 4]]]
+            [
+                [
+                    [0, 1, 2, 3, 4],
+                    [10, 1, 2, 3, 4],
+                    [0, -1, 2, 3, 4],
+                    [100, 100, 2, 3, 4],
+                ]
+            ]
             * 2
         ).astype("float32")
         bounding_boxes = np.array(
             [
                 [
                     [0, 0, 0, 4, 4, 4, 0, 1],
                     [10, 1, 2, 2, 2, 2, 0, 1],
@@ -52,55 +59,38 @@
         inputs = {
             POINT_CLOUDS: point_clouds,
             BOUNDING_BOXES: bounding_boxes,
             "dummy_item": np.random.uniform(size=(2, 2, 2)),
         }
         outputs = add_layer(inputs)
         object_point_clouds = np.array(
-            [[[[0, 1, 2, 3, 4], [0, -1, 2, 3, 4]], [[10, 1, 2, 3, 4], [0, 0, 0, 0, 0]]]]
+            [
+                [
+                    [[0, 1, 2, 3, 4], [0, -1, 2, 3, 4]],
+                    [[10, 1, 2, 3, 4], [0, 0, 0, 0, 0]],
+                ]
+            ]
             * 2
         ).astype("float32")
         object_bounding_boxes = np.array(
             [[[0, 0, 0, 4, 4, 4, 0, 1], [10, 1, 2, 2, 2, 2, 0, 1]]] * 2
         ).astype("float32")
         self.assertAllClose(inputs[POINT_CLOUDS], outputs[POINT_CLOUDS])
         self.assertAllClose(inputs[BOUNDING_BOXES], outputs[BOUNDING_BOXES])
         self.assertAllClose(inputs["dummy_item"], outputs["dummy_item"])
-        # Sort the point clouds due to the orders of points are different when using Tensorflow and Metal+Tensorflow (MAC).
-        outputs[OBJECT_POINT_CLOUDS] = tf.sort(outputs[OBJECT_POINT_CLOUDS], axis=-2)
+        # Sort the point clouds due to the orders of points are different when
+        # using Tensorflow and Metal+Tensorflow (MAC).
+        outputs[OBJECT_POINT_CLOUDS] = tf.sort(
+            outputs[OBJECT_POINT_CLOUDS], axis=-2
+        )
         object_point_clouds = tf.sort(object_point_clouds, axis=-2)
         self.assertAllClose(outputs[OBJECT_POINT_CLOUDS], object_point_clouds)
-        self.assertAllClose(outputs[OBJECT_BOUNDING_BOXES], object_bounding_boxes)
-
-    def test_not_augment_point_clouds_and_bounding_boxes(self):
-        add_layer = GroupPointsByBoundingBoxes(
-            label_index=1,
-            min_points_per_bounding_boxes=1,
-            max_points_per_bounding_boxes=2,
+        self.assertAllClose(
+            outputs[OBJECT_BOUNDING_BOXES], object_bounding_boxes
         )
-        point_clouds = np.array(
-            [[[0, 1, 2, 3, 4], [10, 1, 2, 3, 4], [0, -1, 2, 3, 4], [100, 100, 2, 3, 4]]]
-            * 2
-        ).astype("float32")
-        bounding_boxes = np.array(
-            [
-                [
-                    [0, 0, 0, 4, 4, 4, 0, 1],
-                    [10, 1, 2, 2, 2, 2, 0, 1],
-                    [20, 20, 20, 1, 1, 1, 0, 1],
-                ]
-            ]
-            * 2
-        ).astype("float32")
-        inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
-        outputs = add_layer(inputs, training=False)
-        self.assertAllClose(inputs[POINT_CLOUDS], outputs[POINT_CLOUDS])
-        self.assertAllClose(inputs[BOUNDING_BOXES], outputs[BOUNDING_BOXES])
-        self.assertIsNone(outputs.get(OBJECT_POINT_CLOUDS, None))
-        self.assertIsNone(outputs.get(OBJECT_BOUNDING_BOXES, None))
 
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = GroupPointsByBoundingBoxes(
             label_index=1,
             min_points_per_bounding_boxes=1,
             max_points_per_bounding_boxes=2,
         )
@@ -144,32 +134,45 @@
             * 2
         ).astype("float32")
         object_bounding_boxes = np.array(
             [[[0, 0, 0, 4, 4, 4, 0, 1], [10, 1, 2, 2, 2, 2, 0, 1]] * 3] * 2
         ).astype("float32")
         self.assertAllClose(inputs[POINT_CLOUDS], outputs[POINT_CLOUDS])
         self.assertAllClose(inputs[BOUNDING_BOXES], outputs[BOUNDING_BOXES])
-        # Sort the point clouds due to the orders of points are different when using Tensorflow and Metal+Tensorflow (MAC).
-        outputs[OBJECT_POINT_CLOUDS] = tf.sort(outputs[OBJECT_POINT_CLOUDS], axis=-2)
+        # Sort the point clouds due to the orders of points are different when
+        # using Tensorflow and Metal+Tensorflow (MAC).
+        outputs[OBJECT_POINT_CLOUDS] = tf.sort(
+            outputs[OBJECT_POINT_CLOUDS], axis=-2
+        )
         object_point_clouds = tf.sort(object_point_clouds, axis=-2)
         self.assertAllClose(outputs[OBJECT_POINT_CLOUDS], object_point_clouds)
-        self.assertAllClose(outputs[OBJECT_BOUNDING_BOXES], object_bounding_boxes)
+        self.assertAllClose(
+            outputs[OBJECT_BOUNDING_BOXES], object_bounding_boxes
+        )
 
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def test_augment_point_clouds_and_bounding_boxes_v2(self):
         add_layer = GroupPointsByBoundingBoxes(
             label_index=1,
             min_points_per_bounding_boxes=1,
             max_points_per_bounding_boxes=2,
         )
         point_clouds = np.array(
-            [[[0, 1, 2, 3, 4], [10, 1, 2, 3, 4], [0, -1, 2, 3, 4], [100, 100, 2, 3, 4]]]
+            [
+                [
+                    [0, 1, 2, 3, 4],
+                    [10, 1, 2, 3, 4],
+                    [0, -1, 2, 3, 4],
+                    [100, 100, 2, 3, 4],
+                ]
+            ]
             * 2
         ).astype("float32")
         bounding_boxes = np.array(
             [
                 [
                     [0, 0, 0, 4, 4, 4, 0, 1],
                     [10, 1, 2, 2, 2, 2, 0, 1],
@@ -181,15 +184,20 @@
         point_clouds = tf.convert_to_tensor(point_clouds)
         bounding_boxes = tf.convert_to_tensor(bounding_boxes)
         outputs = add_layer.augment_point_clouds_bounding_boxes_v2(
             point_clouds=point_clouds, bounding_boxes=bounding_boxes
         )
         object_point_clouds, object_bounding_boxes = outputs[0], outputs[1]
         expected_object_point_clouds = np.array(
-            [[[[0, 1, 2, 3, 4], [0, -1, 2, 3, 4]], [[10, 1, 2, 3, 4], [0, 0, 0, 0, 0]]]]
+            [
+                [
+                    [[0, 1, 2, 3, 4], [0, -1, 2, 3, 4]],
+                    [[10, 1, 2, 3, 4], [0, 0, 0, 0, 0]],
+                ]
+            ]
             * 2
         ).astype("float32")
         expected_object_bounding_boxes = np.array(
             [[[0, 0, 0, 4, 4, 4, 0, 1], [10, 1, 2, 2, 2, 2, 0, 1]]] * 2
         ).astype("float32")
         self.assertAllClose(
             expected_object_point_clouds, object_point_clouds.to_tensor()
```

## keras_cv/layers/preprocessing_3d/random_copy_paste.py

```diff
@@ -9,57 +9,67 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 from keras_cv.ops import iou_3d
-from keras_cv.ops.point_cloud import is_within_any_box3d
+from keras_cv.point_cloud import is_within_any_box3d
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 OBJECT_POINT_CLOUDS = base_augmentation_layer_3d.OBJECT_POINT_CLOUDS
 OBJECT_BOUNDING_BOXES = base_augmentation_layer_3d.OBJECT_BOUNDING_BOXES
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomCopyPaste(base_augmentation_layer_3d.BaseAugmentationLayer3D):
-    """A preprocessing layer which randomly pastes object point clouds and bounding boxes during training.
+    """A preprocessing layer which randomly pastes object point clouds and
+    bounding boxes during training.
 
     This layer will randomly paste object point clouds and bounding boxes.
-    OBJECT_POINT_CLOUDS and OBJECT_BOUNDING_BOXES are generated by running group_points_by_bounding_boxes function on additional input frames.
-    We use the first frame to check overlap between existing bounding boxes and pasted bounding boxes
-    If a to-be-pasted bounding box overlaps with an existing bounding box and object point clouds, we do not paste the additional bounding box.
-    We load 5 times max_paste_bounding_boxes to check overlap.
-    If a to-be-pasted bounding box overlaps with existing background point clouds, we paste the additional bounding box and replace the
-    background point clouds with object point clouds.
-    During inference time, the output will be identical to input. Call the layer with `training=True` to paste bounding boxes.
+    OBJECT_POINT_CLOUDS and OBJECT_BOUNDING_BOXES are generated by running
+    group_points_by_bounding_boxes function on additional input frames. We use
+    the first frame to check overlap between existing bounding boxes and pasted
+    bounding boxes.
+    If a to-be-pasted bounding box overlaps with an existing bounding box and
+    object point clouds, we do not paste the additional bounding box. We load 5
+    times max_paste_bounding_boxes to check overlap.
+    If a to-be-pasted bounding box overlaps with existing background point
+    clouds, we paste the additional bounding box and replace the background
+    point clouds with object point clouds.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
-        [num of frames, num of boxes, num of box features].  Boxes are expected
+        [num of frames, num of boxes, num of box features]. Boxes are expected
         to follow the CENTER_XYZ_DXDYDZ_PHI format. Refer to
         https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box_3d/formats.py
 
     Output shape:
-      A tuple of two Tensors (point_clouds, bounding_boxes) with the same shape as input Tensors.
+      A tuple of two Tensors (point_clouds, bounding_boxes) with the same shape
+      as input Tensors.
 
     Arguments:
       label_index: An optional int scalar sets the target object index.
-        Bounding boxes and corresponding point clouds with box class == label_index will be saved as OBJECT_BOUNDING_BOXES and OBJECT_POINT_CLOUDS.
-        If label index is None, all valid bounding boxes (box class !=0) are used.
-      min_paste_bounding_boxes: A int scalar sets the min number of pasted bounding boxes.
-      max_paste_bounding_boxes: A int scalar sets the max number of pasted bounding boxes.
+        Bounding boxes and corresponding point clouds with box class ==
+        label_index will be saved as OBJECT_BOUNDING_BOXES and
+        OBJECT_POINT_CLOUDS. If label index is None, all valid bounding boxes
+        (box class !=0) are used.
+      min_paste_bounding_boxes: A int scalar sets the min number of pasted
+        bounding boxes.
+      max_paste_bounding_boxes: A int scalar sets the max number of pasted
+        bounding boxes.
 
     """
 
     def __init__(
         self,
         label_index=None,
         min_paste_bounding_boxes=0,
@@ -99,57 +109,72 @@
     ):
         del point_clouds
         num_paste_bounding_boxes = self._random_generator.random_uniform(
             (),
             minval=self._min_paste_bounding_boxes,
             maxval=self._max_paste_bounding_boxes,
         )
-        num_paste_bounding_boxes = tf.cast(num_paste_bounding_boxes, dtype=tf.int32)
+        num_paste_bounding_boxes = tf.cast(
+            num_paste_bounding_boxes, dtype=tf.int32
+        )
         num_existing_bounding_boxes = tf.shape(bounding_boxes)[1]
         if self._label_index:
             object_mask = (
                 object_bounding_boxes[0, :, CENTER_XYZ_DXDYDZ_PHI.CLASS]
                 == self._label_index
             )
             object_point_clouds = tf.boolean_mask(
                 object_point_clouds, object_mask, axis=1
             )
             object_bounding_boxes = tf.boolean_mask(
                 object_bounding_boxes, object_mask, axis=1
             )
         shuffle_index = tf.range(tf.shape(object_point_clouds)[1])
         shuffle_index = tf.random.shuffle(shuffle_index)
-        object_point_clouds = tf.gather(object_point_clouds, shuffle_index, axis=1)
-        object_bounding_boxes = tf.gather(object_bounding_boxes, shuffle_index, axis=1)
+        object_point_clouds = tf.gather(
+            object_point_clouds, shuffle_index, axis=1
+        )
+        object_bounding_boxes = tf.gather(
+            object_bounding_boxes, shuffle_index, axis=1
+        )
 
         # Load at most 5 times num_paste_bounding_boxes to check overlaps.
         num_compare_bounding_boxes = tf.math.minimum(
             num_paste_bounding_boxes * 5,
             tf.shape(object_point_clouds)[1],
         )
 
-        object_point_clouds = object_point_clouds[:, :num_compare_bounding_boxes, :]
-        object_bounding_boxes = object_bounding_boxes[:, :num_compare_bounding_boxes, :]
-        # Use the current frame to check overlap between existing bounding boxes and pasted bounding boxes
-        all_bounding_boxes = tf.concat([bounding_boxes, object_bounding_boxes], axis=1)[
-            0, :, :7
+        object_point_clouds = object_point_clouds[
+            :, :num_compare_bounding_boxes, :
+        ]
+        object_bounding_boxes = object_bounding_boxes[
+            :, :num_compare_bounding_boxes, :
         ]
+        # Use the current frame to check overlap between existing bounding boxes
+        # and pasted bounding boxes
+        all_bounding_boxes = tf.concat(
+            [bounding_boxes, object_bounding_boxes], axis=1
+        )[0, :, :7]
         iou = iou_3d(all_bounding_boxes, all_bounding_boxes)
         iou = tf.linalg.band_part(iou, -1, 0)
         iou_sum = tf.reduce_sum(iou[num_existing_bounding_boxes:], axis=1)
         # A non overlapping bounding box has a 1.0 IoU with itself.
         non_overlapping_mask = tf.reshape(iou_sum <= 1, [-1])
         object_point_clouds = tf.boolean_mask(
             object_point_clouds, non_overlapping_mask, axis=1
         )
         object_bounding_boxes = tf.boolean_mask(
             object_bounding_boxes, non_overlapping_mask, axis=1
         )
-        object_point_clouds = object_point_clouds[:, :num_paste_bounding_boxes, :]
-        object_bounding_boxes = object_bounding_boxes[:, :num_paste_bounding_boxes, :]
+        object_point_clouds = object_point_clouds[
+            :, :num_paste_bounding_boxes, :
+        ]
+        object_bounding_boxes = object_bounding_boxes[
+            :, :num_paste_bounding_boxes, :
+        ]
         return {
             OBJECT_POINT_CLOUDS: object_point_clouds,
             OBJECT_BOUNDING_BOXES: object_bounding_boxes,
         }
 
     def augment_point_clouds_bounding_boxes(
         self, point_clouds, bounding_boxes, transformation, **kwargs
@@ -182,29 +207,32 @@
             point_clouds_list += [
                 tf.concat([paste_point_clouds, existing_point_clouds], axis=0)
             ]
 
             existing_bounding_boxes = tf.boolean_mask(
                 bounding_boxes[frame_index],
                 tf.math.greater(
-                    bounding_boxes[frame_index, :, CENTER_XYZ_DXDYDZ_PHI.CLASS], 0.0
+                    bounding_boxes[frame_index, :, CENTER_XYZ_DXDYDZ_PHI.CLASS],
+                    0.0,
                 ),
             )
             paste_bounding_boxes = tf.boolean_mask(
                 additional_object_bounding_boxes[frame_index],
                 tf.math.greater(
                     additional_object_bounding_boxes[
                         frame_index, :, CENTER_XYZ_DXDYDZ_PHI.CLASS
                     ],
                     0.0,
                 ),
                 axis=0,
             )
             bounding_boxes_list += [
-                tf.concat([paste_bounding_boxes, existing_bounding_boxes], axis=0)
+                tf.concat(
+                    [paste_bounding_boxes, existing_bounding_boxes], axis=0
+                )
             ]
 
         point_clouds = tf.ragged.stack(point_clouds_list)
         bounding_boxes = tf.ragged.stack(bounding_boxes_list)
 
         return (
             point_clouds.to_tensor(shape=original_point_clouds_shape),
@@ -224,47 +252,48 @@
             object_bounding_boxes=object_bounding_boxes,
         )
         point_clouds, bounding_boxes = self.augment_point_clouds_bounding_boxes(
             point_clouds,
             bounding_boxes=bounding_boxes,
             transformation=transformation,
         )
-        result.update({POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes})
+        result.update(
+            {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
+        )
         return result
 
-    def call(self, inputs, training=True):
-        if training:
-            point_clouds = inputs[POINT_CLOUDS]
-            bounding_boxes = inputs[BOUNDING_BOXES]
-            if point_clouds.shape.rank == 3 and bounding_boxes.shape.rank == 3:
-                return self._augment(inputs)
-            elif point_clouds.shape.rank == 4 and bounding_boxes.shape.rank == 4:
-                batch = point_clouds.get_shape().as_list()[0]
-                point_clouds_list = []
-                bounding_boxes_list = []
-                for i in range(batch):
-                    no_batch_inputs = {
-                        POINT_CLOUDS: inputs[POINT_CLOUDS][i],
-                        BOUNDING_BOXES: inputs[BOUNDING_BOXES][i],
-                        OBJECT_POINT_CLOUDS: inputs[OBJECT_POINT_CLOUDS][i],
-                        OBJECT_BOUNDING_BOXES: inputs[OBJECT_BOUNDING_BOXES][i],
-                    }
-                    no_batch_result = self._augment(no_batch_inputs)
-                    point_clouds_list += [
-                        no_batch_result[POINT_CLOUDS][tf.newaxis, ...]
-                    ]
-                    bounding_boxes_list += [
-                        no_batch_result[BOUNDING_BOXES][tf.newaxis, ...]
-                    ]
-
-                inputs[POINT_CLOUDS] = tf.concat(point_clouds_list, axis=0)
-                inputs[BOUNDING_BOXES] = tf.concat(bounding_boxes_list, axis=0)
-                return inputs
-            else:
-                raise ValueError(
-                    "Point clouds augmentation layers are expecting inputs point clouds and bounding boxes to "
-                    "be rank 3D (Frame, Point, Feature) or 4D (Batch, Frame, Point, Feature) tensors. Got shape: {} and {}".format(
-                        point_clouds.shape, bounding_boxes.shape
-                    )
-                )
-        else:
+    def call(self, inputs):
+        point_clouds = inputs[POINT_CLOUDS]
+        bounding_boxes = inputs[BOUNDING_BOXES]
+        if point_clouds.shape.rank == 3 and bounding_boxes.shape.rank == 3:
+            return self._augment(inputs)
+        elif point_clouds.shape.rank == 4 and bounding_boxes.shape.rank == 4:
+            batch = point_clouds.get_shape().as_list()[0]
+            point_clouds_list = []
+            bounding_boxes_list = []
+            for i in range(batch):
+                no_batch_inputs = {
+                    POINT_CLOUDS: inputs[POINT_CLOUDS][i],
+                    BOUNDING_BOXES: inputs[BOUNDING_BOXES][i],
+                    OBJECT_POINT_CLOUDS: inputs[OBJECT_POINT_CLOUDS][i],
+                    OBJECT_BOUNDING_BOXES: inputs[OBJECT_BOUNDING_BOXES][i],
+                }
+                no_batch_result = self._augment(no_batch_inputs)
+                point_clouds_list += [
+                    no_batch_result[POINT_CLOUDS][tf.newaxis, ...]
+                ]
+                bounding_boxes_list += [
+                    no_batch_result[BOUNDING_BOXES][tf.newaxis, ...]
+                ]
+
+            inputs[POINT_CLOUDS] = tf.concat(point_clouds_list, axis=0)
+            inputs[BOUNDING_BOXES] = tf.concat(bounding_boxes_list, axis=0)
             return inputs
+        else:
+            raise ValueError(
+                "Point clouds augmentation layers are expecting inputs "
+                "point clouds and bounding boxes to be rank 3D (Frame, "
+                "Point, Feature) or 4D (Batch, Frame, Point, Feature) "
+                "tensors. Got shape: {} and {}".format(
+                    point_clouds.shape, bounding_boxes.shape
+                )
+            )
```

## keras_cv/layers/preprocessing_3d/random_copy_paste_test.py

```diff
@@ -24,15 +24,16 @@
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 OBJECT_POINT_CLOUDS = base_augmentation_layer_3d.OBJECT_POINT_CLOUDS
 OBJECT_BOUNDING_BOXES = base_augmentation_layer_3d.OBJECT_BOUNDING_BOXES
 
 
 class RandomCopyPasteTest(tf.test.TestCase):
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def test_augment_point_clouds_and_bounding_boxes(self):
         add_layer = RandomCopyPaste(
             label_index=1,
             min_paste_bounding_boxes=1,
             max_paste_bounding_boxes=1,
@@ -87,18 +88,18 @@
         inputs = {
             POINT_CLOUDS: point_clouds,
             BOUNDING_BOXES: bounding_boxes,
             OBJECT_POINT_CLOUDS: object_point_clouds,
             OBJECT_BOUNDING_BOXES: object_bounding_boxes,
         }
         outputs = add_layer(inputs)
-        # The first object bounding box [0, 0, 1, 4, 4, 4, 0, 1] overlaps with existing bounding
-        # box [0, 0, 0, 4, 4, 4, 0, 1], thus not used.
-        # The second object bounding box [100, 100, 2, 5, 5, 5, 0, 1] and object point clouds
-        # [100, 101, 2, 3, 4] are pasted.
+        # The first object bounding box [0, 0, 1, 4, 4, 4, 0, 1] overlaps with
+        # existing bounding box [0, 0, 0, 4, 4, 4, 0, 1], thus not used.
+        # The second object bounding box [100, 100, 2, 5, 5, 5, 0, 1] and object
+        # point clouds [100, 101, 2, 3, 4] are pasted.
         augmented_point_clouds = np.array(
             [
                 [
                     [100, 101, 2, 3, 4],
                     [0, 1, 2, 3, 4],
                     [10, 1, 2, 3, 4],
                     [0, -1, 2, 3, 4],
@@ -114,23 +115,26 @@
                     [0, 0, 0, 4, 4, 4, 0, 1],
                     [20, 20, 20, 1, 1, 1, 0, 1],
                     [0, 0, 0, 0, 0, 0, 0, 0],
                 ]
             ]
             * 2
         ).astype("float32")
-        self.assertAllClose(inputs[OBJECT_POINT_CLOUDS], outputs[OBJECT_POINT_CLOUDS])
+        self.assertAllClose(
+            inputs[OBJECT_POINT_CLOUDS], outputs[OBJECT_POINT_CLOUDS]
+        )
         self.assertAllClose(
             inputs[OBJECT_BOUNDING_BOXES], outputs[OBJECT_BOUNDING_BOXES]
         )
         self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
         self.assertAllClose(outputs[BOUNDING_BOXES], augmented_bounding_boxes)
 
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = RandomCopyPaste(
             label_index=1,
             min_paste_bounding_boxes=1,
             max_paste_bounding_boxes=1,
@@ -191,18 +195,18 @@
         inputs = {
             POINT_CLOUDS: point_clouds,
             BOUNDING_BOXES: bounding_boxes,
             OBJECT_POINT_CLOUDS: object_point_clouds,
             OBJECT_BOUNDING_BOXES: object_bounding_boxes,
         }
         outputs = add_layer(inputs)
-        # The first object bounding box [0, 0, 1, 4, 4, 4, 0, 1] overlaps with existing bounding
-        # box [0, 0, 0, 4, 4, 4, 0, 1], thus not used.
-        # The second object bounding box [100, 100, 2, 5, 5, 5, 0, 1] and object point clouds
-        # [100, 101, 2, 3, 4] are pasted.
+        # The first object bounding box [0, 0, 1, 4, 4, 4, 0, 1] overlaps with
+        # existing bounding box [0, 0, 0, 4, 4, 4, 0, 1], thus not used.
+        # The second object bounding box [100, 100, 2, 5, 5, 5, 0, 1] and object
+        # point clouds [100, 101, 2, 3, 4] are pasted.
         augmented_point_clouds = np.array(
             [
                 [
                     [
                         [100, 101, 2, 3, 4],
                         [0, 1, 2, 3, 4],
                         [10, 1, 2, 3, 4],
@@ -224,13 +228,15 @@
                         [0, 0, 0, 0, 0, 0, 0, 0],
                     ]
                 ]
                 * 2
             ]
             * 3
         ).astype("float32")
-        self.assertAllClose(inputs[OBJECT_POINT_CLOUDS], outputs[OBJECT_POINT_CLOUDS])
+        self.assertAllClose(
+            inputs[OBJECT_POINT_CLOUDS], outputs[OBJECT_POINT_CLOUDS]
+        )
         self.assertAllClose(
             inputs[OBJECT_BOUNDING_BOXES], outputs[OBJECT_BOUNDING_BOXES]
         )
         self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
         self.assertAllClose(outputs[BOUNDING_BOXES], augmented_bounding_boxes)
```

## keras_cv/layers/preprocessing_3d/random_drop_box.py

```diff
@@ -9,51 +9,57 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.ops.point_cloud import is_within_any_box3d
+from keras_cv.point_cloud import is_within_any_box3d
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 BOX_LABEL_INDEX = base_augmentation_layer_3d.BOX_LABEL_INDEX
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RandomDropBox(base_augmentation_layer_3d.BaseAugmentationLayer3D):
-    """A preprocessing layer which randomly drops object bounding boxes and points during training.
+    """A preprocessing layer which randomly drops object bounding boxes and
+    points during training.
 
-    This layer will randomly drop object point clouds and bounding boxes. Number of dropped bounding boxes
-    is sampled uniformly sampled between 0 and max_drop_bounding_boxes. If label_index is set, only bounding boxes with
-    box class == label_index will be sampled and dropped; otherwise, all valid bounding boxes (box class > 0) will be sampled and dropped.
-
-    During inference time, the output will be identical to input. Call the layer with `training=True` to drop object bounding boxes and points.
+    This layer will randomly drop object point clouds and bounding boxes. Number
+    of dropped bounding boxes is sampled uniformly sampled between 0 and
+    max_drop_bounding_boxes. If label_index is set, only bounding boxes with box
+    class == label_index will be sampled and dropped; otherwise, all valid
+    bounding boxes (box class > 0) will be sampled and dropped.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features].
         The first 8 features are [x, y, z, dx, dy, dz, phi, box class].
 
 
     Output shape:
-      A tuple of two Tensors (point_clouds, bounding_boxes) with the same shape as input Tensors.
+      A tuple of two Tensors (point_clouds, bounding_boxes) with the same shape
+      as input Tensors.
 
     Arguments:
-      max_drop_bounding_boxes: A int non negative scalar sets the maximum number of dropped bounding boxes.
-        Do not drop any bounding boxe when max_drop_bounding_boxes = 0.
+      max_drop_bounding_boxes: A int non-negative scalar sets the maximum number
+        of dropped bounding boxes. Do not drop any bounding boxes when
+        max_drop_bounding_boxes = 0.
       label_index: An optional int scalar sets the target object index.
-        If label index is set, randomly drop bounding boxes, where box class == label_index.
-        If label index is None, randomly drop bounding boxes, where box class > 0.
+        If label index is set, randomly drop bounding boxes, where box
+        class == label_index.
+        If label index is None, randomly drop bounding boxes, where box
+        class > 0.
 
     """
 
     def __init__(self, max_drop_bounding_boxes, label_index=None, **kwargs):
         super().__init__(**kwargs)
         self.auto_vectorize = False
         if label_index and label_index < 0:
@@ -82,15 +88,17 @@
                 bounding_boxes[0, :, BOX_LABEL_INDEX], 0
             )
         max_drop_bounding_boxes = tf.random.uniform(
             (), maxval=self._max_drop_bounding_boxes, dtype=tf.int32
         )
         # Randomly remove max_drop_bounding_boxes number of bounding boxes.
         num_bounding_boxes = bounding_boxes.get_shape().as_list()[1]
-        random_scores_for_selected_boxes = tf.random.uniform(shape=[num_bounding_boxes])
+        random_scores_for_selected_boxes = tf.random.uniform(
+            shape=[num_bounding_boxes]
+        )
         random_scores_for_selected_boxes = tf.where(
             selected_boxes_mask, random_scores_for_selected_boxes, 0.0
         )
         topk, _ = tf.math.top_k(
             random_scores_for_selected_boxes, k=max_drop_bounding_boxes + 1
         )
         drop_bounding_boxes_mask = tf.math.greater(
```

## keras_cv/layers/preprocessing_3d/random_drop_box_test.py

```diff
@@ -7,29 +7,31 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import numpy as np
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 from keras_cv.layers.preprocessing_3d.random_drop_box import RandomDropBox
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 ADDITIONAL_POINT_CLOUDS = base_augmentation_layer_3d.ADDITIONAL_POINT_CLOUDS
 ADDITIONAL_BOUNDING_BOXES = base_augmentation_layer_3d.ADDITIONAL_BOUNDING_BOXES
 
 
 class RandomDropBoxTest(tf.test.TestCase):
     def test_drop_class1_box_point_clouds_and_bounding_boxes(self):
-        tf.keras.utils.set_random_seed(2)
+        keras.utils.set_random_seed(2)
         add_layer = RandomDropBox(label_index=1, max_drop_bounding_boxes=4)
         # point_clouds: 3D (multi frames) float32 Tensor with shape
         # [num of frames, num of points, num of point features].
         # The first 5 features are [x, y, z, class, range].
         point_clouds = np.array(
             [
                 [
@@ -59,15 +61,16 @@
         ).astype("float32")
 
         inputs = {
             POINT_CLOUDS: point_clouds,
             BOUNDING_BOXES: bounding_boxes,
         }
         outputs = add_layer(inputs)
-        # Drop the first object bounding box [0, 0, 0, 4, 4, 4, 0, 1] and points.
+        # Drop the first object bounding box [0, 0, 0, 4, 4, 4, 0, 1] and
+        # points.
         augmented_point_clouds = np.array(
             [
                 [
                     [0, 0, 0, 0, 0],
                     [0, 0, 0, 0, 0],
                     [10, 1, 2, 3, 4],
                     [0, 0, 0, 0, 0],
@@ -88,15 +91,15 @@
             ]
             * 2
         ).astype("float32")
         self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
         self.assertAllClose(outputs[BOUNDING_BOXES], augmented_bounding_boxes)
 
     def test_drop_both_boxes_point_clouds_and_bounding_boxes(self):
-        tf.keras.utils.set_random_seed(2)
+        keras.utils.set_random_seed(2)
         add_layer = RandomDropBox(max_drop_bounding_boxes=4)
         # point_clouds: 3D (multi frames) float32 Tensor with shape
         # [num of frames, num of points, num of point features].
         # The first 5 features are [x, y, z, class, range].
         point_clouds = np.array(
             [
                 [
@@ -155,15 +158,15 @@
             ]
             * 2
         ).astype("float32")
         self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
         self.assertAllClose(outputs[BOUNDING_BOXES], augmented_bounding_boxes)
 
     def test_not_drop_any_box_point_clouds_and_bounding_boxes(self):
-        tf.keras.utils.set_random_seed(2)
+        keras.utils.set_random_seed(2)
         add_layer = RandomDropBox(max_drop_bounding_boxes=0)
         # point_clouds: 3D (multi frames) float32 Tensor with shape
         # [num of frames, num of points, num of point features].
         # The first 5 features are [x, y, z, class, range].
         point_clouds = np.array(
             [
                 [
@@ -222,15 +225,15 @@
             ]
             * 2
         ).astype("float32")
         self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
         self.assertAllClose(outputs[BOUNDING_BOXES], augmented_bounding_boxes)
 
     def test_batch_drop_one_of_the_box_point_clouds_and_bounding_boxes(self):
-        tf.keras.utils.set_random_seed(4)
+        keras.utils.set_random_seed(4)
         add_layer = RandomDropBox(max_drop_bounding_boxes=2)
         # point_clouds: 3D (multi frames) float32 Tensor with shape
         # [num of frames, num of points, num of point features].
         # The first 5 features are [x, y, z, class, range].
         point_clouds = np.array(
             [
                 [
@@ -266,16 +269,18 @@
         ).astype("float32")
 
         inputs = {
             POINT_CLOUDS: point_clouds,
             BOUNDING_BOXES: bounding_boxes,
         }
         outputs = add_layer(inputs)
-        # Batch 0: drop the first bounding box [0, 0, 0, 4, 4, 4, 0, 1] and points,
-        # Batch 1,2: drop the second bounding box [20, 20, 20, 3, 3, 3, 0, 2] and points,
+        # Batch 0: drop the first bounding box [0, 0, 0, 4, 4, 4, 0, 1] and
+        #       points,
+        # Batch 1,2: drop the second bounding box [20, 20, 20, 3, 3, 3, 0, 2]
+        #       and points,
         augmented_point_clouds = np.array(
             [
                 [
                     [
                         [0, 0, 0, 0, 0],
                         [0, 0, 0, 0, 0],
                         [10, 1, 2, 3, 4],
```

## keras_cv/layers/preprocessing_3d/swap_background.py

```diff
@@ -9,51 +9,54 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.ops.point_cloud import is_within_any_box3d
+from keras_cv.point_cloud import is_within_any_box3d
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 ADDITIONAL_POINT_CLOUDS = base_augmentation_layer_3d.ADDITIONAL_POINT_CLOUDS
 ADDITIONAL_BOUNDING_BOXES = base_augmentation_layer_3d.ADDITIONAL_BOUNDING_BOXES
 POINTCLOUD_LABEL_INDEX = base_augmentation_layer_3d.POINTCLOUD_LABEL_INDEX
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class SwapBackground(base_augmentation_layer_3d.BaseAugmentationLayer3D):
-    """A preprocessing layer which swaps the backgrounds of two scenes during training.
+    """A preprocessing layer which swaps the backgrounds of two scenes during
+    training.
 
-    This layer will extract object point clouds and bounding boxes from an additional scene and paste it on to the training
-    scene while removing the objects in the training scene.
-    First, removing all the objects point clouds and bounding boxes in the training scene.
-    Second, extracting object point clouds and bounding boxes from an additional scene.
-    Third, removing backgrounds points clouds in the training scene that overlap with the additional object bounding boxes.
-    Last, pasting the additional object point clouds and bounding boxes to the training background scene.
-
-    During inference time, the output will be identical to input. Call the layer with `training=True` to swap backgrounds between two scenes.
+    This layer will extract object point clouds and bounding boxes from an
+    additional scene and paste it on to the training scene while removing the
+    objects in the training scene. First, removing all the objects point clouds
+    and bounding boxes in the training scene. Second, extracting object point
+    clouds and bounding boxes from an additional scene. Third, removing
+    backgrounds points clouds in the training scene that overlap with the
+    additional object bounding boxes. Last, pasting the additional object point
+    clouds and bounding boxes to the training background scene.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
         The first 5 features are [x, y, z, class, range].
       bounding_boxes: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of boxes, num of box features]. Boxes are expected
         to follow the CENTER_XYZ_DXDYDZ_PHI format. Refer to
         https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box_3d/formats.py
         for more details on supported bounding box formats.
 
     Output shape:
-      A tuple of two Tensors (point_clouds, bounding_boxes) with the same shape as input Tensors.
+      A tuple of two Tensors (point_clouds, bounding_boxes) with the same shape
+      as input Tensors.
 
     """
 
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.auto_vectorize = False
 
@@ -64,15 +67,16 @@
         self,
         point_clouds,
         bounding_boxes,
         additional_point_clouds,
         additional_bounding_boxes,
         **kwargs
     ):
-        # Use the current frame bounding boxes to determine valid bounding boxes.
+        # Use the current frame bounding boxes to determine valid bounding
+        # boxes.
         bounding_boxes = tf.boolean_mask(
             bounding_boxes,
             bounding_boxes[0, :, CENTER_XYZ_DXDYDZ_PHI.CLASS] > 0,
             axis=1,
         )
         additional_bounding_boxes = tf.boolean_mask(
             additional_bounding_boxes,
@@ -82,27 +86,32 @@
 
         # Remove objects in point_clouds.
         objects_points_in_point_clouds = is_within_any_box3d(
             point_clouds[..., :3],
             bounding_boxes[..., : CENTER_XYZ_DXDYDZ_PHI.CLASS],
             keepdims=True,
         )
-        point_clouds = tf.where(~objects_points_in_point_clouds, point_clouds, 0.0)
+        point_clouds = tf.where(
+            ~objects_points_in_point_clouds, point_clouds, 0.0
+        )
 
         # Extract objects from additional_point_clouds.
         objects_points_in_additional_point_clouds = is_within_any_box3d(
             additional_point_clouds[..., :3],
             additional_bounding_boxes[..., : CENTER_XYZ_DXDYDZ_PHI.CLASS],
             keepdims=True,
         )
         additional_point_clouds = tf.where(
-            objects_points_in_additional_point_clouds, additional_point_clouds, 0.0
+            objects_points_in_additional_point_clouds,
+            additional_point_clouds,
+            0.0,
         )
 
-        # Remove backgorund points in point_clouds overlaps with additional_bounding_boxes.
+        # Remove background points in point_clouds overlaps with
+        # additional_bounding_boxes.
         points_overlaps_additional_bounding_boxes = is_within_any_box3d(
             point_clouds[..., :3],
             additional_bounding_boxes[..., : CENTER_XYZ_DXDYDZ_PHI.CLASS],
             keepdims=True,
         )
         point_clouds = tf.where(
             ~points_overlaps_additional_bounding_boxes, point_clouds, 0.0
@@ -126,20 +135,23 @@
             background_point_clouds = tf.boolean_mask(
                 point_clouds[frame_index],
                 point_clouds[frame_index, :, POINTCLOUD_LABEL_INDEX] > 0,
                 axis=0,
             )
             object_point_clouds = tf.boolean_mask(
                 additional_point_clouds[frame_index],
-                additional_point_clouds[frame_index, :, POINTCLOUD_LABEL_INDEX] > 0,
+                additional_point_clouds[frame_index, :, POINTCLOUD_LABEL_INDEX]
+                > 0,
                 axis=0,
             )
 
             point_clouds_list += [
-                tf.concat([object_point_clouds, background_point_clouds], axis=0)
+                tf.concat(
+                    [object_point_clouds, background_point_clouds], axis=0
+                )
             ]
 
         point_clouds = tf.ragged.stack(point_clouds_list)
         bounding_boxes = tf.RaggedTensor.from_tensor(
             transformation[ADDITIONAL_BOUNDING_BOXES]
         )
         return (
@@ -161,9 +173,11 @@
             additional_bounding_boxes=additional_bounding_boxes,
         )
         point_clouds, bounding_boxes = self.augment_point_clouds_bounding_boxes(
             point_clouds,
             bounding_boxes=bounding_boxes,
             transformation=transformation,
         )
-        result.update({POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes})
+        result.update(
+            {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
+        )
         return result
```

## keras_cv/layers/preprocessing_3d/swap_background_test.py

```diff
@@ -82,25 +82,35 @@
             POINT_CLOUDS: point_clouds,
             BOUNDING_BOXES: bounding_boxes,
             ADDITIONAL_POINT_CLOUDS: additional_point_clouds,
             ADDITIONAL_BOUNDING_BOXES: additional_bounding_boxes,
         }
         outputs = add_layer(inputs)
         # The following points in additional_point_clouds.
-        # [0, 2, 1, 3, 4], -> kept because it is in additional_point_clouds [0, 0, 1, 4, 4, 4, 0, 1].
-        # [0, 0, 2, 0, 2] -> removed because it is a background point (not in any bounding_boxes and additional_point_clouds).
-        # [0, 11, 2, 3, 4] -> removed because it is a background point (not in any bounding_boxes and additional_point_clouds).
-        # [100, 101, 2, 3, 4] -> kept because it is in additional_point_clouds [100, 100, 2, 5, 5, 5, 0, 1].
-        # [10, 10, 10, 10, 10] -> removed because it is a background point (not in any bounding_boxes and additional_point_clouds).
+        # [0, 2, 1, 3, 4], -> kept because it is in additional_point_clouds
+        #               [0, 0, 1, 4, 4, 4, 0, 1].
+        # [0, 0, 2, 0, 2] -> removed because it is a background point (not in
+        #               any bounding_boxes and additional_point_clouds).
+        # [0, 11, 2, 3, 4] -> removed because it is a background point (not in
+        #               any bounding_boxes and additional_point_clouds).
+        # [100, 101, 2, 3, 4] -> kept because it is in additional_point_clouds
+        #               [100, 100, 2, 5, 5, 5, 0, 1].
+        # [10, 10, 10, 10, 10] -> removed because it is a background point (not
+        #               in any bounding_boxes and additional_point_clouds).
         # The following points in point_clouds.
-        # [0, 1, 2, 3, 4] -> removed because it is in bounding_boxes [0, 0, 0, 4, 4, 4, 0, 1].
-        # [10, 1, 2, 3, 4] -> kept because it is a background point (not in any bounding_boxes and additional_point_clouds).
-        # [0, -1, 2, 3, 4] -> removed becuase it overlaps with additional_bounding_boxes [0, 0, 1, 4, 4, 4, 0, 1].
-        # [100, 100, 2, 3, 4] -> removed becuase it overlaps with additional_bounding_boxes [100, 100, 2, 5, 5, 5, 0, 1].
-        # [20, 20, 21, 1, 0] -> kept because it is a background point (not in any bounding_boxes and additional_point_clouds).
+        # [0, 1, 2, 3, 4] -> removed because it is in bounding_boxes
+        #               [0, 0, 0, 4, 4, 4, 0, 1].
+        # [10, 1, 2, 3, 4] -> kept because it is a background point (not in any
+        #               bounding_boxes and additional_point_clouds).
+        # [0, -1, 2, 3, 4] -> removed because it overlaps with
+        #               additional_bounding_boxes [0, 0, 1, 4, 4, 4, 0, 1].
+        # [100, 100, 2, 3, 4] -> removed because it overlaps with
+        #               additional_bounding_boxes [100, 100, 2, 5, 5, 5, 0, 1].
+        # [20, 20, 21, 1, 0] -> kept because it is a background point (not in
+        #               any bounding_boxes and additional_point_clouds).
         augmented_point_clouds = np.array(
             [
                 [
                     [0, 2, 1, 3, 4],
                     [100, 101, 2, 3, 4],
                     [10, 1, 2, 3, 4],
                     [20, 20, 21, 1, 0],
@@ -120,15 +130,16 @@
             ]
             * 2
         ).astype("float32")
         self.assertAllClose(
             inputs[ADDITIONAL_POINT_CLOUDS], outputs[ADDITIONAL_POINT_CLOUDS]
         )
         self.assertAllClose(
-            inputs[ADDITIONAL_BOUNDING_BOXES], outputs[ADDITIONAL_BOUNDING_BOXES]
+            inputs[ADDITIONAL_BOUNDING_BOXES],
+            outputs[ADDITIONAL_BOUNDING_BOXES],
         )
         self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
         self.assertAllClose(outputs[BOUNDING_BOXES], augmented_bounding_boxes)
 
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
         add_layer = SwapBackground()
         # point_clouds: 3D (multi frames) float32 Tensor with shape
@@ -199,25 +210,35 @@
             POINT_CLOUDS: point_clouds,
             BOUNDING_BOXES: bounding_boxes,
             ADDITIONAL_POINT_CLOUDS: additional_point_clouds,
             ADDITIONAL_BOUNDING_BOXES: additional_bounding_boxes,
         }
         outputs = add_layer(inputs)
         # The following points in additional_point_clouds.
-        # [0, 2, 1, 3, 4], -> kept because it is in additional_point_clouds [0, 0, 1, 4, 4, 4, 0, 1].
-        # [0, 0, 2, 0, 2] -> removed because it is a background point (not in any bounding_boxes and additional_point_clouds).
-        # [0, 11, 2, 3, 4] -> removed because it is a background point (not in any bounding_boxes and additional_point_clouds).
-        # [100, 101, 2, 3, 4] -> kept because it is in additional_point_clouds [100, 100, 2, 5, 5, 5, 0, 1].
-        # [10, 10, 10, 10, 10] -> removed because it is a background point (not in any bounding_boxes and additional_point_clouds).
+        # [0, 2, 1, 3, 4], -> kept because it is in additional_point_clouds
+        #               [0, 0, 1, 4, 4, 4, 0, 1].
+        # [0, 0, 2, 0, 2] -> removed because it is a background point (not in
+        #               any bounding_boxes and additional_point_clouds).
+        # [0, 11, 2, 3, 4] -> removed because it is a background point (not in
+        #               any bounding_boxes and additional_point_clouds).
+        # [100, 101, 2, 3, 4] -> kept because it is in additional_point_clouds
+        #               [100, 100, 2, 5, 5, 5, 0, 1].
+        # [10, 10, 10, 10, 10] -> removed because it is a background point (not
+        #               in any bounding_boxes and additional_point_clouds).
         # The following points in point_clouds.
-        # [0, 1, 2, 3, 4] -> removed because it is in bounding_boxes [0, 0, 0, 4, 4, 4, 0, 1].
-        # [10, 1, 2, 3, 4] -> kept because it is a background point (not in any bounding_boxes and additional_point_clouds).
-        # [0, -1, 2, 3, 4] -> removed becuase it overlaps with additional_bounding_boxes [0, 0, 1, 4, 4, 4, 0, 1].
-        # [100, 100, 2, 3, 4] -> removed becuase it overlaps with additional_bounding_boxes [100, 100, 2, 5, 5, 5, 0, 1].
-        # [20, 20, 21, 1, 0] -> kept because it is a background point (not in any bounding_boxes and additional_point_clouds).
+        # [0, 1, 2, 3, 4] -> removed because it is in bounding_boxes\
+        #               [0, 0, 0, 4, 4, 4, 0, 1].
+        # [10, 1, 2, 3, 4] -> kept because it is a background point (not in any
+        #               bounding_boxes and additional_point_clouds).
+        # [0, -1, 2, 3, 4] -> removed because it overlaps with
+        #               additional_bounding_boxes [0, 0, 1, 4, 4, 4, 0, 1].
+        # [100, 100, 2, 3, 4] -> removed because it overlaps with
+        #               additional_bounding_boxes [100, 100, 2, 5, 5, 5, 0, 1].
+        # [20, 20, 21, 1, 0] -> kept because it is a background point (not in
+        #               any bounding_boxes and additional_point_clouds).
         augmented_point_clouds = np.array(
             [
                 [
                     [
                         [0, 2, 1, 3, 4],
                         [100, 101, 2, 3, 4],
                         [10, 1, 2, 3, 4],
@@ -243,11 +264,12 @@
             ]
             * 3
         ).astype("float32")
         self.assertAllClose(
             inputs[ADDITIONAL_POINT_CLOUDS], outputs[ADDITIONAL_POINT_CLOUDS]
         )
         self.assertAllClose(
-            inputs[ADDITIONAL_BOUNDING_BOXES], outputs[ADDITIONAL_BOUNDING_BOXES]
+            inputs[ADDITIONAL_BOUNDING_BOXES],
+            outputs[ADDITIONAL_BOUNDING_BOXES],
         )
         self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
         self.assertAllClose(outputs[BOUNDING_BOXES], augmented_bounding_boxes)
```

## keras_cv/layers/regularization/drop_path.py

```diff
@@ -7,58 +7,59 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import tensorflow as tf
 
+from tensorflow import keras
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class DropPath(tf.keras.__internal__.layers.BaseRandomLayer):
+
+@keras.utils.register_keras_serializable(package="keras_cv")
+class DropPath(keras.__internal__.layers.BaseRandomLayer):
     """
-    Implements the DropPath layer. DropPath randomly drops samples during training
-     with a probability of `rate`. Note that this layer drops individual samples
-    within a batch and not the entire batch. DropPath randomly drops some of the
-    individual samples from a batch, whereas StachasticDepth randomly drops the
-    entire batch.
+    Implements the DropPath layer. DropPath randomly drops samples during
+    training with a probability of `rate`. Note that this layer drops individual
+    samples within a batch and not the entire batch. DropPath randomly drops
+    some individual samples from a batch, whereas StochasticDepth
+    randomly drops the entire batch.
 
     References:
         - [FractalNet](https://arxiv.org/abs/1605.07648v4).
         - [rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models/blob/7c67d6aca992f039eece0af5f7c29a43d48c00e4/timm/models/layers/drop.py#L135)
 
     Args:
         rate: float, the probability of the residual branch being dropped.
-        seed: (Optional) Integer. Used to create a random seed.
+        seed: (Optional) integer. Used to create a random seed.
 
     Usage:
     `DropPath` can be used in any network as follows:
     ```python
 
     # (...)
     input = tf.ones((1, 3, 3, 1), dtype=tf.float32)
-    residual = tf.keras.layers.Conv2D(1, 1)(input)
+    residual = keras.layers.Conv2D(1, 1)(input)
     output = keras_cv.layers.DropPath()(input)
     # (...)
     ```
-    """
+    """  # noqa: E501
 
     def __init__(self, rate=0.5, seed=None, **kwargs):
         super().__init__(seed=seed, **kwargs)
         self.rate = rate
         self.seed = seed
 
     def call(self, x, training=None):
         if self.rate == 0.0 or not training:
             return x
         else:
             keep_prob = 1 - self.rate
             drop_map_shape = (x.shape[0],) + (1,) * (len(x.shape) - 1)
-            drop_map = tf.keras.backend.random_bernoulli(
+            drop_map = keras.backend.random_bernoulli(
                 drop_map_shape, p=keep_prob, seed=self.seed
             )
             x = x / keep_prob
             x = x * drop_map
             return x
 
     def get_config(self):
```

## keras_cv/layers/regularization/dropblock_2d.py

```diff
@@ -7,134 +7,140 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras.__internal__.layers import BaseRandomLayer
 
 from keras_cv.utils import conv_utils
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class DropBlock2D(BaseRandomLayer):
     """Applies DropBlock regularization to input features.
 
     DropBlock is a form of structured dropout, where units in a contiguous
     region of a feature map are dropped together. DropBlock works better than
     dropout on convolutional layers due to the fact that activation units in
     convolutional layers are spatially correlated.
 
-    It is advised to use DropBlock after activation in Conv -> BatchNorm -> Activation
-    block in further layers of the network. For example, the paper mentions using
-    DropBlock in 3rd and 4th group of ResNet blocks.
+    It is advised to use DropBlock after activation in Conv -> BatchNorm ->
+    Activation block in further layers of the network. For example, the paper
+    mentions using DropBlock in 3rd and 4th group of ResNet blocks.
 
     Reference:
-    - [DropBlock: A regularization method for convolutional networks](
-        https://arxiv.org/abs/1810.12890
-    )
+    - [DropBlock: A regularization method for convolutional networks](https://arxiv.org/abs/1810.12890)
 
     Args:
         rate: float. Probability of dropping a unit. Must be between 0 and 1.
             For best results, the value should be between 0.05-0.25.
         block_size: integer, or tuple of integers. The size of the block to be
-            dropped. In case of an integer a square block will be dropped. In case of a
-            tuple, the numbers are block's (height, width).
-            Must be bigger than 0, and should not be bigger than the input feature map
-            size. The paper authors use `block_size=7` for input feature's of size
-            `14x14xchannels`.
-            If this value is greater or equal to the input feature map size you will
-            encounter `nan` values.
+            dropped. In case of an integer a square block will be dropped. In
+            case of a tuple, the numbers are block's (height, width). Must be
+            bigger than 0, and should not be bigger than the input feature map
+            size. The paper authors use `block_size=7` for input feature's of
+            size `14x14xchannels`. If this value is greater or equal to the
+            input feature map size you will encounter `nan` values.
         seed: integer. To use as random seed.
         name: string. The name of the layer.
 
     Usage:
-    DropBlock2D can be used inside a `tf.keras.Model`:
+    DropBlock2D can be used inside a `keras.Model`:
     ```python
     # (...)
     x = Conv2D(32, (1, 1))(x)
     x = BatchNormalization()(x)
     x = ReLU()(x)
     x = DropBlock2D(0.1, block_size=7)(x)
     # (...)
     ```
-    When used directly, the layer will zero-out some inputs in a contiguous region and
-    normalize the remaining values.
+    When used directly, the layer will zero-out some inputs in a contiguous
+    region and normalize the remaining values.
 
     ```python
     # Small feature map shape for demonstration purposes:
     features = tf.random.stateless_uniform((1, 4, 4, 1), seed=[0, 1])
 
     # Preview the feature map
     print(features[..., 0])
     # tf.Tensor(
     # [[[0.08216608 0.40928006 0.39318466 0.3162533 ]
     #   [0.34717774 0.73199546 0.56369007 0.9769211 ]
     #   [0.55243933 0.13101244 0.2941643  0.5130266 ]
     #   [0.38977218 0.80855536 0.6040567  0.10502195]]], shape=(1, 4, 4),
     # dtype=float32)
 
-    layer = DropBlock2D(0.1, block_size=2, seed=1234) # Small size for demonstration
+    layer = DropBlock2D(0.1, block_size=2, seed=1234) # Small size for
+        demonstration
     output = layer(features, training=True)
 
     # Preview the feature map after dropblock:
     print(output[..., 0])
     # tf.Tensor(
-    # [[[0.10955477 0.54570675 0.5242462  0.42167106]
-    #   [0.46290365 0.97599393 0.         0.        ]
-    #   [0.7365858  0.17468326 0.         0.        ]
-    #   [0.51969624 1.0780739  0.80540895 0.14002927]]], shape=(1, 4, 4),
-    # dtype=float32)
+    #     [[[0.10955477 0.54570675 0.5242462  0.42167106]
+    #       [0.46290365 0.97599393 0.         0.        ]
+    #       [0.7365858  0.17468326 0.         0.        ]
+    #       [0.51969624 1.0780739  0.80540895 0.14002927]]],
+    #     shape=(1, 4, 4),
+    #     dtype=float32)
 
     # We can observe two things:
     # 1. A 2x2 block has been dropped
     # 2. The inputs have been slightly scaled to account for missing values.
 
-    # The number of blocks dropped can vary, between the channels - sometimes no blocks
-    # will be dropped, and sometimes there will be multiple overlapping blocks.
-    # Let's present on a larger feature map:
+    # The number of blocks dropped can vary, between the channels - sometimes no
+    # blocks will be dropped, and sometimes there will be multiple overlapping
+    # blocks. Let's present on a larger feature map:
 
     features = tf.random.stateless_uniform((1, 4, 4, 36), seed=[0, 1])
     layer = DropBlock2D(0.1, (2, 2), seed=123)
     output = layer(features, training=True)
 
     print(output[..., 0])  # no drop
     # tf.Tensor(
-    # [[[0.09136613 0.98085546 0.15265216 0.19690938]
-    #   [0.48835075 0.52433217 0.1661478  0.7067729 ]
-    #   [0.07383626 0.9938906  0.14309917 0.06882786]
-    #   [0.43242374 0.04158871 0.24213943 0.1903095 ]]], shape=(1, 4, 4),
-    # dtype=float32)
+    #     [[[0.09136613 0.98085546 0.15265216 0.19690938]
+    #       [0.48835075 0.52433217 0.1661478  0.7067729 ]
+    #       [0.07383626 0.9938906  0.14309917 0.06882786]
+    #       [0.43242374 0.04158871 0.24213943 0.1903095 ]]],
+    #     shape=(1, 4, 4),
+    #     dtype=float32)
 
     print(output[..., 9])  # drop single block
     # tf.Tensor(
-    # [[[0.14568178 0.01571623 0.9082305  1.0545396 ]
-    #   [0.24126057 0.86874676 0.         0.        ]
-    #   [0.44101703 0.29805306 0.         0.        ]
-    #   [0.56835717 0.04925899 0.6745584  0.20550345]]], shape=(1, 4, 4), dtype=float32)
+    #     [[[0.14568178 0.01571623 0.9082305  1.0545396 ]
+    #       [0.24126057 0.86874676 0.         0.        ]
+    #       [0.44101703 0.29805306 0.         0.        ]
+    #       [0.56835717 0.04925899 0.6745584  0.20550345]]],
+    #     shape=(1, 4, 4),
+    #     dtype=float32)
 
     print(output[..., 22])  # drop two blocks
     # tf.Tensor(
-    # [[[0.69479376 0.49463132 1.0627024  0.58349967]
-    #   [0.         0.         0.36143216 0.58699244]
-    #   [0.         0.         0.         0.        ]
-    #   [0.0315055  1.0117861  0.         0.        ]]], shape=(1, 4, 4),
-    # dtype=float32)
+    #     [[[0.69479376 0.49463132 1.0627024  0.58349967]
+    #       [0.         0.         0.36143216 0.58699244]
+    #       [0.         0.         0.         0.        ]
+    #       [0.0315055  1.0117861  0.         0.        ]]],
+    #     shape=(1, 4, 4),
+    #     dtype=float32)
 
     print(output[..., 29])  # drop two blocks with overlap
     # tf.Tensor(
-    # [[[0.2137237  0.9120104  0.9963533  0.33937347]
-    #   [0.21868704 0.44030213 0.5068906  0.20034194]
-    #   [0.         0.         0.         0.5915383 ]
-    #   [0.         0.         0.         0.9526224 ]]], shape=(1, 4, 4),
-    # dtype=float32)
+    #     [[[0.2137237  0.9120104  0.9963533  0.33937347]
+    #       [0.21868704 0.44030213 0.5068906  0.20034194]
+    #       [0.         0.         0.         0.5915383 ]
+    #       [0.         0.         0.         0.9526224 ]]],
+    #     shape=(1, 4, 4),
+    #     dtype=float32)
     ```
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         rate,
         block_size,
         seed=None,
         **kwargs,
@@ -142,15 +148,18 @@
         super().__init__(seed=seed, **kwargs)
         if not 0.0 <= rate <= 1.0:
             raise ValueError(
                 f"rate must be a number between 0 and 1. " f"Received: {rate}"
             )
 
         self._rate = rate
-        self._dropblock_height, self._dropblock_width = conv_utils.normalize_tuple(
+        (
+            self._dropblock_height,
+            self._dropblock_width,
+        ) = conv_utils.normalize_tuple(
             value=block_size, n=2, name="block_size", allow_zero=False
         )
         self.seed = seed
 
     def call(self, x, training=None):
         if not training or self._rate == 0.0:
             return x
@@ -205,18 +214,20 @@
             -block_pattern,
             ksize=window_size,
             strides=[1, 1, 1, 1],
             padding="SAME",
         )
 
         # Slightly scale the values, to account for magnitude change
-        percent_ones = tf.cast(tf.reduce_sum(block_pattern), tf.float32) / tf.cast(
-            tf.size(block_pattern), tf.float32
+        percent_ones = tf.cast(
+            tf.reduce_sum(block_pattern), tf.float32
+        ) / tf.cast(tf.size(block_pattern), tf.float32)
+        return (
+            x / tf.cast(percent_ones, x.dtype) * tf.cast(block_pattern, x.dtype)
         )
-        return x / tf.cast(percent_ones, x.dtype) * tf.cast(block_pattern, x.dtype)
 
     def get_config(self):
         config = {
             "rate": self._rate,
             "block_size": (self._dropblock_height, self._dropblock_width),
             "seed": self.seed,
         }
```

## keras_cv/layers/regularization/squeeze_excite.py

```diff
@@ -9,77 +9,84 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras import layers
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class SqueezeAndExcite2D(layers.Layer):
     """
     Implements Squeeze and Excite block as in
     [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf).
     This layer tries to use a content aware mechanism to assign channel-wise
     weights adaptively. It first squeezes the feature maps into a single value
     using global average pooling, which are then fed into two Conv1D layers,
     which act like fully-connected layers. The first layer reduces the
-    dimensionality of the feature maps by a factor of `ratio`, whereas the second
-    layer restores it to its original value.
+    dimensionality of the feature maps, and second layer restores it to its
+    original value.
 
     The resultant values are the adaptive weights for each channel. These
     weights are then multiplied with the original inputs to scale the outputs
     based on their individual weightages.
 
 
     Args:
         filters: Number of input and output filters. The number of input and
             output filters is same.
-        ratio: Ratio for bottleneck filters. Number of bottleneck filters =
-            filters * ratio. Defaults to 0.25.
-        squeeze_activation: (Optional) String, callable (or tf.keras.layers.Layer) or
-            tf.keras.activations.Activation instance denoting activation to
-            be applied after squeeze convolution. Defaults to `relu`.
-        excite_activation: (Optional) String, callable (or tf.keras.layers.Layer) or
-            tf.keras.activations.Activation instance denoting activation to
-            be applied after excite convolution. Defaults to `sigmoid`.
+        bottleneck_filters: (Optional) Number of bottleneck filters. Defaults
+            to `0.25 * filters`
+        squeeze_activation: (Optional) String, callable (or
+            keras.layers.Layer) or keras.activations.Activation instance
+            denoting activation to be applied after squeeze convolution.
+            Defaults to `relu`.
+        excite_activation: (Optional) String, callable (or
+            keras.layers.Layer) or keras.activations.Activation instance
+            denoting activation to be applied after excite convolution.
+            Defaults to `sigmoid`.
     Usage:
 
     ```python
     # (...)
     input = tf.ones((1, 5, 5, 16), dtype=tf.float32)
-    x = tf.keras.layers.Conv2D(16, (3, 3))(input)
+    x = keras.layers.Conv2D(16, (3, 3))(input)
     output = keras_cv.layers.SqueezeAndExciteBlock(16)(x)
     # (...)
     ```
     """
 
     def __init__(
         self,
         filters,
-        ratio=0.25,
+        bottleneck_filters=None,
         squeeze_activation="relu",
         excite_activation="sigmoid",
         **kwargs,
     ):
         super().__init__(**kwargs)
 
         self.filters = filters
 
-        if ratio <= 0.0 or ratio >= 1.0:
-            raise ValueError(f"`ratio` should be a float between 0 and 1. Got {ratio}")
+        if bottleneck_filters and bottleneck_filters >= filters:
+            raise ValueError(
+                "`bottleneck_filters` should be smaller than `filters`. Got "
+                f"`filters={filters}`, and "
+                f"`bottleneck_filters={bottleneck_filters}`."
+            )
 
         if filters <= 0 or not isinstance(filters, int):
-            raise ValueError(f"`filters` should be a positive integer. Got {filters}")
-
-        self.ratio = ratio
-        self.bottleneck_filters = int(self.filters * self.ratio)
+            raise ValueError(
+                f"`filters` should be a positive integer. Got {filters}"
+            )
 
+        self.bottleneck_filters = bottleneck_filters or (filters // 4)
         self.squeeze_activation = squeeze_activation
         self.excite_activation = excite_activation
 
         self.global_average_pool = layers.GlobalAveragePooling2D(keepdims=True)
         self.squeeze_conv = layers.Conv2D(
             self.bottleneck_filters,
             (1, 1),
@@ -95,13 +102,25 @@
         x = self.excite_conv(x)  # x: (batch_size, 1, 1, filters)
         x = tf.math.multiply(x, inputs)  # x: (batch_size, h, w, filters)
         return x
 
     def get_config(self):
         config = {
             "filters": self.filters,
-            "ratio": self.ratio,
+            "bottleneck_filters": self.bottleneck_filters,
             "squeeze_activation": self.squeeze_activation,
             "excite_activation": self.excite_activation,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        if isinstance(config["squeeze_activation"], dict):
+            config["squeeze_activation"] = keras.utils.deserialize_keras_object(
+                config["squeeze_activation"]
+            )
+        if isinstance(config["excite_activation"], dict):
+            config["excite_activation"] = keras.utils.deserialize_keras_object(
+                config["excite_activation"]
+            )
+        return cls(**config)
```

## keras_cv/layers/regularization/squeeze_excite_test.py

```diff
@@ -18,38 +18,39 @@
 
 
 class SqueezeAndExcite2DTest(tf.test.TestCase):
     def test_maintains_shape(self):
         input_shape = (1, 4, 4, 8)
         inputs = tf.random.uniform(input_shape)
 
-        layer = SqueezeAndExcite2D(8, ratio=0.25)
+        layer = SqueezeAndExcite2D(8, 2)
         outputs = layer(inputs)
         self.assertEquals(inputs.shape, outputs.shape)
 
     def test_custom_activation(self):
         def custom_activation(x):
             return x * tf.random.uniform(x.shape, seed=42)
 
         input_shape = (1, 4, 4, 8)
         inputs = tf.random.uniform(input_shape)
 
         layer = SqueezeAndExcite2D(
             8,
-            ratio=0.25,
+            2,
             squeeze_activation=custom_activation,
             excite_activation=custom_activation,
         )
         outputs = layer(inputs)
         self.assertEquals(inputs.shape, outputs.shape)
 
     def test_raises_invalid_ratio_error(self):
         with self.assertRaisesRegex(
-            ValueError, "`ratio` should be a float" " between 0 and 1. Got (.*?)"
+            ValueError,
+            "`bottleneck_filters` should be smaller than `filters`",
         ):
-            _ = SqueezeAndExcite2D(8, ratio=1.1)
+            _ = SqueezeAndExcite2D(8, 9)
 
     def test_raises_invalid_filters_error(self):
         with self.assertRaisesRegex(
             ValueError, "`filters` should be a positive" " integer. Got (.*?)"
         ):
             _ = SqueezeAndExcite2D(-8.7)
```

## keras_cv/layers/regularization/stochastic_depth.py

```diff
@@ -7,54 +7,55 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import tensorflow as tf
 
+from tensorflow import keras
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class StochasticDepth(tf.keras.layers.Layer):
+
+@keras.utils.register_keras_serializable(package="keras_cv")
+class StochasticDepth(keras.layers.Layer):
     """
     Implements the Stochastic Depth layer. It randomly drops residual branches
     in residual architectures. It is used as a drop-in replacement for addition
     operation. Note that this layer DOES NOT drop a residual block across
     individual samples but across the entire batch.
 
     Reference:
-        - [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382).
-        - Docstring taken from [stochastic_depth.py](https://tinyurl.com/mr3y2af6)
+        - [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382)
+        - [Docstring taken from [stochastic_depth.py](https://tinyurl.com/mr3y2af6)
 
     Args:
         rate: float, the probability of the residual branch being dropped.
 
     Usage:
 
     `StochasticDepth` can be used in a residual network as follows:
     ```python
     # (...)
     input = tf.ones((1, 3, 3, 1), dtype=tf.float32)
-    residual = tf.keras.layers.Conv2D(1, 1)(input)
+    residual = keras.layers.Conv2D(1, 1)(input)
     output = keras_cv.layers.StochasticDepth()([input, residual])
     # (...)
     ```
 
     At train time, StochasticDepth returns:
     $$
     x[0] + b_l * x[1],
     $$
     where $b_l$ is a random Bernoulli variable with probability
     $P(b_l = 1) = rate$. At test time, StochasticDepth rescales the activations
     of the residual branch based on the drop rate ($rate$):
     $$
     x[0] + (1 - rate) * x[1]
     $$
-    """
+    """  # noqa: E501
 
     def __init__(self, rate=0.5, **kwargs):
         super().__init__(**kwargs)
         self.rate = rate
         self.survival_probability = 1.0 - self.rate
 
     def call(self, x, training=None):
@@ -62,15 +63,15 @@
             raise ValueError(
                 f"""Input must be a list of length 2. """
                 f"""Got input with length={len(x)}."""
             )
 
         shortcut, residual = x
 
-        b_l = tf.keras.backend.random_bernoulli([], p=self.survival_probability)
+        b_l = keras.backend.random_bernoulli([], p=self.survival_probability)
 
         if training:
             return shortcut + b_l * residual
         else:
             return shortcut + self.survival_probability * residual
 
     def get_config(self):
```

## keras_cv/layers/regularization/stochastic_depth_test.py

```diff
@@ -20,15 +20,16 @@
     FEATURE_SHAPE = (1, 14, 14, 256)
 
     def test_inputs_have_two_elements(self):
         inputs = tf.random.uniform(self.FEATURE_SHAPE, 0, 1)
         inputs = [inputs, inputs, inputs]
 
         with self.assertRaisesRegex(
-            ValueError, "Input must be a list of length 2. " "Got input with length=3."
+            ValueError,
+            "Input must be a list of length 2. " "Got input with length=3.",
         ):
             StochasticDepth()(inputs)
 
     def test_eval_mode(self):
         inputs = tf.random.uniform(self.FEATURE_SHAPE, 0, 1)
         inputs = [inputs, inputs]
```

## keras_cv/losses/__init__.py

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from keras_cv.losses.centernet_box_loss import CenterNetBoxLoss
 from keras_cv.losses.focal import FocalLoss
 from keras_cv.losses.giou_loss import GIoULoss
 from keras_cv.losses.iou_loss import IoULoss
 from keras_cv.losses.penalty_reduced_focal_loss import (
     BinaryPenaltyReducedFocalCrossEntropy,
 )
 from keras_cv.losses.simclr_loss import SimCLRLoss
```

## keras_cv/losses/focal.py

```diff
@@ -10,37 +10,38 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 import tensorflow.keras.backend as K
+from tensorflow import keras
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class FocalLoss(tf.keras.losses.Loss):
+@keras.utils.register_keras_serializable(package="keras_cv")
+class FocalLoss(keras.losses.Loss):
     """Implements Focal loss
 
     Focal loss is a modified cross-entropy designed to perform better with
     class imbalance. For this reason, it's commonly used with object detectors.
 
     Args:
         alpha: a float value between 0 and 1 representing a weighting factor
             used to deal with class imbalance. Positive classes and negative
             classes have alpha and (1 - alpha) as their weighting factors
             respectively. Defaults to 0.25.
         gamma: a positive float value representing the tunable focusing
-            parameter. Defaults to 2.
+            parameter, defaults to 2.
         from_logits: Whether `y_pred` is expected to be a logits tensor. By
             default, `y_pred` is assumed to encode a probability distribution.
             Default to `False`.
         label_smoothing: Float in `[0, 1]`. If higher than 0 then smooth the
-            labels by squeezing them towards `0.5`, i.e., using `1. - 0.5 * label_smoothing`
-            for the target class and `0.5 * label_smoothing` for the non-target
-            class.
+            labels by squeezing them towards `0.5`, i.e., using
+            `1. - 0.5 * label_smoothing` for the target class and
+            `0.5 * label_smoothing` for the non-target class.
 
     References:
         - [Focal Loss paper](https://arxiv.org/abs/1708.02002)
 
     Standalone usage:
     ```python
     y_true = tf.random.uniform([10], 0, maxval=4)
@@ -65,15 +66,17 @@
         super().__init__(**kwargs)
         self.alpha = float(alpha)
         self.gamma = float(gamma)
         self.from_logits = from_logits
         self.label_smoothing = label_smoothing
 
     def _smooth_labels(self, y_true):
-        return y_true * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing
+        return (
+            y_true * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing
+        )
 
     def call(self, y_true, y_pred):
         y_pred = tf.convert_to_tensor(y_pred)
         y_true = tf.cast(y_true, y_pred.dtype)
 
         if self.label_smoothing:
             y_true = self._smooth_labels(y_true)
@@ -84,15 +87,15 @@
         cross_entropy = K.binary_crossentropy(y_true, y_pred)
 
         alpha = tf.where(tf.equal(y_true, 1.0), self.alpha, (1.0 - self.alpha))
         pt = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)
         loss = alpha * tf.pow(1.0 - pt, self.gamma) * cross_entropy
         # In most losses you mean over the final axis to achieve a scalar
         # Focal loss however is a special case in that it is meant to focus on
-        # a small number of hard examples in a batch.  Most of the time this
+        # a small number of hard examples in a batch. Most of the time this
         # comes in the form of thousands of background class boxes and a few
         # positive boxes.
         # If you mean over the final axis you will get a number close to 0,
         # which will encourage your model to exclusively predict background
         # class boxes.
         return K.sum(loss, axis=-1)
```

## keras_cv/losses/focal_test.py

```diff
@@ -19,26 +19,30 @@
 
 class FocalTest(tf.test.TestCase):
     def test_output_shape(self):
         y_true = tf.cast(
             tf.random.uniform(shape=[2, 5], minval=0, maxval=2, dtype=tf.int32),
             tf.float32,
         )
-        y_pred = tf.random.uniform(shape=[2, 5], minval=0, maxval=1, dtype=tf.float32)
+        y_pred = tf.random.uniform(
+            shape=[2, 5], minval=0, maxval=1, dtype=tf.float32
+        )
 
         focal_loss = FocalLoss(reduction="sum")
 
         self.assertAllEqual(focal_loss(y_true, y_pred).shape, [])
 
     def test_output_shape_reduction_none(self):
         y_true = tf.cast(
             tf.random.uniform(shape=[2, 5], minval=0, maxval=2, dtype=tf.int32),
             tf.float32,
         )
-        y_pred = tf.random.uniform(shape=[2, 5], minval=0, maxval=1, dtype=tf.float32)
+        y_pred = tf.random.uniform(
+            shape=[2, 5], minval=0, maxval=1, dtype=tf.float32
+        )
 
         focal_loss = FocalLoss(reduction="none")
 
         self.assertAllEqual(
             focal_loss(y_true, y_pred).shape,
             [
                 2,
```

## keras_cv/losses/giou_loss.py

```diff
@@ -9,52 +9,64 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
+import warnings
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 
 
-class GIoULoss(tf.keras.losses.Loss):
+class GIoULoss(keras.losses.Loss):
     """Implements the Generalized IoU Loss
 
-    GIoU loss is a modified IoU loss commonly used for object detection. This loss aims
-    to directly optimize the IoU score between true boxes and predicted boxes. GIoU loss
-    adds a penalty term to the IoU loss that takes in account the area of the
-    smallest box enclosing both the boxes being considered for the iou. The length of
-    the last dimension should be 4 to represent the bounding boxes.
+    GIoU loss is a modified IoU loss commonly used for object detection. This
+    loss aims to directly optimize the IoU score between true boxes and
+    predicted boxes. GIoU loss adds a penalty term to the IoU loss that takes in
+    account the area of the smallest box enclosing both the boxes being
+    considered for the iou. The length of the last dimension should be 4 to
+    represent the bounding boxes.
 
     Args:
         bounding_box_format: a case-insensitive string (for example, "xyxy").
-            Each bounding box is defined by these 4 values.For detailed information
-            on the supported formats, see the
-            [KerasCV bounding box documentation](https://keras.io/api/keras_cv/bounding_box/formats/).
-        axis: the axis along which to mean the ious. Defaults to -1.
+            Each bounding box is defined by these 4 values.For detailed
+            information on the supported formats, see the [KerasCV bounding box
+            documentation](https://keras.io/api/keras_cv/bounding_box/formats/).
+        axis: the axis along which to mean the ious, defaults to -1.
 
     References:
         - [GIoU paper](https://arxiv.org/pdf/1902.09630)
         - [TFAddons Implementation](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/GIoULoss)
 
     Sample Usage:
     ```python
-    y_true = tf.random.uniform((5, 10, 5), minval=0, maxval=10, dtype=tf.dtypes.int32)
-    y_pred = tf.random.uniform((5, 10, 4), minval=0, maxval=10, dtype=tf.dtypes.int32)
+    y_true = tf.random.uniform(
+        (5, 10, 5),
+        minval=0,
+        maxval=10,
+        dtype=tf.dtypes.int32)
+    y_pred = tf.random.uniform(
+        (5, 10, 4),
+        minval=0,
+        maxval=10,
+        dtype=tf.dtypes.int32)
     loss = GIoULoss(bounding_box_format = "xyWH")
     loss(y_true, y_pred).numpy()
     ```
 
     Usage with the `compile()` API:
     ```python
     model.compile(optimizer='adam', loss=keras_cv.losses.GIoULoss())
     ```
-    """
+    """  # noqa: E501
 
     def __init__(self, bounding_box_format, axis=-1, **kwargs):
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         self.axis = axis
 
     def _compute_enclosure(self, boxes1, boxes2):
@@ -81,25 +93,25 @@
 
     def _compute_giou(self, boxes1, boxes2):
         boxes1_rank = len(boxes1.shape)
         boxes2_rank = len(boxes2.shape)
 
         if boxes1_rank not in [2, 3]:
             raise ValueError(
-                "compute_iou() expects boxes1 to be batched, or "
-                f"to be unbatched. Received len(boxes1.shape)={boxes1_rank}, "
-                f"len(boxes2.shape)={boxes2_rank}. Expected either len(boxes1.shape)=2 AND "
-                "or len(boxes1.shape)=3."
+                "compute_iou() expects boxes1 to be batched, or to be "
+                f"unbatched. Received len(boxes1.shape)={boxes1_rank}, "
+                f"len(boxes2.shape)={boxes2_rank}. Expected either "
+                "len(boxes1.shape)=2 AND or len(boxes1.shape)=3."
             )
         if boxes2_rank not in [2, 3]:
             raise ValueError(
-                "compute_iou() expects boxes2 to be batched, or "
-                f"to be unbatched. Received len(boxes1.shape)={boxes1_rank}, "
-                f"len(boxes2.shape)={boxes2_rank}. Expected either len(boxes2.shape)=2 AND "
-                "or len(boxes2.shape)=3."
+                "compute_iou() expects boxes2 to be batched, or to be "
+                f"unbatched. Received len(boxes1.shape)={boxes1_rank}, "
+                f"len(boxes2.shape)={boxes2_rank}. Expected either "
+                "len(boxes2.shape)=2 AND or len(boxes2.shape)=3."
             )
 
         target_format = "yxyx"
         if bounding_box.is_relative(self.bounding_box_format):
             target_format = bounding_box.as_relative(target_format)
 
         boxes1 = bounding_box.convert_format(
@@ -119,48 +131,58 @@
         boxes2_area = tf.expand_dims(boxes2_area, axis=boxes2_axis)
         union_area = boxes1_area + boxes2_area - intersect_area
         iou = tf.math.divide_no_nan(intersect_area, union_area)
 
         # giou calculation
         enclose_area = self._compute_enclosure(boxes1, boxes2)
 
-        return iou - tf.math.divide_no_nan((enclose_area - union_area), enclose_area)
+        return iou - tf.math.divide_no_nan(
+            (enclose_area - union_area), enclose_area
+        )
 
     def call(self, y_true, y_pred, sample_weight=None):
         if sample_weight is not None:
             raise ValueError(
-                "GIoULoss does not support sample_weight. Please ensure that sample_weight=None."
-                f"got sample_weight={sample_weight}"
+                "GIoULoss does not support sample_weight. Please ensure "
+                f"sample_weight=None. Got sample_weight={sample_weight}"
             )
 
         y_pred = tf.convert_to_tensor(y_pred)
         y_true = tf.cast(y_true, y_pred.dtype)
 
         if y_pred.shape[-1] != 4:
             raise ValueError(
-                "GIoULoss expects y_pred.shape[-1] to be 4 to represent "
-                f"the bounding boxes. Received y_pred.shape[-1]={y_pred.shape[-1]}."
+                "GIoULoss expects y_pred.shape[-1] to be 4 to represent the "
+                f"bounding boxes. Received y_pred.shape[-1]={y_pred.shape[-1]}."
             )
 
         if y_true.shape[-1] != 4:
             raise ValueError(
-                "GIoULoss expects y_true.shape[-1] to be 4 to represent "
-                f"the bounding boxes. Received y_true.shape[-1]={y_true.shape[-1]}."
+                "GIoULoss expects y_true.shape[-1] to be 4 to represent the "
+                f"bounding boxes. Received y_true.shape[-1]={y_true.shape[-1]}."
             )
 
         if y_true.shape[-2] != y_pred.shape[-2]:
             raise ValueError(
-                "GIoULoss expects number of boxes in y_pred to be equal to the number "
-                f"of boxes in y_true. Received number of boxes in y_true={y_true.shape[-2]} "
-                f"and number of boxes in y_pred={y_pred.shape[-2]}."
+                "GIoULoss expects number of boxes in y_pred to be equal to the "
+                "number of boxes in y_true. Received number of boxes in "
+                f"y_true={y_true.shape[-2]} and number of boxes in "
+                f"y_pred={y_pred.shape[-2]}."
             )
 
         giou = self._compute_giou(y_true, y_pred)
         giou = tf.linalg.diag_part(giou)
-        giou = tf.reduce_mean(giou, axis=self.axis)
+        if self.axis == "no_reduction":
+            warnings.warn(
+                "`axis='no_reduction'` is a temporary API, and the API "
+                "contract will be replaced in the future with a more generic "
+                "solution covering all losses."
+            )
+        else:
+            giou = tf.reduce_mean(giou, axis=self.axis)
 
         return 1 - giou
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
```

## keras_cv/losses/giou_loss_test.py

```diff
@@ -15,24 +15,32 @@
 import tensorflow as tf
 
 from keras_cv.losses.giou_loss import GIoULoss
 
 
 class GIoUTest(tf.test.TestCase):
     def test_output_shape(self):
-        y_true = tf.random.uniform(shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32)
-        y_pred = tf.random.uniform(shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32)
+        y_true = tf.random.uniform(
+            shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32
+        )
+        y_pred = tf.random.uniform(
+            shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32
+        )
 
         giou_loss = GIoULoss(bounding_box_format="xywh")
 
         self.assertAllEqual(giou_loss(y_true, y_pred).shape, ())
 
     def test_output_shape_reduction_none(self):
-        y_true = tf.random.uniform(shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32)
-        y_pred = tf.random.uniform(shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32)
+        y_true = tf.random.uniform(
+            shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32
+        )
+        y_pred = tf.random.uniform(
+            shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32
+        )
 
         giou_loss = GIoULoss(bounding_box_format="xywh", reduction="none")
 
         self.assertAllEqual(
             giou_loss(y_true, y_pred).shape,
             [
                 2,
```

## keras_cv/losses/iou_loss.py

```diff
@@ -9,97 +9,116 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
+import warnings
+
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import bounding_box
 
 
-class IoULoss(tf.keras.losses.Loss):
+class IoULoss(keras.losses.Loss):
     """Implements the IoU Loss
 
     IoU loss is commonly used for object detection. This loss aims to directly
-    optimize the IoU score between true boxes and predicted boxes. The length of the
-    last dimension should be 4 to represent the bounding boxes. This loss
-    uses IoUs according to box pairs and therefore, the number of boxes in both y_true
-    and y_pred are expected to be equal i.e. the i<sup>th</sup> y_true box in a batch
-    will be compared the i<sup>th</sup> y_pred box.
+    optimize the IoU score between true boxes and predicted boxes. The length of
+    the last dimension should be 4 to represent the bounding boxes. This loss
+    uses IoUs according to box pairs and therefore, the number of boxes in both
+    y_true and y_pred are expected to be equal i.e. the i<sup>th</sup>
+    y_true box in a batch will be compared the i<sup>th</sup> y_pred box.
 
     Args:
         bounding_box_format: a case-insensitive string (for example, "xyxy").
             Each bounding box is defined by these 4 values. For detailed
             information on the supported formats, see the
             [KerasCV bounding box documentation](https://keras.io/api/keras_cv/bounding_box/formats/).
         mode: must be one of
             - `"linear"`. The loss will be calculated as 1 - iou
             - `"quadratic"`. The loss will be calculated as 1 - iou<sup>2</sup>
             - `"log"`. The loss will be calculated as -ln(iou)
             Defaults to "log".
-        axis: the axis along which to mean the ious. Defaults to -1.
+        axis: the axis along which to mean the ious, defaults to -1.
 
     References:
         - [UnitBox paper](https://arxiv.org/pdf/1608.01471)
 
     Sample Usage:
     ```python
-    y_true = tf.random.uniform((5, 10, 5), minval=0, maxval=10, dtype=tf.dtypes.int32)
-    y_pred = tf.random.uniform((5, 10, 4), minval=0, maxval=10, dtype=tf.dtypes.int32)
+    y_true = tf.random.uniform(
+        (5, 10, 5),
+        minval=0,
+        maxval=10,
+        dtype=tf.dtypes.int32)
+    y_pred = tf.random.uniform(
+        (5, 10, 4),
+        minval=0,
+        maxval=10,
+        dtype=tf.dtypes.int32)
     loss = IoULoss(bounding_box_format = "xyWH")
     loss(y_true, y_pred).numpy()
     ```
 
     Usage with the `compile()` API:
     ```python
     model.compile(optimizer='adam', loss=keras_cv.losses.IoULoss())
     ```
-    """
+    """  # noqa: E501
 
     def __init__(self, bounding_box_format, mode="log", axis=-1, **kwargs):
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         self.mode = mode
         self.axis = axis
 
         if self.mode not in ["linear", "quadratic", "log"]:
             raise ValueError(
-                "IoULoss expects mode to be one of 'linear', 'quadratic' or 'log' "
-                f"Received mode={self.mode}, "
+                "IoULoss expects mode to be one of 'linear', 'quadratic' or "
+                f"'log' Received mode={self.mode}, "
             )
 
     def call(self, y_true, y_pred):
         y_pred = tf.convert_to_tensor(y_pred)
         y_true = tf.cast(y_true, y_pred.dtype)
 
         if y_pred.shape[-1] != 4:
             raise ValueError(
-                "IoULoss expects y_pred.shape[-1] to be 4 to represent "
-                f"the bounding boxes. Received y_pred.shape[-1]={y_pred.shape[-1]}."
+                "IoULoss expects y_pred.shape[-1] to be 4 to represent the "
+                f"bounding boxes. Received y_pred.shape[-1]={y_pred.shape[-1]}."
             )
 
         if y_true.shape[-1] != 4:
             raise ValueError(
-                "IoULoss expects y_true.shape[-1] to be 4 to represent "
-                f"the bounding boxes. Received y_true.shape[-1]={y_true.shape[-1]}."
+                "IoULoss expects y_true.shape[-1] to be 4 to represent the "
+                f"bounding boxes. Received y_true.shape[-1]={y_true.shape[-1]}."
             )
 
         if y_true.shape[-2] != y_pred.shape[-2]:
             raise ValueError(
-                "IoULoss expects number of boxes in y_pred to be equal to the number "
-                f"of boxes in y_true. Received number of boxes in y_true={y_true.shape[-2]} "
-                f"and number of boxes in y_pred={y_pred.shape[-2]}."
+                "IoULoss expects number of boxes in y_pred to be equal to the "
+                "number of boxes in y_true. Received number of boxes in "
+                f"y_true={y_true.shape[-2]} and number of boxes in "
+                f"y_pred={y_pred.shape[-2]}."
             )
 
         iou = bounding_box.compute_iou(y_true, y_pred, self.bounding_box_format)
         # pick out the diagonal for corresponding ious
         iou = tf.linalg.diag_part(iou)
-        iou = tf.reduce_mean(iou, axis=self.axis)
+        if self.axis == "no_reduction":
+            warnings.warn(
+                "`axis='no_reduction'` is a temporary API, and the API "
+                "contract will be replaced in the future with a more generic "
+                "solution covering all losses."
+            )
+        else:
+            iou = tf.reduce_mean(iou, axis=self.axis)
 
         if self.mode == "linear":
             loss = 1 - iou
         elif self.mode == "quadratic":
             loss = 1 - iou**2
         elif self.mode == "log":
             loss = -tf.math.log(iou)
```

## keras_cv/losses/iou_loss_test.py

```diff
@@ -15,24 +15,32 @@
 import tensorflow as tf
 
 from keras_cv.losses.iou_loss import IoULoss
 
 
 class IoUTest(tf.test.TestCase):
     def test_output_shape(self):
-        y_true = tf.random.uniform(shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32)
-        y_pred = tf.random.uniform(shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32)
+        y_true = tf.random.uniform(
+            shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32
+        )
+        y_pred = tf.random.uniform(
+            shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32
+        )
 
         iou_loss = IoULoss(bounding_box_format="xywh")
 
         self.assertAllEqual(iou_loss(y_true, y_pred).shape, ())
 
     def test_output_shape_reduction_none(self):
-        y_true = tf.random.uniform(shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32)
-        y_pred = tf.random.uniform(shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32)
+        y_true = tf.random.uniform(
+            shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32
+        )
+        y_pred = tf.random.uniform(
+            shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32
+        )
 
         iou_loss = IoULoss(bounding_box_format="xywh", reduction="none")
 
         self.assertAllEqual(
             iou_loss(y_true, y_pred).shape,
             [
                 2,
```

## keras_cv/losses/penalty_reduced_focal_loss.py

```diff
@@ -9,57 +9,64 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 
 # TODO(tanzhenyu): consider inherit from LossFunctionWrapper to
-# get the dimension squeeze.
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class BinaryPenaltyReducedFocalCrossEntropy(tf.keras.losses.Loss):
+#  get the dimension squeeze.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class BinaryPenaltyReducedFocalCrossEntropy(keras.losses.Loss):
     """Implements CenterNet modified Focal loss.
 
-    Compared with `keras.losses.BinaryFocalCrossentropy`, this loss discounts for negative
-    labels that have value less than `positive_threshold`, the larger value the negative label
-    is, the more discount to the final loss.
+    Compared with `keras.losses.BinaryFocalCrossentropy`, this loss discounts
+    for negative labels that have value less than `positive_threshold`, the
+    larger value the negative label is, the more discount to the final loss.
 
-    User can choose to divide the number of keypoints outside the loss computation, or by
-    passing in `sample_weight` as 1.0/num_key_points.
+    User can choose to divide the number of keypoints outside the loss
+    computation, or by passing in `sample_weight` as 1.0/num_key_points.
 
     Args:
       alpha: a focusing parameter used to compute the focal factor.
-        Defaults to 2.0. Note, this is equivalent to the `gamma` parameter in `keras.losses.BinaryFocalCrossentropy`.
-      beta: a float parameter, penalty exponent for negative labels. Defaults to 4.0.
-      from_logits: Whether `y_pred` is expected to be a logits tensor. Defaults
+        Defaults to 2.0. Note, this is equivalent to the `gamma` parameter in
+        `keras.losses.BinaryFocalCrossentropy`.
+      beta: a float parameter, penalty exponent for negative labels, defaults to
+        4.0.
+      from_logits: Whether `y_pred` is expected to be a logits tensor, defaults
         to `False`.
-      positive_threshold: Anything bigger than this is treated as positive label. Defaults to 0.99.
-      positive_weight: single scalar weight on positive examples. Defaults to 1.0.
-      negative_weight: single scalar weight on negative examples. Defaults to 1.0.
+      positive_threshold: Anything bigger than this is treated as positive
+        label, defaults to 0.99.
+      positive_weight: single scalar weight on positive examples, defaults to
+        1.0.
+      negative_weight: single scalar weight on negative examples, defaults to
+        1.0.
 
     Inputs:
       y_true: [batch_size, ...] float tensor
       y_pred: [batch_size, ...] float tensor with same shape as y_true.
 
     References:
         - [Objects as Points](https://arxiv.org/pdf/1904.07850.pdf) Eq 1.
-        - [Cornernet: Detecting objects as paired keypoints](https://arxiv.org/abs/1808.01244) for `alpha` and `beta`.
-    """
+        - [Cornernet: Detecting objects as paired keypoints](https://arxiv.org/abs/1808.01244) for `alpha` and
+            `beta`.
+    """  # noqa: E501
 
     def __init__(
         self,
         alpha=2.0,
         beta=4.0,
         from_logits=False,
         positive_threshold=0.99,
         positive_weight=1.0,
         negative_weight=1.0,
-        reduction=tf.keras.losses.Reduction.AUTO,
+        reduction=keras.losses.Reduction.AUTO,
         name="binary_penalty_reduced_focal_cross_entropy",
     ):
         super().__init__(reduction=reduction, name=name)
         self.alpha = alpha
         self.beta = beta
         self.from_logits = from_logits
         self.positive_threshold = positive_threshold
@@ -69,16 +76,16 @@
     def call(self, y_true, y_pred):
         y_pred = tf.convert_to_tensor(y_pred)
         y_true = tf.cast(y_true, y_pred.dtype)
 
         if self.from_logits:
             y_pred = tf.nn.sigmoid(y_pred)
 
-        # TODO(tanzhenyu): Evaluate whether we need clipping after
-        # model is trained.
+        # TODO(tanzhenyu): Evaluate whether we need clipping after model is
+        #  trained.
         y_pred = tf.clip_by_value(y_pred, 1e-4, 0.9999)
         y_true = tf.clip_by_value(y_true, 0.0, 1.0)
 
         pos_loss = tf.math.pow(1.0 - y_pred, self.alpha) * tf.math.log(y_pred)
         neg_loss = (
             tf.math.pow(1.0 - y_true, self.beta)
             * tf.math.pow(y_pred, self.alpha)
```

## keras_cv/losses/penalty_reduced_focal_loss_test.py

```diff
@@ -20,26 +20,30 @@
 
 class BinaryPenaltyReducedFocalLossTest(tf.test.TestCase):
     def test_output_shape(self):
         y_true = tf.cast(
             tf.random.uniform(shape=[2, 5], minval=0, maxval=2, dtype=tf.int32),
             tf.float32,
         )
-        y_pred = tf.random.uniform(shape=[2, 5], minval=0, maxval=1, dtype=tf.float32)
+        y_pred = tf.random.uniform(
+            shape=[2, 5], minval=0, maxval=1, dtype=tf.float32
+        )
 
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
 
         self.assertAllEqual(focal_loss(y_true, y_pred).shape, [])
 
     def test_output_shape_reduction_none(self):
         y_true = tf.cast(
             tf.random.uniform(shape=[2, 5], minval=0, maxval=2, dtype=tf.int32),
             tf.float32,
         )
-        y_pred = tf.random.uniform(shape=[2, 5], minval=0, maxval=1, dtype=tf.float32)
+        y_pred = tf.random.uniform(
+            shape=[2, 5], minval=0, maxval=1, dtype=tf.float32
+        )
 
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="none")
 
         self.assertAllEqual(
             [2, 5],
             focal_loss(y_true, y_pred).shape,
         )
@@ -51,33 +55,39 @@
         self.assertAllClose(0.0, focal_loss(y_true, y_pred))
 
     def test_output_with_pos_label_neg_pred(self):
         y_true = tf.constant([1.0])
         y_pred = tf.constant([np.exp(-1.0)])
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
         # (1-1/e)^2 * log(1/e)
-        self.assertAllClose(np.square(1 - np.exp(-1.0)), focal_loss(y_true, y_pred))
+        self.assertAllClose(
+            np.square(1 - np.exp(-1.0)), focal_loss(y_true, y_pred)
+        )
 
     def test_output_with_neg_label_pred(self):
         y_true = tf.constant([0.0])
         y_pred = tf.constant([0.0])
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
         self.assertAllClose(0.0, focal_loss(y_true, y_pred))
 
     def test_output_with_neg_label_pos_pred(self):
         y_true = tf.constant([0.0])
         y_pred = tf.constant([1.0 - np.exp(-1.0)])
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
         # (1-0)^4 * (1-1/e)^2 * log(1/e)
-        self.assertAllClose(np.square(1 - np.exp(-1.0)), focal_loss(y_true, y_pred))
+        self.assertAllClose(
+            np.square(1 - np.exp(-1.0)), focal_loss(y_true, y_pred)
+        )
 
     def test_output_with_weak_label_pos_pred(self):
         y_true = tf.constant([0.5])
         y_pred = tf.constant([1.0 - np.exp(-1.0)])
-        focal_loss = BinaryPenaltyReducedFocalCrossEntropy(beta=2.0, reduction="sum")
+        focal_loss = BinaryPenaltyReducedFocalCrossEntropy(
+            beta=2.0, reduction="sum"
+        )
         # (1-0.5)^2 * (1-1/e)^2 * log(1/e)
         self.assertAllClose(
             0.25 * np.square(1 - np.exp(-1.0)), focal_loss(y_true, y_pred)
         )
 
     def test_output_with_sample_weight(self):
         y_true = tf.constant([0.0])
```

## keras_cv/losses/serialization_test.py

```diff
@@ -40,21 +40,25 @@
         loss = loss_cls(**init_args)
         config = loss.get_config()
         self.assertAllInitParametersAreInConfig(loss_cls, config)
 
         reconstructed_loss = loss_cls.from_config(config)
 
         self.assertTrue(
-            test_utils.config_equals(loss.get_config(), reconstructed_loss.get_config())
+            test_utils.config_equals(
+                loss.get_config(), reconstructed_loss.get_config()
+            )
         )
 
     def assertAllInitParametersAreInConfig(self, loss_cls, config):
         excluded_name = ["args", "kwargs", "*"]
         parameter_names = {
             v
             for v in inspect.signature(loss_cls).parameters.keys()
             if v not in excluded_name
         }
 
-        intersection_with_config = {v for v in config.keys() if v in parameter_names}
+        intersection_with_config = {
+            v for v in config.keys() if v in parameter_names
+        }
 
         self.assertSetEqual(parameter_names, intersection_with_config)
```

## keras_cv/losses/simclr_loss.py

```diff
@@ -14,39 +14,44 @@
 
 import tensorflow as tf
 from tensorflow import keras
 
 LARGE_NUM = 1e9
 
 
-class SimCLRLoss(tf.keras.losses.Loss):
+class SimCLRLoss(keras.losses.Loss):
     """Implements SimCLR Cosine Similarity loss.
 
     SimCLR loss is used for contrastive self-supervised learning.
 
     Args:
-        temperature: a float value between 0 and 1, used as a scaling factor for cosine similarity.
+        temperature: a float value between 0 and 1, used as a scaling factor for
+            cosine similarity.
 
     References:
         - [SimCLR paper](https://arxiv.org/pdf/2002.05709)
     """
 
     def __init__(self, temperature, **kwargs):
         super().__init__(**kwargs)
         self.temperature = temperature
 
     def call(self, projections_1, projections_2):
-        """Computes SimCLR loss for a pair of projections in a contrastive learning trainer.
+        """Computes SimCLR loss for a pair of projections in a contrastive
+        learning trainer.
 
-        Note that unlike most loss functions, this should not be called with y_true and y_pred,
-        but with two unlabeled projections. It can otherwise be treated as a normal loss function.
+        Note that unlike most loss functions, this should not be called with
+        y_true and y_pred, but with two unlabeled projections. It can otherwise
+        be treated as a normal loss function.
 
         Args:
-            projections_1: a tensor with the output of the first projection model in a contrastive learning trainer
-            projections_2: a tensor with the output of the second projection model in a contrastive learning trainer
+            projections_1: a tensor with the output of the first projection
+                model in a contrastive learning trainer
+            projections_2: a tensor with the output of the second projection
+                model in a contrastive learning trainer
 
         Returns:
             A tensor with the SimCLR loss computed from the input projections
         """
         # Normalize the projections
         projections_1 = tf.math.l2_normalize(projections_1, axis=1)
         projections_2 = tf.math.l2_normalize(projections_2, axis=1)
@@ -54,26 +59,30 @@
         # Produce artificial labels, 1 for each image in the batch.
         batch_size = tf.shape(projections_1)[0]
         labels = tf.one_hot(tf.range(batch_size), batch_size * 2)
         masks = tf.one_hot(tf.range(batch_size), batch_size)
 
         # Compute logits
         logits_11 = (
-            tf.matmul(projections_1, projections_1, transpose_b=True) / self.temperature
+            tf.matmul(projections_1, projections_1, transpose_b=True)
+            / self.temperature
         )
         logits_11 = logits_11 - tf.cast(masks * LARGE_NUM, logits_11.dtype)
         logits_22 = (
-            tf.matmul(projections_2, projections_2, transpose_b=True) / self.temperature
+            tf.matmul(projections_2, projections_2, transpose_b=True)
+            / self.temperature
         )
         logits_22 = logits_22 - tf.cast(masks * LARGE_NUM, logits_22.dtype)
         logits_12 = (
-            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature
+            tf.matmul(projections_1, projections_2, transpose_b=True)
+            / self.temperature
         )
         logits_21 = (
-            tf.matmul(projections_2, projections_1, transpose_b=True) / self.temperature
+            tf.matmul(projections_2, projections_1, transpose_b=True)
+            / self.temperature
         )
 
         loss_a = keras.losses.categorical_crossentropy(
             labels, tf.concat([logits_12, logits_11], 1), from_logits=True
         )
         loss_b = keras.losses.categorical_crossentropy(
             labels, tf.concat([logits_21, logits_22], 1), from_logits=True
```

## keras_cv/losses/simclr_loss_test.py

```diff
@@ -36,15 +36,17 @@
         )
         projections_2 = tf.random.uniform(
             shape=(10, 128), minval=0, maxval=10, dtype=tf.float32
         )
 
         simclr_loss = SimCLRLoss(temperature=1, reduction="none")
 
-        self.assertAllEqual(simclr_loss(projections_1, projections_2).shape, (10,))
+        self.assertAllEqual(
+            simclr_loss(projections_1, projections_2).shape, (10,)
+        )
 
     def test_output_value(self):
         projections_1 = [
             [1.0, 2.0, 3.0, 4.0],
             [2.0, 3.0, 4.0, 5.0],
             [3.0, 4.0, 5.0, 6.0],
         ]
```

## keras_cv/losses/smooth_l1.py

```diff
@@ -9,27 +9,29 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 
 # --- Implementing Smooth L1 loss and Focal Loss as keras custom losses ---
-class SmoothL1Loss(tf.keras.losses.Loss):
+class SmoothL1Loss(keras.losses.Loss):
     """Implements Smooth L1 loss.
 
-    SmoothL1Loss implements the SmoothL1 function, where values less than `l1_cutoff`
-    contribute to the overall loss based on their squared difference, and values greater
-    than l1_cutoff contribute based on their raw difference.
+    SmoothL1Loss implements the SmoothL1 function, where values less than
+    `l1_cutoff` contribute to the overall loss based on their squared
+    difference, and values greater than l1_cutoff contribute based on their raw
+    difference.
 
     Args:
-        l1_cutoff: differences between y_true and y_pred that are larger than `l1_cutoff` are
-            treated as `L1` values
+        l1_cutoff: differences between y_true and y_pred that are larger than
+            `l1_cutoff` are treated as `L1` values
     """
 
     def __init__(self, l1_cutoff=1.0, **kwargs):
         super().__init__(**kwargs)
         self.l1_cutoff = l1_cutoff
 
     def call(self, y_true, y_pred):
@@ -37,15 +39,15 @@
         absolute_difference = tf.abs(difference)
         squared_difference = difference**2
         loss = tf.where(
             absolute_difference < self.l1_cutoff,
             0.5 * squared_difference,
             absolute_difference - 0.5,
         )
-        return tf.keras.backend.mean(loss, axis=-1)
+        return keras.backend.mean(loss, axis=-1)
 
     def get_config(self):
         config = {
             "l1_cutoff": self.l1_cutoff,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
```

## keras_cv/metrics/__init__.py

```diff
@@ -8,9 +8,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from keras_cv.metrics.coco.mean_average_precision import _COCOMeanAveragePrecision
-from keras_cv.metrics.coco.recall import _COCORecall
+from keras_cv.metrics.object_detection.box_coco_metrics import BoxCOCOMetrics
```

## keras_cv/metrics/coco/__init__.py

```diff
@@ -7,19 +7,9 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-from keras_cv.metrics.coco.mean_average_precision import _COCOMeanAveragePrecision
-
-try:
-    from keras_cv.metrics.coco.pycoco_wrapper import PyCOCOWrapper
-    from keras_cv.metrics.coco.pycoco_wrapper import compute_pycoco_metrics
-except ImportError:
-    print(
-        "You do not have pycocotools installed, so KerasCV pycoco metrics are not available. "
-        "Please run `pip install pycocotools`."
-    )
-    pass
+from keras_cv.metrics.coco.pycoco_wrapper import PyCOCOWrapper
+from keras_cv.metrics.coco.pycoco_wrapper import compute_pycoco_metrics
```

## keras_cv/metrics/coco/pycoco_wrapper.py

```diff
@@ -11,16 +11,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import copy
 
 import numpy as np
 import tensorflow as tf
-from pycocotools.coco import COCO
-from pycocotools.cocoeval import COCOeval
+
+try:
+    from pycocotools.coco import COCO
+    from pycocotools.cocoeval import COCOeval
+except ImportError:
+    COCO = object
+    COCOeval = None
+
+from keras_cv.utils.conditional_imports import assert_pycocotools_installed
 
 METRIC_NAMES = [
     "AP",
     "AP50",
     "AP75",
     "APs",
     "APm",
@@ -45,44 +52,46 @@
          dictionary.
     """
 
     def __init__(self, gt_dataset=None):
         """Instantiates a COCO-style API object.
         Args:
           eval_type: either 'box' or 'mask'.
-          annotation_file: a JSON file that stores annotations of the eval dataset.
-            This is required if `gt_dataset` is not provided.
+          annotation_file: a JSON file that stores annotations of the eval
+            dataset. This is required if `gt_dataset` is not provided.
           gt_dataset: the groundtruth eval datatset in COCO API format.
         """
-
+        assert_pycocotools_installed("PyCOCOWrapper")
         COCO.__init__(self, annotation_file=None)
         self._eval_type = "box"
         if gt_dataset:
             self.dataset = gt_dataset
             self.createIndex()
 
     def loadRes(self, predictions):
         """Loads result file and return a result api object.
         Args:
-          predictions: a list of dictionary each representing an annotation in COCO
-            format. The required fields are `image_id`, `category_id`, `score`,
-            `bbox`, `segmentation`.
+          predictions: a list of dictionary each representing an annotation in
+            COCO format. The required fields are `image_id`, `category_id`,
+            `score`, `bbox`, `segmentation`.
         Returns:
           res: result COCO api object.
         Raises:
-          ValueError: if the set of image id from predictions is not the subset of
-            the set of image id of the groundtruth dataset.
+          ValueError: if the set of image id from predictions is not the subset
+            of the set of image id of the groundtruth dataset.
         """
         res = COCO()
         res.dataset["images"] = copy.deepcopy(self.dataset["images"])
         res.dataset["categories"] = copy.deepcopy(self.dataset["categories"])
 
         image_ids = [ann["image_id"] for ann in predictions]
         if set(image_ids) != (set(image_ids) & set(self.getImgIds())):
-            raise ValueError("Results do not correspond to the current dataset!")
+            raise ValueError(
+                "Results do not correspond to the current dataset!"
+            )
         for ann in predictions:
             x1, x2, y1, y2 = [
                 ann["bbox"][0],
                 ann["bbox"][0] + ann["bbox"][2],
                 ann["bbox"][1],
                 ann["bbox"][1] + ann["bbox"][3],
             ]
@@ -101,15 +110,17 @@
             "boxes.shape[-1] is {:d}, but must be 4.".format(boxes.shape[-1])
         )
 
     boxes_ymin = boxes[..., 0]
     boxes_xmin = boxes[..., 1]
     boxes_width = boxes[..., 3] - boxes[..., 1]
     boxes_height = boxes[..., 2] - boxes[..., 0]
-    new_boxes = np.stack([boxes_xmin, boxes_ymin, boxes_width, boxes_height], axis=-1)
+    new_boxes = np.stack(
+        [boxes_xmin, boxes_ymin, boxes_width, boxes_height], axis=-1
+    )
 
     return new_boxes
 
 
 def _convert_predictions_to_coco_annotations(predictions):
     coco_predictions = []
     num_batches = len(predictions["source_id"])
@@ -132,20 +143,15 @@
         ann["id"] = i + 1
 
     return coco_predictions
 
 
 def _convert_groundtruths_to_coco_dataset(groundtruths, label_map=None):
     source_ids = np.concatenate(groundtruths["source_id"], axis=0)
-    heights = np.concatenate(groundtruths["height"], axis=0)
-    widths = np.concatenate(groundtruths["width"], axis=0)
-    gt_images = [
-        {"id": i, "height": int(h), "width": int(w)}
-        for i, h, w in zip(source_ids, heights, widths)
-    ]
+    gt_images = [{"id": i} for i in source_ids]
 
     gt_annotations = []
     num_batches = len(groundtruths["source_id"])
     for i in range(num_batches):
         max_num_instances = groundtruths["classes"][i].shape[1]
         batch_size = groundtruths["source_id"][i].shape[0]
         for j in range(batch_size):
@@ -208,14 +214,16 @@
             val = np.concatenate(val)
         numpy_predictions[key] = val
 
     return numpy_groundtruths, numpy_predictions
 
 
 def compute_pycoco_metrics(groundtruths, predictions):
+    assert_pycocotools_installed("compute_pycoco_metrics")
+
     groundtruths, predictions = _convert_to_numpy(groundtruths, predictions)
 
     gt_dataset = _convert_groundtruths_to_coco_dataset(groundtruths)
     coco_gt = PyCOCOWrapper(gt_dataset=gt_dataset)
     coco_predictions = _convert_predictions_to_coco_annotations(predictions)
     coco_dt = coco_gt.loadRes(predictions=coco_predictions)
     image_ids = [ann["image_id"] for ann in coco_predictions]
```

## keras_cv/models/__init__.py

```diff
@@ -1,109 +1,114 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from keras_cv.models.convmixer import ConvMixer_512_16
-from keras_cv.models.convmixer import ConvMixer_768_32
-from keras_cv.models.convmixer import ConvMixer_1024_16
-from keras_cv.models.convmixer import ConvMixer_1536_20
-from keras_cv.models.convmixer import ConvMixer_1536_24
-from keras_cv.models.convnext import ConvNeXtBase
-from keras_cv.models.convnext import ConvNeXtLarge
-from keras_cv.models.convnext import ConvNeXtSmall
-from keras_cv.models.convnext import ConvNeXtTiny
-from keras_cv.models.convnext import ConvNeXtXLarge
-from keras_cv.models.csp_darknet import CSPDarkNetL
-from keras_cv.models.csp_darknet import CSPDarkNetM
-from keras_cv.models.csp_darknet import CSPDarkNetS
-from keras_cv.models.csp_darknet import CSPDarkNetTiny
-from keras_cv.models.csp_darknet import CSPDarkNetX
-from keras_cv.models.darknet import DarkNet21
-from keras_cv.models.darknet import DarkNet53
-from keras_cv.models.densenet import DenseNet121
-from keras_cv.models.densenet import DenseNet169
-from keras_cv.models.densenet import DenseNet201
-from keras_cv.models.efficientnet_lite import EfficientNetLiteB0
-from keras_cv.models.efficientnet_lite import EfficientNetLiteB1
-from keras_cv.models.efficientnet_lite import EfficientNetLiteB2
-from keras_cv.models.efficientnet_lite import EfficientNetLiteB3
-from keras_cv.models.efficientnet_lite import EfficientNetLiteB4
-from keras_cv.models.efficientnet_v1 import EfficientNetB0
-from keras_cv.models.efficientnet_v1 import EfficientNetB1
-from keras_cv.models.efficientnet_v1 import EfficientNetB2
-from keras_cv.models.efficientnet_v1 import EfficientNetB3
-from keras_cv.models.efficientnet_v1 import EfficientNetB4
-from keras_cv.models.efficientnet_v1 import EfficientNetB5
-from keras_cv.models.efficientnet_v1 import EfficientNetB6
-from keras_cv.models.efficientnet_v1 import EfficientNetB7
-from keras_cv.models.efficientnet_v2 import EfficientNetV2B0
-from keras_cv.models.efficientnet_v2 import EfficientNetV2B1
-from keras_cv.models.efficientnet_v2 import EfficientNetV2B2
-from keras_cv.models.efficientnet_v2 import EfficientNetV2B3
-from keras_cv.models.efficientnet_v2 import EfficientNetV2L
-from keras_cv.models.efficientnet_v2 import EfficientNetV2M
-from keras_cv.models.efficientnet_v2 import EfficientNetV2S
-from keras_cv.models.mlp_mixer import MLPMixerB16
-from keras_cv.models.mlp_mixer import MLPMixerB32
-from keras_cv.models.mlp_mixer import MLPMixerL16
-from keras_cv.models.mobilenet_v3 import MobileNetV3Large
-from keras_cv.models.mobilenet_v3 import MobileNetV3Small
-from keras_cv.models.object_detection.faster_rcnn import FasterRCNN
-from keras_cv.models.object_detection.retina_net.retina_net import RetinaNet
-from keras_cv.models.object_detection_3d.center_pillar import MultiHeadCenterPillar
-from keras_cv.models.regnet import RegNetX002
-from keras_cv.models.regnet import RegNetX004
-from keras_cv.models.regnet import RegNetX006
-from keras_cv.models.regnet import RegNetX008
-from keras_cv.models.regnet import RegNetX016
-from keras_cv.models.regnet import RegNetX032
-from keras_cv.models.regnet import RegNetX040
-from keras_cv.models.regnet import RegNetX064
-from keras_cv.models.regnet import RegNetX080
-from keras_cv.models.regnet import RegNetX120
-from keras_cv.models.regnet import RegNetX160
-from keras_cv.models.regnet import RegNetX320
-from keras_cv.models.regnet import RegNetY002
-from keras_cv.models.regnet import RegNetY004
-from keras_cv.models.regnet import RegNetY006
-from keras_cv.models.regnet import RegNetY008
-from keras_cv.models.regnet import RegNetY016
-from keras_cv.models.regnet import RegNetY032
-from keras_cv.models.regnet import RegNetY040
-from keras_cv.models.regnet import RegNetY064
-from keras_cv.models.regnet import RegNetY080
-from keras_cv.models.regnet import RegNetY120
-from keras_cv.models.regnet import RegNetY160
-from keras_cv.models.regnet import RegNetY320
-from keras_cv.models.resnet_v1 import ResNet18
-from keras_cv.models.resnet_v1 import ResNet34
-from keras_cv.models.resnet_v1 import ResNet50
-from keras_cv.models.resnet_v1 import ResNet101
-from keras_cv.models.resnet_v1 import ResNet152
-from keras_cv.models.resnet_v2 import ResNet50V2
-from keras_cv.models.resnet_v2 import ResNet101V2
-from keras_cv.models.resnet_v2 import ResNet152V2
-from keras_cv.models.segmentation.deeplab import DeepLabV3
+from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
+    CSPDarkNetBackbone,
+)
+from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
+    CSPDarkNetLBackbone,
+)
+from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
+    CSPDarkNetMBackbone,
+)
+from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
+    CSPDarkNetSBackbone,
+)
+from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
+    CSPDarkNetTinyBackbone,
+)
+from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
+    CSPDarkNetXLBackbone,
+)
+from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
+    EfficientNetV2B0Backbone,
+)
+from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
+    EfficientNetV2B1Backbone,
+)
+from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
+    EfficientNetV2B2Backbone,
+)
+from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
+    EfficientNetV2B3Backbone,
+)
+from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
+    EfficientNetV2Backbone,
+)
+from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
+    EfficientNetV2LBackbone,
+)
+from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
+    EfficientNetV2MBackbone,
+)
+from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
+    EfficientNetV2SBackbone,
+)
+from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
+    MobileNetV3Backbone,
+)
+from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
+    MobileNetV3LargeBackbone,
+)
+from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
+    MobileNetV3SmallBackbone,
+)
+from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+    ResNet18Backbone,
+)
+from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+    ResNet34Backbone,
+)
+from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+    ResNet50Backbone,
+)
+from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+    ResNet101Backbone,
+)
+from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+    ResNet152Backbone,
+)
+from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+    ResNetBackbone,
+)
+from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+    ResNet18V2Backbone,
+)
+from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+    ResNet34V2Backbone,
+)
+from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+    ResNet50V2Backbone,
+)
+from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+    ResNet101V2Backbone,
+)
+from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+    ResNet152V2Backbone,
+)
+from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+    ResNetV2Backbone,
+)
+from keras_cv.models.classification.image_classifier import ImageClassifier
+from keras_cv.models.object_detection.retinanet.retinanet import RetinaNet
+from keras_cv.models.object_detection.yolo_v8.yolo_v8_backbone import (
+    YOLOV8Backbone,
+)
+from keras_cv.models.object_detection.yolo_v8.yolo_v8_detector import (
+    YOLOV8Detector,
+)
+from keras_cv.models.object_detection_3d.center_pillar import (
+    MultiHeadCenterPillar,
+)
 from keras_cv.models.stable_diffusion import StableDiffusion
 from keras_cv.models.stable_diffusion import StableDiffusionV2
-from keras_cv.models.vgg16 import VGG16
-from keras_cv.models.vgg19 import VGG19
-from keras_cv.models.vit import ViTB16
-from keras_cv.models.vit import ViTB32
-from keras_cv.models.vit import ViTH16
-from keras_cv.models.vit import ViTH32
-from keras_cv.models.vit import ViTL16
-from keras_cv.models.vit import ViTL32
-from keras_cv.models.vit import ViTS16
-from keras_cv.models.vit import ViTS32
-from keras_cv.models.vit import ViTTiny16
-from keras_cv.models.vit import ViTTiny32
```

## keras_cv/models/utils.py

```diff
@@ -10,80 +10,19 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 """Utility functions for models"""
 
-import tensorflow as tf
+from keras import layers
 from tensorflow import keras
-from tensorflow.keras import layers
 
 
 def parse_model_inputs(input_shape, input_tensor):
     if input_tensor is None:
         return layers.Input(shape=input_shape)
     else:
         if not keras.backend.is_keras_tensor(input_tensor):
             return layers.Input(tensor=input_tensor, shape=input_shape)
         else:
             return input_tensor
-
-
-def as_backbone(self, min_level=None, max_level=None):
-    """Convert the application model into a model backbone for other tasks.
-
-    The backbone model will usually take same inputs as the original application
-    model, but produce multiple outputs, one for each feature level. Those outputs
-    can be feed to network downstream, like FPN and RPN.
-
-    The output of the backbone model will be a dict with int as key and tensor as
-    value. The int key represent the level of the feature output.
-    A typical feature pyramid has five levels corresponding to scales P3, P4, P5,
-    P6, P7 in the backbone. Scale Pn represents a feature map 2n times smaller in
-    width and height than the input image.
-
-    Args:
-        min_level: optional int, the lowest level of feature to be included in the
-            output. Default to model's lowest feature level (based on the model structure).
-        max_level: optional int, the highest level of feature to be included in the
-            output. Default to model's highest feature level (based on the model structure).
-
-    Returns:
-        a `tf.keras.Model` which has dict as outputs.
-    Raises:
-        ValueError: When the model is lack of information for feature level, and can't
-        be converted to backbone model, or the min_level/max_level param is out of
-        range based on the model structure.
-    """
-    if hasattr(self, "_backbone_level_outputs"):
-        backbone_level_outputs = self._backbone_level_outputs
-        model_levels = list(sorted(backbone_level_outputs.keys()))
-        if min_level is not None:
-            if min_level < model_levels[0]:
-                raise ValueError(
-                    f"The min_level provided: {min_level} should be in "
-                    f"the range of {model_levels}"
-                )
-        else:
-            min_level = model_levels[0]
-
-        if max_level is not None:
-            if max_level > model_levels[-1]:
-                raise ValueError(
-                    f"The max_level provided: {max_level} should be in "
-                    f"the range of {model_levels}"
-                )
-        else:
-            max_level = model_levels[-1]
-
-        outputs = {}
-        for level in range(min_level, max_level + 1):
-            outputs[level] = backbone_level_outputs[level]
-
-        return tf.keras.Model(inputs=self.inputs, outputs=outputs)
-
-    else:
-        raise ValueError(
-            "The current model doesn't have any feature level "
-            "information and can't be convert to backbone model."
-        )
```

## keras_cv/models/utils_test.py

```diff
@@ -23,45 +23,10 @@
     def test_parse_model_inputs(self):
         input_shape = (224, 244, 3)
 
         inputs = utils.parse_model_inputs(input_shape, None)
         self.assertEqual(inputs.shape.as_list(), list((None,) + input_shape))
 
         input_tensor = layers.Input(shape=input_shape)
-        self.assertIs(utils.parse_model_inputs(input_shape, input_tensor), input_tensor)
-
-    def test_as_backbone_missing_backbone_level_outputs(self):
-        model = tf.keras.models.Sequential()
-        model.add(layers.Conv2D(64, kernel_size=3, input_shape=(16, 16, 3)))
-        model.add(
-            layers.Conv2D(
-                32,
-                kernel_size=3,
-            )
+        self.assertIs(
+            utils.parse_model_inputs(input_shape, input_tensor), input_tensor
         )
-        model.add(layers.Dense(10))
-        with self.assertRaises(ValueError):
-            utils.as_backbone(model)
-
-    def test_as_backbone_util(self):
-        inp = layers.Input((16, 16, 3))
-        _backbone_level_outputs = {}
-
-        x = layers.Conv2D(64, kernel_size=3, input_shape=(16, 16, 3))(inp)
-        _backbone_level_outputs[2] = x
-
-        x = layers.Conv2D(
-            32,
-            kernel_size=3,
-        )(x)
-        _backbone_level_outputs[3] = x
-
-        out = layers.Dense(10)(x)
-        _backbone_level_outputs[4] = out
-
-        model = tf.keras.models.Model(inputs=inp, outputs=out)
-
-        # when model has _backbone_level_outputs, it should not raise an error
-        model._backbone_level_outputs = _backbone_level_outputs
-
-        backbone = utils.as_backbone(model)
-        self.assertEqual(len(backbone.outputs), 3)
```

## keras_cv/models/__internal__/__init__.py

```diff
@@ -7,14 +7,7 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-from keras_cv.models.__internal__.darknet_utils import CrossStagePartial
-from keras_cv.models.__internal__.darknet_utils import DarknetConvBlock
-from keras_cv.models.__internal__.darknet_utils import DarknetConvBlockDepthwise
-from keras_cv.models.__internal__.darknet_utils import Focus
-from keras_cv.models.__internal__.darknet_utils import ResidualBlocks
-from keras_cv.models.__internal__.darknet_utils import SpatialPyramidPoolingBottleneck
```

## keras_cv/models/__internal__/unet.py

```diff
@@ -175,16 +175,16 @@
     included.
 
     All function parameters require curried functions as inputs which return a
     function that acts on tensors as inputs.
 
     Args:
         input_shape: the rank 3 shape of the input to the UNet
-        down_block_configs: a list of (filter_count, num_blocks) tuples indicating the
-            number of filters and sub-blocks in each down block
+        down_block_configs: a list of (filter_count, num_blocks) tuples
+            indicating the number of filters and sub-blocks in each down block
         up_block_configs: a list of filter counts, one for each up block
         down_block: a downsampling block
         up_block: an upsampling block
     """
 
     input = layers.Input(shape=input_shape)
     x = input
```

## keras_cv/models/object_detection/__internal__.py

```diff
@@ -34,23 +34,26 @@
 def _get_tensor_types():
     if pd is None:
         return (tf.Tensor, np.ndarray)
     else:
         return (tf.Tensor, np.ndarray, pd.Series, pd.DataFrame)
 
 
-def convert_inputs_to_tf_dataset(x=None, y=None, sample_weight=None, batch_size=None):
+def convert_inputs_to_tf_dataset(
+    x=None, y=None, sample_weight=None, batch_size=None
+):
     if sample_weight is not None:
         raise ValueError("RetinaNet does not yet support `sample_weight`.")
 
     if isinstance(x, tf.data.Dataset):
         if y is not None or batch_size is not None:
             raise ValueError(
-                "When `x` is a `tf.data.Dataset`, please do not provide a value for "
-                f"`y` or `batch_size`.  Got `y={y}`, `batch_size={batch_size}`."
+                "When `x` is a `tf.data.Dataset`, please do not provide a "
+                f"value for `y` or `batch_size`. Got `y={y}`, "
+                f"`batch_size={batch_size}`."
             )
         return x
 
     # batch_size defaults to 32, as it does in fit().
     batch_size = batch_size or 32
     # Parse inputs
     inputs = x
```

## keras_cv/models/object_detection/__test_utils__.py

```diff
@@ -12,28 +12,49 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 
 import keras_cv
 
 
-def _create_bounding_box_dataset(bounding_box_format, use_dictionary_box_format=False):
-    # Just about the easiest dataset you can have, all classes are 0, all boxes are
-    # exactly the same.  [1, 1, 2, 2] are the coordinates in xyxy
-    xs = tf.ones((5, 256, 256, 3), dtype=tf.float32)
-    y_classes = tf.zeros((5, 10), dtype=tf.float32)
+def _create_bounding_box_dataset(
+    bounding_box_format, use_dictionary_box_format=False
+):
+    # Just about the easiest dataset you can have, all classes are 0, all boxes
+    # are exactly the same. [1, 1, 2, 2] are the coordinates in xyxy.
+    xs = tf.random.normal(shape=(1, 512, 512, 3), dtype=tf.float32)
+    xs = tf.tile(xs, [5, 1, 1, 1])
 
-    ys = tf.constant([0.25, 0.25, 0.1, 0.1], dtype=tf.float32)
-    ys = tf.expand_dims(ys, axis=0)
+    y_classes = tf.zeros((5, 3), dtype=tf.float32)
+
+    ys = tf.constant(
+        [
+            [0.1, 0.1, 0.23, 0.23],
+            [0.67, 0.75, 0.23, 0.23],
+            [0.25, 0.25, 0.23, 0.23],
+        ],
+        dtype=tf.float32,
+    )
     ys = tf.expand_dims(ys, axis=0)
-    ys = tf.tile(ys, [5, 10, 1])
+    ys = tf.tile(ys, [5, 1, 1])
     ys = keras_cv.bounding_box.convert_format(
-        ys, source="rel_xywh", target=bounding_box_format, images=xs, dtype=tf.float32
+        ys,
+        source="rel_xywh",
+        target=bounding_box_format,
+        images=xs,
+        dtype=tf.float32,
     )
     num_dets = tf.ones([5])
 
     if use_dictionary_box_format:
         return tf.data.Dataset.from_tensor_slices(
-            (xs, {"boxes": ys, "classes": y_classes, "num_dets": num_dets})
+            {
+                "images": xs,
+                "bounding_boxes": {
+                    "boxes": ys,
+                    "classes": y_classes,
+                    "num_dets": num_dets,
+                },
+            }
         ).batch(5, drop_remainder=True)
     else:
         return xs, {"boxes": ys, "classes": y_classes}
```

## keras_cv/models/object_detection/predict_utils.py

```diff
@@ -29,15 +29,17 @@
             outputs = model.predict_step(data)
             # Ensure counter is updated only if `test_step` succeeds.
             with tf.control_dependencies(_minimum_control_deps(outputs)):
                 model._predict_counter.assign_add(1)
             return outputs
 
         if model._jit_compile:
-            run_step = tf.function(run_step, jit_compile=True, reduce_retracing=True)
+            run_step = tf.function(
+                run_step, jit_compile=True, reduce_retracing=True
+            )
 
         data = next(iterator)
         outputs = model.distribute_strategy.run(run_step, args=(data,))
         outputs = reduce_per_replica(
             outputs, model.distribute_strategy, reduction="concat"
         )
         # Note that this is the only deviation from the base keras.Model
```

## keras_cv/models/object_detection_3d/center_pillar.py

```diff
@@ -12,89 +12,90 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import List
 from typing import Sequence
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.object_detection_3d.heatmap_decoder import HeatmapDecoder
 
 
-class MultiClassDetectionHead(tf.keras.layers.Layer):
+class MultiClassDetectionHead(keras.layers.Layer):
     """Multi-class object detection head."""
 
     def __init__(
         self,
-        num_class: int,
+        num_classes: int,
         num_head_bin: Sequence[int],
         share_head: bool = False,
         name: str = "detection_head",
     ):
         super().__init__(name=name)
 
         self._heads = {}
         self._head_names = []
         self._per_class_prediction_size = []
-        self._num_class = num_class
+        self._num_classes = num_classes
         self._num_head_bin = num_head_bin
-        for i in range(num_class):
+        for i in range(num_classes):
             self._head_names.append(f"class_{i + 1}")
             size = 0
             # 0:1 outputs is for classification
             size += 2
             # 2:4 outputs is for location offset
             size += 3
             # 5:7 outputs is for dimension offset
             size += 3
             # 8:end outputs is for bin-based classification and regression
             size += 2 * num_head_bin[i]
             self._per_class_prediction_size.append(size)
 
         if not share_head:
-            for i in range(num_class):
+            for i in range(num_classes):
                 # 1x1 conv for each voxel/pixel.
-                self._heads[self._head_names[i]] = tf.keras.layers.Conv2D(
+                self._heads[self._head_names[i]] = keras.layers.Conv2D(
                     filters=self._per_class_prediction_size[i],
                     kernel_size=(1, 1),
                     name=f"head_{i + 1}",
                 )
         else:
-            shared_layer = tf.keras.layers.Conv2D(
+            shared_layer = keras.layers.Conv2D(
                 filters=self._per_class_prediction_size[0],
                 kernel_size=(1, 1),
                 name="shared_head",
             )
-            for i in range(num_class):
+            for i in range(num_classes):
                 self._heads[self._head_names[i]] = shared_layer
 
     def call(self, feature: tf.Tensor, training: bool) -> List[tf.Tensor]:
         del training
         outputs = {}
         for head_name in self._head_names:
             outputs[head_name] = self._heads[head_name](feature)
         return outputs
 
 
-class MultiClassHeatmapDecoder(tf.keras.layers.Layer):
+class MultiClassHeatmapDecoder(keras.layers.Layer):
     def __init__(
         self,
-        num_class,
+        num_classes,
         num_head_bin: Sequence[int],
         anchor_size: Sequence[Sequence[float]],
         max_pool_size: Sequence[int],
         max_num_box: Sequence[int],
         heatmap_threshold: Sequence[float],
         voxel_size: Sequence[float],
         spatial_size: Sequence[float],
         **kwargs,
     ):
         super().__init__(**kwargs)
-        self.num_class = num_class
-        self.class_ids = list(range(1, num_class + 1))
+        self.num_classes = num_classes
+        self.class_ids = list(range(1, num_classes + 1))
         self.num_head_bin = num_head_bin
         self.anchor_size = anchor_size
         self.max_pool_size = max_pool_size
         self.max_num_box = max_num_box
         self.heatmap_threshold = heatmap_threshold
         self.voxel_size = voxel_size
         self.spatial_size = spatial_size
@@ -108,112 +109,155 @@
                 max_num_box=self.max_num_box[i],
                 heatmap_threshold=self.heatmap_threshold[i],
                 voxel_size=self.voxel_size,
                 spatial_size=self.spatial_size,
             )
 
     def call(self, predictions):
-        decoded_predictions = {}
+        box_predictions = []
+        class_predictions = []
+        box_confidence = []
         for k, v in predictions.items():
-            decoded_predictions[k] = self.decoders[k](v)
-        return decoded_predictions
+            boxes, classes, confidence = self.decoders[k](v)
+            box_predictions.append(boxes)
+            class_predictions.append(classes)
+            box_confidence.append(confidence)
+
+        return {
+            "3d_boxes": {
+                "boxes": tf.concat(box_predictions, axis=1),
+                "classes": tf.concat(class_predictions, axis=1),
+                "confidence": tf.concat(box_confidence, axis=1),
+            }
+        }
 
 
-class MultiHeadCenterPillar(tf.keras.Model):
+class MultiHeadCenterPillar(keras.Model):
     """Multi headed model based on CenterNet heatmap and PointPillar.
 
     This model builds box classification and regression for each class
     separately. It voxelizes the point cloud feature, applies feature extraction
     on top of voxelized feature, and applies multi-class classification and
     regression heads on the feature map.
 
     Args:
       backbone: the backbone to apply to voxelized features.
       voxel_net: the voxel_net that takes point cloud feature and convert
         to voxelized features.
-      multiclass_head: a multi class head which returns a dict of heatmap prediction
-        and regression prediction per class.
+      multiclass_head: a multi class head which returns a dict of heatmap
+        prediction and regression prediction per class.
       label_encoder: a LabelEncoder that takes point cloud xyz and point cloud
         features and returns a multi class labels which is a dict of heatmap,
         box location and top_k heatmap index per class.
-      prediction_decoder: a multi class heatmap prediction decoder that returns a dict
-        of decoded boxes, box class, and box confidence score per class.
+      prediction_decoder: a multi class heatmap prediction decoder that returns
+        a dict of decoded boxes, box class, and box confidence score per class.
 
 
     """
 
     def __init__(
         self,
         backbone,
         voxel_net,
         multiclass_head,
-        label_encoder,
         prediction_decoder,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self._voxelization_layer = voxel_net
         self._unet_layer = backbone
         self._multiclass_head = multiclass_head
-        self._label_encoder = label_encoder
         self._prediction_decoder = prediction_decoder
         self._head_names = self._multiclass_head._head_names
 
-    def call(self, point_xyz, point_feature, point_mask, training=None):
+    def call(self, input_dict, training=None):
+        point_xyz, point_feature, point_mask = (
+            input_dict["point_xyz"],
+            input_dict["point_feature"],
+            input_dict["point_mask"],
+        )
         voxel_feature = self._voxelization_layer(
             point_xyz, point_feature, point_mask, training=training
         )
         voxel_feature = self._unet_layer(voxel_feature, training=training)
         predictions = self._multiclass_head(voxel_feature)
         if not training:
             predictions = self._prediction_decoder(predictions)
         # returns dict {"class_1": concat_pred_1, "class_2": concat_pred_2}
         return predictions
 
-    def compute_loss(self, predictions, box_dict, heatmap_dict, top_k_index_dict):
+    def compile(self, heatmap_loss=None, box_loss=None, **kwargs):
+        """Compiles the MultiHeadCenterPillar.
+
+        `compile()` mirrors the standard Keras `compile()` method, but allows
+        for specification of heatmap and box-specific losses.
+
+        Args:
+            heatmap_loss: a Keras loss to use for heatmap regression.
+            box_loss: a Keras loss to use for box regression, or a list of Keras
+                losses for box regression, one for each class. If only one loss
+                is specified, it will be used for all classes, otherwise exactly
+                one loss should be specified per class.
+            kwargs: other `keras.Model.compile()` arguments are supported and
+                propagated to the `keras.Model` class.
+        """
+        losses = {}
+
+        if box_loss is not None and not isinstance(box_loss, list):
+            box_loss = [
+                box_loss for _ in range(self._multiclass_head._num_classes)
+            ]
+        for i in range(self._multiclass_head._num_classes):
+            losses[f"heatmap_class_{i+1}"] = heatmap_loss
+            losses[f"box_class_{i+1}"] = box_loss[i]
+
+        super().compile(loss=losses, **kwargs)
+
+    def compute_loss(self, predictions=None, targets=None):
         y_pred = {}
         y_true = {}
         sample_weight = {}
+
         for head_name in self._head_names:
             prediction = predictions[head_name]
             heatmap_pred = tf.nn.softmax(prediction[..., :2])[..., 1]
             box_pred = prediction[..., 2:]
-            box = box_dict[head_name]
-            heatmap = heatmap_dict[head_name]
-            index = top_k_index_dict[head_name]
+            box = targets[head_name]["boxes"]
+            heatmap = targets[head_name]["heatmap"]
+            index = targets[head_name]["top_k_index"]
+
             # the prediction returns 2 outputs for background vs object
             y_pred["heatmap_" + head_name] = heatmap_pred
             y_true["heatmap_" + head_name] = heatmap
-            sample_weight["heatmap_" + head_name] = tf.ones_like(heatmap)
-            # heatmap_groundtruth_gather = tf.gather_nd(heatmap, index, batch_dims=1)
-            # TODO(tanzhenyu): loss heatmap threshold be configurable.
-            # box_regression_mask = heatmap_groundtruth_gather >= 0.95
+
+            # TODO(ianstenbit): loss heatmap threshold should be configurable.
+            box_regression_mask = (
+                tf.gather_nd(heatmap, index, batch_dims=1) >= 0.95
+            )
             box = tf.gather_nd(box, index, batch_dims=1)
             box_pred = tf.gather_nd(box_pred, index, batch_dims=1)
-            y_pred["bin_" + head_name] = box_pred
-            y_true["bin_" + head_name] = box
+
+            num_boxes = tf.math.maximum(
+                tf.reduce_sum(tf.cast(box_regression_mask, tf.float32)), 1
+            )
+
+            sample_weight["box_" + head_name] = (
+                tf.cast(box_regression_mask, tf.float32) / num_boxes
+            )
+            sample_weight["heatmap_" + head_name] = (
+                tf.ones_like(heatmap) / num_boxes
+            )
+
+            y_pred["box_" + head_name] = box_pred
+            y_true["box_" + head_name] = box
 
         return super().compute_loss(
             x={}, y=y_true, y_pred=y_pred, sample_weight=sample_weight
         )
 
     def train_step(self, data):
-        x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)
-        box_3d_dict = y["box_3d"]
-        heatmap_dict = y["heatmap"]
-        top_k_index_dict = y["top_k_index"]
-        losses = []
+        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)
         with tf.GradientTape() as tape:
-            predictions = self(
-                x["point_xyz"], x["point_feature"], x["point_mask"], training=True
-            )
-            losses.append(
-                self.compute_loss(
-                    predictions, box_3d_dict, heatmap_dict, top_k_index_dict
-                )
-            )
-            if self.weight_decay:
-                for var in self.trainable_variables:
-                    losses.append(self.weight_decay * tf.nn.l2_loss(var))
-            total_loss = tf.math.add_n(losses)
-        self.optimizer.minimize(total_loss, self.trainable_variables, tape=tape)
+            predictions = self(x, training=True)
+            loss = self.compute_loss(predictions, y)
+        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
         return self.compute_metrics({}, {}, {}, sample_weight={})
```

## keras_cv/models/object_detection_3d/center_pillar_test.py

```diff
@@ -9,125 +9,145 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.object_detection_3d.voxelization import DynamicVoxelization
 from keras_cv.models.__internal__.unet import Block
 from keras_cv.models.__internal__.unet import UNet
-from keras_cv.models.object_detection_3d.center_pillar import MultiClassDetectionHead
-from keras_cv.models.object_detection_3d.center_pillar import MultiClassHeatmapDecoder
-from keras_cv.models.object_detection_3d.center_pillar import MultiHeadCenterPillar
+from keras_cv.models.object_detection_3d.center_pillar import (
+    MultiClassDetectionHead,
+)
+from keras_cv.models.object_detection_3d.center_pillar import (
+    MultiClassHeatmapDecoder,
+)
+from keras_cv.models.object_detection_3d.center_pillar import (
+    MultiHeadCenterPillar,
+)
 
 down_block_configs = [(128, 6), (256, 2), (512, 2)]
 up_block_configs = [512, 256, 256]
 
 
 class CenterPillarTest(tf.test.TestCase):
     def get_point_net(self):
-        return tf.keras.Sequential(
+        return keras.Sequential(
             [
-                tf.keras.layers.Dense(10),
-                tf.keras.layers.Dense(20),
+                keras.layers.Dense(10),
+                keras.layers.Dense(20),
             ]
         )
 
     def build_centerpillar_unet(self, input_shape):
-        input = tf.keras.layers.Input(shape=input_shape)
-        x = tf.keras.layers.Conv2D(
+        input = keras.layers.Input(shape=input_shape)
+        x = keras.layers.Conv2D(
             128,
             1,
             1,
             padding="same",
-            kernel_initializer=tf.keras.initializers.VarianceScaling(),
-            kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4),
+            kernel_initializer=keras.initializers.VarianceScaling(),
+            kernel_regularizer=keras.regularizers.L2(l2=1e-4),
         )(input)
-        x = tf.keras.layers.BatchNormalization(
-            beta_regularizer=tf.keras.regularizers.L2(l2=1e-8),
-            gamma_regularizer=tf.keras.regularizers.L2(l2=1e-8),
+        x = keras.layers.BatchNormalization(
+            beta_regularizer=keras.regularizers.L2(l2=1e-8),
+            gamma_regularizer=keras.regularizers.L2(l2=1e-8),
         )(x)
-        x = tf.keras.layers.ReLU()(x)
+        x = keras.layers.ReLU()(x)
         x = Block(128, downsample=False, sync_bn=False)(x)
-        output = UNet(x.shape[1:], down_block_configs, up_block_configs, sync_bn=False)(
-            x
-        )
-        return tf.keras.Model(input, output)
+        output = UNet(
+            x.shape[1:], down_block_configs, up_block_configs, sync_bn=False
+        )(x)
+        return keras.Model(input, output)
 
     def test_center_pillar_call(self):
         voxel_net = DynamicVoxelization(
             point_net=self.get_point_net(),
             voxel_size=[0.1, 0.1, 1000],
             spatial_size=[-20, 20, -20, 20, -20, 20],
         )
         # dimensions computed from voxel_net
         unet = self.build_centerpillar_unet([400, 400, 20])
         decoder = MultiClassHeatmapDecoder(
-            num_class=2,
+            num_classes=2,
             num_head_bin=[2, 2],
             anchor_size=[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],
             max_pool_size=[3, 3],
             max_num_box=[3, 4],
             heatmap_threshold=[0.2, 0.2],
             voxel_size=voxel_net._voxel_size,
             spatial_size=voxel_net._spatial_size,
         )
         multiclass_head = MultiClassDetectionHead(
-            num_class=2,
+            num_classes=2,
             num_head_bin=[2, 2],
         )
         model = MultiHeadCenterPillar(
             backbone=unet,
             voxel_net=voxel_net,
             multiclass_head=multiclass_head,
-            label_encoder=None,
             prediction_decoder=decoder,
         )
         point_xyz = tf.random.normal([2, 1000, 3])
         point_feature = tf.random.normal([2, 1000, 4])
         point_mask = tf.constant(True, shape=[2, 1000])
-        outputs = model(point_xyz, point_feature, point_mask, training=True)
+        outputs = model(
+            {
+                "point_xyz": point_xyz,
+                "point_feature": point_feature,
+                "point_mask": point_mask,
+            },
+            training=True,
+        )
         self.assertEqual(outputs["class_1"].shape, [2, 400, 400, 12])
         self.assertEqual(outputs["class_2"].shape, [2, 400, 400, 12])
 
     def test_center_pillar_predict(self):
         voxel_net = DynamicVoxelization(
             point_net=self.get_point_net(),
             voxel_size=[0.1, 0.1, 1000],
             spatial_size=[-20, 20, -20, 20, -20, 20],
         )
         # dimensions computed from voxel_net
         unet = self.build_centerpillar_unet([400, 400, 20])
         decoder = MultiClassHeatmapDecoder(
-            num_class=2,
+            num_classes=2,
             num_head_bin=[2, 2],
             anchor_size=[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]],
             max_pool_size=[3, 3],
             max_num_box=[3, 4],
             heatmap_threshold=[0.2, 0.2],
             voxel_size=voxel_net._voxel_size,
             spatial_size=voxel_net._spatial_size,
         )
         multiclass_head = MultiClassDetectionHead(
-            num_class=2,
+            num_classes=2,
             num_head_bin=[2, 2],
         )
         model = MultiHeadCenterPillar(
             backbone=unet,
             voxel_net=voxel_net,
             multiclass_head=multiclass_head,
-            label_encoder=None,
             prediction_decoder=decoder,
         )
         point_xyz = tf.random.normal([2, 1000, 3])
         point_feature = tf.random.normal([2, 1000, 4])
         point_mask = tf.constant(True, shape=[2, 1000])
-        outputs = model(point_xyz, point_feature, point_mask, training=False)
+        outputs = model(
+            {
+                "point_xyz": point_xyz,
+                "point_feature": point_feature,
+                "point_mask": point_mask,
+            },
+            training=False,
+        )
         # max number boxes is 3
-        self.assertEqual(outputs["class_1"][0].shape, [2, 3, 7])
-        self.assertEqual(outputs["class_1"][1].shape, [2, 3])
-        self.assertEqual(outputs["class_1"][2].shape, [2, 3])
-        self.assertEqual(outputs["class_2"][0].shape, [2, 4, 7])
-        self.assertEqual(outputs["class_2"][1].shape, [2, 4])
-        self.assertEqual(outputs["class_2"][2].shape, [2, 4])
+        self.assertEqual(outputs["3d_boxes"]["boxes"].shape, [2, 7, 7])
+        self.assertEqual(outputs["3d_boxes"]["classes"].shape, [2, 7])
+        self.assertEqual(outputs["3d_boxes"]["confidence"].shape, [2, 7])
+        self.assertAllEqual(
+            outputs["3d_boxes"]["classes"],
+            tf.constant([1, 1, 1, 2, 2, 2, 2] * 2, shape=(2, 7)),
+        )
```

## keras_cv/models/stable_diffusion/clip_tokenizer.py

```diff
@@ -7,33 +7,35 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""This code is taken nearly verbatim from https://github.com/divamgupta/stable-diffusion-tensorflow."""
+"""This code is taken nearly verbatim from
+https://github.com/divamgupta/stable-diffusion-tensorflow."""
 
 import gzip
 import html
 from functools import lru_cache
 
 import regex as re
 from tensorflow import keras
 
 
 @lru_cache()
 def bytes_to_unicode():
     """Return a list of utf-8 bytes and a corresponding list of unicode strings.
 
     The reversible bpe codes work on unicode strings.
-    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
-    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
-    This is a signficant percentage of your normal, say, 32K bpe vocab.
-    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
+    This means you need a large # of unicode characters in your vocab if you
+    want to avoid UNKs. When you're at something like a 10B token dataset you
+    end up needing around 5K for decent coverage. This is a significant
+    percentage of your normal, say, 32K bpe vocab. To avoid that, we want
+    lookup tables between utf-8 bytes and unicode strings.
     And avoids mapping to whitespace/control characters the bpe code barfs on.
     """
     bs = (
         list(range(ord("!"), ord("~") + 1))
         + list(range(ord("¡"), ord("¬") + 1))
         + list(range(ord("®"), ord("ÿ") + 1))
     )
@@ -47,15 +49,16 @@
     cs = [chr(n) for n in cs]
     return dict(zip(bs, cs))
 
 
 def get_pairs(word):
     """Return set of symbol pairs in a word.
 
-    A word is represented as tuple of symbols (symbols being variable-length strings).
+    A word is represented as tuple of symbols(symbols being variable-length
+    strings).
     """
     pairs = set()
     prev_char = word[0]
     for char in word[1:]:
         pairs.add((prev_char, char))
         prev_char = char
     return pairs
@@ -72,16 +75,16 @@
     return text
 
 
 class SimpleTokenizer:
     def __init__(self, bpe_path=None):
         bpe_path = bpe_path or keras.utils.get_file(
             "bpe_simple_vocab_16e6.txt.gz",
-            "https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true",
-            file_hash="924691ac288e54409236115652ad4aa250f48203de50a9e4722a6ecd48d6804a",
+            "https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true",  # noqa: E501
+            file_hash="924691ac288e54409236115652ad4aa250f48203de50a9e4722a6ecd48d6804a",  # noqa: E501
         )
         self.byte_encoder = bytes_to_unicode()
         self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
         merges = gzip.open(bpe_path).read().decode("utf-8").split("\n")
         merges = merges[1 : 49152 - 256 - 2 + 1]
         merges = [tuple(merge.split()) for merge in merges]
         vocab = list(bytes_to_unicode().values())
@@ -147,30 +150,36 @@
         word = tuple(token[:-1]) + (token[-1] + "</w>",)
         pairs = get_pairs(word)
 
         if not pairs:
             return token + "</w>"
 
         while True:
-            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))
+            bigram = min(
+                pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf"))
+            )
             if bigram not in self.bpe_ranks:
                 break
             first, second = bigram
             new_word = []
             i = 0
             while i < len(word):
                 try:
                     j = word.index(first, i)
                     new_word.extend(word[i:j])
                     i = j
                 except:
                     new_word.extend(word[i:])
                     break
 
-                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
+                if (
+                    word[i] == first
+                    and i < len(word) - 1
+                    and word[i + 1] == second
+                ):
                     new_word.append(first + second)
                     i += 2
                 else:
                     new_word.append(word[i])
                     i += 1
             new_word = tuple(new_word)
             word = new_word
@@ -184,15 +193,16 @@
 
     def encode(self, text):
         bpe_tokens = []
         text = whitespace_clean(basic_clean(text)).lower()
         for token in re.findall(self.pat, text):
             token = "".join(self.byte_encoder[b] for b in token.encode("utf-8"))
             bpe_tokens.extend(
-                self.encoder[bpe_token] for bpe_token in self.bpe(token).split(" ")
+                self.encoder[bpe_token]
+                for bpe_token in self.bpe(token).split(" ")
             )
         return [self.start_of_text] + bpe_tokens + [self.end_of_text]
 
     def decode(self, tokens):
         text = "".join([self.decoder[token] for token in tokens])
         text = (
             bytearray([self.byte_decoder[c] for c in text])
```

## keras_cv/models/stable_diffusion/decoder.py

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from tensorflow import keras
 
-from keras_cv.models.stable_diffusion.__internal__.layers.attention_block import (
+from keras_cv.models.stable_diffusion.__internal__.layers.attention_block import (  # noqa: E501
     AttentionBlock,
 )
 from keras_cv.models.stable_diffusion.__internal__.layers.padded_conv2d import (
     PaddedConv2D,
 )
 from keras_cv.models.stable_diffusion.__internal__.layers.resnet_block import (
     ResnetBlock,
@@ -59,11 +59,11 @@
                 PaddedConv2D(3, 3, padding=1),
             ],
             name=name,
         )
 
         if download_weights:
             decoder_weights_fpath = keras.utils.get_file(
-                origin="https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5",
-                file_hash="ad350a65cc8bc4a80c8103367e039a3329b4231c2469a1093869a345f55b1962",
+                origin="https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5",  # noqa: E501
+                file_hash="ad350a65cc8bc4a80c8103367e039a3329b4231c2469a1093869a345f55b1962",  # noqa: E501
             )
             self.load_weights(decoder_weights_fpath)
```

## keras_cv/models/stable_diffusion/diffusion_model.py

```diff
@@ -18,15 +18,20 @@
 from keras_cv.models.stable_diffusion.__internal__.layers.padded_conv2d import (
     PaddedConv2D,
 )
 
 
 class DiffusionModel(keras.Model):
     def __init__(
-        self, img_height, img_width, max_text_length, name=None, download_weights=True
+        self,
+        img_height,
+        img_width,
+        max_text_length,
+        name=None,
+        download_weights=True,
     ):
         context = keras.layers.Input((max_text_length, 768))
         t_embed_input = keras.layers.Input((320,))
         latent = keras.layers.Input((img_height // 8, img_width // 8, 4))
 
         t_emb = keras.layers.Dense(1280)(t_embed_input)
         t_emb = keras.layers.Activation("swish")(t_emb)
@@ -99,23 +104,28 @@
         x = keras.layers.Activation("swish")(x)
         output = PaddedConv2D(4, kernel_size=3, padding=1)(x)
 
         super().__init__([latent, t_embed_input, context], output, name=name)
 
         if download_weights:
             diffusion_model_weights_fpath = keras.utils.get_file(
-                origin="https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5",
-                file_hash="8799ff9763de13d7f30a683d653018e114ed24a6a819667da4f5ee10f9e805fe",
+                origin="https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5",  # noqa: E501
+                file_hash="8799ff9763de13d7f30a683d653018e114ed24a6a819667da4f5ee10f9e805fe",  # noqa: E501
             )
             self.load_weights(diffusion_model_weights_fpath)
 
 
 class DiffusionModelV2(keras.Model):
     def __init__(
-        self, img_height, img_width, max_text_length, name=None, download_weights=True
+        self,
+        img_height,
+        img_width,
+        max_text_length,
+        name=None,
+        download_weights=True,
     ):
         context = keras.layers.Input((max_text_length, 1024))
         t_embed_input = keras.layers.Input((320,))
         latent = keras.layers.Input((img_height // 8, img_width // 8, 4))
 
         t_emb = keras.layers.Dense(1280)(t_embed_input)
         t_emb = keras.layers.Activation("swish")(t_emb)
@@ -188,16 +198,16 @@
         x = keras.layers.Activation("swish")(x)
         output = PaddedConv2D(4, kernel_size=3, padding=1)(x)
 
         super().__init__([latent, t_embed_input, context], output, name=name)
 
         if download_weights:
             diffusion_model_weights_fpath = keras.utils.get_file(
-                origin="https://huggingface.co/ianstenbit/keras-sd2.1/resolve/main/diffusion_model_v2_1.h5",
-                file_hash="c31730e91111f98fe0e2dbde4475d381b5287ebb9672b1821796146a25c5132d",
+                origin="https://huggingface.co/ianstenbit/keras-sd2.1/resolve/main/diffusion_model_v2_1.h5",  # noqa: E501
+                file_hash="c31730e91111f98fe0e2dbde4475d381b5287ebb9672b1821796146a25c5132d",  # noqa: E501
             )
             self.load_weights(diffusion_model_weights_fpath)
 
 
 class ResBlock(keras.layers.Layer):
     def __init__(self, output_dim, **kwargs):
         super().__init__(**kwargs)
@@ -241,15 +251,17 @@
         super().__init__(**kwargs)
         self.norm = keras.layers.GroupNormalization(epsilon=1e-5)
         channels = num_heads * head_size
         if fully_connected:
             self.proj1 = keras.layers.Dense(num_heads * head_size)
         else:
             self.proj1 = PaddedConv2D(num_heads * head_size, 1)
-        self.transformer_block = BasicTransformerBlock(channels, num_heads, head_size)
+        self.transformer_block = BasicTransformerBlock(
+            channels, num_heads, head_size
+        )
         if fully_connected:
             self.proj2 = keras.layers.Dense(channels)
         else:
             self.proj2 = PaddedConv2D(channels, 1)
 
     def call(self, inputs):
         inputs, context = inputs
@@ -292,26 +304,36 @@
         self.out_proj = keras.layers.Dense(num_heads * head_size)
 
     def call(self, inputs):
         inputs, context = inputs
         context = inputs if context is None else context
         q, k, v = self.to_q(inputs), self.to_k(context), self.to_v(context)
         q = tf.reshape(q, (-1, inputs.shape[1], self.num_heads, self.head_size))
-        k = tf.reshape(k, (-1, context.shape[1], self.num_heads, self.head_size))
-        v = tf.reshape(v, (-1, context.shape[1], self.num_heads, self.head_size))
+        k = tf.reshape(
+            k, (-1, context.shape[1], self.num_heads, self.head_size)
+        )
+        v = tf.reshape(
+            v, (-1, context.shape[1], self.num_heads, self.head_size)
+        )
 
         q = tf.transpose(q, (0, 2, 1, 3))  # (bs, num_heads, time, head_size)
         k = tf.transpose(k, (0, 2, 3, 1))  # (bs, num_heads, head_size, time)
         v = tf.transpose(v, (0, 2, 1, 3))  # (bs, num_heads, time, head_size)
 
         score = td_dot(q, k) * self.scale
-        weights = keras.activations.softmax(score)  # (bs, num_heads, time, time)
+        weights = keras.activations.softmax(
+            score
+        )  # (bs, num_heads, time, time)
         attn = td_dot(weights, v)
-        attn = tf.transpose(attn, (0, 2, 1, 3))  # (bs, time, num_heads, head_size)
-        out = tf.reshape(attn, (-1, inputs.shape[1], self.num_heads * self.head_size))
+        attn = tf.transpose(
+            attn, (0, 2, 1, 3)
+        )  # (bs, time, num_heads, head_size)
+        out = tf.reshape(
+            attn, (-1, inputs.shape[1], self.num_heads * self.head_size)
+        )
         return self.out_proj(out)
 
 
 class Upsample(keras.layers.Layer):
     def __init__(self, channels, **kwargs):
         super().__init__(**kwargs)
         self.ups = keras.layers.UpSampling2D(2)
```

## keras_cv/models/stable_diffusion/image_encoder.py

```diff
@@ -10,57 +10,58 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from tensorflow import keras
 
-from keras_cv.models.stable_diffusion.__internal__.layers.attention_block import (
+from keras_cv.models.stable_diffusion.__internal__.layers.attention_block import (  # noqa: E501
     AttentionBlock,
 )
 from keras_cv.models.stable_diffusion.__internal__.layers.padded_conv2d import (
     PaddedConv2D,
 )
 from keras_cv.models.stable_diffusion.__internal__.layers.resnet_block import (
     ResnetBlock,
 )
 
 
 class ImageEncoder(keras.Sequential):
     """ImageEncoder is the VAE Encoder for StableDiffusion."""
 
-    def __init__(self, img_height=512, img_width=512, download_weights=True):
+    def __init__(self, download_weights=True):
         super().__init__(
             [
-                keras.layers.Input((img_height, img_width, 3)),
+                keras.layers.Input((None, None, 3)),
                 PaddedConv2D(128, 3, padding=1),
                 ResnetBlock(128),
                 ResnetBlock(128),
-                PaddedConv2D(128, 3, padding=1, strides=2),
+                PaddedConv2D(128, 3, padding=((0, 1), (0, 1)), strides=2),
                 ResnetBlock(256),
                 ResnetBlock(256),
-                PaddedConv2D(256, 3, padding=1, strides=2),
+                PaddedConv2D(256, 3, padding=((0, 1), (0, 1)), strides=2),
                 ResnetBlock(512),
                 ResnetBlock(512),
-                PaddedConv2D(512, 3, padding=1, strides=2),
+                PaddedConv2D(512, 3, padding=((0, 1), (0, 1)), strides=2),
                 ResnetBlock(512),
                 ResnetBlock(512),
                 ResnetBlock(512),
                 AttentionBlock(512),
                 ResnetBlock(512),
                 keras.layers.GroupNormalization(epsilon=1e-5),
                 keras.layers.Activation("swish"),
                 PaddedConv2D(8, 3, padding=1),
                 PaddedConv2D(8, 1),
-                # TODO(lukewood): can this be refactored to be a Rescaling layer?
-                # Perhaps some sort of rescale and gather?
-                # Either way, we may need a lambda to gather the first 4 dimensions.
+                # TODO(lukewood): can this be refactored to be a Rescaling
+                #  layer? Perhaps some sort of rescale and gather?
+                #  Either way, we may need a lambda to gather the first 4
+                #  dimensions.
                 keras.layers.Lambda(lambda x: x[..., :4] * 0.18215),
             ]
         )
 
         if download_weights:
             image_encoder_weights_fpath = keras.utils.get_file(
-                origin="https://huggingface.co/fchollet/stable-diffusion/resolve/main/vae_encoder.h5",
-                file_hash="c60fb220a40d090e0f86a6ab4c312d113e115c87c40ff75d11ffcf380aab7ebb",
+                origin="https://huggingface.co/fchollet/stable-diffusion/resolve/main/vae_encoder.h5",  # noqa: E501
+                file_hash="c60fb220a40d090e0f86a6ab4c312d113e115c87c40ff75d11ffcf380aab7ebb",  # noqa: E501
             )
             self.load_weights(image_encoder_weights_fpath)
```

## keras_cv/models/stable_diffusion/noise_scheduler.py

```diff
@@ -10,34 +10,35 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """StableDiffusion Noise scheduler
 
 Adapted from https://github.com/huggingface/diffusers/blob/v0.3.0/src/diffusers/schedulers/scheduling_ddpm.py#L56
-"""
+"""  # noqa: E501
 
 import tensorflow as tf
 
 
 class NoiseScheduler:
     """
     Args:
         train_timesteps: number of diffusion steps used to train the model.
         beta_start: the starting `beta` value of inference.
         beta_end: the final `beta` value.
-        beta_schedule:
-            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
-            `linear` or `quadratic`.
-        betas: a complete set of betas, in lieu of using one of the existing schedules.
-        variance_type:
-            options to clip the variance used when adding noise to the denoised sample. Choose from `fixed_small`,
-            `fixed_small_log`, `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.
-        clip_sample:
-            option to clip predicted sample between -1 and 1 for numerical stability.
+        beta_schedule: the beta schedule, a mapping from a beta range to a
+            sequence of betas for stepping the model. Choose from `linear` or
+            `quadratic`.
+        betas: a complete set of betas, in lieu of using one of the existing
+            schedules.
+        variance_type: options to clip the variance used when adding noise to
+            the denoised sample. Choose from `fixed_small`, `fixed_small_log`,
+            `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.
+        clip_sample: option to clip predicted sample between -1 and 1 for
+            numerical stability.
     """
 
     def __init__(
         self,
         train_timesteps=1000,
         beta_start=0.0001,
         beta_end=0.02,
@@ -49,38 +50,47 @@
         self.train_timesteps = train_timesteps
 
         if beta_schedule == "linear":
             self.betas = tf.linspace(beta_start, beta_end, train_timesteps)
         elif beta_schedule == "scaled_linear":
             # this schedule is very specific to the latent diffusion model.
             self.betas = (
-                tf.linspace(beta_start**0.5, beta_end**0.5, train_timesteps) ** 2
+                tf.linspace(beta_start**0.5, beta_end**0.5, train_timesteps)
+                ** 2
             )
         else:
             raise ValueError(f"Invalid beta schedule: {beta_schedule}.")
 
         self.alphas = 1.0 - self.betas
         self.alphas_cumprod = tf.math.cumprod(self.alphas)
 
         self.variance_type = variance_type
         self.clip_sample = clip_sample
 
     def _get_variance(self, timestep, predicted_variance=None):
         alpha_prod = self.alphas_cumprod[timestep]
-        alpha_prod_prev = self.alphas_cumprod[timestep - 1] if timestep > 0 else 1.0
+        alpha_prod_prev = (
+            self.alphas_cumprod[timestep - 1] if timestep > 0 else 1.0
+        )
 
-        variance = (1 - alpha_prod_prev) / (1 - alpha_prod) * self.betas[timestep]
+        variance = (
+            (1 - alpha_prod_prev) / (1 - alpha_prod) * self.betas[timestep]
+        )
 
         if self.variance_type == "fixed_small":
             variance = tf.clip_by_value(
                 variance, clip_value_min=1e-20, clip_value_max=1
             )
         elif self.variance_type == "fixed_small_log":
             variance = tf.log(
-                (tf.clip_by_value(variance, clip_value_min=1e-20, clip_value_max=1))
+                (
+                    tf.clip_by_value(
+                        variance, clip_value_min=1e-20, clip_value_max=1
+                    )
+                )
             )
         elif self.variance_type == "fixed_large":
             variance = self.betas[timestep]
         elif self.variance_type == "fixed_large_log":
             variance = tf.log(self.betas[timestep])
         elif self.variance_type == "learned":
             return predicted_variance
@@ -98,55 +108,64 @@
         self,
         model_output,
         timestep,
         sample,
         predict_epsilon=True,
     ):
         """
-        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
-        process from the learned model outputs (usually the predicted noise).
+        Predict the sample at the previous timestep by reversing the SDE. Core
+        function to propagate the diffusion process from the learned model
+        outputs (usually the predicted noise).
         Args:
-            model_output: a Tensor containing direct output from learned diffusion model
+            model_output: a Tensor containing direct output from learned
+                diffusion model
             timestep: current discrete timestep in the diffusion chain.
-            sample: a Tensor containing the current instance of sample being created by diffusion process.
-            predict_epsilon: whether the model is predicting noise (epsilon) or samples
+            sample: a Tensor containing the current instance of sample being
+                created by diffusion process.
+            predict_epsilon: whether the model is predicting noise (epsilon) or
+                samples
         Returns:
             The predicted sample at the previous timestep
         """
 
-        if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in [
+        if model_output.shape[1] == sample.shape[
+            1
+        ] * 2 and self.variance_type in [
             "learned",
             "learned_range",
         ]:
             model_output, predicted_variance = tf.split(
                 model_output, sample.shape[1], axis=1
             )
         else:
             predicted_variance = None
 
         # 1. compute alphas, betas
         alpha_prod = self.alphas_cumprod[timestep]
-        alpha_prod_prev = self.alphas_cumprod[timestep - 1] if timestep > 0 else 1.0
+        alpha_prod_prev = (
+            self.alphas_cumprod[timestep - 1] if timestep > 0 else 1.0
+        )
         beta_prod = 1 - alpha_prod
         beta_prod_prev = 1 - alpha_prod_prev
 
         # 2. compute predicted original sample from predicted noise also called
-        # "predicted x_0" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf
+        # "predicted x_0" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf  # noqa: E501
         if predict_epsilon:
             pred_original_sample = (
                 sample - beta_prod ** (0.5) * model_output
             ) / alpha_prod ** (0.5)
         else:
             pred_original_sample = model_output
 
         # 3. Clip "predicted x_0"
         if self.clip_sample:
             pred_original_sample = tf.clip_by_value(pred_original_sample, -1, 1)
 
-        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t
+        # 4. Compute coefficients for pred_original_sample x_0 and current
+        # sample x_t
         # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf
         pred_original_sample_coeff = (
             alpha_prod_prev ** (0.5) * self.betas[timestep]
         ) / beta_prod
         current_sample_coeff = (
             self.alphas[timestep] ** (0.5) * beta_prod_prev / beta_prod
         )
@@ -159,15 +178,17 @@
         )
 
         # 6. Add noise
         variance = 0
         if timestep > 0:
             noise = tf.random.normal(model_output.shape)
             variance = (
-                self._get_variance(timestep, predicted_variance=predicted_variance)
+                self._get_variance(
+                    timestep, predicted_variance=predicted_variance
+                )
                 ** 0.5
             ) * noise
 
         pred_prev_sample = pred_prev_sample + variance
 
         return pred_prev_sample
 
@@ -185,13 +206,14 @@
         for _ in range(3):
             sqrt_alpha_prod = tf.expand_dims(sqrt_alpha_prod, axis=-1)
             sqrt_one_minus_alpha_prod = tf.expand_dims(
                 sqrt_one_minus_alpha_prod, axis=-1
             )
 
         noisy_samples = (
-            sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise
+            sqrt_alpha_prod * original_samples
+            + sqrt_one_minus_alpha_prod * noise
         )
         return noisy_samples
 
     def __len__(self):
         return self.train_timesteps
```

## keras_cv/models/stable_diffusion/stable_diffusion.py

```diff
@@ -11,18 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Keras implementation of StableDiffusion.
 
 Credits:
 
-- Original implementation: https://github.com/CompVis/stable-diffusion
-- Initial TF/Keras port: https://github.com/divamgupta/stable-diffusion-tensorflow
+- Original implementation:
+  https://github.com/CompVis/stable-diffusion
+- Initial TF/Keras port:
+  https://github.com/divamgupta/stable-diffusion-tensorflow
 
-The current implementation is a rewrite of the initial TF/Keras port by Divam Gupta.
+The current implementation is a rewrite of the initial TF/Keras port by
+Divam Gupta.
 """
 
 import math
 
 import numpy as np
 import tensorflow as tf
 from tensorflow import keras
@@ -111,15 +114,17 @@
         if len(inputs) > MAX_PROMPT_LENGTH:
             raise ValueError(
                 f"Prompt is too long (should be <= {MAX_PROMPT_LENGTH} tokens)"
             )
         phrase = inputs + [49407] * (MAX_PROMPT_LENGTH - len(inputs))
         phrase = tf.convert_to_tensor([phrase], dtype=tf.int32)
 
-        context = self.text_encoder.predict_on_batch([phrase, self._get_pos_ids()])
+        context = self.text_encoder.predict_on_batch(
+            [phrase, self._get_pos_ids()]
+        )
 
         return context
 
     def generate_image(
         self,
         encoded_text,
         negative_prompt=None,
@@ -132,27 +137,26 @@
         """Generates an image based on encoded text.
 
         The encoding passed to this method should be derived from
         `StableDiffusion.encode_text`.
 
         Args:
             encoded_text: Tensor of shape (`batch_size`, 77, 768), or a Tensor
-            of shape (77, 768). When the batch axis is omitted, the same encoded
-            text will be used to produce every generated image.
-            batch_size: number of images to generate. Default: 1.
+                of shape (77, 768). When the batch axis is omitted, the same
+                encoded text will be used to produce every generated image.
+            batch_size: int, number of images to generate, defaults to 1.
             negative_prompt: a string containing information to negatively guide
-            the image generation (e.g. by removing or altering certain aspects
-            of the generated image).
-                Default: None.
-            num_steps: number of diffusion steps (controls image quality).
-                Default: 50.
-            unconditional_guidance_scale: float controling how closely the image
-                should adhere to the prompt. Larger values result in more
+                the image generation (e.g. by removing or altering certain
+                aspects of the generated image), defaults to None.
+            num_steps: int, number of diffusion steps (controls image quality),
+                defaults to 50.
+            unconditional_guidance_scale: float, controlling how closely the
+                image should adhere to the prompt. Larger values result in more
                 closely adhering to the prompt, but will make the image noisier.
-                Default: 7.5.
+                Defaults to 7.5.
             diffusion_noise: Tensor of shape (`batch_size`, img_height // 8,
                 img_width // 8, 4), or a Tensor of shape (img_height // 8,
                 img_width // 8, 4). Optional custom noise to seed the diffusion
                 process. When the batch axis is omitted, the same noise will be
                 used to seed diffusion for every generated image.
             seed: integer which is used to seed the random generation of
                 diffusion noise, only to be specified if `diffusion_noise` is
@@ -208,175 +212,29 @@
         iteration = 0
         for index, timestep in list(enumerate(timesteps))[::-1]:
             latent_prev = latent  # Set aside the previous latent vector
             t_emb = self._get_timestep_embedding(timestep, batch_size)
             unconditional_latent = self.diffusion_model.predict_on_batch(
                 [latent, t_emb, unconditional_context]
             )
-            latent = self.diffusion_model.predict_on_batch([latent, t_emb, context])
+            latent = self.diffusion_model.predict_on_batch(
+                [latent, t_emb, context]
+            )
             latent = unconditional_latent + unconditional_guidance_scale * (
                 latent - unconditional_latent
             )
             a_t, a_prev = alphas[index], alphas_prev[index]
-            pred_x0 = (latent_prev - math.sqrt(1 - a_t) * latent) / math.sqrt(a_t)
-            latent = latent * math.sqrt(1.0 - a_prev) + math.sqrt(a_prev) * pred_x0
-            iteration += 1
-            progbar.update(iteration)
-
-        # Decoding stage
-        decoded = self.decoder.predict_on_batch(latent)
-        decoded = ((decoded + 1) / 2) * 255
-        return np.clip(decoded, 0, 255).astype("uint8")
-
-    def inpaint(
-        self,
-        prompt,
-        image,
-        mask,
-        negative_prompt=None,
-        num_resamples=1,
-        batch_size=1,
-        num_steps=25,
-        unconditional_guidance_scale=7.5,
-        diffusion_noise=None,
-        seed=None,
-        verbose=True,
-    ):
-        """Inpaints a masked section of the provided image based on the provided prompt.
-        Note that this currently does not support mixed precision.
-
-        Args:
-            prompt: A string representing the prompt for generation.
-            image: Tensor of shape (`batch_size`, `image_height`, `image_width`,
-                3) with RGB values in [0, 255]. When the batch is omitted, the same
-                image will be used as the starting image.
-            mask: Tensor of shape (`batch_size`, `image_height`, `image_width`)
-                with binary values 0 or 1. When the batch is omitted, the same mask
-                will be used on all images.
-            negative_prompt: a string containing information to negatively guide
-            the image generation (e.g. by removing or altering certain aspects
-            of the generated image).
-                Default: None.
-            num_resamples: number of times to resample the generated mask region.
-                Increasing the number of resamples improves the semantic fit of the
-                generated mask region w.r.t the rest of the image. Default: 1.
-            batch_size: number of images to generate. Default: 1.
-            num_steps: number of diffusion steps (controls image quality).
-                Default: 25.
-            unconditional_guidance_scale: float controlling how closely the image
-                should adhere to the prompt. Larger values result in more
-                closely adhering to the prompt, but will make the image noisier.
-                Default: 7.5.
-            diffusion_noise: (Optional) Tensor of shape (`batch_size`,
-                img_height // 8, img_width // 8, 4), or a Tensor of shape
-                (img_height // 8, img_width // 8, 4). Optional custom noise to
-                seed the diffusion process. When the batch axis is omitted, the
-                same noise will be used to seed diffusion for every generated image.
-            seed: (Optional) integer which is used to seed the random generation of
-                diffusion noise, only to be specified if `diffusion_noise` is None.
-            verbose: whether to print progress bar. Default: True.
-        """
-        if diffusion_noise is not None and seed is not None:
-            raise ValueError(
-                "Please pass either diffusion_noise or seed to inpaint(), seed "
-                "is only used to generate diffusion noise when it is not provided. "
-                "Received both diffusion_noise and seed."
+            pred_x0 = (latent_prev - math.sqrt(1 - a_t) * latent) / math.sqrt(
+                a_t
             )
-
-        encoded_text = self.encode_text(prompt)
-        encoded_text = tf.squeeze(encoded_text)
-        if encoded_text.shape.rank == 2:
-            encoded_text = tf.repeat(
-                tf.expand_dims(encoded_text, axis=0), batch_size, axis=0
-            )
-
-        image = tf.squeeze(image)
-        image = tf.cast(image, dtype=tf.float32) / 255.0 * 2.0 - 1.0
-        image = tf.expand_dims(image, axis=0)
-        known_x0 = self.image_encoder(image)
-        if image.shape.rank == 3:
-            known_x0 = tf.repeat(known_x0, batch_size, axis=0)
-
-        mask = tf.expand_dims(mask, axis=-1)
-        mask = tf.cast(
-            tf.nn.max_pool2d(mask, ksize=8, strides=8, padding="SAME"),
-            dtype=tf.float32,
-        )
-        mask = tf.squeeze(mask)
-        if mask.shape.rank == 2:
-            mask = tf.repeat(tf.expand_dims(mask, axis=0), batch_size, axis=0)
-        mask = tf.expand_dims(mask, axis=-1)
-
-        context = encoded_text
-        if negative_prompt is None:
-            unconditional_context = tf.repeat(
-                self._get_unconditional_context(), batch_size, axis=0
-            )
-        else:
-            unconditional_context = self.encode_text(negative_prompt)
-            unconditional_context = self._expand_tensor(
-                unconditional_context, batch_size
+            latent = (
+                latent * math.sqrt(1.0 - a_prev) + math.sqrt(a_prev) * pred_x0
             )
-
-        if diffusion_noise is not None:
-            diffusion_noise = tf.squeeze(diffusion_noise)
-            if diffusion_noise.shape.rank == 3:
-                diffusion_noise = tf.repeat(
-                    tf.expand_dims(diffusion_noise, axis=0), batch_size, axis=0
-                )
-            latent = diffusion_noise
-        else:
-            latent = self._get_initial_diffusion_noise(batch_size, seed)
-
-        # Iterative reverse diffusion stage
-        timesteps = tf.range(1, 1000, 1000 // num_steps)
-        alphas, alphas_prev = self._get_initial_alphas(timesteps)
-        if verbose:
-            progbar = keras.utils.Progbar(len(timesteps))
-            iteration = 0
-
-        for index, timestep in list(enumerate(timesteps))[::-1]:
-            a_t, a_prev = alphas[index], alphas_prev[index]
-            latent_prev = latent  # Set aside the previous latent vector
-            t_emb = self._get_timestep_embedding(timestep, batch_size)
-
-            for resample_index in range(num_resamples):
-                unconditional_latent = self.diffusion_model.predict_on_batch(
-                    [latent, t_emb, unconditional_context]
-                )
-                latent = self.diffusion_model.predict_on_batch([latent, t_emb, context])
-                latent = unconditional_latent + unconditional_guidance_scale * (
-                    latent - unconditional_latent
-                )
-                pred_x0 = (latent_prev - math.sqrt(1 - a_t) * latent) / math.sqrt(a_t)
-                latent = latent * math.sqrt(1.0 - a_prev) + math.sqrt(a_prev) * pred_x0
-
-                # Use known image (x0) to compute latent
-                if timestep > 1:
-                    noise = tf.random.normal(tf.shape(known_x0), seed=seed)
-                else:
-                    noise = 0.0
-                known_latent = (
-                    math.sqrt(a_prev) * known_x0 + math.sqrt(1 - a_prev) * noise
-                )
-                # Use known latent in unmasked regions
-                latent = mask * known_latent + (1 - mask) * latent
-                # Resample latent
-                if resample_index < num_resamples - 1 and timestep > 1:
-                    beta_prev = 1 - (a_t / a_prev)
-                    latent_prev = tf.random.normal(
-                        tf.shape(latent),
-                        mean=latent * math.sqrt(1 - beta_prev),
-                        stddev=math.sqrt(beta_prev),
-                        seed=seed,
-                    )
-
-            if verbose:
-                iteration += 1
-                progbar.update(iteration)
+            iteration += 1
+            progbar.update(iteration)
 
         # Decoding stage
         decoded = self.decoder.predict_on_batch(latent)
         decoded = ((decoded + 1) / 2) * 255
         return np.clip(decoded, 0, 255).astype("uint8")
 
     def _get_unconditional_context(self):
@@ -386,15 +244,16 @@
         unconditional_context = self.text_encoder.predict_on_batch(
             [unconditional_tokens, self._get_pos_ids()]
         )
 
         return unconditional_context
 
     def _expand_tensor(self, text_embedding, batch_size):
-        """Extends a tensor by repeating it to fit the shape of the given batch size."""
+        """Extends a tensor by repeating it to fit the shape of the given batch
+        size."""
         text_embedding = tf.squeeze(text_embedding)
         if text_embedding.shape.rank == 2:
             text_embedding = tf.repeat(
                 tf.expand_dims(text_embedding, axis=0), batch_size, axis=0
             )
         return text_embedding
 
@@ -406,48 +265,52 @@
         ```python
         sd = keras_cv.models.StableDiffusion()
         my_image = np.ones((512, 512, 3))
         latent_representation = sd.image_encoder.predict(my_image)
         ```
         """
         if self._image_encoder is None:
-            self._image_encoder = ImageEncoder(self.img_height, self.img_width)
+            self._image_encoder = ImageEncoder()
             if self.jit_compile:
                 self._image_encoder.compile(jit_compile=True)
         return self._image_encoder
 
     @property
     def text_encoder(self):
         pass
 
     @property
     def diffusion_model(self):
         pass
 
     @property
     def decoder(self):
-        """decoder returns the diffusion image decoder model with pretrained weights.
-        Can be overriden for tasks where the decoder needs to be modified.
+        """decoder returns the diffusion image decoder model with pretrained
+        weights. Can be overriden for tasks where the decoder needs to be
+        modified.
         """
         if self._decoder is None:
             self._decoder = Decoder(self.img_height, self.img_width)
             if self.jit_compile:
                 self._decoder.compile(jit_compile=True)
         return self._decoder
 
     @property
     def tokenizer(self):
         """tokenizer returns the tokenizer used for text inputs.
-        Can be overriden for tasks like textual inversion where the tokenizer needs to be modified.
+        Can be overriden for tasks like textual inversion where the tokenizer
+        needs to be modified.
         """
         if self._tokenizer is None:
             self._tokenizer = SimpleTokenizer()
         return self._tokenizer
 
-    def _get_timestep_embedding(self, timestep, batch_size, dim=320, max_period=10000):
+    def _get_timestep_embedding(
+        self, timestep, batch_size, dim=320, max_period=10000
+    ):
         half = dim // 2
         freqs = tf.math.exp(
             -math.log(max_period) * tf.range(0, half, dtype=tf.float32) / half
         )
         args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs
         embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)
         embedding = tf.reshape(embedding, [1, -1])
@@ -468,38 +331,41 @@
         else:
             return tf.random.normal(
                 (batch_size, self.img_height // 8, self.img_width // 8, 4)
             )
 
     @staticmethod
     def _get_pos_ids():
-        return tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)
+        return tf.convert_to_tensor(
+            [list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32
+        )
 
 
 class StableDiffusion(StableDiffusionBase):
     """Keras implementation of Stable Diffusion.
 
     Note that the StableDiffusion API, as well as the APIs of the sub-components
     of StableDiffusion (e.g. ImageEncoder, DiffusionModel) should be considered
     unstable at this point. We do not guarantee backwards compatability for
     future changes to these APIs.
 
     Stable Diffusion is a powerful image generation model that can be used,
-    among other things, to generate pictures according to a short text description
-    (called a "prompt").
+    among other things, to generate pictures according to a short text
+    description (called a "prompt").
 
     Arguments:
-        img_height: Height of the images to generate, in pixel. Note that only
-            multiples of 128 are supported; the value provided will be rounded
-            to the nearest valid value. Default: 512.
-        img_width: Width of the images to generate, in pixel. Note that only
-            multiples of 128 are supported; the value provided will be rounded
-            to the nearest valid value. Default: 512.
-        jit_compile: Whether to compile the underlying models to XLA.
-            This can lead to a significant speedup on some systems. Default: False.
+        img_height: int, height of the images to generate, in pixel. Note that
+            only multiples of 128 are supported; the value provided will be
+            rounded to the nearest valid value. Defaults to 512.
+        img_width: int, width of the images to generate, in pixel. Note that
+            only multiples of 128 are supported; the value provided will be
+            rounded to the nearest valid value. Defaults to 512.
+        jit_compile: bool, whether to compile the underlying models to XLA.
+            This can lead to a significant speedup on some systems. Defaults to
+            False.
 
     Example:
 
     ```python
     from keras_cv.models import StableDiffusion
     from PIL import Image
 
@@ -513,27 +379,27 @@
     Image.fromarray(img[0]).save("horse.png")
     print("saved at horse.png")
     ```
 
     References:
     - [About Stable Diffusion](https://stability.ai/blog/stable-diffusion-announcement)
     - [Original implementation](https://github.com/CompVis/stable-diffusion)
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         img_height=512,
         img_width=512,
         jit_compile=False,
     ):
         super().__init__(img_height, img_width, jit_compile)
         print(
             "By using this model checkpoint, you acknowledge that its usage is "
             "subject to the terms of the CreativeML Open RAIL-M license at "
-            "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE"
+            "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE"  # noqa: E501
         )
 
     @property
     def text_encoder(self):
         """text_encoder returns the text encoder with pretrained weights.
         Can be overriden for tasks like textual inversion where the text encoder
         needs to be modified.
@@ -543,46 +409,48 @@
             if self.jit_compile:
                 self._text_encoder.compile(jit_compile=True)
         return self._text_encoder
 
     @property
     def diffusion_model(self):
         """diffusion_model returns the diffusion model with pretrained weights.
-        Can be overriden for tasks where the diffusion model needs to be modified.
+        Can be overriden for tasks where the diffusion model needs to be
+        modified.
         """
         if self._diffusion_model is None:
             self._diffusion_model = DiffusionModel(
                 self.img_height, self.img_width, MAX_PROMPT_LENGTH
             )
             if self.jit_compile:
                 self._diffusion_model.compile(jit_compile=True)
         return self._diffusion_model
 
 
 class StableDiffusionV2(StableDiffusionBase):
     """Keras implementation of Stable Diffusion v2.
 
     Note that the StableDiffusion API, as well as the APIs of the sub-components
-    of StableDiffusionV2 (e.g. ImageEncoder, DiffusionModelV2) should be considered
-    unstable at this point. We do not guarantee backwards compatability for
-    future changes to these APIs.
+    of StableDiffusionV2 (e.g. ImageEncoder, DiffusionModelV2) should be
+    considered unstable at this point. We do not guarantee backwards
+    compatability for future changes to these APIs.
 
     Stable Diffusion is a powerful image generation model that can be used,
-    among other things, to generate pictures according to a short text description
-    (called a "prompt").
+    among other things, to generate pictures according to a short text
+    description (called a "prompt").
 
     Arguments:
-        img_height: Height of the images to generate, in pixel. Note that only
-            multiples of 128 are supported; the value provided will be rounded
-            to the nearest valid value. Default: 512.
-        img_width: Width of the images to generate, in pixel. Note that only
-            multiples of 128 are supported; the value provided will be rounded
-            to the nearest valid value. Default: 512.
-        jit_compile: Whether to compile the underlying models to XLA.
-            This can lead to a significant speedup on some systems. Default: False.
+        img_height: int, height of the images to generate, in pixel. Note that
+            only multiples of 128 are supported; the value provided will be
+            rounded to the nearest valid value. Defaults to 512.
+        img_width: int, width of the images to generate, in pixel. Note that
+            only multiples of 128 are supported; the value provided will be
+            rounded to the nearest valid value. Defaults to 512.
+        jit_compile: bool, whether to compile the underlying models to XLA.
+            This can lead to a significant speedup on some systems. Defaults to
+            False.
     Example:
 
     ```python
     from keras_cv.models import StableDiffusionV2
     from PIL import Image
 
     model = StableDiffusionV2(img_height=512, img_width=512, jit_compile=True)
@@ -596,15 +464,15 @@
     print("saved at horse.png")
     ```
 
     References:
 
     - [About Stable Diffusion](https://stability.ai/blog/stable-diffusion-announcement)
     - [Original implementation](https://github.com/Stability-AI/stablediffusion)
-    """
+    """  # noqa: E501
 
     def __init__(
         self,
         img_height=512,
         img_width=512,
         jit_compile=False,
     ):
@@ -626,15 +494,16 @@
             if self.jit_compile:
                 self._text_encoder.compile(jit_compile=True)
         return self._text_encoder
 
     @property
     def diffusion_model(self):
         """diffusion_model returns the diffusion model with pretrained weights.
-        Can be overriden for tasks where the diffusion model needs to be modified.
+        Can be overriden for tasks where the diffusion model needs to be
+        modified.
         """
         if self._diffusion_model is None:
             self._diffusion_model = DiffusionModelV2(
                 self.img_height, self.img_width, MAX_PROMPT_LENGTH
             )
             if self.jit_compile:
                 self._diffusion_model.compile(jit_compile=True)
```

## keras_cv/models/stable_diffusion/stable_diffusion_test.py

```diff
@@ -20,32 +20,45 @@
 
 class StableDiffusionTest(tf.test.TestCase):
     def DISABLED_test_end_to_end_golden_value(self):
         prompt = "a caterpillar smoking a hookah while sitting on a mushroom"
         stablediff = StableDiffusion(128, 128)
 
         img = stablediff.text_to_image(prompt, seed=1337)
-        self.assertAllClose(img[0][13:14, 13:14, :][0][0], [15, 248, 229], atol=1e-4)
+        self.assertAllClose(
+            img[0][13:14, 13:14, :][0][0], [15, 248, 229], atol=1e-4
+        )
 
         # Verify that the step-by-step creation flow creates an identical output
         text_encoding = stablediff.encode_text(prompt)
         self.assertAllClose(
             img, stablediff.generate_image(text_encoding, seed=1337), atol=1e-4
         )
 
+    def DISABLED_test_image_encoder_golden_value(self):
+        stablediff = StableDiffusion(128, 128)
+
+        outputs = stablediff.image_encoder.predict(tf.ones((1, 128, 128, 3)))
+        self.assertAllClose(
+            outputs[0][1:4][0][0],
+            [2.451568, 1.607522, -0.546311, -1.194388],
+            atol=1e-4,
+        )
+
     def DISABLED_test_mixed_precision(self):
         mixed_precision.set_global_policy("mixed_float16")
         stablediff = StableDiffusion(128, 128)
         _ = stablediff.text_to_image("Testing123 haha!")
 
     def DISABLED_test_generate_image_rejects_noise_and_seed(self):
         stablediff = StableDiffusion(128, 128)
 
         with self.assertRaisesRegex(
-            ValueError, r"`diffusion_noise` and `seed` should not both be passed"
+            ValueError,
+            r"`diffusion_noise` and `seed` should not both be passed",
         ):
             _ = stablediff.generate_image(
                 stablediff.encode_text("thou shall not render"),
                 diffusion_noise=tf.random.normal((1, 16, 16, 4)),
                 seed=1337,
             )
```

## keras_cv/models/stable_diffusion/text_encoder.py

```diff
@@ -14,59 +14,69 @@
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.experimental import numpy as tfnp
 
 
 class TextEncoder(keras.Model):
-    def __init__(self, max_length, vocab_size=49408, name=None, download_weights=True):
-        tokens = keras.layers.Input(shape=(max_length,), dtype="int32", name="tokens")
+    def __init__(
+        self, max_length, vocab_size=49408, name=None, download_weights=True
+    ):
+        tokens = keras.layers.Input(
+            shape=(max_length,), dtype="int32", name="tokens"
+        )
         positions = keras.layers.Input(
             shape=(max_length,), dtype="int32", name="positions"
         )
         x = CLIPEmbedding(vocab_size, 768, max_length)([tokens, positions])
         for _ in range(12):
             x = CLIPEncoderLayer(768, 12, activation=quick_gelu)(x)
         embedded = keras.layers.LayerNormalization(epsilon=1e-5)(x)
         super().__init__([tokens, positions], embedded, name=name)
 
         if download_weights:
             text_encoder_weights_fpath = keras.utils.get_file(
-                origin="https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5",
-                file_hash="4789e63e07c0e54d6a34a29b45ce81ece27060c499a709d556c7755b42bb0dc4",
+                origin="https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5",  # noqa: E501
+                file_hash="4789e63e07c0e54d6a34a29b45ce81ece27060c499a709d556c7755b42bb0dc4",  # noqa: E501
             )
             self.load_weights(text_encoder_weights_fpath)
 
 
 class TextEncoderV2(keras.Model):
-    def __init__(self, max_length, vocab_size=49408, name=None, download_weights=True):
-        tokens = keras.layers.Input(shape=(max_length,), dtype="int32", name="tokens")
+    def __init__(
+        self, max_length, vocab_size=49408, name=None, download_weights=True
+    ):
+        tokens = keras.layers.Input(
+            shape=(max_length,), dtype="int32", name="tokens"
+        )
         positions = keras.layers.Input(
             shape=(max_length,), dtype="int32", name="positions"
         )
         x = CLIPEmbedding(vocab_size, 1024, max_length)([tokens, positions])
         for _ in range(23):
             x = CLIPEncoderLayer(1024, 16, activation=tf.nn.gelu)(x)
         embedded = keras.layers.LayerNormalization(epsilon=1e-5)(x)
         super().__init__([tokens, positions], embedded, name=name)
 
         if download_weights:
             text_encoder_weights_fpath = keras.utils.get_file(
-                origin="https://huggingface.co/ianstenbit/keras-sd2.1/resolve/main/text_encoder_v2_1.h5",
-                file_hash="985002e68704e1c5c3549de332218e99c5b9b745db7171d5f31fcd9a6089f25b",
+                origin="https://huggingface.co/ianstenbit/keras-sd2.1/resolve/main/text_encoder_v2_1.h5",  # noqa: E501
+                file_hash="985002e68704e1c5c3549de332218e99c5b9b745db7171d5f31fcd9a6089f25b",  # noqa: E501
             )
             self.load_weights(text_encoder_weights_fpath)
 
 
 def quick_gelu(x):
     return x * tf.sigmoid(x * 1.702)
 
 
 class CLIPEmbedding(keras.layers.Layer):
-    def __init__(self, input_dim=49408, output_dim=768, max_length=77, **kwargs):
+    def __init__(
+        self, input_dim=49408, output_dim=768, max_length=77, **kwargs
+    ):
         super().__init__(**kwargs)
         self.token_embedding = keras.layers.Embedding(input_dim, output_dim)
         self.position_embedding = keras.layers.Embedding(max_length, output_dim)
 
     def call(self, inputs):
         tokens, positions = inputs
         tokens = self.token_embedding(tokens)
@@ -107,22 +117,27 @@
         self.scale = self.head_dim**-0.5
         self.q_proj = keras.layers.Dense(self.embed_dim)
         self.k_proj = keras.layers.Dense(self.embed_dim)
         self.v_proj = keras.layers.Dense(self.embed_dim)
         self.out_proj = keras.layers.Dense(self.embed_dim)
 
     def reshape_states(self, x, sequence_length, batch_size):
-        x = tf.reshape(x, (batch_size, sequence_length, self.num_heads, self.head_dim))
-        return tf.transpose(x, (0, 2, 1, 3))  # bs, heads, sequence_length, head_dim
+        x = tf.reshape(
+            x, (batch_size, sequence_length, self.num_heads, self.head_dim)
+        )
+        return tf.transpose(
+            x, (0, 2, 1, 3)
+        )  # bs, heads, sequence_length, head_dim
 
     def call(self, inputs, attention_mask=None):
         if attention_mask is None and self.causal:
             length = tf.shape(inputs)[1]
             attention_mask = tfnp.triu(
-                tf.ones((1, 1, length, length), dtype=self.compute_dtype) * -tfnp.inf,
+                tf.ones((1, 1, length, length), dtype=self.compute_dtype)
+                * -tfnp.inf,
                 k=1,
             )
 
         _, tgt_len, embed_dim = inputs.shape
         query_states = self.q_proj(inputs) * self.scale
         key_states = self.reshape_states(self.k_proj(inputs), tgt_len, -1)
         value_states = self.reshape_states(self.v_proj(inputs), tgt_len, -1)
@@ -132,15 +147,17 @@
         query_states = tf.reshape(query_states, proj_shape)
         key_states = tf.reshape(key_states, proj_shape)
 
         src_len = tgt_len
         value_states = tf.reshape(value_states, proj_shape)
         attn_weights = query_states @ tf.transpose(key_states, (0, 2, 1))
 
-        attn_weights = tf.reshape(attn_weights, (-1, self.num_heads, tgt_len, src_len))
+        attn_weights = tf.reshape(
+            attn_weights, (-1, self.num_heads, tgt_len, src_len)
+        )
         attn_weights = attn_weights + attention_mask
         attn_weights = tf.reshape(attn_weights, (-1, tgt_len, src_len))
 
         attn_weights = tf.nn.softmax(attn_weights)
         attn_output = attn_weights @ value_states
 
         attn_output = tf.reshape(
```

## keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py

```diff
@@ -31,20 +31,21 @@
         self.proj_out = PaddedConv2D(output_dim, 1)
 
     def call(self, inputs):
         x = self.norm(inputs)
         q, k, v = self.q(x), self.k(x), self.v(x)
 
         # Compute attention
-        _, h, w, c = q.shape
+        shape = tf.shape(q)
+        h, w, c = shape[1], shape[2], shape[3]
         q = tf.reshape(q, (-1, h * w, c))  # b, hw, c
         k = tf.transpose(k, (0, 3, 1, 2))
         k = tf.reshape(k, (-1, c, h * w))  # b, c, hw
         y = q @ k
-        y = y * (c**-0.5)
+        y = y * 1 / tf.sqrt(tf.cast(c, self.compute_dtype))
         y = keras.activations.softmax(y)
 
         # Attend to values
         v = tf.transpose(v, (0, 3, 1, 2))
         v = tf.reshape(v, (-1, c, h * w))
         y = tf.transpose(y, (0, 2, 1))
         x = v @ y
```

## keras_cv/ops/__init__.py

```diff
@@ -9,19 +9,7 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from keras_cv.ops.iou_3d import iou_3d
-from keras_cv.ops.point_cloud import _box_area
-from keras_cv.ops.point_cloud import _center_xyzWHD_to_corner_xyz
-from keras_cv.ops.point_cloud import _is_on_lefthand_side
-from keras_cv.ops.point_cloud import coordinate_transform
-from keras_cv.ops.point_cloud import group_points_by_boxes
-from keras_cv.ops.point_cloud import is_within_any_box3d
-from keras_cv.ops.point_cloud import is_within_any_box3d_v2
-from keras_cv.ops.point_cloud import is_within_box2d
-from keras_cv.ops.point_cloud import is_within_box3d
-from keras_cv.ops.point_cloud import spherical_coordinate_transform
-from keras_cv.ops.point_cloud import within_a_frustum
-from keras_cv.ops.point_cloud import within_box3d_index
```

## keras_cv/ops/iou_3d.py

```diff
@@ -18,16 +18,16 @@
 
 keras_cv_custom_ops = LazySO("custom_ops/_keras_cv_custom_ops.so")
 
 
 def iou_3d(y_true, y_pred):
     """Implements IoU computation for 3D upright rotated bounding boxes.
 
-    Note that this is implemented using a custom TensorFlow op. If you don't have
-    KerasCV installed with custom ops, calling this will fail.
+    Note that this is implemented using a custom TensorFlow op. If you don't
+    have KerasCV installed with custom ops, calling this will fail.
 
     Boxes should have the format CENTER_XYZ_DXDYDZ_PHI. Refer to
     https://github.com/keras-team/keras-cv/blob/master/keras_cv/bounding_box_3d/formats.py
     for more details on supported bounding box formats.
 
     Sample Usage:
     ```python
```

## keras_cv/ops/iou_3d_test.py

```diff
@@ -20,33 +20,37 @@
 import tensorflow as tf
 
 from keras_cv.ops import iou_3d
 
 
 class IoU3DTest(tf.test.TestCase):
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def testOpCall(self):
         # Predicted boxes:
         # 0: a 2x2x2 box centered at 0,0,0, rotated 0 degrees
         # 1: a 2x2x2 box centered at 1,1,1, rotated 135 degrees
         # Ground Truth boxes:
-        # 0: a 2x2x2 box centered at 1,1,1, rotated 45 degrees (idential to predicted box 1)
+        # 0: a 2x2x2 box centered at 1,1,1, rotated 45 degrees
+        #    (idential to predicted box 1)
         # 1: a 2x2x2 box centered at 1,1,1, rotated 0 degrees
         box_preds = [[0, 0, 0, 2, 2, 2, 0], [1, 1, 1, 2, 2, 2, 3 * math.pi / 4]]
         box_gt = [[1, 1, 1, 2, 2, 2, math.pi / 4], [1, 1, 1, 2, 2, 2, 0]]
 
-        # Predicted box 0 and both ground truth boxes overlap by 1/8th of the box.
-        # Therefore, IiU is 1/15
-        # Predicted box 1 is the same as ground truth box 0, therefore IoU is 1
-        # Predicted box 1 shares an origin with ground truth box 1, but is rotated by 135 degrees.
-        # Their IoU can be reduced to that of two overlapping squares that share a center with
-        # the same offset of 135 degrees, which reduces to the square root of 0.5.
+        # Predicted box 0 and both ground truth boxes overlap by 1/8th of the
+        # box. Therefore, IiU is 1/15.
+        # Predicted box 1 is the same as ground truth box 0, therefore IoU is 1.
+        # Predicted box 1 shares an origin with ground truth box 1, but is
+        # rotated by 135 degrees.
+        # Their IoU can be reduced to that of two overlapping squares that
+        # share a center with the same offset of 135 degrees, which reduces to
+        # the square root of 0.5.
         expected_ious = [[1 / 15, 1 / 15], [1, 0.5**0.5]]
 
         self.assertAllClose(iou_3d(box_preds, box_gt), expected_ious)
 
 
 if __name__ == "__main__":
     tf.test.main()
```

## keras_cv/training/contrastive/contrastive_trainer.py

```diff
@@ -21,36 +21,40 @@
 
 class ContrastiveTrainer(keras.Model):
     """Creates a self-supervised contrastive trainer for a model.
 
     Args:
         encoder: a `keras.Model` to be pre-trained. In most cases, this encoder
             should not include a top dense layer.
-        augmenter: a preprocessing layer to randomly augment input images for contrastive learning,
-            or a tuple of two separate augmenters for the two sides of the contrastive pipeline.
-        projector: a projection model for contrastive training, or a tuple of two separate
-            projectors for the two sides of the contrastive pipeline. This shrinks
-            the feature map produced by the encoder, and is usually a 1 or
-            2-layer dense MLP.
+        augmenter: a preprocessing layer to randomly augment input images for
+            contrastive learning, or a tuple of two separate augmenters for the
+            two sides of the contrastive pipeline.
+        projector: a projection model for contrastive training, or a tuple of
+            two separate projectors for the two sides of the contrastive
+            pipeline. This shrinks the feature map produced by the encoder, and
+            is usually a 1 or 2-layer dense MLP.
         probe: An optional Keras layer or model which will be trained against
             class labels at train-time using the encoder output as input.
             Note that this should be specified iff training with labeled images.
             This predicts class labels based on the feature map produced by the
             encoder and is usually a 1 or 2-layer dense MLP.
 
     Returns:
       A `keras.Model` instance.
 
 
     Usage:
     ```python
-    encoder = keras_cv.models.DenseNet121(include_rescaling=True, include_top=False, pooling="avg")
+    encoder = keras_cv.models.DenseNet121(
+        include_rescaling=True,
+        include_top=False,
+        pooling="avg")
     augmenter = keras_cv.layers.preprocessing.RandomFlip()
     projector = keras.layers.Dense(64)
-    probe = keras_cv.training.ContrastiveTrainer.linear_probe(classes=10)
+    probe = keras_cv.training.ContrastiveTrainer.linear_probe(num_classes=10)
 
     trainer = keras_cv.training.ContrastiveTrainer(
         encoder=encoder,
         augmenter=augmenter,
         projector=projector,
         probe=probe
     )
@@ -78,25 +82,29 @@
         projector,
         probe=None,
     ):
         super().__init__()
 
         if encoder.output.shape.rank != 2:
             raise ValueError(
-                f"`encoder` must have a flattened output.  Expected rank(encoder.output.shape)=2, got encoder.output.shape={encoder.output.shape}"
+                f"`encoder` must have a flattened output. Expected "
+                f"rank(encoder.output.shape)=2, got "
+                f"encoder.output.shape={encoder.output.shape}"
             )
 
         if type(augmenter) is tuple and len(augmenter) != 2:
             raise ValueError(
-                "`augmenter` must be either a single augmenter or a tuple of exactly 2 augmenters."
+                "`augmenter` must be either a single augmenter or a tuple of "
+                "exactly 2 augmenters."
             )
 
         if type(projector) is tuple and len(projector) != 2:
             raise ValueError(
-                "`projector` must be either a single augmenter or a tuple of exactly 2 augmenters."
+                "`projector` must be either a single augmenter or a tuple of "
+                "exactly 2 augmenters."
             )
 
         self.augmenters = (
             augmenter if type(augmenter) is tuple else (augmenter, augmenter)
         )
         self.encoder = encoder
         self.projectors = (
@@ -129,29 +137,36 @@
 
         if self.probe and not probe_optimizer:
             raise ValueError(
                 "`probe_optimizer` must be specified when a probe is included."
             )
 
         if self.probe and not probe_loss:
-            raise ValueError("`probe_loss` must be specified when a probe is included.")
+            raise ValueError(
+                "`probe_loss` must be specified when a probe is included."
+            )
 
         if "loss" in kwargs:
             raise ValueError(
-                "`loss` parameter in ContrastiveTrainer.compile is ambiguous. Please specify `encoder_loss` or `probe_loss`."
+                "`loss` parameter in ContrastiveTrainer.compile is ambiguous. "
+                "Please specify `encoder_loss` or `probe_loss`."
             )
 
         if "optimizer" in kwargs:
             raise ValueError(
-                "`optimizer` parameter in ContrastiveTrainer.compile is ambiguous. Please specify `encoder_optimizer` or `probe_optimizer`."
+                "`optimizer` parameter in ContrastiveTrainer.compile is "
+                "ambiguous. Please specify `encoder_optimizer` or "
+                "`probe_optimizer`."
             )
 
         if "metrics" in kwargs:
             raise ValueError(
-                "`metrics` parameter in ContrastiveTrainer.compile is ambiguous. Please specify `encoder_metrics` or `probe_metrics`."
+                "`metrics` parameter in ContrastiveTrainer.compile is "
+                "ambiguous. Please specify `encoder_metrics` or "
+                "`probe_metrics`."
             )
 
         if self.probe:
             self.probe_loss = probe_loss
             self.probe_optimizer = probe_optimizer
             self.probe_metrics = probe_metrics or []
 
@@ -173,15 +188,17 @@
         batch_size=None,
         **kwargs,
     ):
         dataset = convert_inputs_to_tf_dataset(
             x=x, y=y, sample_weight=sample_weight, batch_size=batch_size
         )
 
-        dataset = dataset.map(self.run_augmenters, num_parallel_calls=tf.data.AUTOTUNE)
+        dataset = dataset.map(
+            self.run_augmenters, num_parallel_calls=tf.data.AUTOTUNE
+        )
         dataset = dataset.prefetch(tf.data.AUTOTUNE)
 
         return super().fit(x=dataset, **kwargs)
 
     def run_augmenters(self, x, y=None):
         inputs = {"images": x}
         if y is not None:
@@ -202,15 +219,17 @@
             features_0 = self.encoder(augmented_images_0, training=True)
             features_1 = self.encoder(augmented_images_1, training=True)
 
             projections_0 = self.projectors[0](features_0, training=True)
             projections_1 = self.projectors[1](features_1, training=True)
 
             loss = self.compiled_loss(
-                projections_0, projections_1, regularization_losses=self.encoder.losses
+                projections_0,
+                projections_1,
+                regularization_losses=self.encoder.losses,
             )
 
         gradients = tape.gradient(
             loss,
             self.encoder.trainable_weights
             + self.projectors[0].trainable_weights
             + self.projectors[1].trainable_weights,
@@ -224,30 +243,35 @@
                 + self.projectors[1].trainable_weights,
             )
         )
         self.loss_metric.update_state(loss)
 
         if self.probe:
             if labels is None:
-                raise ValueError("Targets must be provided when a probe is specified")
+                raise ValueError(
+                    "Targets must be provided when a probe is specified"
+                )
             with tf.GradientTape() as tape:
-                features = tf.stop_gradient(self.encoder(images, training=False))
+                features = tf.stop_gradient(
+                    self.encoder(images, training=False)
+                )
                 class_logits = self.probe(features, training=True)
                 probe_loss = self.probe_loss(labels, class_logits)
             gradients = tape.gradient(probe_loss, self.probe.trainable_weights)
             self.probe_optimizer.apply_gradients(
                 zip(gradients, self.probe.trainable_weights)
             )
             self.probe_loss_metric.update_state(probe_loss)
             for metric in self.probe_metrics:
                 metric.update_state(labels, class_logits)
 
         return {metric.name: metric.result() for metric in self.metrics}
 
     def call(self, inputs):
         raise NotImplementedError(
-            "ContrastiveTrainer.call() is not implemented - please call your model directly."
+            "ContrastiveTrainer.call() is not implemented - "
+            "please call your model directly."
         )
 
     @staticmethod
-    def linear_probe(classes, **kwargs):
-        return keras.Sequential(keras.layers.Dense(classes), **kwargs)
+    def linear_probe(num_classes, **kwargs):
+        return keras.Sequential(keras.layers.Dense(num_classes), **kwargs)
```

## keras_cv/training/contrastive/contrastive_trainer_test.py

```diff
@@ -16,15 +16,15 @@
 from tensorflow import keras
 from tensorflow.keras import layers
 from tensorflow.keras import metrics
 from tensorflow.keras import optimizers
 
 from keras_cv.layers import preprocessing
 from keras_cv.losses import SimCLRLoss
-from keras_cv.models import DenseNet121
+from keras_cv.models.legacy import DenseNet121
 from keras_cv.training import ContrastiveTrainer
 
 
 class ContrastiveTrainerTest(tf.test.TestCase):
     def test_probe_requires_probe_optimizer(self):
         trainer = ContrastiveTrainer(
             encoder=self.build_encoder(),
@@ -69,24 +69,26 @@
             trainer_with_probing.fit(images)
 
     def test_train_with_probing(self):
         trainer_with_probing = ContrastiveTrainer(
             encoder=self.build_encoder(),
             augmenter=self.build_augmenter(),
             projector=self.build_projector(),
-            probe=self.build_probe(classes=20),
+            probe=self.build_probe(num_classes=20),
         )
 
         images = tf.random.uniform((1, 50, 50, 3))
         targets = tf.ones((1, 20))
 
         trainer_with_probing.compile(
             encoder_optimizer=optimizers.Adam(),
             encoder_loss=SimCLRLoss(temperature=0.5),
-            probe_metrics=[metrics.TopKCategoricalAccuracy(3, "top3_probe_accuracy")],
+            probe_metrics=[
+                metrics.TopKCategoricalAccuracy(3, "top3_probe_accuracy")
+            ],
             probe_optimizer=optimizers.Adam(),
             probe_loss=keras.losses.CategoricalCrossentropy(from_logits=True),
         )
 
         trainer_with_probing.fit(images, targets)
 
     def test_train_without_probing(self):
@@ -158,14 +160,16 @@
 
         trainer_without_probing.fit(images)
 
     def build_augmenter(self):
         return preprocessing.RandomFlip("horizontal")
 
     def build_encoder(self):
-        return DenseNet121(include_rescaling=False, include_top=False, pooling="avg")
+        return DenseNet121(
+            include_rescaling=False, include_top=False, pooling="avg"
+        )
 
     def build_projector(self):
         return layers.Dense(128)
 
-    def build_probe(self, classes=20):
-        return layers.Dense(classes)
+    def build_probe(self, num_classes=20):
+        return layers.Dense(num_classes)
```

## keras_cv/training/contrastive/simclr_trainer.py

```diff
@@ -46,15 +46,15 @@
                 ],
                 name="projector",
             ),
             **kwargs,
         )
 
 
-class SimCLRAugmenter(preprocessing.Augmenter):
+class SimCLRAugmenter(keras.Sequential):
     def __init__(
         self,
         value_range,
         height=128,
         width=128,
         crop_area_factor=(0.08, 1.0),
         aspect_ratio_factor=(3 / 4, 4 / 3),
@@ -71,15 +71,16 @@
                 preprocessing.RandomFlip("horizontal"),
                 preprocessing.RandomCropAndResize(
                     target_size=(height, width),
                     crop_area_factor=crop_area_factor,
                     aspect_ratio_factor=aspect_ratio_factor,
                 ),
                 preprocessing.MaybeApply(
-                    preprocessing.Grayscale(output_channels=3), rate=grayscale_rate
+                    preprocessing.Grayscale(output_channels=3),
+                    rate=grayscale_rate,
                 ),
                 preprocessing.MaybeApply(
                     preprocessing.RandomColorJitter(
                         value_range=value_range,
                         brightness_factor=brightness_factor,
                         contrast_factor=contrast_factor,
                         saturation_factor=saturation_factor,
```

## keras_cv/training/contrastive/simclr_trainer_test.py

```diff
@@ -9,18 +9,20 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
+from tensorflow.keras import layers
 from tensorflow.keras import optimizers
 
 from keras_cv.losses import SimCLRLoss
-from keras_cv.models import ResNet50V2
+from keras_cv.models import ResNet50V2Backbone
 from keras_cv.training import SimCLRAugmenter
 from keras_cv.training import SimCLRTrainer
 
 
 class SimCLRTrainerTest(tf.test.TestCase):
     def test_train_without_probing(self):
         simclr_without_probing = SimCLRTrainer(
@@ -33,8 +35,13 @@
         simclr_without_probing.compile(
             encoder_optimizer=optimizers.Adam(),
             encoder_loss=SimCLRLoss(temperature=0.5),
         )
         simclr_without_probing.fit(images)
 
     def build_encoder(self):
-        return ResNet50V2(include_rescaling=False, include_top=False, pooling="avg")
+        return keras.Sequential(
+            [
+                ResNet50V2Backbone(include_rescaling=False),
+                layers.GlobalAveragePooling2D(name="avg_pool"),
+            ]
+        )
```

## keras_cv/utils/__init__.py

```diff
@@ -9,16 +9,22 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from keras_cv.utils import test_utils
+from keras_cv.utils.conditional_imports import assert_cv2_installed
+from keras_cv.utils.conditional_imports import assert_matplotlib_installed
+from keras_cv.utils.conditional_imports import (
+    assert_waymo_open_dataset_installed,
+)
 from keras_cv.utils.fill_utils import fill_rectangle
 from keras_cv.utils.preprocessing import blend
 from keras_cv.utils.preprocessing import ensure_tensor
 from keras_cv.utils.preprocessing import get_interpolation
 from keras_cv.utils.preprocessing import parse_factor
 from keras_cv.utils.preprocessing import transform
 from keras_cv.utils.preprocessing import transform_value_range
+from keras_cv.utils.to_numpy import to_numpy
 from keras_cv.utils.train import convert_inputs_to_tf_dataset
 from keras_cv.utils.train import scale_loss_for_distribution
```

## keras_cv/utils/conv_utils.py

```diff
@@ -27,15 +27,16 @@
       A tuple of n integers.
     Raises:
       ValueError: If something else than an int/long or iterable thereof or a
       negative value is
         passed.
     """
     error_msg = (
-        f"The `{name}` argument must be a tuple of {n} " f"integers. Received: {value}"
+        f"The `{name}` argument must be a tuple of {n} "
+        f"integers. Received: {value}"
     )
 
     if isinstance(value, int):
         value_tuple = (value,) * n
     else:
         try:
             value_tuple = tuple(value)
@@ -44,15 +45,16 @@
         if len(value_tuple) != n:
             raise ValueError(error_msg)
         for single_value in value_tuple:
             try:
                 int(single_value)
             except (ValueError, TypeError):
                 error_msg += (
-                    f"including element {single_value} of " f"type {type(single_value)}"
+                    f"including element {single_value} of "
+                    f"type {type(single_value)}"
                 )
                 raise ValueError(error_msg)
 
     if allow_zero:
         unqualified_values = {v for v in value_tuple if v < 0}
         req_msg = ">= 0"
     else:
```

## keras_cv/utils/fill_utils.py

```diff
@@ -20,26 +20,28 @@
     # index range of axis
     batch_size = tf.shape(starts)[0]
     axis_indices = tf.range(mask_len, dtype=starts.dtype)
     axis_indices = tf.expand_dims(axis_indices, 0)
     axis_indices = tf.tile(axis_indices, [batch_size, 1])
 
     # mask of index bounds
-    axis_mask = tf.greater_equal(axis_indices, starts) & tf.less(axis_indices, ends)
+    axis_mask = tf.greater_equal(axis_indices, starts) & tf.less(
+        axis_indices, ends
+    )
     return axis_mask
 
 
 def corners_to_mask(bounding_boxes, mask_shape):
     """Converts bounding boxes in corners format to boolean masks
 
     Args:
-        bounding_boxes: tensor of rectangle coordinates with shape (batch_size, 4) in
-            corners format (x0, y0, x1, y1).
-        mask_shape: a shape tuple as (width, height) indicating the output
-            width and height of masks.
+        bounding_boxes: tensor of rectangle coordinates with shape
+            (batch_size, 4) in corners format (x0, y0, x1, y1).
+        mask_shape: a tuple or list of shape (width, height) indicating the
+            output width and height of masks.
 
     Returns:
         boolean masks with shape (batch_size, width, height) where True values
             indicate positions within bounding box coordinates.
     """
     mask_width, mask_height = mask_shape
     x0, y0, x1, y1 = tf.split(bounding_boxes, [1, 1, 1, 1], axis=-1)
@@ -53,29 +55,31 @@
     return masks
 
 
 def fill_rectangle(images, centers_x, centers_y, widths, heights, fill_values):
     """Fill rectangles with fill value into images.
 
     Args:
-        images: Tensor of images to fill rectangles into.
-        centers_x: Tensor of positions of the rectangle centers on the x-axis.
-        centers_y: Tensor of positions of the rectangle centers on the y-axis.
+        images: Tensor of images to fill rectangles into
+        centers_x: Tensor of positions of the rectangle centers on the x-axis
+        centers_y: Tensor of positions of the rectangle centers on the y-axis
         widths: Tensor of widths of the rectangles
         heights: Tensor of heights of the rectangles
-        fill_values: Tensor with same shape as images to get rectangle fill from.
+        fill_values: Tensor with same shape as images to get rectangle fill from
     Returns:
         images with filled rectangles.
     """
     images_shape = tf.shape(images)
     images_height = images_shape[1]
     images_width = images_shape[2]
 
     xywh = tf.stack([centers_x, centers_y, widths, heights], axis=1)
     xywh = tf.cast(xywh, tf.float32)
-    corners = bounding_box.convert_format(xywh, source="center_xywh", target="xyxy")
+    corners = bounding_box.convert_format(
+        xywh, source="center_xywh", target="xyxy"
+    )
     mask_shape = (images_width, images_height)
     is_rectangle = corners_to_mask(corners, mask_shape)
     is_rectangle = tf.expand_dims(is_rectangle, -1)
 
     images = tf.where(is_rectangle, fill_values, images)
     return images
```

## keras_cv/utils/fill_utils_test.py

```diff
@@ -307,15 +307,17 @@
         batch_shape = (batch_size, img_h, img_w, 1)
         images = tf.ones(batch_shape, dtype=tf.int32)
 
         centers_x = tf.fill([batch_size], cent_x)
         centers_y = tf.fill([batch_size], cent_y)
         width = tf.fill([batch_size], rec_w)
         height = tf.fill([batch_size], rec_h)
-        fill = tf.stack([tf.fill(images[0].shape, 2), tf.fill(images[1].shape, 3)])
+        fill = tf.stack(
+            [tf.fill(images[0].shape, 2), tf.fill(images[1].shape, 3)]
+        )
 
         filled_images = fill_utils.fill_rectangle(
             images, centers_x, centers_y, width, height, fill
         )
         # remove channel dimension
         filled_images = filled_images[..., 0]
```

## keras_cv/utils/preprocessing.py

```diff
@@ -7,15 +7,17 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras import backend
 
 from keras_cv import core
 
 _TF_INTERPOLATION_METHODS = {
     "bilinear": tf.image.ResizeMethod.BILINEAR,
     "nearest": tf.image.ResizeMethod.NEAREST_NEIGHBOR,
@@ -44,25 +46,27 @@
         raise NotImplementedError(
             "Value not recognized for `interpolation`: {}. Supported values "
             "are: {}".format(interpolation, _TF_INTERPOLATION_METHODS.keys())
         )
     return _TF_INTERPOLATION_METHODS[interpolation]
 
 
-def transform_value_range(images, original_range, target_range, dtype=tf.float32):
+def transform_value_range(
+    images, original_range, target_range, dtype=tf.float32
+):
     """transforms values in input tensor from original_range to target_range.
     This function is intended to be used in preprocessing layers that
-    rely upon color values.  This allows us to assume internally that
+    rely upon color values. This allows us to assume internally that
     the input tensor is always in the range [0, 255].
 
     Args:
-        images: the set of images to transform to the target range range.
+        images: the set of images to transform to the target range.
         original_range: the value range to transform from.
         target_range: the value range to transform to.
-        dtype: the dtype to compute the conversion with.  Defaults to tf.float32.
+        dtype: the dtype to compute the conversion with, defaults to tf.float32.
 
     Returns:
         a new Tensor with values in the target range.
 
     Usage:
     ```python
     original_range = [0, 1]
@@ -76,25 +80,32 @@
     images = keras_cv.utils.preprocessing.transform_value_range(
         images,
         target_range,
         original_range
     )
     ```
     """
-    if original_range[0] == target_range[0] and original_range[1] == target_range[1]:
+    if (
+        original_range[0] == target_range[0]
+        and original_range[1] == target_range[1]
+    ):
         return images
 
     images = tf.cast(images, dtype=dtype)
     original_min_value, original_max_value = _unwrap_value_range(
         original_range, dtype=dtype
     )
-    target_min_value, target_max_value = _unwrap_value_range(target_range, dtype=dtype)
+    target_min_value, target_max_value = _unwrap_value_range(
+        target_range, dtype=dtype
+    )
 
     # images in the [0, 1] scale
-    images = (images - original_min_value) / (original_max_value - original_min_value)
+    images = (images - original_min_value) / (
+        original_max_value - original_min_value
+    )
 
     scale_factor = target_max_value - target_min_value
     return (images * scale_factor) + target_min_value
 
 
 def _unwrap_value_range(value_range, dtype=tf.float32):
     min_value, max_value = value_range
@@ -102,18 +113,18 @@
     max_value = tf.cast(max_value, dtype=dtype)
     return min_value, max_value
 
 
 def blend(image1: tf.Tensor, image2: tf.Tensor, factor: float) -> tf.Tensor:
     """Blend image1 and image2 using 'factor'.
 
-    FactorSampler should be in the range [0, 1].  A value of 0.0 means only image1
-    is used. A value of 1.0 means only image2 is used.  A value between 0.0
-    and 1.0 means we linearly interpolate the pixel values between the two
-    images.  A value greater than 1.0 "extrapolates" the difference
+    FactorSampler should be in the range [0, 1]. A value of 0.0 means only
+    image1 is used. A value of 1.0 means only image2 is used. A value between
+    0.0 and 1.0 means we linearly interpolate the pixel values between the two
+    images. A value greater than 1.0 "extrapolates" the difference
     between the two pixel values, and we clip the results to values
     between 0 and 255.
     Args:
       image1: An image Tensor of type tf.float32 with value range [0, 255].
       image2: An image Tensor of type tf.float32 with value range [0, 255].
       factor: A floating point value above 0.0.
     Returns:
@@ -121,82 +132,106 @@
     """
     difference = image2 - image1
     scaled = factor * difference
     temp = image1 + scaled
     return tf.clip_by_value(temp, 0.0, 255.0)
 
 
-def parse_factor(param, min_value=0.0, max_value=1.0, param_name="factor", seed=None):
+def parse_factor(
+    param, min_value=0.0, max_value=1.0, param_name="factor", seed=None
+):
+    if isinstance(param, dict):
+        # For all classes missing a `from_config` implementation.
+        # (RandomHue, RandomShear, etc.)
+        # To be removed with addition of `keras.__internal__` namespace support
+        param = keras.utils.deserialize_keras_object(param)
+
     if isinstance(param, core.FactorSampler):
         return param
 
     if isinstance(param, float) or isinstance(param, int):
         param = (min_value, param)
 
     if param[0] > param[1]:
         raise ValueError(
-            f"`{param_name}[0] > {param_name}[1]`, `{param_name}[0]` must be <= "
-            f"`{param_name}[1]`.  Got `{param_name}={param}`"
+            f"`{param_name}[0] > {param_name}[1]`, `{param_name}[0]` must be "
+            f"<= `{param_name}[1]`. Got `{param_name}={param}`"
         )
     if (min_value is not None and param[0] < min_value) or (
         max_value is not None and param[1] > max_value
     ):
         raise ValueError(
-            f"`{param_name}` should be inside of range [{min_value}, {max_value}]. "
-            f"Got {param_name}={param}"
+            f"`{param_name}` should be inside of range "
+            f"[{min_value}, {max_value}]. Got {param_name}={param}"
         )
 
     if param[0] == param[1]:
         return core.ConstantFactorSampler(param[0])
 
     return core.UniformFactorSampler(param[0], param[1], seed=seed)
 
 
 def random_inversion(random_generator):
     """Randomly returns a -1 or a 1 based on the provided random_generator.
 
     This can be used by KPLs to randomly invert sampled values.
 
     Args:
-        random_generator: a Keras random number generator.  An instance can be passed
-        from the `self._random_generator` attribute of a `BaseImageAugmentationLayer`.
+        random_generator: a Keras random number generator. An instance can be
+            passed from the `self._random_generator` attribute of
+            a `BaseImageAugmentationLayer`.
 
     Returns:
-        either -1, or -1.
+      either -1, or -1.
     """
     negate = random_generator.random_uniform((), 0, 1, dtype=tf.float32) > 0.5
     negate = tf.cond(negate, lambda: -1.0, lambda: 1.0)
     return negate
 
 
+def batch_random_inversion(random_generator, batch_size):
+    """Same as `random_inversion` but for batched inputs."""
+    negate = random_generator.random_uniform(
+        (batch_size, 1), 0, 1, dtype=tf.float32
+    )
+    negate = tf.where(negate > 0.5, -1.0, 1.0)
+    return negate
+
+
 def get_rotation_matrix(angles, image_height, image_width, name=None):
     """Returns projective transform(s) for the given angle(s).
     Args:
-      angles: A scalar angle to rotate all images by, or (for batches of images) a
-        vector with an angle to rotate each image in the batch. The rank must be
-        statically known (the shape is not `TensorShape(None)`).
+      angles: A scalar angle to rotate all images by, or (for batches of images)
+        a vector with an angle to rotate each image in the batch. The rank
+        must be statically known (the shape is not `TensorShape(None)`).
       image_height: Height of the image(s) to be transformed.
       image_width: Width of the image(s) to be transformed.
       name: The name of the op.
     Returns:
-      A tensor of shape (num_images, 8). Projective transforms which can be given
-        to operation `image_projective_transform_v2`. If one row of transforms is
-         [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the *output* point
-         `(x, y)` to a transformed *input* point
-         `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,
-         where `k = c0 x + c1 y + 1`.
+      A tensor of shape (num_images, 8). Projective transforms which can be
+        given to operation `image_projective_transform_v2`. If one row of
+        transforms is [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the
+        *output* point `(x, y)` to a transformed *input* point
+        `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,
+        where `k = c0 x + c1 y + 1`.
     """
     with backend.name_scope(name or "rotation_matrix"):
         x_offset = (
             (image_width - 1)
-            - (tf.cos(angles) * (image_width - 1) - tf.sin(angles) * (image_height - 1))
+            - (
+                tf.cos(angles) * (image_width - 1)
+                - tf.sin(angles) * (image_height - 1)
+            )
         ) / 2.0
         y_offset = (
             (image_height - 1)
-            - (tf.sin(angles) * (image_width - 1) + tf.cos(angles) * (image_height - 1))
+            - (
+                tf.sin(angles) * (image_width - 1)
+                + tf.cos(angles) * (image_height - 1)
+            )
         ) / 2.0
         num_angles = tf.shape(angles)[0]
         return tf.concat(
             values=[
                 tf.cos(angles)[:, None],
                 -tf.sin(angles)[:, None],
                 x_offset[:, None],
@@ -212,16 +247,16 @@
 def get_translation_matrix(translations, name=None):
     """Returns projective transform(s) for the given translation(s).
     Args:
       translations: A matrix of 2-element lists representing `[dx, dy]`
         to translate for each image (for a batch of images).
       name: The name of the op.
     Returns:
-      A tensor of shape `(num_images, 8)` projective transforms which can be given
-        to `transform`.
+      A tensor of shape `(num_images, 8)` projective transforms which can be
+        given to `transform`.
     """
     with backend.name_scope(name or "translation_matrix"):
         num_translations = tf.shape(translations)[0]
         # The translation matrix looks like:
         #     [[1 0 -dx]
         #      [0 1 -dy]
         #      [0 0 1]]
@@ -250,27 +285,28 @@
     output_shape=None,
     name=None,
 ):
     """Applies the given transform(s) to the image(s).
 
     Args:
       images: A tensor of shape
-        `(num_images, num_rows, num_columns, num_channels)` (NHWC). The rank must
-        be statically known (the shape is not `TensorShape(None)`).
+        `(num_images, num_rows, num_columns, num_channels)` (NHWC). The rank
+        must be statically known (the shape is not `TensorShape(None)`).
       transforms: Projective transform matrix/matrices. A vector of length 8 or
-        tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0, b1, b2,
-        c0, c1], then it maps the *output* point `(x, y)` to a transformed *input*
-        point `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`, where
+        tensor of size N x 8. If one row of transforms is
+        [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the *output* point
+        `(x, y)` to a transformed *input* point
+        `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`, where
         `k = c0 x + c1 y + 1`. The transforms are *inverted* compared to the
         transform mapping input points to output points. Note that gradients are
         not backpropagated into transformation parameters.
       fill_mode: Points outside the boundaries of the input are filled according
         to the given mode (one of `{"constant", "reflect", "wrap", "nearest"}`).
-      fill_value: a float represents the value to be filled outside the boundaries
-        when `fill_mode="constant"`.
+      fill_value: a float represents the value to be filled outside the
+        boundaries when `fill_mode="constant"`.
       interpolation: Interpolation mode. Supported values: `"nearest"`,
         `"bilinear"`.
       output_shape: Output dimension after the transform, `[height, width]`.
         If `None`, output is the same size as input image.
       name: The name of the op.
 
     Fill mode behavior for each valid value is as follows:
@@ -309,24 +345,28 @@
         if output_shape is None:
             output_shape = tf.shape(images)[1:3]
             if not tf.executing_eagerly():
                 output_shape_value = tf.get_static_value(output_shape)
                 if output_shape_value is not None:
                     output_shape = output_shape_value
 
-        output_shape = tf.convert_to_tensor(output_shape, tf.int32, name="output_shape")
+        output_shape = tf.convert_to_tensor(
+            output_shape, tf.int32, name="output_shape"
+        )
 
         if not output_shape.get_shape().is_compatible_with([2]):
             raise ValueError(
                 "output_shape must be a 1-D Tensor of 2 elements: "
                 "new_height, new_width, instead got "
                 "{}".format(output_shape)
             )
 
-        fill_value = tf.convert_to_tensor(fill_value, tf.float32, name="fill_value")
+        fill_value = tf.convert_to_tensor(
+            fill_value, tf.float32, name="fill_value"
+        )
 
         return tf.raw_ops.ImageProjectiveTransformV3(
             images=images,
             output_shape=output_shape,
             fill_value=fill_value,
             transforms=transforms,
             fill_mode=fill_mode.upper(),
```

## keras_cv/utils/resource_loader.py

```diff
@@ -50,32 +50,35 @@
         self.relative_path = relative_path
         self._ops = None
 
     @property
     def ops(self):
         if self._ops is None:
             self.display_warning_if_incompatible()
-            self._ops = tf.load_op_library(get_path_to_datafile(self.relative_path))
+            self._ops = tf.load_op_library(
+                get_path_to_datafile(self.relative_path)
+            )
         return self._ops
 
     def display_warning_if_incompatible(self):
         global abi_warning_already_raised
         if abi_warning_already_raised or abi_is_compatible():
             return
 
         user_version = tf.__version__
         warnings.warn(
-            f"You are currently using TensorFlow {user_version} and trying to load a KerasCV custom op."
-            "\n"
-            f"KerasCV has compiled its custom ops against TensorFlow {TF_VERSION_FOR_ABI_COMPATIBILITY}, "
-            "and there are no compatibility guarantees between the two versions. "
-            "\n"
-            "This means that you might get segfaults when loading the custom op, "
-            "or other kind of low-level errors.\n If you do, do not file an issue "
-            "on Github. This is a known limitation.",
+            f"You are currently using TensorFlow {user_version} and "
+            f"trying to load a KerasCV custom op.\n"
+            f"KerasCV has compiled its custom ops against TensorFlow "
+            f"{TF_VERSION_FOR_ABI_COMPATIBILITY}, and there are no "
+            f"compatibility guarantees between the two versions.\n"
+            "This means that you might get segfaults when loading the custom "
+            "op, or other kind of low-level errors.\n"
+            "If you do, do not file an issue on Github. "
+            "This is a known limitation.",
             UserWarning,
         )
         abi_warning_already_raised = True
 
 
 def abi_is_compatible():
     return tf.__version__.startswith(TF_VERSION_FOR_ABI_COMPATIBILITY)
```

## keras_cv/utils/target_gather.py

```diff
@@ -25,26 +25,27 @@
      1) both batched and unbatched `targets`
      2) when unbatched `targets` have empty rows, the result will be filled
         with `mask_val`
      3) target masking.
 
     Args:
      targets: [N, ...] or [batch_size, N, ...] Tensor representing targets such
-       as boxes, keypoints, etc.
+        as boxes, keypoints, etc.
      indices: [M] or [batch_size, M] int32 Tensor representing indices within
-       `targets` to gather.
-     mask: optional [M, ...] or [batch_size, M, ...] boolean
-       Tensor representing the masking for each target. `True` means the corresponding
-       entity should be masked to `mask_val`, `False` means the corresponding entity
-       should be the target value.
-     mask_val: optinal float representing the masking value if `mask` is True on
-       the entity.
-
-     Returns:
-       targets: [M, ...] or [batch_size, M, ...] Tensor representing selected targets.
+        `targets` to gather.
+     mask: optional [M, ...] or [batch_size, M, ...] boolean Tensor representing
+        the masking for each target. `True` means the corresponding entity
+        should be masked to `mask_val`, `False` means the corresponding
+        entity should be the target value.
+     mask_val: optional float representing the masking value if `mask` is True
+        on the entity.
+
+    Returns:
+     targets: [M, ...] or [batch_size, M, ...] Tensor representing
+       selected targets.
 
      Raise:
        ValueError: If `targets` is higher than rank 3.
     """
     targets_shape = targets.get_shape().as_list()
     if len(targets_shape) > 3:
         raise ValueError(
@@ -102,15 +103,17 @@
             return tf.expand_dims(result, axis=0)
         else:
             indices_shape = tf.shape(match_indices)
             indices_dtype = match_indices.dtype
             batch_indices = tf.expand_dims(
                 tf.range(indices_shape[0], dtype=indices_dtype), axis=-1
             ) * tf.ones([1, indices_shape[-1]], dtype=indices_dtype)
-            gather_nd_indices = tf.stack([batch_indices, match_indices], axis=-1)
+            gather_nd_indices = tf.stack(
+                [batch_indices, match_indices], axis=-1
+            )
             targets = tf.gather_nd(labels, gather_nd_indices)
             if mask is None:
                 return targets
             else:
                 masked_targets = tf.cast(mask_val, labels.dtype) * tf.ones_like(
                     mask, dtype=labels.dtype
                 )
```

## keras_cv/utils/test_utils.py

```diff
@@ -10,35 +10,36 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import inspect
 
-import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv import core
 
 
 def exhaustive_compare(obj1, obj2):
-    """Exhaustively compared config of any two python or Keras objects recursively.
+    """Exhaustively compared config of any two python
+    or Keras objects recursively.
 
-    If objects are python objects, a standard equality check is run.  If the objects are
-    Keras objects a `get_config()` call is made.  The subsequent configs are then
-    compared to determine if equality holds.
+    If objects are python objects, a standard equality check is run.
+    If the objects are Keras objects a `get_config()` call is made.
+    The subsequent configs are then compared to determine if equality holds.
 
     Args:
         obj1: any object, can be a Keras object or python object.
         obj2: any object, can be a Keras object or python object.
     """
 
     classes_supporting_get_config = (
         core.FactorSampler,
-        tf.keras.layers.Layer,
-        tf.keras.losses.Loss,
+        keras.layers.Layer,
+        keras.losses.Loss,
     )
 
     # If both objects are either one of list or tuple then their individual
     # elements also must be checked exhaustively.
     if isinstance(obj1, (list, tuple)) and isinstance(obj2, (list, tuple)):
         # Length based checks.
         if len(obj1) == 0 and len(obj2) == 0:
@@ -62,21 +63,21 @@
     ):
         return config_equals(obj1.get_config(), obj2.get_config())
 
     # Following checks are if either of the objects are _functions_, not methods
     # or callables, since Layers and other unforeseen objects may also fit into
     # this category. Specifically for Keras activation functions.
     elif inspect.isfunction(obj1) and inspect.isfunction(obj2):
-        return tf.keras.utils.serialize_keras_object(
+        return keras.utils.serialize_keras_object(
             obj1
-        ) == tf.keras.utils.serialize_keras_object(obj2)
+        ) == keras.utils.serialize_keras_object(obj2)
     elif inspect.isfunction(obj1) and not inspect.isfunction(obj2):
-        return tf.keras.utils.serialize_keras_object(obj1) == obj2
+        return keras.utils.serialize_keras_object(obj1) == obj2
     elif inspect.isfunction(obj2) and not inspect.isfunction(obj1):
-        return obj1 == tf.keras.utils.serialize_keras_object(obj2)
+        return obj1 == keras.utils.serialize_keras_object(obj2)
 
     # Lastly check for primitive datatypes and objects that don't need
     # additional preprocessing.
     else:
         return obj1 == obj2
```

## keras_cv/utils/train.py

```diff
@@ -9,33 +9,39 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 
 
 def scale_loss_for_distribution(loss_value):
     """Scales and returns the given loss value by the number of replicas."""
     num_replicas = tf.distribute.get_strategy().num_replicas_in_sync
     if num_replicas > 1:
         loss_value *= 1.0 / num_replicas
     return loss_value
 
 
-def convert_inputs_to_tf_dataset(x=None, y=None, sample_weight=None, batch_size=None):
+def convert_inputs_to_tf_dataset(
+    x=None, y=None, sample_weight=None, batch_size=None
+):
     if sample_weight is not None:
-        raise ValueError("Contrastive trainers do not yet support `sample_weight`.")
+        raise ValueError(
+            "Contrastive trainers do not yet support `sample_weight`."
+        )
 
     if isinstance(x, tf.data.Dataset):
         if y is not None or batch_size is not None:
             raise ValueError(
-                "When `x` is a `tf.data.Dataset`, please do not provide a value for "
-                f"`y` or `batch_size`.  Got `y={y}`, `batch_size={batch_size}`."
+                "When `x` is a `tf.data.Dataset`, please do not "
+                "provide a value for `y` or `batch_size`. "
+                "Got `y={y}`, `batch_size={batch_size}`."
             )
         return x
 
     # batch_size defaults to 32, as it does in fit().
     batch_size = batch_size or 32
     # Parse inputs
     inputs = x
@@ -43,7 +49,33 @@
         inputs = (x, y)
 
     # Construct tf.data.Dataset
     dataset = tf.data.Dataset.from_tensor_slices(inputs)
     if batch_size is not None:
         dataset = dataset.batch(batch_size)
     return dataset
+
+
+def get_feature_extractor(model, layer_names, output_keys=None):
+    """Create a feature extractor model with augmented output.
+
+    This method produces a new `keras.Model` with the same input signature
+    as the source but with the layers in `layer_names` as the output.
+    This is useful for downstream tasks that require more output than the
+    final layer of the backbone.
+
+    Args:
+        model: keras.Model. The source model.
+        layer_names: list of strings. Names of layers to include in the
+            output signature.
+        output_keys: optional, list of strings. Key to use for each layer in
+            the model's output dictionary.
+
+    Returns:
+        `keras.Model` which has dict as outputs.
+    """
+
+    if not output_keys:
+        output_keys = layer_names
+    items = zip(output_keys, layer_names)
+    outputs = {key: model.get_layer(name).output for key, name in items}
+    return keras.Model(inputs=model.inputs, outputs=outputs)
```

## Comparing `keras_cv/layers/object_detection/retina_net_label_encoder.py` & `keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,44 +9,50 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras import layers
 
 from keras_cv import bounding_box
 from keras_cv.layers.object_detection import box_matcher
 from keras_cv.utils import target_gather
 
 
+@keras.utils.register_keras_serializable(package="keras_cv")
 class RetinaNetLabelEncoder(layers.Layer):
     """Transforms the raw labels into targets for training.
 
     This class has operations to generate targets for a batch of samples which
     is made up of the input images, bounding boxes for the objects present and
-    their class ids.
+    their class ids. Targets are always represented in `center_yxwh` format.
+    This done for numerical reasons, to ensure numerical consistency when
+    training in any format.
 
     Args:
-        bounding_box_format:  The format of bounding boxes of input dataset. Refer
-            [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
-            for more details on supported bounding box formats.
-        anchor_generator: `keras_cv.layers.AnchorGenerator` instance to produce anchor
-            boxes.  Boxes are then used to encode labels on a per-image basis.
-        positive_threshold: the float threshold to set an anchor to positive match to gt box.
-            values above it are positive matches.
-        negative_threshold: the float threshold to set an anchor to negative match to gt box.
-            values below it are negative matches.
-        box_variance: The scaling factors used to scale the bounding box targets.
-            Defaults to (0.1, 0.1, 0.2, 0.2).
-        background_class: (Optional) The class ID used for the background class.
-            Defaults to -1.
-        ignore_class: (Optional) The class ID used for the ignore class. Defaults to -2.
-    """
+        bounding_box_format: The format of bounding boxes of input dataset.
+            Refer [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/) for more
+            details on supported bounding box formats.
+        anchor_generator: `keras_cv.layers.AnchorGenerator` instance to produce
+            anchor boxes. Boxes are then used to encode labels on a per-image
+            basis.
+        positive_threshold: the float threshold to set an anchor to positive
+            match to gt box. Values above it are positive matches.
+        negative_threshold: the float threshold to set an anchor to negative
+            match to gt box. Values below it are negative matches.
+        box_variance: The scaling factors used to scale the bounding box
+            targets, defaults to (0.1, 0.1, 0.2, 0.2).
+        background_class: (Optional) The class ID used for the background class,
+            defaults to -1.
+        ignore_class: (Optional) The class ID used for the ignore class,
+            defaults to -2.
+    """  # noqa: E501
 
     def __init__(
         self,
         bounding_box_format,
         anchor_generator,
         positive_threshold=0.5,
         negative_threshold=0.4,
@@ -57,28 +63,28 @@
     ):
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         self.anchor_generator = anchor_generator
         self.box_variance = tf.convert_to_tensor(box_variance, dtype=self.dtype)
         self.background_class = background_class
         self.ignore_class = ignore_class
-        self.matched_boxes_metric = tf.keras.metrics.BinaryAccuracy(
+        self.matched_boxes_metric = keras.metrics.BinaryAccuracy(
             name="percent_boxes_matched_with_anchor"
         )
         self.positive_threshold = positive_threshold
         self.negative_threshold = negative_threshold
         self.box_matcher = box_matcher.BoxMatcher(
             thresholds=[negative_threshold, positive_threshold],
             match_values=[-1, -2, 1],
             force_match_for_each_col=False,
         )
         self.box_variance_tuple = box_variance
         self.built = True
 
-    def _encode_sample(self, box_labels, anchor_boxes):
+    def _encode_sample(self, box_labels, anchor_boxes, image_shape):
         """Creates box and classification targets for a batched sample
         Matches ground truth boxes to anchor boxes based on IOU.
         1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`
           to get a `(M, N)` shaped matrix.
         2. The ground truth box with the maximum IOU in each row is assigned to
           the anchor box provided the IOU is greater than `match_iou`.
         3. If the maximum IOU in a row is less than `ignore_iou`, the anchor
@@ -100,42 +106,56 @@
             truth boxes.
           ignore_mask: A mask for anchor boxes that need to by ignored during
             training
         """
         gt_boxes = box_labels["boxes"]
         gt_classes = box_labels["classes"]
         iou_matrix = bounding_box.compute_iou(
-            anchor_boxes, gt_boxes, bounding_box_format="xywh"
+            anchor_boxes,
+            gt_boxes,
+            bounding_box_format=self.bounding_box_format,
+            image_shape=image_shape,
         )
         matched_gt_idx, matched_vals = self.box_matcher(iou_matrix)
         matched_vals = matched_vals[..., tf.newaxis]
         positive_mask = tf.cast(tf.math.equal(matched_vals, 1), self.dtype)
         ignore_mask = tf.cast(tf.math.equal(matched_vals, -2), self.dtype)
-        matched_gt_boxes = target_gather._target_gather(gt_boxes, matched_gt_idx)
+        matched_gt_boxes = target_gather._target_gather(
+            gt_boxes, matched_gt_idx
+        )
         box_target = bounding_box._encode_box_to_deltas(
             anchors=anchor_boxes,
             boxes=matched_gt_boxes,
             anchor_format=self.bounding_box_format,
-            box_format="xywh",
+            box_format=self.bounding_box_format,
             variance=self.box_variance,
+            image_shape=image_shape,
+        )
+        matched_gt_cls_ids = target_gather._target_gather(
+            gt_classes, matched_gt_idx
+        )
+        cls_target = tf.where(
+            tf.not_equal(positive_mask, 1.0),
+            self.background_class,
+            matched_gt_cls_ids,
         )
-        matched_gt_cls_ids = target_gather._target_gather(gt_classes, matched_gt_idx)
         cls_target = tf.where(
-            tf.not_equal(positive_mask, 1.0), self.background_class, matched_gt_cls_ids
+            tf.equal(ignore_mask, 1.0), self.ignore_class, cls_target
         )
-        cls_target = tf.where(tf.equal(ignore_mask, 1.0), self.ignore_class, cls_target)
         label = tf.concat([box_target, cls_target], axis=-1)
 
-        # In the case that a box in the corner of an image matches with an all -1 box
-        # that is outside of the image, we should assign the box to the ignore class
-        # There are rare cases where a -1 box can be matched, resulting in a NaN during
-        # training.  The unit test passing all -1s to the label encoder ensures that we
-        # properly handle this edge-case.
+        # In the case that a box in the corner of an image matches with an all
+        # -1 box that is outside the image, we should assign the box to the
+        # ignore class. There are rare cases where a -1 box can be matched,
+        # resulting in a NaN during training. The unit test passing all -1s to
+        # the label encoder ensures that we properly handle this edge-case.
         label = tf.where(
-            tf.expand_dims(tf.math.reduce_any(tf.math.is_nan(label), axis=-1), axis=-1),
+            tf.expand_dims(
+                tf.math.reduce_any(tf.math.is_nan(label), axis=-1), axis=-1
+            ),
             self.ignore_class,
             label,
         )
 
         result = {"boxes": label[:, :, :4], "classes": label[:, :, 4]}
 
         box_shape = tf.shape(gt_boxes)
@@ -159,45 +179,37 @@
 
     def call(self, images, box_labels):
         """Creates box and classification targets for a batch
 
         Args:
           images: a batched [batch_size, H, W, C] image float `tf.Tensor`.
           box_labels: a batched KerasCV style bounding box dictionary containing
-            bounding boxes and class labels.  Should be in `bounding_box_format`.
+            bounding boxes and class labels. Should be in `bounding_box_format`.
         """
         if isinstance(images, tf.RaggedTensor):
             raise ValueError(
                 "`RetinaNetLabelEncoder`'s `call()` method does not "
-                "support RaggedTensor inputs for the `images` argument.  Received "
-                f"`type(images)={type(images)}`."
+                "support RaggedTensor inputs for the `images` argument. "
+                f"Received `type(images)={type(images)}`."
             )
 
+        image_shape = tf.shape(images[0])
         box_labels = bounding_box.to_dense(box_labels)
-        box_labels = bounding_box.convert_format(
-            box_labels,
-            source=self.bounding_box_format,
-            target="xywh",
-            images=images,
-        )
         if box_labels["classes"].get_shape().rank == 2:
             box_labels["classes"] = box_labels["classes"][..., tf.newaxis]
-        anchor_boxes = self.anchor_generator(image_shape=tf.shape(images[0]))
+        anchor_boxes = self.anchor_generator(image_shape=image_shape)
         anchor_boxes = tf.concat(list(anchor_boxes.values()), axis=0)
         anchor_boxes = bounding_box.convert_format(
             anchor_boxes,
             source=self.anchor_generator.bounding_box_format,
             target=self.bounding_box_format,
-            images=images[0],
+            image_shape=image_shape,
         )
 
-        result = self._encode_sample(box_labels, anchor_boxes)
-        result = bounding_box.convert_format(
-            result, source="xywh", target=self.bounding_box_format, images=images
-        )
+        result = self._encode_sample(box_labels, anchor_boxes, image_shape)
         encoded_box_targets = result["boxes"]
         class_targets = result["classes"]
         return encoded_box_targets, class_targets
 
     def get_config(self):
         config = {
             "bounding_box_format": self.bounding_box_format,
```

## Comparing `keras_cv/layers/object_detection/retina_net_label_encoder_test.py` & `keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 
 from keras_cv import layers as cv_layers
+from keras_cv.models.object_detection.retinanet import RetinaNetLabelEncoder
 
 
 class RetinaNetLabelEncoderTest(tf.test.TestCase):
     def test_label_encoder_output_shapes(self):
         images_shape = (8, 512, 512, 3)
         boxes_shape = (8, 10, 4)
         classes_shape = (8, 10)
@@ -38,15 +39,15 @@
         anchor_generator = cv_layers.AnchorGenerator(
             bounding_box_format="yxyx",
             sizes=sizes,
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
-        encoder = cv_layers.RetinaNetLabelEncoder(
+        encoder = RetinaNetLabelEncoder(
             anchor_generator=anchor_generator,
             bounding_box_format="xyxy",
         )
         bounding_boxes = {"boxes": boxes, "classes": classes}
         box_targets, class_targets = encoder(images, bounding_boxes)
 
         self.assertEqual(box_targets.shape, [8, 49104, 4])
@@ -69,15 +70,15 @@
         anchor_generator = cv_layers.AnchorGenerator(
             bounding_box_format="yxyx",
             sizes=sizes,
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
-        encoder = cv_layers.RetinaNetLabelEncoder(
+        encoder = RetinaNetLabelEncoder(
             anchor_generator=anchor_generator,
             bounding_box_format="xyxy",
         )
 
         bounding_boxes = {"boxes": boxes, "classes": classes}
         box_targets, class_targets = encoder(images, bounding_boxes)
 
@@ -91,29 +92,32 @@
         boxes = tf.ragged.stack(
             [
                 tf.constant([[0, 0, 10, 10], [5, 5, 10, 10]], tf.float32),
                 tf.constant([[0, 0, 10, 10]], tf.float32),
             ]
         )
         classes = tf.ragged.stack(
-            [tf.constant([[1], [1]], tf.float32), tf.constant([[1]], tf.float32)]
+            [
+                tf.constant([[1], [1]], tf.float32),
+                tf.constant([[1]], tf.float32),
+            ]
         )
         strides = [2**i for i in range(3, 8)]
         scales = [2**x for x in [0, 1 / 3, 2 / 3]]
         sizes = [x**2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]
         aspect_ratios = [0.5, 1.0, 2.0]
 
         anchor_generator = cv_layers.AnchorGenerator(
             bounding_box_format="xywh",
             sizes=sizes,
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
-        encoder = cv_layers.RetinaNetLabelEncoder(
+        encoder = RetinaNetLabelEncoder(
             anchor_generator=anchor_generator,
             bounding_box_format="xywh",
         )
 
         bounding_boxes = {"boxes": boxes, "classes": classes}
         box_targets, class_targets = encoder(images, bounding_boxes)
```

## Comparing `keras_cv/layers/object_detection_3d/center_net_label_encoder.py` & `keras_cv/layers/object_detection_3d/centernet_label_encoder.py`

 * *Files 9% similar despite different names*

```diff
@@ -15,14 +15,15 @@
 import math
 from typing import Sequence
 from typing import Tuple
 from typing import Union
 
 import numpy as np
 import tensorflow as tf
+from tensorflow import keras
 
 from keras_cv.layers.object_detection_3d import voxel_utils
 
 # Infinite voxel size.
 INF_VOXEL_SIZE = 100
 
 
@@ -65,17 +66,19 @@
       box_3d: 3d boxes in xyz format, vehicle frame, [B, boxes, 7].
       box_mask: box masking, [B, boxes]
       voxel_size: the size on each voxel dimension (xyz)
       min_radius: the minimum radius on each voxel dimension (xyz)
       max_radius: the maximum radius on each voxel dimension (xyz)
 
     Returns:
-      point_xyz: the point location w.r.t. vehicle frame, [B, boxes, max_voxels_per_box, 3]
+      point_xyz: the point location w.r.t. vehicle frame, [B, boxes,
+        max_voxels_per_box, 3]
       mask: point mask, [B, boxes, max_voxels_per_box]
-      heatmap: the returned heatmap w.r.t box frame, [B, boxes, max_voxels_per_box]
+      heatmap: the returned heatmap w.r.t box frame, [B, boxes,
+        max_voxels_per_box]
       box_id: the box id each point belongs to, [B, boxes, max_voxels_per_box]
 
     """
     # convert radius from point unit to voxel unit.
     max_radius_in_voxels = [
         math.ceil(mr / vs) for mr, vs in zip(max_radius, voxel_size)
     ]
@@ -86,18 +89,22 @@
     box_center = box_3d[:, :, :3]
     # voxelize and de-voxelize point_xyz
     # This ensures that we are computing heatmap for each voxel with these
     # quantized x,y,z.
     # [B, N, max_num_voxels_per_box, 3]
     point_xyz = (
         box_center[:, :, tf.newaxis, :]
-        + tf.constant(points_numpy, dtype=tf.float32)[tf.newaxis, tf.newaxis, :, :]
+        + tf.constant(points_numpy, dtype=tf.float32)[
+            tf.newaxis, tf.newaxis, :, :
+        ]
     )
     # [B, N, max_num_voxels_per_box, 3]
-    point_xyz = voxel_utils.point_to_voxel_coord(point_xyz, voxel_size, dtype=tf.int32)
+    point_xyz = voxel_utils.point_to_voxel_coord(
+        point_xyz, voxel_size, dtype=tf.int32
+    )
     # Map voxel back to xyz to get quantized version.
     # [B, N, max_num_voxels_per_box, 3]
     point_xyz = voxel_utils.voxel_coord_to_point(
         point_xyz, voxel_size, dtype=tf.float32
     )
 
     # Transforms these points to the box frame from vehicle frame.
@@ -105,21 +112,25 @@
     # [B, N, 3, 3]
     rot = voxel_utils.get_yaw_rotation(heading)
     # [B, N, max_num_voxels_per_box, 3]
     point_xyz_rot = tf.linalg.matmul(point_xyz, rot)
     # convert from box frame to vehicle frame.
     # [B, N, max_num_voxels_per_box, 3]
     point_xyz_transform = (
-        point_xyz_rot + voxel_utils.inv_loc(rot, box_center)[:, :, tf.newaxis, :]
+        point_xyz_rot
+        + voxel_utils.inv_loc(rot, box_center)[:, :, tf.newaxis, :]
     )
-    # Due to the transform above, z=0 can be transformed to a non-zero value. For
-    # 2d headmap, we do not want to use z.
+    # Due to the transform above, z=0 can be transformed to a non-zero value.
+    # For 2d heatmap, we do not want to use z.
     if voxel_size[2] > INF_VOXEL_SIZE:
         point_xyz_transform = tf.concat(
-            [point_xyz_transform[..., :2], tf.zeros_like(point_xyz_transform[..., :1])],
+            [
+                point_xyz_transform[..., :2],
+                tf.zeros_like(point_xyz_transform[..., :1]),
+            ],
             axis=-1,
         )
 
     # The Gaussian radius is set as the dimension of the boxes
     # [B, N, 3]
     radius = box_3d[:, :, 3:6]
     # [B, N, 1, 3]
@@ -147,19 +158,24 @@
         batch_size,
         num_box,
         max_num_voxels_per_box,
         _,
     ) = voxel_utils.combined_static_and_dynamic_shape(point_xyz)
     box_id = tf.range(num_box, dtype=tf.int32)
     box_id = tf.tile(
-        box_id[tf.newaxis, :, tf.newaxis], [batch_size, 1, max_num_voxels_per_box]
+        box_id[tf.newaxis, :, tf.newaxis],
+        [batch_size, 1, max_num_voxels_per_box],
     )
 
-    point_xyz = tf.reshape(point_xyz, [batch_size, num_box * max_num_voxels_per_box, 3])
-    heatmap = tf.reshape(heatmap, [batch_size, num_box * max_num_voxels_per_box])
+    point_xyz = tf.reshape(
+        point_xyz, [batch_size, num_box * max_num_voxels_per_box, 3]
+    )
+    heatmap = tf.reshape(
+        heatmap, [batch_size, num_box * max_num_voxels_per_box]
+    )
     box_id = tf.reshape(box_id, [batch_size, num_box * max_num_voxels_per_box])
     mask = tf.reshape(mask, [batch_size, num_box * max_num_voxels_per_box])
 
     return point_xyz, mask, heatmap, box_id
 
 
 def scatter_to_dense_heatmap(
@@ -173,25 +189,25 @@
     """Scatter the heatmap to a dense grid.
 
     N = num_boxes * max_voxels_per_box
 
     Args:
       point_xyz: [B, N, 3] 3d points, point coordinate in vehicle frame.
       point_mask: [B, N] valid point mask.
-      point_box_id: [B, N] box id of each point. The ID indexes into the input box
-        tensors. See compute_heatmap for more details.
+      point_box_id: [B, N] box id of each point. The ID indexes into the input
+        box tensors. See compute_heatmap for more details.
       heatmap: [B, N] heatmap value of each point.
       voxel_size: voxel size.
       spatial_size: the spatial size.
 
     Returns:
       dense_heatmap: [B, H, W] heatmap value.
-      dense_box_id: [B, H, W] box id associated with each feature map pixel. Only
-        pixels with positive heatmap value have valid box id set. Other locations
-        have random values.
+      dense_box_id: [B, H, W] box id associated with each feature map pixel.
+        Only pixels with positive heatmap value have valid box id set. Other
+        locations have random values.
 
     """
     # [B, N, 3]
     # convert to voxel units.
     point_voxel_xyz = voxel_utils.point_to_voxel_coord(
         point_xyz, voxel_size, dtype=tf.int32
     )
@@ -201,19 +217,23 @@
     # shift point voxel coordinates to positive voxel index.
     point_voxel_xyz = point_voxel_xyz - voxel_origin[tf.newaxis, tf.newaxis, :]
     voxel_spatial_size = voxel_utils.compute_voxel_spatial_size(
         spatial_size, voxel_size
     )
     # [B, N]
     point_voxel_valid_mask = tf.math.reduce_all(
-        tf.math.logical_and(point_voxel_xyz >= 0, point_voxel_xyz < voxel_spatial_size),
+        tf.math.logical_and(
+            point_voxel_xyz >= 0, point_voxel_xyz < voxel_spatial_size
+        ),
         axis=-1,
     )
     # [B, N]
-    point_voxel_valid_mask = tf.math.logical_and(point_voxel_valid_mask, point_mask)
+    point_voxel_valid_mask = tf.math.logical_and(
+        point_voxel_valid_mask, point_mask
+    )
     # [B, N]
     point_voxel_xyz = point_voxel_xyz * tf.cast(
         point_voxel_valid_mask[..., tf.newaxis], dtype=point_voxel_xyz.dtype
     )
     # [B, N]
     # filtered heatmap with out of range voxels.
     heatmap = heatmap * tf.cast(point_voxel_valid_mask, dtype=heatmap.dtype)
@@ -224,15 +244,16 @@
         point_voxel_xyz_i, mask_i, heatmap_i, point_box_id_i = args
         mask_index = tf.where(mask_i)
 
         point_voxel_xyz_i = tf.gather_nd(point_voxel_xyz_i, mask_index)
         heatmap_i = tf.gather_nd(heatmap_i, mask_index)
         point_box_id_i = tf.gather_nd(point_box_id_i, mask_index)
 
-        # scatter from local heatmap to global heatmap based on point_xyz voxel units
+        # scatter from local heatmap to global heatmap based on point_xyz voxel
+        # units
         dense_heatmap_i = tf.tensor_scatter_nd_update(
             tf.zeros(voxel_spatial_size, dtype=heatmap_i.dtype),
             point_voxel_xyz_i,
             heatmap_i,
         )
         dense_box_id_i = tf.tensor_scatter_nd_update(
             tf.zeros(voxel_spatial_size, dtype=tf.int32),
@@ -246,23 +267,26 @@
         elems=[point_voxel_xyz, point_voxel_valid_mask, heatmap, point_box_id],
         fn_output_signature=(heatmap.dtype, point_box_id.dtype),
     )
 
     return dense_heatmap, dense_box_id
 
 
-def decode_tensor(t: tf.Tensor, dims: Sequence[Union[tf.Tensor, int]]) -> tf.Tensor:
+def decode_tensor(
+    t: tf.Tensor, dims: Sequence[Union[tf.Tensor, int]]
+) -> tf.Tensor:
     """
 
     Args:
       t: int32 or int64 tensor of shape [shape], [B, k]
       dims: list of ints., [H, W, Z]
 
     Returns:
-      t_decoded: int32 or int64 decoded tensor of shape [shape, len(dims)], [B, k, 3]
+      t_decoded: int32 or int64 decoded tensor of shape [shape, len(dims)],
+        [B, k, 3]
     """
     with tf.name_scope("decode_tensor"):
         multipliers = []
         multiplier = 1
         assert dims
         for d in reversed(dims):
             multipliers.append(multiplier)
@@ -289,81 +313,99 @@
 
     # [B, H*W*Z]
     heatmap_reshape = tf.reshape(heatmap, [shape[0], -1])
     # [B, k]
     # each index in the range of [0, H*W*Z)
     _, indices = tf.math.top_k(heatmap_reshape, k=k, sorted=False)
     # [B, k, 2] or [B, k, 3]
-    # shape[1:] = [H, W, Z], convert the indices from 1 dimension to 3 dimensions
-    # in the range of [0, H), [0, W), [0, Z)
+    # shape[1:] = [H, W, Z], convert the indices from 1 dimension to 3
+    # dimensions in the range of [0, H), [0, W), [0, Z)
     res = decode_tensor(indices, shape[1:])
     return res
 
 
-class CenterNetLabelEncoder(tf.keras.layers.Layer):
-    """Transforms the raw sparse labels into class specific dense training labels.
+class CenterNetLabelEncoder(keras.layers.Layer):
+    """Transforms the raw sparse labels into class specific dense training
+    labels.
 
     This layer takes the box locations, box classes and box masks, voxelizes
     and compute the Gaussian radius for each box, then computes class specific
-    heatmap for classification and class specific box offset w.r.t to feature map
-    for regression.
+    heatmap for classification and class specific box offset w.r.t to feature
+    map for regression.
 
     Args:
       voxel_size: the x, y, z dimension (in meters) of each voxel.
-      min_radius: minimum Gasussian radius in each dimension in meters.
-      max_radius: maximum Gasussian radius in each dimension in meters.
+      min_radius: minimum Gaussian radius in each dimension in meters.
+      max_radius: maximum Gaussian radius in each dimension in meters.
       spatial_size: the x, y, z boundary of voxels
-      classes: number of object classes.
+      num_classes: number of object classes.
       top_k_heatmap: A sequence of integers, top k for each class. Can be None.
     """
 
     def __init__(
         self,
         voxel_size: Sequence[float],
         min_radius: Sequence[float],
         max_radius: Sequence[float],
         spatial_size: Sequence[float],
-        classes: int,
+        num_classes: int,
         top_k_heatmap: Sequence[int],
         **kwargs,
     ):
         super().__init__(**kwargs)
         self._voxel_size = voxel_size
         self._min_radius = min_radius
         self._max_radius = max_radius
         self._spatial_size = spatial_size
-        self._classes = classes
+        self._num_classes = num_classes
         self._top_k_heatmap = top_k_heatmap
 
-    def call(self, box_3d, box_classes, box_mask):
+    def call(self, inputs):
         """
         Args:
-          box_3d: [B, num_boxes, 7] 3d boxes in vehicle frame.
-          box_classes: [B, num_boxes, 1] 3d box classes, 0 represents background.
-          box_mask: [B, num_boxes] 3d box masks, True means valid box, False means invalid box.
+          inputs: dictionary of Tensors representing a batch of data. Must
+          contain 3D box targets under the key "3d_boxes".
         Returns:
-          heatmap: dict of class specific float Tensor in [B, H, W, Z] or [B, H, W]
-          dense_box_3d:  dict of class specific float Tensor in [B, H, W, Z, 7] or [B, H, W, 7]
-          top_k_heatmap_feature_idx_dict: dict of int Tensor in [B, k, 3] or [B, k, 2].
+          A dictionary of Tensors with all of the original inputs, plus, for
+          each class, a new key with encoded CenterNet targets in the format:
+          ```
+          "class_{class_index}": {
+            "heatmap": float Tensor [B, H, W, Z] or [B, H, W]
+            "boxes": float Tensor [B, H, W, Z, 7] or [B, H, W, 7]
+            "tok_k_index": int Tensor [B, k, 3] or [B, k, 2]
+          }
+          ```
         where:
           H: number of voxels in y dimension
           W: number of voxels in x dimension
           Z: number of voxels in z dimension
           k: `top_k_heatmap` slice
         """
+        box_3d = inputs["3d_boxes"]["boxes"]
+        box_mask = inputs["3d_boxes"]["mask"]
+        box_classes = inputs["3d_boxes"]["classes"]
         # point_xyz - [B, num_boxes * max_num_voxels_per_box, 3]
         # heatmap - [B, num_boxes * max_num_voxels_per_box]
         # compute localized heatmap around its radius.
         point_xyz, point_mask, heatmap, box_id = compute_heatmap(
-            box_3d, box_mask, self._voxel_size, self._min_radius, self._max_radius
+            box_3d,
+            box_mask,
+            self._voxel_size,
+            self._min_radius,
+            self._max_radius,
         )
         # heatmap - [B, H, W, Z]
         # scatter the localized heatmap to global heatmap in vehicle frame.
         dense_heatmap, dense_box_id = scatter_to_dense_heatmap(
-            point_xyz, point_mask, box_id, heatmap, self._voxel_size, self._spatial_size
+            point_xyz,
+            point_mask,
+            box_id,
+            heatmap,
+            self._voxel_size,
+            self._spatial_size,
         )
         b, h, w, z = voxel_utils.combined_static_and_dynamic_shape(dense_box_id)
         # [B, H * W * Z]
         dense_box_id = tf.reshape(dense_box_id, [b, h * w * z])
         # mask out invalid boxes to 0, which represents background
         box_classes = box_classes * tf.cast(box_mask, box_classes.dtype)
         # [B, H, W, Z]
@@ -375,43 +417,44 @@
             tf.gather(box_3d, dense_box_id, batch_dims=1), [b, h, w, z, -1]
         )
         global_xyz = tf.zeros([b, 3], dtype=point_xyz.dtype)
         # [B, H, W, Z, 3]
         feature_map_ref_xyz = voxel_utils.compute_feature_map_ref_xyz(
             self._voxel_size, self._spatial_size, global_xyz
         )
-        # convert from global box point xyz to offset w.r.t center of feature map.
+        # convert from global box point xyz to offset w.r.t center of feature
+        # map.
         # [B, H, W, Z, 3]
         dense_box_3d_center = dense_box_3d[..., :3] - feature_map_ref_xyz
         # [B, H, W, Z, 7]
-        dense_box_3d = tf.concat([dense_box_3d_center, dense_box_3d[..., 3:]], axis=-1)
+        dense_box_3d = tf.concat(
+            [dense_box_3d_center, dense_box_3d[..., 3:]], axis=-1
+        )
 
-        heatmap_dict = {}
-        box_3d_dict = {}
-        top_k_heatmap_feature_idx_dict = {}
-        for i in range(self._classes):
-            class_key = f"class_{i+1}"
+        centernet_targets = {}
+        for i in range(self._num_classes):
             # Object class is 1-indexed (0 is background).
-            dense_box_class_i = tf.cast(
-                tf.math.equal(dense_box_classes, i + 1), dtype=dense_heatmap.dtype
+            dense_box_classes_i = tf.cast(
+                tf.math.equal(dense_box_classes, i + 1),
+                dtype=dense_heatmap.dtype,
             )
-            # [B, H, W, Z]
-            dense_heatmap_i = dense_heatmap * dense_box_class_i
-            # [B, H, W, Z, 7]
-            dense_box_3d_i = dense_box_3d * dense_box_class_i[..., tf.newaxis]
+            dense_heatmap_i = dense_heatmap * dense_box_classes_i
+            dense_box_3d_i = dense_box_3d * dense_box_classes_i[..., tf.newaxis]
             # Remove z-dimension if this is 2D setup.
             if self._voxel_size[2] > INF_VOXEL_SIZE:
                 dense_heatmap_i = tf.squeeze(dense_heatmap_i, axis=-1)
                 dense_box_3d_i = tf.squeeze(dense_box_3d_i, axis=-2)
 
-            heatmap_dict[class_key] = dense_heatmap_i
-            box_3d_dict[class_key] = dense_box_3d_i
-
             top_k_heatmap_feature_idx_i = None
             if self._top_k_heatmap[i] > 0:
-                # [B, k, 2] or [B, k, 3]
                 top_k_heatmap_feature_idx_i = compute_top_k_heatmap_idx(
                     dense_heatmap_i, self._top_k_heatmap[i]
                 )
-            top_k_heatmap_feature_idx_dict[class_key] = top_k_heatmap_feature_idx_i
 
-        return heatmap_dict, box_3d_dict, top_k_heatmap_feature_idx_dict
+            centernet_targets[f"class_{i+1}"] = {
+                "heatmap": dense_heatmap_i,
+                "boxes": dense_box_3d_i,
+                "top_k_index": top_k_heatmap_feature_idx_i,
+            }
+
+        inputs.update(centernet_targets)
+        return inputs
```

## Comparing `keras_cv/layers/object_detection_3d/center_net_label_encoder_test.py` & `keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,90 +11,111 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import tensorflow as tf
 
-from keras_cv.layers.object_detection_3d.center_net_label_encoder import (
+from keras_cv.layers.object_detection_3d.centernet_label_encoder import (
     CenterNetLabelEncoder,
 )
 
 
 class CenterNetLabelEncoderTest(tf.test.TestCase):
     def test_voxelization_output_shape_no_z(self):
         layer = CenterNetLabelEncoder(
             voxel_size=[0.1, 0.1, 1000],
             min_radius=[0.8, 0.8, 0.0],
             max_radius=[8.0, 8.0, 0.0],
             spatial_size=[-20, 20, -20, 20, -20, 20],
-            classes=2,
+            num_classes=2,
             top_k_heatmap=[10, 20],
         )
         box_3d = tf.random.uniform(
             shape=[2, 100, 7], minval=-5, maxval=5, dtype=tf.float32
         )
         box_classes = tf.random.uniform(
             shape=[2, 100], minval=0, maxval=2, dtype=tf.int32
         )
         box_mask = tf.constant(True, shape=[2, 100])
-        output = layer(box_3d, box_classes, box_mask)
+        inputs = {
+            "3d_boxes": {
+                "boxes": box_3d,
+                "classes": box_classes,
+                "mask": box_mask,
+            }
+        }
+        output = layer(inputs)
         # # (20 - (-20)) / 0.1 = 400
-        self.assertEqual(output[0]["class_1"].shape, [2, 400, 400])
-        self.assertEqual(output[0]["class_2"].shape, [2, 400, 400])
-        self.assertEqual(output[1]["class_1"].shape, [2, 400, 400, 7])
-        self.assertEqual(output[1]["class_2"].shape, [2, 400, 400, 7])
+        self.assertEqual(output["class_1"]["heatmap"].shape, [2, 400, 400])
+        self.assertEqual(output["class_2"]["heatmap"].shape, [2, 400, 400])
+        self.assertEqual(output["class_1"]["boxes"].shape, [2, 400, 400, 7])
+        self.assertEqual(output["class_2"]["boxes"].shape, [2, 400, 400, 7])
         # last dimension only has x, y
-        self.assertEqual(output[2]["class_1"].shape, [2, 10, 2])
-        self.assertEqual(output[2]["class_2"].shape, [2, 20, 2])
+        self.assertEqual(output["class_1"]["top_k_index"].shape, [2, 10, 2])
+        self.assertEqual(output["class_2"]["top_k_index"].shape, [2, 20, 2])
 
     def test_voxelization_output_shape_with_z(self):
         layer = CenterNetLabelEncoder(
             voxel_size=[0.1, 0.1, 10],
             min_radius=[0.8, 0.8, 0.0],
             max_radius=[8.0, 8.0, 0.0],
             spatial_size=[-20, 20, -20, 20, -20, 20],
-            classes=2,
+            num_classes=2,
             top_k_heatmap=[10, 20],
         )
         box_3d = tf.random.uniform(
             shape=[2, 100, 7], minval=-5, maxval=5, dtype=tf.float32
         )
         box_classes = tf.random.uniform(
             shape=[2, 100], minval=0, maxval=2, dtype=tf.int32
         )
         box_mask = tf.constant(True, shape=[2, 100])
-        output = layer(box_3d, box_classes, box_mask)
+        inputs = {
+            "3d_boxes": {
+                "boxes": box_3d,
+                "classes": box_classes,
+                "mask": box_mask,
+            }
+        }
+        output = layer(inputs)
         # # (20 - (-20)) / 0.1 = 400
-        self.assertEqual(output[0]["class_1"].shape, [2, 400, 400, 4])
-        self.assertEqual(output[0]["class_2"].shape, [2, 400, 400, 4])
-        self.assertEqual(output[1]["class_1"].shape, [2, 400, 400, 4, 7])
-        self.assertEqual(output[1]["class_2"].shape, [2, 400, 400, 4, 7])
+        self.assertEqual(output["class_1"]["heatmap"].shape, [2, 400, 400, 4])
+        self.assertEqual(output["class_2"]["heatmap"].shape, [2, 400, 400, 4])
+        self.assertEqual(output["class_1"]["boxes"].shape, [2, 400, 400, 4, 7])
+        self.assertEqual(output["class_2"]["boxes"].shape, [2, 400, 400, 4, 7])
         # last dimension has x, y, z
-        self.assertEqual(output[2]["class_1"].shape, [2, 10, 3])
-        self.assertEqual(output[2]["class_2"].shape, [2, 20, 3])
+        self.assertEqual(output["class_1"]["top_k_index"].shape, [2, 10, 3])
+        self.assertEqual(output["class_2"]["top_k_index"].shape, [2, 20, 3])
 
     def test_voxelization_output_shape_missing_topk(self):
         layer = CenterNetLabelEncoder(
             voxel_size=[0.1, 0.1, 1000],
             min_radius=[0.8, 0.8, 0.0],
             max_radius=[8.0, 8.0, 0.0],
             spatial_size=[-20, 20, -20, 20, -20, 20],
-            classes=2,
+            num_classes=2,
             top_k_heatmap=[10, 0],
         )
         box_3d = tf.random.uniform(
             shape=[2, 100, 7], minval=-5, maxval=5, dtype=tf.float32
         )
         box_classes = tf.random.uniform(
             shape=[2, 100], minval=0, maxval=2, dtype=tf.int32
         )
         box_mask = tf.constant(True, shape=[2, 100])
-        output = layer(box_3d, box_classes, box_mask)
+        inputs = {
+            "3d_boxes": {
+                "boxes": box_3d,
+                "classes": box_classes,
+                "mask": box_mask,
+            }
+        }
+        output = layer(inputs)
         # # (20 - (-20)) / 0.1 = 400
-        self.assertEqual(output[0]["class_1"].shape, [2, 400, 400])
-        self.assertEqual(output[0]["class_2"].shape, [2, 400, 400])
-        self.assertEqual(output[1]["class_1"].shape, [2, 400, 400, 7])
-        self.assertEqual(output[1]["class_2"].shape, [2, 400, 400, 7])
+        self.assertEqual(output["class_1"]["heatmap"].shape, [2, 400, 400])
+        self.assertEqual(output["class_2"]["heatmap"].shape, [2, 400, 400])
+        self.assertEqual(output["class_1"]["boxes"].shape, [2, 400, 400, 7])
+        self.assertEqual(output["class_2"]["boxes"].shape, [2, 400, 400, 7])
         # last dimension only has x, y
-        self.assertEqual(output[2]["class_1"].shape, [2, 10, 2])
-        self.assertEqual(output[2]["class_2"], None)
+        self.assertEqual(output["class_1"]["top_k_index"].shape, [2, 10, 2])
+        self.assertEqual(output["class_2"]["top_k_index"], None)
```

## Comparing `keras_cv/metrics/coco/utils.py` & `keras_cv/visualization/draw_bounding_boxes.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,155 +1,154 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Contains shared utilities for Keras COCO metrics."""
-import tensorflow as tf
 
-from keras_cv import bounding_box
-
-
-def filter_boxes_by_area_range(boxes, min_area, max_area):
-    areas = bounding_box_area(boxes)
-    inds = tf.where(tf.math.logical_and(areas >= min_area, areas < max_area))
-    return tf.gather_nd(boxes, inds)
+try:
+    import cv2
+except:
+    cv2 = None
 
+import numpy as np
 
-def bounding_box_area(boxes):
-    """box_areas returns the area of the provided bounding boxes.
-    Args:
-        boxes: Tensor of bounding boxes of shape `[..., 4+]` in corners format.
-    Returns:
-        areas: Tensor of areas of shape `[...]`.
-    """
-    w = boxes[..., bounding_box.XYXY.RIGHT] - boxes[..., bounding_box.XYXY.LEFT]
-    h = boxes[..., bounding_box.XYXY.BOTTOM] - boxes[..., bounding_box.XYXY.TOP]
-    return tf.math.multiply(w, h)
-
-
-def filter_boxes(boxes, value, axis=4):
-    """filter_boxes is used to select only boxes matching a given class.
-    The most common use case for this is to filter to accept only a specific
-    bounding_box.CLASS.
-    Args:
-        boxes: Tensor of bounding boxes in format `[images, bounding_boxes, 6]`
-        value: Value the specified axis must match
-        axis: Integer identifying the axis on which to sort, default 4
-    Returns:
-        boxes: A new Tensor of bounding boxes, where boxes[axis]==value
-    """
-    return tf.gather_nd(boxes, tf.where(boxes[:, axis] == value))
-
-
-def to_sentinel_padded_bounding_box_tensor(box_sets):
-    """pad_with_sentinels returns a Tensor of bounding_boxes padded with -1s
-    to ensure that each bounding_box set has identical dimensions.  This is to
-    be used before passing bounding_box predictions, or bounding_box ground truths to
-    the keras COCO metrics.
-    Args:
-        box_sets: List of Tensors representing bounding boxes, or a list of lists of
-            Tensors.
-    Returns:
-        boxes: A new Tensor where each value missing is populated with -1.
-    """
-    return tf.ragged.stack(box_sets).to_tensor(default_value=-1)
-
-
-def filter_out_sentinels(boxes):
-    """filter_out_sentinels to filter out boxes that were padded on to the prediction
-    or ground truth bounding_box tensor to ensure dimensions match.
-    Args:
-        boxes: Tensor of bounding boxes in format `[bounding_boxes, 6]`, usually from a
-            single image.
-    Returns:
-        boxes: A new Tensor of bounding boxes, where boxes[axis]!=-1.
-    """
-    return tf.gather_nd(boxes, tf.where(boxes[:, bounding_box.XYXY.CLASS] != -1))
+from keras_cv import bounding_box
+from keras_cv import utils
+from keras_cv.utils import assert_cv2_installed
 
 
-def sort_bounding_boxes(boxes, axis=5):
-    """sort_bounding_boxes is used to sort a list of bounding boxes by a given axis.
+def draw_bounding_boxes(
+    images,
+    bounding_boxes,
+    color,
+    bounding_box_format,
+    line_thickness=1,
+    text_thickness=1,
+    font_scale=1.0,
+    class_mapping=None,
+):
+    """Internal utility to draw bounding boxes on the target image.
+
+    Accepts a batch of images and batch of bounding boxes. The function draws
+    the bounding boxes onto the image, and returns a new image tensor with the
+    annotated images. This API is intentionally not exported, and is considered
+    an implementation detail.
+
+    Args:
+        images: a batch Tensor of images to plot bounding boxes onto.
+        bounding_boxes: a Tensor of batched bounding boxes to plot onto the
+            provided images.
+        color: the color in which to plot the bounding boxes
+        bounding_box_format: The format of bounding boxes to plot onto the
+            images. Refer
+            [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
+            for more details on supported bounding box formats.
+        line_thickness: (Optional) line_thickness for the box and text labels.
+            Defaults to 2.
+        text_thickness: (Optional) the thickness for the text, defaults to
+            `1.0`.
+        font_scale: (Optional) scale of font to draw in, defaults to `1.0`.
+        class_mapping: (Optional) dictionary from class ID to class label.
+
+    Returns:
+        the input `images` with provided bounding boxes plotted on top of them
+    """  # noqa: E501
+    assert_cv2_installed("draw_bounding_boxes")
+    bounding_boxes = bounding_box.convert_format(
+        bounding_boxes, source=bounding_box_format, target="xyxy", images=images
+    )
+    text_thickness = text_thickness or line_thickness
 
-    The most common use case for this is to sort by bounding_box.XYXY.CONFIDENCE, as
-    this is a part of computing both _COCORecall and _COCOMeanAveragePrecision.
-    Args:
-        boxes: Tensor of bounding boxes in format `[images, bounding_boxes, 6]`
-        axis: Integer identifying the axis on which to sort, default 5
-    Returns:
-        boxes: A new Tensor of Bounding boxes, sorted on an image-wise basis.
-    """
-    num_images = tf.shape(boxes)[0]
-    boxes_sorted_list = tf.TensorArray(tf.float32, size=num_images, dynamic_size=False)
-    for img in tf.range(num_images):
-        preds_for_img = boxes[img, :, :]
-        prediction_scores = preds_for_img[:, axis]
-        _, idx = tf.math.top_k(prediction_scores, tf.shape(preds_for_img)[0])
-        boxes_sorted_list = boxes_sorted_list.write(
-            img, tf.gather(preds_for_img, idx, axis=0)
+    bounding_boxes["boxes"] = utils.to_numpy(bounding_boxes["boxes"])
+    bounding_boxes["classes"] = utils.to_numpy(bounding_boxes["classes"])
+    images = utils.to_numpy(images)
+    image_width = images.shape[-2]
+    outline_factor = image_width // 100
+
+    class_mapping = class_mapping or {}
+    result = []
+
+    if len(images.shape) != 4:
+        raise ValueError(
+            "Images must be a batched np-like with elements of shape "
+            "(height, width, 3)"
         )
 
-    return boxes_sorted_list.stack()
-
-
-def match_boxes(ious, threshold):
-    """matches bounding boxes from y_true to boxes in y_pred.
-
-    Args:
-        ious: lookup table from [y_true, y_pred] => IoU.
-        threshold: minimum IoU for a pair to be considered a match.
-    Returns:
-        a mapping from [y_pred] => matching y_true index.  Dimension
-        of result tensor is equal to the number of boxes in y_pred.
-    """
-    num_true = tf.shape(ious)[0]
-    num_pred = tf.shape(ious)[1]
-
-    gt_matches = tf.TensorArray(
-        tf.int32,
-        size=num_true,
-        dynamic_size=False,
-        infer_shape=False,
-        element_shape=(),
-    )
-    pred_matches = tf.TensorArray(
-        tf.int32,
-        size=num_pred,
-        dynamic_size=False,
-        infer_shape=False,
-        element_shape=(),
-    )
-    for i in tf.range(num_true):
-        gt_matches = gt_matches.write(i, -1)
-    for i in tf.range(num_pred):
-        pred_matches = pred_matches.write(i, -1)
-
-    for detection_idx in tf.range(num_pred):
-        match_index = -1
-        iou = tf.math.minimum(threshold, 1 - 1e-10)
-
-        for gt_idx in tf.range(num_true):
-            if gt_matches.gather([gt_idx]) > -1:
-                continue
-            # TODO(lukewood): update clause to account for gtIg
-            # if m > -1 and gtIg[m] == 0 and gtIg[gind] == 1:
+    for i in range(images.shape[0]):
+        bounding_box_batch = {
+            "boxes": bounding_boxes["boxes"][i],
+            "classes": bounding_boxes["classes"][i],
+        }
+        if "confidence" in bounding_boxes:
+            bounding_box_batch["confidence"] = bounding_boxes["confidence"][i]
+
+        image = utils.to_numpy(images[i]).astype("uint8")
+        for b_id in range(bounding_box_batch["boxes"].shape[0]):
+            x, y, x2, y2 = bounding_box_batch["boxes"][b_id].astype(int)
+            class_id = bounding_box_batch["classes"][b_id].astype(int)
+            confidence = bounding_box_batch.get("confidence", None)
 
-            if ious[gt_idx, detection_idx] < iou:
+            if class_id == -1:
                 continue
-
-            iou = ious[gt_idx, detection_idx]
-            match_index = gt_idx
-
-        # Write back the match indices
-        pred_matches = pred_matches.write(detection_idx, match_index)
-        if match_index == -1:
-            continue
-        gt_matches = gt_matches.write(match_index, detection_idx)
-    return pred_matches.stack()
+            # force conversion back to contiguous array
+            x, y, x2, y2 = int(x), int(y), int(x2), int(y2)
+            cv2.rectangle(
+                image,
+                (x, y),
+                (x2, y2),
+                (0, 0, 0, 0.5),
+                line_thickness + outline_factor,
+            )
+            cv2.rectangle(image, (x, y), (x2, y2), color, line_thickness)
+            class_id = int(class_id)
+
+            if class_id in class_mapping:
+                label = class_mapping[class_id]
+                if confidence is not None:
+                    label = f"{label} | {confidence[b_id]:.2f}"
+
+                x, y = _find_text_location(
+                    x, y, font_scale, line_thickness, outline_factor
+                )
+                cv2.putText(
+                    image,
+                    label,
+                    (x, y),
+                    cv2.FONT_HERSHEY_SIMPLEX,
+                    font_scale,
+                    (0, 0, 0, 0.5),
+                    text_thickness + outline_factor,
+                )
+                cv2.putText(
+                    image,
+                    label,
+                    (x, y),
+                    cv2.FONT_HERSHEY_SIMPLEX,
+                    font_scale,
+                    color,
+                    text_thickness,
+                )
+        result.append(image)
+    return np.array(result).astype(int)
+
+
+def _find_text_location(x, y, font_scale, line_thickness, outline_factor):
+    font_height = int(font_scale * 12)
+    target_y = y - int(8 + outline_factor)
+    if target_y - (2 * font_height) > 0:
+        return x, y - int(8 + outline_factor)
+
+    line_offset = line_thickness + outline_factor
+    static_offset = 3
+
+    return (
+        x + outline_factor + static_offset,
+        y + (2 * font_height) + line_offset + static_offset,
+    )
```

## Comparing `keras_cv/metrics/coco/numerical_tests/__init__.py` & `keras_cv/metrics/object_detection/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
```

## Comparing `keras_cv/models/convmixer.py` & `keras_cv/models/legacy/convmixer.py`

 * *Files 8% similar despite different names*

```diff
@@ -19,19 +19,24 @@
 - [Patches Are All You Need?](https://arxiv.org/abs/2201.09792)
 """
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 
-from keras_cv.models import utils
-from keras_cv.models.weights import parse_weights
+from keras_cv.models.legacy import utils
+from keras_cv.models.legacy.weights import parse_weights
 
 MODEL_CONFIGS = {
-    "ConvMixer_1536_20": {"dim": 1536, "depth": 20, "patch_size": 7, "kernel_size": 9},
+    "ConvMixer_1536_20": {
+        "dim": 1536,
+        "depth": 20,
+        "patch_size": 7,
+        "kernel_size": 9,
+    },
     "ConvMixer_1536_24": {
         "dim": 1536,
         "depth": 24,
         "patch_size": 14,
         "kernel_size": 9,
     },
     "ConvMixer_768_32": {
@@ -50,193 +55,230 @@
         "dim": 512,
         "depth": 16,
         "patch_size": 7,
         "kernel_size": 8,
     },
 }
 
-
 BASE_DOCSTRING = """Instantiates the {name} architecture.
+
     Reference:
         - [Patches Are All You Need?](https://arxiv.org/abs/2201.09792)
-    This function returns a Keras {name} model.
+
+    This class represents a Keras {name} model.
+
     For transfer learning use cases, make sure to read the [guide to transfer
         learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: whether to include the fully-connected layer at the top of the
-            network.  If provided, classes must be provided.
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True.
+        include_rescaling: bool, whether to rescale the inputs. If set to
+            True, inputs will be passed through a `Rescaling(1/255.0)` layer.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, num_classes must be provided.
+        num_classes: integer, optional number of classes to classify images
+            into. Only to be specified if `include_top` is True.
         weights: one of `None` (random initialization), a pretrained weight file
-            path, or a reference to pre-trained weights (e.g. 'imagenet/classification')
-            (see available pre-trained weights in weights.py)
+            path, or a reference to pre-trained weights (e.g.
+            'imagenet/classification')(see available pre-trained weights in
+            weights.py)
         input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+        input_tensor: optional Keras tensor (i.e., output of `layers.Input()`)
             to use as image input for the model.
         pooling: optional pooling mode for feature extraction
             when `include_top` is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
-            - `avg` means that global average pooling will be applied to the output
-                of the last convolutional block, and thus the output of the model will
-                be a 2D tensor.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
+            - `avg` means that global average pooling will be applied to the
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
             - `max` means that global max pooling will be applied.
-        name: (Optional) name to pass to the model.  Defaults to "{name}".
+        name: string, optional name to pass to the model, defaults to "{name}".
+
     Returns:
       A `keras.Model` instance.
 """
 
 
-def CovnMixer_Layer(dim, kernel_size):
-    """CovnMixer Layer module.
+def apply_conv_mixer_layer(x, dim, kernel_size):
+    """ConvMixerLayer module.
     Args:
-        inputs: Input tensor.
+        x: input tensor.
         dim: integer, filters of the layer in a block.
-        kernel_size: integer, kernel size of the Conv2d layers.
+        kernel_size: integer, kernel size of the Conv2D layers.
     Returns:
-        Output tensor for the CovnMixer Layer.
+        the updated input tensor.
     """
 
-    def apply(x):
-        residual = x
-        x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding="same")(x)
-        x = tf.nn.gelu(x)
-        x = layers.BatchNormalization()(x)
-        x = layers.Add()([x, residual])
-
-        x = layers.Conv2D(dim, kernel_size=1)(x)
-        x = tf.nn.gelu(x)
-        x = layers.BatchNormalization()(x)
-        return x
-
-    return apply
+    residual = x
+    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding="same")(x)
+    x = tf.nn.gelu(x)
+    x = layers.BatchNormalization()(x)
+    x = layers.Add()([x, residual])
+
+    x = layers.Conv2D(dim, kernel_size=1)(x)
+    x = tf.nn.gelu(x)
+    x = layers.BatchNormalization()(x)
+    return x
 
 
-def patch_embed(dim, patch_size):
+def apply_patch_embed(x, dim, patch_size):
     """Implementation for Extracting Patch Embeddings.
     Args:
-        inputs: Input tensor.
+        x: input tensor.
+        dim: integer, filters of the layer in a block.
         patch_size: integer, Size of patches.
     Returns:
-        Output tensor for the patch embed.
+        the updated input tensor.
     """
 
-    def apply(x):
-        x = layers.Conv2D(filters=dim, kernel_size=patch_size, strides=patch_size)(x)
-        x = tf.nn.gelu(x)
-        x = layers.BatchNormalization()(x)
-        return x
-
-    return apply
+    x = layers.Conv2D(filters=dim, kernel_size=patch_size, strides=patch_size)(
+        x
+    )
+    x = tf.nn.gelu(x)
+    x = layers.BatchNormalization()(x)
+    return x
 
 
-def ConvMixer(
-    dim,
-    depth,
-    patch_size,
-    kernel_size,
-    include_top,
-    include_rescaling,
-    name="ConvMixer",
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classes=None,
-    classifier_activation="softmax",
-):
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class ConvMixer(keras.Model):
     """Instantiates the ConvMixer architecture.
+
     Args:
-        dim: number of filters.
-        depth: number of CovnMixer Layer.
-        patch_size: Size of the patches.
-        kernel_size: kernel size for conv2d layers.
-        include_rescaling: whether or not to Rescale the inputs. If set to True,
+        dim: integer, number of filters.
+        depth: integer, number of ConvMixer Layer.
+        patch_size: integer, size of the patches.
+        kernel_size: integer, kernel size for Conv2D layers.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network.
+        include_rescaling: bool, whether to rescale the inputs. If set to True,
             inputs will be passed through a `Rescaling(1/255.0)` layer.
-            name: string, model name.
-        include_top: whether to include the fully-connected
-            layer at the top of the network.
-        weights: one of `None` (random initialization),
-            or the path to the weights file to be loaded.
+        name: string, optional name to pass to the model, defaults to
+            "ConvMixer".
+        weights: one of `None` (random initialization) or the path to the
+            weights file to be loaded.
         input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+        input_tensor: optional Keras tensor (i.e., output of `layers.Input()`)
             to use as image input for the model.
-        pooling: optional pooling mode for feature extraction
-            when `include_top` is `False`.
-            - `None` means that the output of the model will be
-                the 4D tensor output of the
-                last convolutional layer.
-            - `avg` means that global average pooling
-                will be applied to the output of the
-                last convolutional layer, and thus
-                the output of the model will be a 2D tensor.
-            - `max` means that global max pooling will
-                be applied.
-        classes: optional number of classes to classify images
-            into, only to be specified if `include_top` is True.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
-        **kwargs: Pass-through keyword arguments to `tf.keras.Model`.
+        pooling: optional pooling mode for feature extraction when `include_top`
+            is `False`.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional layer.
+            - `avg` means that global average pooling will be applied to the
+                output of the last convolutional layer, and thus the output of
+                the model will be a 2D tensor.
+            - `max` means that global max pooling will be applied.
+        num_classes: integer, optional number of classes to classify images
+            into. Only to be specified if `include_top` is True.
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
+        **kwargs: Pass-through keyword arguments to `keras.Model`.
+
     Returns:
       A `keras.Model` instance.
     """
 
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either `None` or the path to the "
-            f"weights file to be loaded. Weights file not found at location: {weights}"
-        )
-
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, you should specify `classes`. "
-            f"Received: classes={classes}"
-        )
-
-    if include_top and pooling:
-        raise ValueError(
-            f"`pooling` must be `None` when `include_top=True`."
-            f"Received pooling={pooling} and include_top={include_top}. "
-        )
-
-    inputs = utils.parse_model_inputs(input_shape, input_tensor)
-    x = inputs
-
-    if include_rescaling:
-        x = layers.Rescaling(1 / 255.0)(x)
-    x = patch_embed(dim, patch_size)(x)
-
-    for _ in range(depth):
-        x = CovnMixer_Layer(dim, kernel_size)(x)
-
-    if include_top:
-        x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        x = layers.Dense(classes, activation=classifier_activation, name="predictions")(
-            x
-        )
-    else:
-        if pooling == "avg":
-            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        elif pooling == "max":
-            x = layers.GlobalMaxPooling2D(name="max_pool")(x)
+    def __init__(
+        self,
+        dim,
+        depth,
+        patch_size,
+        kernel_size,
+        include_top,
+        include_rescaling,
+        name="ConvMixer",
+        weights=None,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        pooling=None,
+        num_classes=None,
+        classifier_activation="softmax",
+        **kwargs,
+    ):
+        if weights and not tf.io.gfile.exists(weights):
+            raise ValueError(
+                "The `weights` argument should be either `None` or the path to "
+                "the weights file to be loaded. Weights file not found at "
+                f"location: {weights}"
+            )
+
+        if include_top and not num_classes:
+            raise ValueError(
+                "If `include_top` is True, you should specify `classes`. "
+                f"Received: classes={num_classes}"
+            )
+
+        if include_top and pooling:
+            raise ValueError(
+                f"`pooling` must be `None` when `include_top=True`."
+                f"Received pooling={pooling} and include_top={include_top}. "
+            )
+
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
+        x = inputs
+
+        if include_rescaling:
+            x = layers.Rescaling(1 / 255.0)(x)
+        x = apply_patch_embed(x, dim, patch_size)
 
-    model = keras.Model(inputs, x, name=name)
-    if weights is not None:
-        model.load_weights(weights)
+        for _ in range(depth):
+            x = apply_conv_mixer_layer(x, dim, kernel_size)
 
-    return model
+        if include_top:
+            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+            x = layers.Dense(
+                num_classes,
+                activation=classifier_activation,
+                name="predictions",
+            )(x)
+        else:
+            if pooling == "avg":
+                x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+            elif pooling == "max":
+                x = layers.GlobalMaxPooling2D(name="max_pool")(x)
+
+        super().__init__(inputs=inputs, outputs=x, name=name, **kwargs)
+
+        if weights is not None:
+            self.load_weights(weights)
+
+        self.dim = dim
+        self.depth = depth
+        self.patch_size = patch_size
+        self.kernel_size = kernel_size
+        self.include_top = include_top
+        self.include_rescaling = include_rescaling
+        self.input_tensor = input_tensor
+        self.pooling = pooling
+        self.num_classes = num_classes
+        self.classifier_activation = classifier_activation
+
+    def get_config(self):
+        return {
+            "dim": self.dim,
+            "depth": self.depth,
+            "patch_size": self.patch_size,
+            "kernel_size": self.kernel_size,
+            "include_top": self.include_top,
+            "include_rescaling": self.include_rescaling,
+            "name": self.name,
+            "input_shape": self.input_shape[1:],
+            "input_tensor": self.input_tensor,
+            "pooling": self.pooling,
+            "num_classes": self.num_classes,
+            "classifier_activation": self.classifier_activation,
+            "trainable": self.trainable,
+        }
 
 
 def ConvMixer_1536_20(
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     name="ConvMixer_1536_20",
     **kwargs,
@@ -249,24 +291,24 @@
         include_rescaling=include_rescaling,
         include_top=include_top,
         name=name,
         weights=weights,
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def ConvMixer_1536_24(
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     name="ConvMixer_1536_24",
     **kwargs,
@@ -279,24 +321,24 @@
         include_rescaling=include_rescaling,
         include_top=include_top,
         name=name,
         weights=weights,
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def ConvMixer_768_32(
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     name="ConvMixer_768_32",
     **kwargs,
@@ -309,24 +351,24 @@
         include_rescaling=include_rescaling,
         include_top=include_top,
         name=name,
         weights=weights,
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def ConvMixer_1024_16(
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     name="ConvMixer_1024_16",
     **kwargs,
@@ -339,24 +381,24 @@
         include_rescaling=include_rescaling,
         include_top=include_top,
         name=name,
         weights=weights,
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def ConvMixer_512_16(
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     name="ConvMixer_512_16",
     **kwargs,
@@ -369,18 +411,34 @@
         include_rescaling=include_rescaling,
         include_top=include_top,
         name=name,
         weights=parse_weights(weights, include_top, "convmixer_512_16"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
-setattr(ConvMixer_1536_20, "__doc__", BASE_DOCSTRING.format(name="ConvMixer_1536_20"))
-setattr(ConvMixer_1536_24, "__doc__", BASE_DOCSTRING.format(name="ConvMixer_1536_24"))
-setattr(ConvMixer_768_32, "__doc__", BASE_DOCSTRING.format(name="ConvMixer_768_32"))
-setattr(ConvMixer_1024_16, "__doc__", BASE_DOCSTRING.format(name="ConvMixer_1024_16"))
-setattr(ConvMixer_512_16, "__doc__", BASE_DOCSTRING.format(name="ConvMixer_512_16"))
+setattr(
+    ConvMixer_1536_20,
+    "__doc__",
+    BASE_DOCSTRING.format(name="ConvMixer_1536_20"),
+)
+setattr(
+    ConvMixer_1536_24,
+    "__doc__",
+    BASE_DOCSTRING.format(name="ConvMixer_1536_24"),
+)
+setattr(
+    ConvMixer_768_32, "__doc__", BASE_DOCSTRING.format(name="ConvMixer_768_32")
+)
+setattr(
+    ConvMixer_1024_16,
+    "__doc__",
+    BASE_DOCSTRING.format(name="ConvMixer_1024_16"),
+)
+setattr(
+    ConvMixer_512_16, "__doc__", BASE_DOCSTRING.format(name="ConvMixer_512_16")
+)
```

## Comparing `keras_cv/models/convmixer_test.py` & `keras_cv/models/legacy/convmixer_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import convmixer
+from keras_cv.models.legacy import convmixer
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
     (
         convmixer.ConvMixer_1024_16,
         1024,
```

## Comparing `keras_cv/models/convnext.py` & `keras_cv/models/legacy/convnext.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,27 +9,26 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """ConvNeXt models for Keras.
-
 References:
 - [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)
   (CVPR 2022)
 """
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import backend
 from tensorflow.keras import layers
 
 from keras_cv.layers.regularization import StochasticDepth
-from keras_cv.models import utils
+from keras_cv.models.legacy import utils
 
 MODEL_CONFIGS = {
     "tiny": {
         "depths": [3, 3, 9, 3],
         "projection_dims": [96, 192, 384, 768],
         "default_size": 224,
     },
@@ -52,80 +51,80 @@
         "depths": [3, 3, 27, 3],
         "projection_dims": [256, 512, 1024, 2048],
         "default_size": 224,
     },
 }
 
 BASE_DOCSTRING = """Instantiates the {name} architecture.
-    - [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)
-    (CVPR 2022)
+    - [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) (CVPR 2022)
 
     This function returns a Keras {name} model.
-
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: whether to include the fully-connected layer at the top of the
-            network.  If provided, classes must be provided.
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, `num_classes` must be provided.
         depths: an iterable containing depths for each individual stages.
         projection_dims: An iterable containing output number of channels of
             each individual stages.
         drop_path_rate: stochastic depth probability, if 0.0, then stochastic
             depth won't be used.
         layer_scale_init_value: layer scale coefficient, if 0.0, layer scaling
             won't be used.
-        weights: one of `None` (random initialization), or a pretrained weight
-            file path.
-        input_shape: optional shape tuple, defaults to `(None, None, 3)`.
+        weights: one of `None` (random initialization), a pretrained weight file
+            path, or a reference to pre-trained weights (e.g.
+            'imagenet/classification')(see available pre-trained weights in
+            weights.py)
+        input_shape: optional shape tuple, defaults to (None, None, 3).
         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
             to use as image input for the model.
         pooling: optional pooling mode for feature extraction
             when `include_top` is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
-            - `avg` means that global average pooling will be applied to the output
-                of the last convolutional block, and thus the output of the model will
-                be a 2D tensor.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
+            - `avg` means that global average pooling will be applied to the
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
             - `max` means that global max pooling will be applied.
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
-            Defaults to `"softmax"`.
-        name: (Optional) name to pass to the model.  Defaults to "{name}".
+        num_classes: optional int, number of classes to classify images into
+            (only to be specified if `include_top` is `True`).
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
+        name: (Optional) name to pass to the model, defaults to "{name}".
 
     Returns:
       A `keras.Model` instance.
 """
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class LayerScale(layers.Layer):
     """Layer scale module.
-
     References:
       - https://arxiv.org/abs/2103.17239
-
     Args:
       init_values (float): Initial value for layer scale. Should be within
         [0, 1].
       projection_dim (int): Projection dimensionality.
-
     Returns:
       Tensor multiplied to the scale.
     """
 
     def __init__(self, init_values, projection_dim, **kwargs):
         super().__init__(**kwargs)
         self.init_values = init_values
         self.projection_dim = projection_dim
 
     def build(self, input_shape):
-        self.gamma = tf.Variable(self.init_values * tf.ones((self.projection_dim,)))
+        self.gamma = tf.Variable(
+            self.init_values * tf.ones((self.projection_dim,))
+        )
 
     def call(self, x):
         return x * self.gamma
 
     def get_config(self):
         config = super().get_config()
         config.update(
@@ -133,288 +132,327 @@
                 "init_values": self.init_values,
                 "projection_dim": self.projection_dim,
             }
         )
         return config
 
 
-def ConvNeXtBlock(
-    projection_dim, drop_path_rate=0.0, layer_scale_init_value=1e-6, name=None
+def apply_block(
+    x,
+    projection_dim,
+    drop_path_rate=0.0,
+    layer_scale_init_value=1e-6,
+    name=None,
 ):
     """ConvNeXt block.
-
     References:
       - https://arxiv.org/abs/2201.03545
       - https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py
-
     Notes:
       In the original ConvNeXt implementation (linked above), the authors use
       `Dense` layers for pointwise convolutions for increased efficiency.
       Following that, this implementation also uses the same.
-
     Args:
       projection_dim (int): Number of filters for convolution layers. In the
         ConvNeXt paper, this is referred to as projection dimension.
       drop_path_rate (float): Probability of dropping paths. Should be within
         [0, 1].
       layer_scale_init_value (float): Layer scale value. Should be a small float
         number.
       name: name to path to the keras layer.
-
     Returns:
       A function representing a ConvNeXtBlock block.
-    """
+    """  # noqa: E501
     if name is None:
         name = "prestem" + str(backend.get_uid("prestem"))
 
-    def apply(inputs):
-        x = inputs
+    inputs = x
 
-        x = layers.Conv2D(
-            filters=projection_dim,
-            kernel_size=7,
-            padding="same",
-            groups=projection_dim,
-            name=name + "_depthwise_conv",
+    x = layers.Conv2D(
+        filters=projection_dim,
+        kernel_size=7,
+        padding="same",
+        groups=projection_dim,
+        name=name + "_depthwise_conv",
+    )(x)
+    x = layers.LayerNormalization(epsilon=1e-6, name=name + "_layernorm")(x)
+    x = layers.Dense(4 * projection_dim, name=name + "_pointwise_conv_1")(x)
+    x = layers.Activation("gelu", name=name + "_gelu")(x)
+    x = layers.Dense(projection_dim, name=name + "_pointwise_conv_2")(x)
+
+    if layer_scale_init_value is not None:
+        x = LayerScale(
+            layer_scale_init_value,
+            projection_dim,
+            name=name + "_layer_scale",
         )(x)
-        x = layers.LayerNormalization(epsilon=1e-6, name=name + "_layernorm")(x)
-        x = layers.Dense(4 * projection_dim, name=name + "_pointwise_conv_1")(x)
-        x = layers.Activation("gelu", name=name + "_gelu")(x)
-        x = layers.Dense(projection_dim, name=name + "_pointwise_conv_2")(x)
-
-        if layer_scale_init_value is not None:
-            x = LayerScale(
-                layer_scale_init_value,
-                projection_dim,
-                name=name + "_layer_scale",
-            )(x)
-        if drop_path_rate:
-            layer = StochasticDepth(drop_path_rate, name=name + "_stochastic_depth")
-            return layer([inputs, x])
-        else:
-            layer = layers.Activation("linear", name=name + "_identity")
-            return inputs + layer(x)
-
-    return apply
+    if drop_path_rate:
+        layer = StochasticDepth(drop_path_rate, name=name + "_stochastic_depth")
+        return layer([inputs, x])
+    else:
+        layer = layers.Activation("linear", name=name + "_identity")
+        return inputs + layer(x)
 
 
-def Head(num_classes, activation="softmax", name=None):
+def apply_head(x, num_classes, activation="softmax", name=None):
     """Implementation of classification head of ConvNeXt.
-
     Args:
       num_classes: number of classes for Dense layer
       activation: activation function for Dense layer
       name: name prefix
-
     Returns:
       Classification head function.
     """
     if name is None:
         name = str(backend.get_uid("head"))
 
-    def apply(x):
-        x = layers.GlobalAveragePooling2D(name=name + "_head_gap")(x)
-        x = layers.LayerNormalization(epsilon=1e-6, name=name + "_head_layernorm")(x)
-        x = layers.Dense(num_classes, activation=activation, name=name + "_head_dense")(
-            x
-        )
-        return x
-
-    return apply
+    x = layers.GlobalAveragePooling2D(name=name + "_head_gap")(x)
+    x = layers.LayerNormalization(epsilon=1e-6, name=name + "_head_layernorm")(
+        x
+    )
+    x = layers.Dense(
+        num_classes, activation=activation, name=name + "_head_dense"
+    )(x)
+    return x
 
 
-def ConvNeXt(
-    include_rescaling,
-    include_top,
-    depths,
-    projection_dims,
-    drop_path_rate=0.0,
-    layer_scale_init_value=1e-6,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classes=None,
-    classifier_activation="softmax",
-    name="convnext",
-):
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class ConvNeXt(keras.Model):
     """Instantiates ConvNeXt architecture given specific configuration.
-
     Args:
-      include_rescaling: whether or not to Rescale the inputs. If set to True,
-        inputs will be passed through a `Rescaling(1/255.0)` layer.
-      include_top: Boolean denoting whether to include classification head to
-        the model.
-      depths: An iterable containing depths for each individual stages.
-      projection_dims: An iterable containing output number of channels of
-      each individual stages.
-      drop_path_rate: Stochastic depth probability. If 0.0, then stochastic
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, `num_classes` must be provided.
+        depths: An iterable containing depths for each individual stages.
+        projection_dims: An iterable containing output number of channels of
+        each individual stages.
+        drop_path_rate: Stochastic depth probability. If 0.0, then stochastic
         depth won't be used.
-      layer_scale_init_value: Layer scale coefficient. If 0.0, layer scaling
+        layer_scale_init_value: Layer scale coefficient. If 0.0, layer scaling
         won't be used.
-      weights: One of `None` (random initialization), or a pretrained weight
-        file path.
-      input_shape: optional shape tuple, defaults to `(None, None, 3)`.
-      input_tensor: optional Keras tensor (i.e. output of `layers.Input()`).
-      pooling: optional pooling mode for feature extraction when `include_top`
-        is `False`.
-        - `None` means that the output of the model will be the 4D tensor output
-          of the last convolutional layer.
-        - `avg` means that global average pooling will be applied to the output
-          of the last convolutional layer, and thus the output of the model will
-          be a 2D tensor.
-        - `max` means that global max pooling will be applied.
-      classes: optional number of classes to classify images into, only to be
-        specified if `include_top` is True.
-      classifier_activation: A `str` or callable. The activation function to use
-        on the "top" layer. Ignored unless `include_top=True`. Set
-        `classifier_activation=None` to return the logits of the "top" layer.
-      name: An optional name for the model.
-
+        weights: one of `None` (random initialization), a pretrained weight file
+            path, or a reference to pre-trained weights (e.g.
+            'imagenet/classification')(see available pre-trained weights in
+            weights.py)
+        input_shape: optional shape tuple, defaults to (None, None, 3).
+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+            to use as image input for the model.
+        pooling: optional pooling mode for feature extraction
+            when `include_top` is `False`.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
+            - `avg` means that global average pooling will be applied to the
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
+            - `max` means that global max pooling will be applied.
+        num_classes: optional int, number of classes to classify images into
+            (only to be specified if `include_top` is `True`).
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
+        name: (Optional) name to pass to the model, defaults to "convnext".
     Returns:
       A `keras.Model` instance.
-
     Raises:
-        ValueError: in case of invalid argument for `weights`,
-          or invalid input shape.
-        ValueError: if `classifier_activation` is not `softmax`, or `None`
-          when using a pretrained top layer.
-        ValueError: if `include_top` is True but `classes` is not specified.
+        ValueError: in case of invalid argument for `weights`, or invalid input
+            shape.
+        ValueError: if `classifier_activation` is not `softmax`, or `None` when
+            using a pretrained top layer.
+        ValueError: if `include_top` is True but `num_classes` is not specified.
     """
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either "
-            "`None` or the path to the weights file to be loaded. "
-            f"Weights file not found at location: {weights}"
-        )
-
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, "
-            "you should specify `classes`. "
-            f"Received: classes={classes}"
-        )
 
-    if include_top and pooling:
-        raise ValueError(
-            f"`pooling` must be `None` when `include_top=True`."
-            f"Received pooling={pooling} and include_top={include_top}. "
-        )
+    def __init__(
+        self,
+        include_rescaling,
+        include_top,
+        depths,
+        projection_dims,
+        drop_path_rate=0.0,
+        layer_scale_init_value=1e-6,
+        weights=None,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        pooling=None,
+        num_classes=None,
+        classifier_activation="softmax",
+        name="convnext",
+        **kwargs,
+    ):
+        if weights and not tf.io.gfile.exists(weights):
+            raise ValueError(
+                "The `weights` argument should be either "
+                "`None` or the path to the weights file to be loaded. "
+                f"Weights file not found at location: {weights}"
+            )
+
+        if include_top and not num_classes:
+            raise ValueError(
+                "If `include_top` is True, "
+                "you should specify `num_classes`. "
+                f"Received: num_classes={num_classes}"
+            )
+
+        if include_top and pooling:
+            raise ValueError(
+                f"`pooling` must be `None` when `include_top=True`."
+                f"Received pooling={pooling} and include_top={include_top}. "
+            )
 
-    inputs = utils.parse_model_inputs(input_shape, input_tensor)
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
 
-    x = inputs
-    if include_rescaling:
-        x = layers.Rescaling(1 / 255.0)(x)
-
-    # Stem block.
-    stem = keras.Sequential(
-        [
-            layers.Conv2D(
-                projection_dims[0],
-                kernel_size=4,
-                strides=4,
-                name=name + "_stem_conv",
-            ),
-            layers.LayerNormalization(epsilon=1e-6, name=name + "_stem_layernorm"),
-        ],
-        name=name + "_stem",
-    )
+        x = inputs
+        if include_rescaling:
+            x = layers.Rescaling(1 / 255.0)(x)
 
-    # Downsampling blocks.
-    downsample_layers = []
-    downsample_layers.append(stem)
-
-    num_downsample_layers = 3
-    for i in range(num_downsample_layers):
-        downsample_layer = keras.Sequential(
+        # Stem block.
+        stem = keras.Sequential(
             [
-                layers.LayerNormalization(
-                    epsilon=1e-6,
-                    name=name + "_downsampling_layernorm_" + str(i),
-                ),
                 layers.Conv2D(
-                    projection_dims[i + 1],
-                    kernel_size=2,
-                    strides=2,
-                    name=name + "_downsampling_conv_" + str(i),
+                    projection_dims[0],
+                    kernel_size=4,
+                    strides=4,
+                    name=name + "_stem_conv",
+                ),
+                layers.LayerNormalization(
+                    epsilon=1e-6, name=name + "_stem_layernorm"
                 ),
             ],
-            name=name + "_downsampling_block_" + str(i),
+            name=name + "_stem",
         )
-        downsample_layers.append(downsample_layer)
-
-    # Stochastic depth schedule.
-    # This is referred from the original ConvNeXt codebase:
-    # https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py#L86
-    depth_drop_rates = [float(x) for x in tf.linspace(0.0, drop_path_rate, sum(depths))]
-
-    # First apply downsampling blocks and then apply ConvNeXt stages.
-    cur = 0
-
-    num_convnext_blocks = 4
-    for i in range(num_convnext_blocks):
-        x = downsample_layers[i](x)
-        for j in range(depths[i]):
-            x = ConvNeXtBlock(
-                projection_dim=projection_dims[i],
-                drop_path_rate=depth_drop_rates[cur + j],
-                layer_scale_init_value=layer_scale_init_value,
-                name=name + f"_stage_{i}_block_{j}",
-            )(x)
-        cur += depths[i]
-
-    if include_top:
-        x = Head(
-            num_classes=classes,
-            activation=classifier_activation,
-            name=name,
-        )(x)
 
-    else:
-        if pooling == "avg":
-            x = layers.GlobalAveragePooling2D()(x)
-        elif pooling == "max":
-            x = layers.GlobalMaxPooling2D()(x)
-        x = layers.LayerNormalization(epsilon=1e-6)(x)
+        # Downsampling blocks.
+        downsample_layers = []
+        downsample_layers.append(stem)
+
+        num_downsample_layers = 3
+        for i in range(num_downsample_layers):
+            downsample_layer = keras.Sequential(
+                [
+                    layers.LayerNormalization(
+                        epsilon=1e-6,
+                        name=name + "_downsampling_layernorm_" + str(i),
+                    ),
+                    layers.Conv2D(
+                        projection_dims[i + 1],
+                        kernel_size=2,
+                        strides=2,
+                        name=name + "_downsampling_conv_" + str(i),
+                    ),
+                ],
+                name=name + "_downsampling_block_" + str(i),
+            )
+            downsample_layers.append(downsample_layer)
+
+        # Stochastic depth schedule.
+        # This is referred from the original ConvNeXt codebase:
+        # https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py#L86
+        depth_drop_rates = [
+            float(x) for x in tf.linspace(0.0, drop_path_rate, sum(depths))
+        ]
+
+        # First apply downsampling blocks and then apply ConvNeXt stages.
+        cur = 0
+
+        num_convnext_blocks = 4
+        for i in range(num_convnext_blocks):
+            x = downsample_layers[i](x)
+            for j in range(depths[i]):
+                x = apply_block(
+                    x,
+                    projection_dim=projection_dims[i],
+                    drop_path_rate=depth_drop_rates[cur + j],
+                    layer_scale_init_value=layer_scale_init_value,
+                    name=name + f"_stage_{i}_block_{j}",
+                )
+            cur += depths[i]
+
+        if include_top:
+            x = apply_head(
+                x,
+                num_classes=num_classes,
+                activation=classifier_activation,
+                name=name,
+            )
 
-    model = keras.Model(inputs=inputs, outputs=x, name=name)
-
-    if weights is not None:
-        model.load_weights(weights)
+        else:
+            if pooling == "avg":
+                x = layers.GlobalAveragePooling2D()(x)
+            elif pooling == "max":
+                x = layers.GlobalMaxPooling2D()(x)
+            x = layers.LayerNormalization(epsilon=1e-6)(x)
+
+        # Create model.
+        super().__init__(inputs=inputs, outputs=x, **kwargs)
+
+        if weights is not None:
+            self.load_weights(weights)
+
+        self.include_rescaling = include_rescaling
+        self.include_top = include_top
+        self.depths = depths
+        self.projection_dims = projection_dims
+        self.drop_path_rate = drop_path_rate
+        self.layer_scale_init_value = layer_scale_init_value
+        self.input_tensor = input_tensor
+        self.pooling = pooling
+        self.num_classes = num_classes
+        self.classifier_activation = classifier_activation
 
-    return model
+    def get_config(self):
+        return {
+            "include_rescaling": self.include_rescaling,
+            "include_top": self.include_top,
+            "depths": self.depths,
+            "projection_dims": self.projection_dims,
+            "drop_path_rate": self.drop_path_rate,
+            "layer_scale_init_value": self.layer_scale_init_value,
+            # Remove batch dimension from `input_shape`
+            "input_shape": self.input_shape[1:],
+            "input_tensor": self.input_tensor,
+            "pooling": self.pooling,
+            "num_classes": self.num_classes,
+            "classifier_activation": self.classifier_activation,
+            "name": self.name,
+            "trainable": self.trainable,
+        }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
 
 
 def ConvNeXtTiny(
     *,
     include_rescaling,
     include_top,
     drop_path_rate,
     layer_scale_init_value,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
+    num_classes=None,
     classifier_activation="softmax",
     name="convnext_tiny",
 ):
     return ConvNeXt(
         include_rescaling=include_rescaling,
         include_top=include_top,
         depths=MODEL_CONFIGS["tiny"]["depths"],
         projection_dims=MODEL_CONFIGS["tiny"]["projection_dims"],
         drop_path_rate=drop_path_rate,
         layer_scale_init_value=layer_scale_init_value,
         weights=weights,
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         name=name,
     )
 
 
 def ConvNeXtSmall(
     *,
@@ -422,30 +460,30 @@
     include_top,
     drop_path_rate,
     layer_scale_init_value,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
+    num_classes=None,
     classifier_activation="softmax",
     name="convnext_small",
 ):
     return ConvNeXt(
         include_rescaling=include_rescaling,
         include_top=include_top,
         depths=MODEL_CONFIGS["small"]["depths"],
         projection_dims=MODEL_CONFIGS["small"]["projection_dims"],
         drop_path_rate=drop_path_rate,
         layer_scale_init_value=layer_scale_init_value,
         weights=weights,
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         name=name,
     )
 
 
 def ConvNeXtBase(
     *,
@@ -453,30 +491,30 @@
     include_top,
     drop_path_rate,
     layer_scale_init_value,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
+    num_classes=None,
     classifier_activation="softmax",
     name="convnext_base",
 ):
     return ConvNeXt(
         include_rescaling=include_rescaling,
         include_top=include_top,
         depths=MODEL_CONFIGS["base"]["depths"],
         projection_dims=MODEL_CONFIGS["base"]["projection_dims"],
         drop_path_rate=drop_path_rate,
         layer_scale_init_value=layer_scale_init_value,
         weights=weights,
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         name=name,
     )
 
 
 def ConvNeXtLarge(
     *,
@@ -484,30 +522,30 @@
     include_top,
     drop_path_rate,
     layer_scale_init_value,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
+    num_classes=None,
     classifier_activation="softmax",
     name="convnext_large",
 ):
     return ConvNeXt(
         include_rescaling=include_rescaling,
         include_top=include_top,
         depths=MODEL_CONFIGS["large"]["depths"],
         projection_dims=MODEL_CONFIGS["large"]["projection_dims"],
         drop_path_rate=drop_path_rate,
         layer_scale_init_value=layer_scale_init_value,
         weights=weights,
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         name=name,
     )
 
 
 def ConvNeXtXLarge(
     *,
@@ -515,30 +553,30 @@
     include_top,
     drop_path_rate,
     layer_scale_init_value,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
+    num_classes=None,
     classifier_activation="softmax",
     name="convnext_xlarge",
 ):
     return ConvNeXt(
         include_rescaling=include_rescaling,
         include_top=include_top,
         depths=MODEL_CONFIGS["xlarge"]["depths"],
         projection_dims=MODEL_CONFIGS["xlarge"]["projection_dims"],
         drop_path_rate=drop_path_rate,
         layer_scale_init_value=layer_scale_init_value,
         weights=weights,
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         name=name,
     )
 
 
 ConvNeXtTiny.__doc__ = BASE_DOCSTRING.format(name="ConvNeXtTiny")
 ConvNeXtSmall.__doc__ = BASE_DOCSTRING.format(name="ConvNeXtSmall")
```

## Comparing `keras_cv/models/convnext_test.py` & `keras_cv/models/legacy/convnext_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import convnext
+from keras_cv.models.legacy import convnext
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
     (
         convnext.ConvNeXtTiny,
         768,
```

## Comparing `keras_cv/models/csp_darknet.py` & `keras_cv/models/legacy/densenet.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,407 +8,384 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""DarkNet models for KerasCV.
+"""DenseNet models for KerasCV.
+
 Reference:
-    - [YoloV4 Paper](https://arxiv.org/abs/1804.02767)
-    - [CSPNet Paper](https://arxiv.org/pdf/1911.11929)
-    - [YoloX Paper](https://arxiv.org/abs/2107.08430)
-    - [YoloX implementation](https://github.com/ultralytics/yolov3)
-"""
-import types
+  - [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)
+  - [Based on the Original keras.applications DenseNet](https://github.com/keras-team/keras/blob/master/keras/applications/densenet.py)
+"""  # noqa: E501
 
-import tensorflow as tf
 from tensorflow import keras
+from tensorflow.keras import backend
 from tensorflow.keras import layers
 
-from keras_cv.models import utils
-from keras_cv.models.__internal__.darknet_utils import CrossStagePartial
-from keras_cv.models.__internal__.darknet_utils import DarknetConvBlock
-from keras_cv.models.__internal__.darknet_utils import DarknetConvBlockDepthwise
-from keras_cv.models.__internal__.darknet_utils import Focus
-from keras_cv.models.__internal__.darknet_utils import SpatialPyramidPoolingBottleneck
-from keras_cv.models.weights import parse_weights
+from keras_cv.models.legacy import utils
+from keras_cv.models.legacy.weights import parse_weights
 
+MODEL_CONFIGS = {
+    "DenseNet121": {
+        "blocks": [6, 12, 24, 16],
+    },
+    "DenseNet169": {
+        "blocks": [6, 12, 32, 32],
+    },
+    "DenseNet201": {
+        "blocks": [6, 12, 48, 32],
+    },
+}
 
-def CSPDarkNet(
-    depth_multiplier,
-    width_multiplier,
-    include_rescaling,
-    include_top,
-    use_depthwise=False,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name="CSPDarkNet",
-    **kwargs,
-):
-    """Instantiates the CSPDarkNet architecture.
+BN_AXIS = 3
+BN_EPSILON = 1.001e-5
 
-    Although the DarkNet architecture is commonly used for detection tasks, it is
-    possible to extract the intermediate dark2 to dark5 layers from the model for
-    creating a feature pyramid Network.
+BASE_DOCSTRING = """Instantiates the {name} architecture.
 
     Reference:
-        - [YoloV4 Paper](https://arxiv.org/abs/1804.02767)
-        - [CSPNet Paper](https://arxiv.org/pdf/1911.11929)
-        - [YoloX Paper](https://arxiv.org/abs/2107.08430)
-        - [YoloX implementation](https://github.com/ultralytics/yolov3)
+        - [Densely Connected Convolutional Networks (CVPR 2017)](https://arxiv.org/abs/1608.06993)
+
+    This function returns a Keras {name} model.
+
     For transfer learning use cases, make sure to read the
     [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
     Args:
-        depth_multiplier: A float value used to calculate the base depth of the model
-            this changes based the detection model being used.
-        width_multiplier: A float value used to calculate the base width of the model
-            this changes based the detection model being used.
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: whether to include the fully-connected layer at the top of
-            the network.  If provided, `classes` must be provided.
-        use_depthwise: a boolean value used to decide whether a depthwise conv block
-            should be used over a regular darknet block. Defaults to False
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True.
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, `num_classes` must be provided.
+        num_classes: optional int, number of classes to classify images into
+            (only to be specified if `include_top` is `True`).
         weights: one of `None` (random initialization), a pretrained weight file
-            path, or a reference to pre-trained weights (e.g. 'imagenet/classification')
-            (see available pre-trained weights in weights.py)
+            path, or a reference to pre-trained weights (e.g.
+            'imagenet/classification')(see available pre-trained weights in
+            weights.py)
+        input_shape: optional shape tuple, defaults to (None, None, 3).
         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
             to use as image input for the model.
-        input_shape: optional shape tuple, defaults to (None, None, 3).
-        pooling: optional pooling mode for feature extraction when `include_top`
-            is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
+        pooling: optional pooling mode for feature extraction
+            when `include_top` is `False`.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
             - `avg` means that global average pooling will be applied to the
-                output of the last convolutional block, and thus the output of the
-                model will be a 2D tensor.
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
             - `max` means that global max pooling will be applied.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
-        name: (Optional) name to pass to the model.  Defaults to "CSPDarkNet".
+        name: (Optional) name to pass to the model, defaults to "{name}".
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
+
+    Returns:
+      A `keras.Model` instance.
+"""  # noqa: E501
+
+
+def apply_dense_block(x, blocks, name=None):
+    """A dense block.
+
+    Args:
+      blocks: int, number of building blocks.
+      name: string, block label.
+
     Returns:
-        A `keras.Model` instance.
+      a function that takes an input Tensor representing an apply_dense_block.
     """
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either `None` or the path to the "
-            f"weights file to be loaded. Weights file not found at location: {weights}"
-        )
-
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, you should specify `classes`. Received: "
-            f"classes={classes}"
-        )
-
-    ConvBlock = DarknetConvBlockDepthwise if use_depthwise else DarknetConvBlock
-
-    base_channels = int(width_multiplier * 64)
-    base_depth = max(round(depth_multiplier * 3), 1)
-
-    inputs = utils.parse_model_inputs(input_shape, input_tensor)
-
-    x = inputs
-    if include_rescaling:
-        x = layers.Rescaling(1 / 255.0)(x)
-
-    # stem
-    x = Focus(name="stem_focus")(x)
-    x = DarknetConvBlock(base_channels, kernel_size=3, strides=1, name="stem_conv")(x)
-
-    _backbone_level_outputs = {}
-    # dark2
-    x = ConvBlock(base_channels * 2, kernel_size=3, strides=2, name="dark2_conv")(x)
-    x = CrossStagePartial(
-        base_channels * 2,
-        num_bottlenecks=base_depth,
-        use_depthwise=use_depthwise,
-        name="dark2_csp",
-    )(x)
-    _backbone_level_outputs[2] = x
+    if name is None:
+        name = f"dense_block_{backend.get_uid('dense_block')}"
 
-    # dark3
-    x = ConvBlock(base_channels * 4, kernel_size=3, strides=2, name="dark3_conv")(x)
-    x = CrossStagePartial(
-        base_channels * 4,
-        num_bottlenecks=base_depth * 3,
-        use_depthwise=use_depthwise,
-        name="dark3_csp",
-    )(x)
-    _backbone_level_outputs[3] = x
+    for i in range(blocks):
+        x = apply_conv_block(x, 32, name=f"{name}_block_{i}")
+    return x
 
-    # dark4
-    x = ConvBlock(base_channels * 8, kernel_size=3, strides=2, name="dark4_conv")(x)
-    x = CrossStagePartial(
-        base_channels * 8,
-        num_bottlenecks=base_depth * 3,
-        use_depthwise=use_depthwise,
-        name="dark4_csp",
-    )(x)
-    _backbone_level_outputs[4] = x
 
-    # dark5
-    x = ConvBlock(base_channels * 16, kernel_size=3, strides=2, name="dark5_conv")(x)
-    x = SpatialPyramidPoolingBottleneck(
-        base_channels * 16, hidden_filters=base_channels * 8, name="dark5_spp"
+def apply_transition_block(x, reduction, name=None):
+    """A transition block.
+
+    Args:
+      reduction: float, compression rate at transition layers.
+      name: string, block label.
+
+    Returns:
+      a function that takes an input Tensor representing an
+      apply_transition_block.
+    """
+    if name is None:
+        name = f"transition_block_{backend.get_uid('transition_block')}"
+
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=f"{name}_bn"
     )(x)
-    x = CrossStagePartial(
-        base_channels * 16,
-        num_bottlenecks=base_depth,
-        residual=False,
-        use_depthwise=use_depthwise,
-        name="dark5_csp",
+    x = layers.Activation("relu", name=f"{name}_relu")(x)
+    x = layers.Conv2D(
+        int(backend.int_shape(x)[BN_AXIS] * reduction),
+        1,
+        use_bias=False,
+        name=f"{name}_conv",
     )(x)
-    _backbone_level_outputs[5] = x
+    x = layers.AveragePooling2D(2, strides=2, name=f"{name}_pool")(x)
+    return x
 
-    if include_top:
-        x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        x = layers.Dense(classes, activation=classifier_activation, name="predictions")(
-            x
-        )
-    elif pooling == "avg":
-        x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-    elif pooling == "max":
-        x = layers.GlobalMaxPooling2D(name="max_pool")(x)
-
-    model = keras.Model(inputs, x, name=name, **kwargs)
-    model._backbone_level_outputs = _backbone_level_outputs
-    # Bind the `to_backbone_model` method to the application model.
-    model.as_backbone = types.MethodType(utils.as_backbone, model)
-
-    if weights is not None:
-        model.load_weights(weights)
-    return model
-
-
-DEPTH_MULTIPLIERS = {
-    "tiny": 0.33,
-    "s": 0.33,
-    "m": 0.67,
-    "l": 1.00,
-    "x": 1.33,
-}
 
-WIDTH_MULTIPLIERS = {
-    "tiny": 0.375,
-    "s": 0.50,
-    "m": 0.75,
-    "l": 1.00,
-    "x": 1.25,
-}
+def apply_conv_block(x, growth_rate, name=None):
+    """A building block for a dense block.
 
-BASE_DOCSTRING = """Instantiates the {name} architecture.
+    Args:
+      growth_rate: float, growth rate at dense layers.
+      name: string, block label.
+
+    Returns:
+      a function that takes an input Tensor representing a apply_conv_block.
+    """
+    if name is None:
+        name = f"conv_block_{backend.get_uid('conv_block')}"
+
+    x1 = x
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=f"{name}_0_bn"
+    )(x)
+    x = layers.Activation("relu", name=f"{name}_0_relu")(x)
+    x = layers.Conv2D(
+        4 * growth_rate, 1, use_bias=False, name=f"{name}_1_conv"
+    )(x)
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=f"{name}_1_bn"
+    )(x)
+    x = layers.Activation("relu", name=f"{name}_1_relu")(x)
+    x = layers.Conv2D(
+        growth_rate,
+        3,
+        padding="same",
+        use_bias=False,
+        name=f"{name}_2_conv",
+    )(x)
+    x = layers.Concatenate(axis=BN_AXIS, name=f"{name}_concat")([x1, x])
+    return x
 
-    The CSPDarkNet architectures are commonly used for detection tasks. It is
-    possible to extract the intermediate dark2 to dark5 layers from the model for
-    creating a feature pyramid Network.
+
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class DenseNet(keras.Model):
+    """Instantiates the DenseNet architecture.
 
     Reference:
-        - [YoloV4 Paper](https://arxiv.org/abs/1804.02767)
-        - [CSPNet Paper](https://arxiv.org/pdf/1911.11929)
-        - [YoloX Paper](https://arxiv.org/abs/2107.08430)
-        - [YoloX implementation](https://github.com/ultralytics/yolov3)
+        - [Densely Connected Convolutional Networks (CVPR 2017)](https://arxiv.org/abs/1608.06993)
+
+    This function returns a Keras DenseNet model.
+
     For transfer learning use cases, make sure to read the
     [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: whether to include the fully-connected layer at the top of
-            the network.  If provided, `classes` must be provided.
-        use_depthwise: a boolean value used to decide whether a depthwise conv block
-            should be used over a regular darknet block. Defaults to False
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True.
+        blocks: numbers of building blocks for the four dense layers.
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, `num_classes` must be provided.
+        num_classes: optional int, number of classes to classify images into
+            (only to be specified if `include_top` is `True`).
         weights: one of `None` (random initialization), a pretrained weight file
-            path, or a reference to pre-trained weights (e.g. 'imagenet/classification')
-            (see available pre-trained weights in weights.py)
+            path, or a reference to pre-trained weights (e.g.
+            'imagenet/classification')(see available pre-trained weights in
+            weights.py)
+        input_shape: optional shape tuple, defaults to (None, None, 3).
         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
             to use as image input for the model.
-        input_shape: optional shape tuple, defaults to (None, None, 3).
-        pooling: optional pooling mode for feature extraction when `include_top`
-            is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
+        pooling: optional pooling mode for feature extraction
+            when `include_top` is `False`.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
             - `avg` means that global average pooling will be applied to the
-                output of the last convolutional block, and thus the output of the
-                model will be a 2D tensor.
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
             - `max` means that global max pooling will be applied.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
+        name: (Optional) name to pass to the model, defaults to "DenseNet".
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
 
-        name: (Optional) name to pass to the model.  Defaults to "{name}".
     Returns:
-        A `keras.Model` instance.
-    """
-
-
-def CSPDarkNetTiny(
-    *,
-    include_rescaling,
-    include_top,
-    use_depthwise=False,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name="CSPDarkNetTiny",
-    **kwargs,
-):
-    return CSPDarkNet(
-        depth_multiplier=DEPTH_MULTIPLIERS["tiny"],
-        width_multiplier=WIDTH_MULTIPLIERS["tiny"],
-        include_rescaling=include_rescaling,
-        include_top=include_top,
-        use_depthwise=use_depthwise,
-        classes=classes,
-        weights=parse_weights(weights, include_top, "cspdarknettiny"),
-        input_shape=input_shape,
-        input_tensor=input_tensor,
-        pooling=pooling,
-        classifier_activation=classifier_activation,
-        name=name,
-        **kwargs,
-    )
-
+      A `keras.Model` instance.
+    """  # noqa: E501
 
-def CSPDarkNetS(
-    *,
-    include_rescaling,
-    include_top,
-    use_depthwise=False,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name="CSPDarkNetS",
-    **kwargs,
-):
-    return CSPDarkNet(
-        depth_multiplier=DEPTH_MULTIPLIERS["s"],
-        width_multiplier=WIDTH_MULTIPLIERS["s"],
-        include_rescaling=include_rescaling,
-        include_top=include_top,
-        use_depthwise=use_depthwise,
-        classes=classes,
-        weights=weights,
-        input_shape=input_shape,
-        input_tensor=input_tensor,
-        pooling=pooling,
-        classifier_activation=classifier_activation,
-        name=name,
+    def __init__(
+        self,
+        blocks,
+        include_rescaling,
+        include_top,
+        num_classes=None,
+        weights=None,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        pooling=None,
+        classifier_activation="softmax",
+        name="DenseNet",
         **kwargs,
-    )
+    ):
+        if include_top and not num_classes:
+            raise ValueError(
+                "If `include_top` is True, you should specify `num_classes`. "
+                f"Received: num_classes={num_classes}"
+            )
+
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
+
+        x = inputs
+        if include_rescaling:
+            x = layers.Rescaling(1 / 255.0)(x)
+
+        x = layers.Conv2D(
+            64, 7, strides=2, use_bias=False, padding="same", name="conv1/conv"
+        )(x)
+        x = layers.BatchNormalization(
+            axis=BN_AXIS, epsilon=BN_EPSILON, name="conv1/bn"
+        )(x)
+        x = layers.Activation("relu", name="conv1/relu")(x)
+        x = layers.MaxPooling2D(3, strides=2, padding="same", name="pool1")(x)
+
+        x = apply_dense_block(x, blocks[0], name="conv2")
+        x = apply_transition_block(x, 0.5, name="pool2")
+        x = apply_dense_block(x, blocks[1], name="conv3")
+        x = apply_transition_block(x, 0.5, name="pool3")
+        x = apply_dense_block(x, blocks[2], name="conv4")
+        x = apply_transition_block(x, 0.5, name="pool4")
+        x = apply_dense_block(x, blocks[3], name="conv5")
+
+        x = layers.BatchNormalization(
+            axis=BN_AXIS, epsilon=BN_EPSILON, name="bn"
+        )(x)
+        x = layers.Activation("relu", name="relu")(x)
+
+        if include_top:
+            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+            x = layers.Dense(
+                num_classes,
+                activation=classifier_activation,
+                name="predictions",
+            )(x)
+        elif pooling == "avg":
+            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+        elif pooling == "max":
+            x = layers.GlobalMaxPooling2D(name="max_pool")(x)
+
+        # Create model.
+        super().__init__(inputs=inputs, outputs=x, **kwargs)
+
+        # All references to `self` below this line
+        if weights is not None:
+            self.load_weights(weights)
+
+        self.blocks = blocks
+        self.include_rescaling = include_rescaling
+        self.include_top = include_top
+        self.num_classes = num_classes
+        self.input_tensor = input_tensor
+        self.pooling = pooling
+        self.classifier_activation = classifier_activation
+
+    def get_config(self):
+        return {
+            "blocks": self.blocks,
+            "include_rescaling": self.include_rescaling,
+            "include_top": self.include_top,
+            # Remove batch dimension from `input_shape`
+            "input_shape": self.input_shape[1:],
+            "num_classes": self.num_classes,
+            "input_tensor": self.input_tensor,
+            "pooling": self.pooling,
+            "classifier_activation": self.classifier_activation,
+            "name": self.name,
+            "trainable": self.trainable,
+        }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
 
 
-def CSPDarkNetM(
+def DenseNet121(
     *,
     include_rescaling,
     include_top,
-    use_depthwise=False,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classifier_activation="softmax",
-    name="CSPDarkNetM",
+    name="DenseNet121",
     **kwargs,
 ):
-    return CSPDarkNet(
-        depth_multiplier=DEPTH_MULTIPLIERS["m"],
-        width_multiplier=WIDTH_MULTIPLIERS["m"],
+    return DenseNet(
+        blocks=MODEL_CONFIGS["DenseNet121"]["blocks"],
         include_rescaling=include_rescaling,
         include_top=include_top,
-        use_depthwise=use_depthwise,
-        classes=classes,
-        weights=weights,
+        num_classes=num_classes,
+        weights=parse_weights(weights, include_top, "densenet121"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classifier_activation=classifier_activation,
         name=name,
         **kwargs,
     )
 
 
-def CSPDarkNetL(
+def DenseNet169(
     *,
     include_rescaling,
     include_top,
-    use_depthwise=False,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classifier_activation="softmax",
-    name="CSPDarkNetL",
+    name="DenseNet169",
     **kwargs,
 ):
-    return CSPDarkNet(
-        depth_multiplier=DEPTH_MULTIPLIERS["l"],
-        width_multiplier=WIDTH_MULTIPLIERS["l"],
+    return DenseNet(
+        blocks=MODEL_CONFIGS["DenseNet169"]["blocks"],
         include_rescaling=include_rescaling,
         include_top=include_top,
-        use_depthwise=use_depthwise,
-        classes=classes,
-        weights=parse_weights(weights, include_top, "cspdarknetl"),
+        num_classes=num_classes,
+        weights=parse_weights(weights, include_top, "densenet169"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classifier_activation=classifier_activation,
         name=name,
         **kwargs,
     )
 
 
-def CSPDarkNetX(
+def DenseNet201(
     *,
     include_rescaling,
     include_top,
-    use_depthwise=False,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classifier_activation="softmax",
-    name="CSPDarkNetX",
+    name="DenseNet201",
     **kwargs,
 ):
-    return CSPDarkNet(
-        depth_multiplier=DEPTH_MULTIPLIERS["x"],
-        width_multiplier=WIDTH_MULTIPLIERS["x"],
+    return DenseNet(
+        blocks=MODEL_CONFIGS["DenseNet201"]["blocks"],
         include_rescaling=include_rescaling,
         include_top=include_top,
-        use_depthwise=use_depthwise,
-        classes=classes,
-        weights=weights,
+        num_classes=num_classes,
+        weights=parse_weights(weights, include_top, "densenet201"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classifier_activation=classifier_activation,
         name=name,
         **kwargs,
     )
 
 
-setattr(CSPDarkNetTiny, "__doc__", BASE_DOCSTRING.format(name="CSPDarkNetTiny"))
-setattr(CSPDarkNetS, "__doc__", BASE_DOCSTRING.format(name="CSPDarkNetS"))
-setattr(CSPDarkNetM, "__doc__", BASE_DOCSTRING.format(name="CSPDarkNetM"))
-setattr(CSPDarkNetL, "__doc__", BASE_DOCSTRING.format(name="CSPDarkNetL"))
-setattr(CSPDarkNetX, "__doc__", BASE_DOCSTRING.format(name="CSPDarkNetX"))
+setattr(DenseNet121, "__doc__", BASE_DOCSTRING.format(name="DenseNet121"))
+setattr(DenseNet169, "__doc__", BASE_DOCSTRING.format(name="DenseNet169"))
+setattr(DenseNet201, "__doc__", BASE_DOCSTRING.format(name="DenseNet201"))
```

## Comparing `keras_cv/models/csp_darknet_test.py` & `keras_cv/models/legacy/darknet_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,26 +11,25 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import csp_darknet
+from keras_cv.models.legacy import darknet
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
-    (csp_darknet.CSPDarkNetTiny, 384, {}),
-    (csp_darknet.CSPDarkNetS, 512, {}),
-    (csp_darknet.CSPDarkNetM, 768, {}),
+    (darknet.DarkNet21, 512, {}),
+    (darknet.DarkNet53, 512, {}),
 ]
 
 
-class CSPDarkNetTest(ModelsTest, tf.test.TestCase, parameterized.TestCase):
+class DarkNetTest(ModelsTest, tf.test.TestCase, parameterized.TestCase):
     @parameterized.parameters(*MODEL_LIST)
     def test_application_base(self, app, _, args):
         super()._test_application_base(app, _, args)
 
     @parameterized.parameters(*MODEL_LIST)
     def test_application_with_rescaling(self, app, last_dim, args):
         super()._test_application_with_rescaling(app, last_dim, args)
```

## Comparing `keras_cv/models/darknet.py` & `keras_cv/models/legacy/darknet.py`

 * *Files 22% similar despite different names*

```diff
@@ -18,243 +18,293 @@
     - [YoloV3 implementation](https://github.com/ultralytics/yolov3)
 """
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 
-from keras_cv.models import utils
-from keras_cv.models.__internal__.darknet_utils import DarknetConvBlock
-from keras_cv.models.__internal__.darknet_utils import ResidualBlocks
-from keras_cv.models.__internal__.darknet_utils import SpatialPyramidPoolingBottleneck
-from keras_cv.models.weights import parse_weights
+from keras_cv.models.backbones.csp_darknet.csp_darknet_utils import (
+    DarknetConvBlock,
+)
+from keras_cv.models.backbones.csp_darknet.csp_darknet_utils import (
+    ResidualBlocks,
+)
+from keras_cv.models.backbones.csp_darknet.csp_darknet_utils import (
+    SpatialPyramidPoolingBottleneck,
+)
+from keras_cv.models.legacy import utils
+from keras_cv.models.legacy.weights import parse_weights
 
-BASE_DOCSTRING = """Instantiates the {name} architecture.
+BASE_DOCSTRING = """Represents the {name} architecture.
 
     Although the {name} architecture is commonly used for detection tasks, it is
-    possible to extract the intermediate dark2 to dark5 layers from the model for
-    creating a feature pyramid Network.
+    possible to extract the intermediate dark2 to dark5 layers from the model
+    for creating a feature pyramid Network.
 
     Reference:
         - [YoloV3 Paper](https://arxiv.org/abs/1804.02767)
         - [YoloV3 implementation](https://github.com/ultralytics/yolov3)
+
     For transfer learning use cases, make sure to read the
-    [guide to transfer learning & fine-tuning](
-        https://keras.io/guides/transfer_learning/).
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: whether to include the fully-connected layer at the top of
-            the network.  If provided, `classes` must be provided.
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True.
+        include_rescaling: bool, whether to rescale the inputs. If set to
+            True, inputs will be passed through a `Rescaling(1/255.0)` layer.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, `num_classes` must be provided.
+        num_classes: integer, optional number of classes to classify images
+            into. Only to be specified if `include_top` is True.
         weights: one of `None` (random initialization), or a pretrained weight
             file path.
         input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+        input_tensor: optional Keras tensor (i.e., output of `layers.Input()`)
             to use as image input for the model.
         pooling: optional pooling mode for feature extraction when `include_top`
             is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
             - `avg` means that global average pooling will be applied to the
-                output of the last convolutional block, and thus the output of the
-                model will be a 2D tensor.
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
             - `max` means that global max pooling will be applied.
-        name: (Optional) name to pass to the model.  Defaults to "{name}".
+        name: string, optional name to pass to the model, defaults to "{name}".
+
     Returns:
         A `keras.Model` instance.
-"""
+"""  # noqa: E501
 
 
-def DarkNet(
-    blocks,
-    include_rescaling,
-    include_top,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name="DarkNet",
-    **kwargs,
-):
-    """Instantiates the DarkNet architecture.
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class DarkNet(keras.Model):
+
+    """Represents the DarkNet architecture.
 
     The DarkNet architecture is commonly used for detection tasks. It is
-    possible to extract the intermediate dark2 to dark5 layers from the model for
-    creating a feature pyramid Network.
+    possible to extract the intermediate dark2 to dark5 layers from the model
+    for creating a feature pyramid Network.
 
     Reference:
         - [YoloV3 Paper](https://arxiv.org/abs/1804.02767)
         - [YoloV3 implementation](https://github.com/ultralytics/yolov3)
     For transfer learning use cases, make sure to read the
-    [guide to transfer learning & fine-tuning](
-        https://keras.io/guides/transfer_learning/).
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
     Args:
-        blocks: numbers of building blocks from the layer dark2 to layer dark5.
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
+        blocks: integer, numbers of building blocks from the layer dark2 to
+            layer dark5.
+        include_rescaling: bool, whether to rescale the inputs. If set to True,
             inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: whether to include the fully-connected layer at the top of
-            the network.  If provided, `classes` must be provided.
-        classes: optional number of classes to classify imagesinto, only to be
-            specified if `include_top` is True.
-        weights: one of `None` (random initialization), or a pretrained weight
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, `num_classes` must be provided.
+        num_classes: integer, optional number of classes to classify images
+            into. Only to be specified if `include_top` is True.
+        weights: one of `None` (random initialization) or a pretrained weight
             file path.
         input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+        input_tensor: optional Keras tensor (i.e., output of `layers.Input()`)
             to use as image input for the model.
         pooling: optional pooling mode for feature extraction when `include_top`
             is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
             - `avg` means that global average pooling will be applied to the
-                output of the last convolutional block, and thus the output of the
-                model will be a 2D tensor.
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
             - `max` means that global max pooling will be applied.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
+        name: string, optional name to pass to the model, defaults to "DarkNet".
 
-        name: (Optional) name to pass to the model.  Defaults to "DarkNet".
     Returns:
         A `keras.Model` instance.
-    """
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either `None` or the path to the "
-            f"weights file to be loaded. Weights file not found at location: {weights}"
-        )
-
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, you should specify `classes`. Received: "
-            f"classes={classes}"
-        )
-
-    inputs = utils.parse_model_inputs(input_shape, input_tensor)
-
-    x = inputs
-    if include_rescaling:
-        x = layers.Rescaling(1 / 255.0)(x)
-
-    # stem
-    x = DarknetConvBlock(
-        filters=32, kernel_size=3, strides=1, activation="leaky_relu", name="stem_conv"
-    )(x)
-    x = ResidualBlocks(filters=64, num_blocks=1, name="stem_residual_block")(x)
+    """  # noqa: E501
 
-    # filters for the ResidualBlock outputs
-    filters = [128, 256, 512, 1024]
+    def __init__(
+        self,
+        blocks,
+        include_rescaling,
+        include_top,
+        num_classes=None,
+        weights=None,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        pooling=None,
+        classifier_activation="softmax",
+        name="DarkNet",
+        **kwargs,
+    ):
+        if weights and not tf.io.gfile.exists(weights):
+            raise ValueError(
+                "The `weights` argument should be either `None` or the path to "
+                "the weights file to be loaded. Weights file not found at "
+                f"location: {weights}"
+            )
+
+        if include_top and not num_classes:
+            raise ValueError(
+                "If `include_top` is True, you should specify `num_classes`. "
+                f"Received: num_classes={num_classes}"
+            )
+
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
+
+        x = inputs
+        if include_rescaling:
+            x = layers.Rescaling(1 / 255.0)(x)
+
+        # stem
+        x = DarknetConvBlock(
+            filters=32,
+            kernel_size=3,
+            strides=1,
+            activation="leaky_relu",
+            name="stem_conv",
+        )(x)
+        x = ResidualBlocks(
+            filters=64, num_blocks=1, name="stem_residual_block"
+        )(x)
 
-    # layer_num is used for naming the residual blocks (starts with dark2, hence 2)
-    layer_num = 2
+        # filters for the ResidualBlock outputs
+        filters = [128, 256, 512, 1024]
 
-    for filter, block in zip(filters, blocks):
-        x = ResidualBlocks(
-            filters=filter, num_blocks=block, name=f"dark{layer_num}_residual_block"
+        # layer_num is used for naming the residual blocks
+        # (starts with dark2, hence 2)
+        layer_num = 2
+
+        for filter, block in zip(filters, blocks):
+            x = ResidualBlocks(
+                filters=filter,
+                num_blocks=block,
+                name=f"dark{layer_num}_residual_block",
+            )(x)
+            layer_num += 1
+
+        # remaining dark5 layers
+        x = DarknetConvBlock(
+            filters=512,
+            kernel_size=1,
+            strides=1,
+            activation="leaky_relu",
+            name="dark5_conv1",
+        )(x)
+        x = DarknetConvBlock(
+            filters=1024,
+            kernel_size=3,
+            strides=1,
+            activation="leaky_relu",
+            name="dark5_conv2",
+        )(x)
+        x = SpatialPyramidPoolingBottleneck(
+            512, activation="leaky_relu", name="dark5_spp"
+        )(x)
+        x = DarknetConvBlock(
+            filters=1024,
+            kernel_size=3,
+            strides=1,
+            activation="leaky_relu",
+            name="dark5_conv3",
+        )(x)
+        x = DarknetConvBlock(
+            filters=512,
+            kernel_size=1,
+            strides=1,
+            activation="leaky_relu",
+            name="dark5_conv4",
         )(x)
-        layer_num += 1
 
-    # remaining dark5 layers
-    x = DarknetConvBlock(
-        filters=512,
-        kernel_size=1,
-        strides=1,
-        activation="leaky_relu",
-        name="dark5_conv1",
-    )(x)
-    x = DarknetConvBlock(
-        filters=1024,
-        kernel_size=3,
-        strides=1,
-        activation="leaky_relu",
-        name="dark5_conv2",
-    )(x)
-    x = SpatialPyramidPoolingBottleneck(512, activation="leaky_relu", name="dark5_spp")(
-        x
-    )
-    x = DarknetConvBlock(
-        filters=1024,
-        kernel_size=3,
-        strides=1,
-        activation="leaky_relu",
-        name="dark5_conv3",
-    )(x)
-    x = DarknetConvBlock(
-        filters=512,
-        kernel_size=1,
-        strides=1,
-        activation="leaky_relu",
-        name="dark5_conv4",
-    )(x)
-
-    if include_top:
-        x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        x = layers.Dense(classes, activation=classifier_activation, name="predictions")(
-            x
-        )
-    elif pooling == "avg":
-        x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-    elif pooling == "max":
-        x = layers.GlobalMaxPooling2D(name="max_pool")(x)
-
-    model = keras.Model(inputs, x, name=name, **kwargs)
-
-    if weights is not None:
-        model.load_weights(weights)
-    return model
+        if include_top:
+            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+            x = layers.Dense(
+                num_classes,
+                activation=classifier_activation,
+                name="predictions",
+            )(x)
+        elif pooling == "avg":
+            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+        elif pooling == "max":
+            x = layers.GlobalMaxPooling2D(name="max_pool")(x)
+
+        super().__init__(inputs=inputs, outputs=x, name=name, **kwargs)
+
+        if weights is not None:
+            self.load_weights(weights)
+
+        self.blocks = blocks
+        self.include_rescaling = include_rescaling
+        self.include_top = include_top
+        self.num_classes = num_classes
+        self.input_tensor = input_tensor
+        self.pooling = pooling
+        self.classifier_activation = classifier_activation
+
+    def get_config(self):
+        return {
+            "blocks": self.blocks,
+            "include_rescaling": self.include_rescaling,
+            "include_top": self.include_top,
+            "num_classes": self.num_classes,
+            "input_shape": self.input_shape[1:],
+            "input_tensor": self.input_tensor,
+            "pooling": self.pooling,
+            "classifier_activation": self.classifier_activation,
+            "name": self.name,
+            "trainable": self.trainable,
+        }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
 
 
 def DarkNet21(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     name="DarkNet21",
     **kwargs,
 ):
     return DarkNet(
         [1, 2, 2, 1],
         include_rescaling=include_rescaling,
         include_top=include_top,
-        classes=classes,
+        num_classes=num_classes,
         weights=parse_weights(weights, include_top, "darknet"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
         name=name,
         **kwargs,
     )
 
 
 def DarkNet53(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     name="DarkNet53",
     **kwargs,
 ):
     return DarkNet(
         [2, 8, 8, 4],
         include_rescaling=include_rescaling,
         include_top=include_top,
-        classes=classes,
+        num_classes=num_classes,
         weights=parse_weights(weights, include_top, "darknet53"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
         name=name,
         **kwargs,
     )
```

## Comparing `keras_cv/models/darknet_test.py` & `keras_cv/models/legacy/vgg16_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,25 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import darknet
+from keras_cv.models.legacy import vgg16
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
-    (darknet.DarkNet21, 512, {}),
-    (darknet.DarkNet53, 512, {}),
+    (vgg16.VGG16, 512, {}),
 ]
 
 
-class DarkNetTest(ModelsTest, tf.test.TestCase, parameterized.TestCase):
+class VGG16Test(ModelsTest, tf.test.TestCase, parameterized.TestCase):
     @parameterized.parameters(*MODEL_LIST)
     def test_application_base(self, app, _, args):
         super()._test_application_base(app, _, args)
 
     @parameterized.parameters(*MODEL_LIST)
     def test_application_with_rescaling(self, app, last_dim, args):
         super()._test_application_with_rescaling(app, last_dim, args)
```

## Comparing `keras_cv/models/densenet_test.py` & `keras_cv/models/legacy/densenet_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import densenet
+from keras_cv.models.legacy import densenet
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
     (densenet.DenseNet121, 1024, {}),
 ]
 """
```

## Comparing `keras_cv/models/efficientnet_lite.py` & `keras_cv/models/legacy/efficientnet_lite.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,27 +12,29 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 """EfficientNet Lite models for Keras.
 
 Reference:
-    - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](
-        https://arxiv.org/abs/1905.11946) (ICML 2019)
+    - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
+        (ICML 2019)
     - [Based on the original EfficientNet Lite's](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite)
-"""
+"""  # noqa: E501
+
 import copy
 import math
 
 import tensorflow as tf
 from keras import backend
 from keras import layers
+from tensorflow import keras
 
-from keras_cv.models import utils
-from keras_cv.models.weights import parse_weights
+from keras_cv.models.legacy import utils
+from keras_cv.models.legacy.weights import parse_weights
 
 DEFAULT_BLOCKS_ARGS = [
     {
         "kernel_size": 3,
         "repeats": 1,
         "filters_in": 32,
         "filters_out": 16,
@@ -112,73 +114,69 @@
         "distribution": "uniform",
     },
 }
 
 BASE_DOCSTRING = """Instantiates the {name} architecture.
 
     Reference:
-    - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](
-        https://arxiv.org/abs/1905.11946) (ICML 2019)
+    - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
+        (ICML 2019)
 
-    This function returns a Keras image classification model.
+    This function returns a Keras {name} model.
 
-    For image classification use cases, see
-    [this page for detailed examples](
-    https://keras.io/api/applications/#usage-examples-for-image-classification-models).
+    For image classification use cases, see [this page for detailed examples](https://keras.io/api/applications/#usage-examples-for-image-classification-models).
 
     For transfer learning use cases, make sure to read the
-    [guide to transfer learning & fine-tuning](
-    https://keras.io/guides/transfer_learning/).
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-                    inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: Whether to include the fully-connected
-            layer at the top of the network.
-        weights: One of `None` (random initialization),
-                or the path to the weights file to be loaded.
-        input_shape: Optional shape tuple.
-            It should have exactly 3 inputs channels.
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, `num_classes` must be provided.
+        num_classes: optional int, number of classes to classify images into
+            (only to be specified if `include_top` is `True`).
+        weights: one of `None` (random initialization), a pretrained weight file
+            path, or a reference to pre-trained weights (e.g.
+            'imagenet/classification')(see available pre-trained weights in
+            weights.py)
+        input_shape: optional shape tuple, defaults to (None, None, 3).
         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
             to use as image input for the model.
-        pooling: Optional pooling mode for feature extraction
-            when `include_top` is `False`. Defaults to None.
-            - `None` means that the output of the model will be
-                the 4D tensor output of the
-                last convolutional layer.
-            - `avg` means that global average pooling
-                will be applied to the output of the
-                last convolutional layer, and thus
-                the output of the model will be a 2D tensor.
-            - `max` means that global max pooling will
-                be applied.
-        classes: Optional number of classes to classify images
-            into, only to be specified if `include_top` is True, and
-            if no `weights` argument is specified. Defaults to None.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
-            Defaults to 'softmax'.
-            When loading pretrained weights, `classifier_activation` can only
-            be `None` or `"softmax"`.
+        pooling: optional pooling mode for feature extraction
+            when `include_top` is `False`.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
+            - `avg` means that global average pooling will be applied to the
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
+            - `max` means that global max pooling will be applied.
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
+        name: (Optional) name to pass to the model, defaults to "{name}".
 
     Returns:
         A `keras.Model` instance.
-"""
+"""  # noqa: E501
 
 BN_AXIS = 3
 
 
 def correct_pad(inputs, kernel_size):
     """Returns a tuple for zero-padding for 2D convolution with downsampling.
+
     Args:
-      inputs: Input tensor.
-      kernel_size: An integer or tuple/list of 2 integers.
+        inputs: Input tensor.
+        kernel_size: An integer or tuple/list of 2 integers.
+
     Returns:
-      A tuple.
+        A tuple.
     """
     img_dim = 1
     input_size = backend.int_shape(inputs)[img_dim : (img_dim + 2)]
     if isinstance(kernel_size, int):
         kernel_size = (kernel_size, kernel_size)
     if input_size[0] is None:
         adjust = (1, 1)
@@ -205,18 +203,19 @@
 
 
 def round_repeats(repeats, depth_coefficient):
     """Round number of repeats based on depth multiplier."""
     return int(math.ceil(depth_coefficient * repeats))
 
 
-def EfficientNetLiteBlock(
+def apply_efficient_net_lite_block(
+    inputs,
     activation="relu6",
     drop_rate=0.0,
-    name="",
+    name=None,
     filters_in=32,
     filters_out=16,
     kernel_size=3,
     strides=1,
     expand_ratio=1,
     id_skip=True,
 ):
@@ -233,97 +232,80 @@
         strides: integer, the stride of the convolution.
         expand_ratio: integer, scaling coefficient for the input filters.
         id_skip: boolean.
 
     Returns:
         output tensor for the block.
     """
+    if name is None:
+        name = f"block_{backend.get_uid('block_')}_"
 
-    def apply(inputs):
-        # Expansion phase
-        filters = filters_in * expand_ratio
-        if expand_ratio != 1:
-            x = layers.Conv2D(
-                filters,
-                1,
-                padding="same",
-                use_bias=False,
-                kernel_initializer=CONV_KERNEL_INITIALIZER,
-                name=name + "expand_conv",
-            )(inputs)
-            x = layers.BatchNormalization(axis=BN_AXIS, name=name + "expand_bn")(x)
-            x = layers.Activation(activation, name=name + "expand_activation")(x)
-        else:
-            x = inputs
-
-        # Depthwise Convolution
-        if strides == 2:
-            x = layers.ZeroPadding2D(
-                padding=correct_pad(x, kernel_size),
-                name=name + "dwconv_pad",
-            )(x)
-            conv_pad = "valid"
-        else:
-            conv_pad = "same"
-        x = layers.DepthwiseConv2D(
-            kernel_size,
-            strides=strides,
-            padding=conv_pad,
-            use_bias=False,
-            depthwise_initializer=CONV_KERNEL_INITIALIZER,
-            name=name + "dwconv",
-        )(x)
-        x = layers.BatchNormalization(axis=BN_AXIS, name=name + "bn")(x)
-        x = layers.Activation(activation, name=name + "activation")(x)
-
-        # Skip SE block
-        # Output phase
+    # Expansion phase
+    filters = filters_in * expand_ratio
+    if expand_ratio != 1:
         x = layers.Conv2D(
-            filters_out,
+            filters,
             1,
             padding="same",
             use_bias=False,
             kernel_initializer=CONV_KERNEL_INITIALIZER,
-            name=name + "project_conv",
+            name=name + "expand_conv",
+        )(inputs)
+        x = layers.BatchNormalization(axis=BN_AXIS, name=name + "expand_bn")(x)
+        x = layers.Activation(activation, name=name + "expand_activation")(x)
+    else:
+        x = inputs
+
+    # Depthwise Convolution
+    if strides == 2:
+        x = layers.ZeroPadding2D(
+            padding=correct_pad(x, kernel_size),
+            name=name + "dwconv_pad",
         )(x)
-        x = layers.BatchNormalization(axis=BN_AXIS, name=name + "project_bn")(x)
-        if id_skip and strides == 1 and filters_in == filters_out:
-            if drop_rate > 0:
-                x = layers.Dropout(
-                    drop_rate, noise_shape=(None, 1, 1, 1), name=name + "drop"
-                )(x)
-            x = layers.add([x, inputs], name=name + "add")
-        return x
+        conv_pad = "valid"
+    else:
+        conv_pad = "same"
+    x = layers.DepthwiseConv2D(
+        kernel_size,
+        strides=strides,
+        padding=conv_pad,
+        use_bias=False,
+        depthwise_initializer=CONV_KERNEL_INITIALIZER,
+        name=name + "dwconv",
+    )(x)
+    x = layers.BatchNormalization(axis=BN_AXIS, name=name + "bn")(x)
+    x = layers.Activation(activation, name=name + "activation")(x)
 
-    return apply
+    # Skip SE block
+    # Output phase
+    x = layers.Conv2D(
+        filters_out,
+        1,
+        padding="same",
+        use_bias=False,
+        kernel_initializer=CONV_KERNEL_INITIALIZER,
+        name=name + "project_conv",
+    )(x)
+    x = layers.BatchNormalization(axis=BN_AXIS, name=name + "project_bn")(x)
+    if id_skip and strides == 1 and filters_in == filters_out:
+        if drop_rate > 0:
+            x = layers.Dropout(
+                drop_rate, noise_shape=(None, 1, 1, 1), name=name + "drop"
+            )(x)
+        x = layers.add([x, inputs], name=name + "add")
+    return x
 
 
-def EfficientNetLite(
-    include_rescaling,
-    include_top,
-    width_coefficient,
-    depth_coefficient,
-    default_size,
-    dropout_rate=0.2,
-    drop_connect_rate=0.2,
-    depth_divisor=8,
-    activation="relu6",
-    blocks_args="default",
-    model_name="efficientnetlite",
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classes=None,
-    classifier_activation="softmax",
-):
-    """Instantiates the EfficientNetLite architecture using given scaling coefficients.
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class EfficientNetLite(keras.Model):
+    """Instantiates the EfficientNetLite architecture using given scaling
+    coefficients.
 
     Args:
-        include_rescaling: whether to Rescale the inputs. If set to True,
+        include_rescaling: whether to rescale the inputs. If set to True,
             inputs will be passed through a `Rescaling(1/255.0)` layer.
         include_top: whether to include the fully-connected
             layer at the top of the network.
         width_coefficient: float, scaling coefficient for network width.
         depth_coefficient: float, scaling coefficient for network depth.
         default_size: integer, default input image size.
         dropout_rate: float, dropout rate before final classifier layer.
@@ -345,301 +327,373 @@
                 last convolutional layer.
             - `avg` means that global average pooling
                 will be applied to the output of the
                 last convolutional layer, and thus
                 the output of the model will be a 2D tensor.
             - `max` means that global max pooling will
                 be applied.
-        classes: optional number of classes to classify images
+        num_classes: optional number of classes to classify images
             into, only to be specified if `include_top` is True, and
             if no `weights` argument is specified.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
 
         Returns:
             A `keras.Model` instance.
 
         Raises:
+            ValueError: if `blocks_args` is invalid.
             ValueError: in case of invalid argument for `weights`,
                 or invalid input shape.
-            ValueError: if `classifier_activation` is not `softmax` or `None` when
-                using a pretrained top layer.
+            ValueError: if `classifier_activation` is not `softmax` or `None`
+                when using a pretrained top layer.
     """
 
-    if blocks_args == "default":
-        blocks_args = DEFAULT_BLOCKS_ARGS
-
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either `None` or the path to the "
-            "weights file to be loaded. Weights file not found at location: {weights}"
-        )
-
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, you should specify `classes`. "
-            f"Received: classes={classes}"
-        )
-
-    if include_top and pooling:
-        raise ValueError(
-            f"`pooling` must be `None` when `include_top=True`."
-            f"Received pooling={pooling} and include_top={include_top}. "
-        )
-
-    img_input = utils.parse_model_inputs(input_shape, input_tensor)
-
-    # Build stem
-    x = img_input
-
-    if include_rescaling:
-        # Use common rescaling strategy across keras_cv
-        x = layers.Rescaling(1.0 / 255.0)(x)
-
-    x = layers.ZeroPadding2D(padding=correct_pad(x, 3), name="stem_conv_pad")(x)
-    x = layers.Conv2D(
-        32,
-        3,
-        strides=2,
-        padding="valid",
-        use_bias=False,
-        kernel_initializer=CONV_KERNEL_INITIALIZER,
-        name="stem_conv",
-    )(x)
-    x = layers.BatchNormalization(axis=BN_AXIS, name="stem_bn")(x)
-    x = layers.Activation(activation, name="stem_activation")(x)
-
-    # Build blocks
-    blocks_args = copy.deepcopy(blocks_args)
+    def __init__(
+        self,
+        include_rescaling,
+        include_top,
+        width_coefficient,
+        depth_coefficient,
+        default_size,
+        dropout_rate=0.2,
+        drop_connect_rate=0.2,
+        depth_divisor=8,
+        activation="relu6",
+        blocks_args=None,
+        weights=None,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        pooling=None,
+        num_classes=None,
+        classifier_activation="softmax",
+        **kwargs,
+    ):
+        if blocks_args is None:
+            blocks_args = DEFAULT_BLOCKS_ARGS
+        if not isinstance(blocks_args, list):
+            raise ValueError(
+                "The `blocks_args` argument should be either `None` or valid"
+                "list of dicts for building blocks. "
+                f"Received: blocks_args={blocks_args}"
+            )
+        intact_blocks_args = copy.deepcopy(blocks_args)  # for configs
+        blocks_args = copy.deepcopy(blocks_args)
 
-    b = 0
-    blocks = float(sum(args["repeats"] for args in blocks_args))
+        if weights and not tf.io.gfile.exists(weights):
+            raise ValueError(
+                "The `weights` argument should be either `None` or the path to "
+                "the weights file to be loaded. "
+                f"Weights file not found at location: {weights}"
+            )
 
-    for i, args in enumerate(blocks_args):
-        assert args["repeats"] > 0
-        # Update block input and output filters based on depth multiplier.
-        args["filters_in"] = round_filters(
-            filters=args["filters_in"],
-            width_coefficient=width_coefficient,
-            depth_divisor=depth_divisor,
-        )
-        args["filters_out"] = round_filters(
-            filters=args["filters_out"],
-            width_coefficient=width_coefficient,
-            depth_divisor=depth_divisor,
-        )
+        if include_top and not num_classes:
+            raise ValueError(
+                "If `include_top` is True, you should specify `num_classes`. "
+                f"Received: num_classes={num_classes}"
+            )
 
-        if i == 0 or i == (len(blocks_args) - 1):
-            repeats = args.pop("repeats")
-        else:
-            repeats = round_repeats(
-                repeats=args.pop("repeats"), depth_coefficient=depth_coefficient
+        if include_top and pooling:
+            raise ValueError(
+                f"`pooling` must be `None` when `include_top=True`."
+                f"Received pooling={pooling} and include_top={include_top}. "
             )
 
-        for j in range(repeats):
-            # The first block needs to take care of stride and filter size
-            # increase.
-            if j > 0:
-                args["strides"] = 1
-                args["filters_in"] = args["filters_out"]
-            x = EfficientNetLiteBlock(
-                activation=activation,
-                drop_rate=drop_connect_rate * b / blocks,
-                name="block{}{}_".format(i + 1, chr(j + 97)),
-                **args,
-            )(x)
+        img_input = utils.parse_model_inputs(input_shape, input_tensor)
 
-            b += 1
+        # Build stem
+        x = img_input
 
-    # Build top
-    x = layers.Conv2D(
-        1280,
-        1,
-        padding="same",
-        use_bias=False,
-        kernel_initializer=CONV_KERNEL_INITIALIZER,
-        name="top_conv",
-    )(x)
-    x = layers.BatchNormalization(axis=BN_AXIS, name="top_bn")(x)
-    x = layers.Activation(activation, name="top_activation")(x)
+        if include_rescaling:
+            # Use common rescaling strategy across keras_cv
+            x = layers.Rescaling(1.0 / 255.0)(x)
 
-    if include_top:
-        x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        if dropout_rate > 0:
-            x = layers.Dropout(dropout_rate, name="top_dropout")(x)
-        x = layers.Dense(
-            classes,
-            activation=classifier_activation,
-            kernel_initializer=DENSE_KERNEL_INITIALIZER,
-            name="predictions",
+        x = layers.ZeroPadding2D(
+            padding=correct_pad(x, 3), name="stem_conv_pad"
         )(x)
-    else:
-        if pooling == "avg":
-            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        elif pooling == "max":
-            x = layers.GlobalMaxPooling2D(name="max_pool")(x)
+        x = layers.Conv2D(
+            32,
+            3,
+            strides=2,
+            padding="valid",
+            use_bias=False,
+            kernel_initializer=CONV_KERNEL_INITIALIZER,
+            name="stem_conv",
+        )(x)
+        x = layers.BatchNormalization(axis=BN_AXIS, name="stem_bn")(x)
+        x = layers.Activation(activation, name="stem_activation")(x)
 
-    inputs = img_input
+        # Build blocks
+        b = 0
+        blocks = float(sum(args["repeats"] for args in blocks_args))
+
+        for i, args in enumerate(blocks_args):
+            assert args["repeats"] > 0
+            # Update block input and output filters based on depth multiplier.
+            args["filters_in"] = round_filters(
+                filters=args["filters_in"],
+                width_coefficient=width_coefficient,
+                depth_divisor=depth_divisor,
+            )
+            args["filters_out"] = round_filters(
+                filters=args["filters_out"],
+                width_coefficient=width_coefficient,
+                depth_divisor=depth_divisor,
+            )
 
-    # Create model.
-    model = tf.keras.Model(inputs, x, name=model_name)
+            if i == 0 or i == (len(blocks_args) - 1):
+                repeats = args.pop("repeats")
+            else:
+                repeats = round_repeats(
+                    repeats=args.pop("repeats"),
+                    depth_coefficient=depth_coefficient,
+                )
+
+            for j in range(repeats):
+                # The first block needs to take care of stride and filter size
+                # increase.
+                if j > 0:
+                    args["strides"] = 1
+                    args["filters_in"] = args["filters_out"]
+                x = apply_efficient_net_lite_block(
+                    x,
+                    activation=activation,
+                    drop_rate=drop_connect_rate * b / blocks,
+                    name="block{}{}_".format(i + 1, chr(j + 97)),
+                    **args,
+                )
 
-    # Load weights.
-    if weights is not None:
-        model.load_weights(weights)
+                b += 1
 
-    return model
+        # Build top
+        x = layers.Conv2D(
+            1280,
+            1,
+            padding="same",
+            use_bias=False,
+            kernel_initializer=CONV_KERNEL_INITIALIZER,
+            name="top_conv",
+        )(x)
+        x = layers.BatchNormalization(axis=BN_AXIS, name="top_bn")(x)
+        x = layers.Activation(activation, name="top_activation")(x)
+
+        if include_top:
+            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+            if dropout_rate > 0:
+                x = layers.Dropout(dropout_rate, name="top_dropout")(x)
+            x = layers.Dense(
+                num_classes,
+                activation=classifier_activation,
+                kernel_initializer=DENSE_KERNEL_INITIALIZER,
+                name="predictions",
+            )(x)
+        else:
+            if pooling == "avg":
+                x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+            elif pooling == "max":
+                x = layers.GlobalMaxPooling2D(name="max_pool")(x)
+
+        inputs = img_input
+
+        # Create model.
+        super().__init__(inputs=inputs, outputs=x, **kwargs)
+
+        # Load weights.
+        if weights is not None:
+            self.load_weights(weights)
+
+        self.include_rescaling = include_rescaling
+        self.include_top = include_top
+        self.width_coefficient = width_coefficient
+        self.depth_coefficient = depth_coefficient
+        self.default_size = default_size
+        self.dropout_rate = dropout_rate
+        self.drop_connect_rate = drop_connect_rate
+        self.depth_divisor = depth_divisor
+        self.activation = activation
+        self.blocks_args = intact_blocks_args
+        self.input_tensor = input_tensor
+        self.pooling = pooling
+        self.num_classes = num_classes
+        self.classifier_activation = classifier_activation
+
+    def get_config(self):
+        return {
+            "include_rescaling": self.include_rescaling,
+            "include_top": self.include_top,
+            "width_coefficient": self.width_coefficient,
+            "depth_coefficient": self.depth_coefficient,
+            "default_size": self.default_size,
+            "dropout_rate": self.dropout_rate,
+            "drop_connect_rate": self.drop_connect_rate,
+            "depth_divisor": self.depth_divisor,
+            "activation": self.activation,
+            "blocks_args": self.blocks_args,
+            # Remove batch dimension from `input_shape`
+            "input_shape": self.input_shape[1:],
+            "input_tensor": self.input_tensor,
+            "pooling": self.pooling,
+            "num_classes": self.num_classes,
+            "classifier_activation": self.classifier_activation,
+            "name": self.name,
+            "trainable": self.trainable,
+        }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
 
 
 def EfficientNetLiteB0(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     **kwargs,
 ):
     return EfficientNetLite(
         include_rescaling,
         include_top,
         width_coefficient=1.0,
         depth_coefficient=1.0,
         default_size=224,
         dropout_rate=0.2,
-        model_name="efficientnetliteb0",
+        name="efficientnetliteb0",
         weights=parse_weights(weights, include_top, "efficientnetliteb0"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetLiteB1(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     **kwargs,
 ):
     return EfficientNetLite(
         include_rescaling,
         include_top,
         width_coefficient=1.0,
         depth_coefficient=1.1,
         default_size=240,
         dropout_rate=0.2,
-        model_name="efficientnetliteb1",
+        name="efficientnetliteb1",
         weights=parse_weights(weights, include_top, "efficientnetliteb1"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetLiteB2(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     **kwargs,
 ):
     return EfficientNetLite(
         include_rescaling,
         include_top,
         width_coefficient=1.1,
         depth_coefficient=1.2,
         default_size=260,
         dropout_rate=0.3,
-        model_name="efficientnetliteb2",
+        name="efficientnetliteb2",
         weights=parse_weights(weights, include_top, "efficientnetliteb2"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetLiteB3(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     **kwargs,
 ):
     return EfficientNetLite(
         include_rescaling,
         include_top,
         width_coefficient=1.2,
         depth_coefficient=1.4,
         default_size=280,
         dropout_rate=0.3,
-        model_name="efficientnetliteb3",
+        name="efficientnetliteb3",
         weights=parse_weights(weights, include_top, "efficientnetliteb3"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetLiteB4(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
     **kwargs,
 ):
     return EfficientNetLite(
         include_rescaling,
         include_top,
         width_coefficient=1.4,
         depth_coefficient=1.8,
         default_size=300,
         dropout_rate=0.3,
-        model_name="efficientnetliteb4",
+        name="efficientnetliteb4",
         weights=parse_weights(weights, include_top, "efficientnetliteb4"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 EfficientNetLiteB0.__doc__ = BASE_DOCSTRING.format(name="EfficientNetLiteB0")
 EfficientNetLiteB1.__doc__ = BASE_DOCSTRING.format(name="EfficientNetLiteB1")
```

## Comparing `keras_cv/models/efficientnet_lite_test.py` & `keras_cv/models/legacy/efficientnet_lite_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import efficientnet_lite
+from keras_cv.models.legacy import efficientnet_lite
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
     (efficientnet_lite.EfficientNetLiteB0, 1280, {}),
 ]
 
@@ -29,15 +29,17 @@
 (efficientnet_lite.EfficientNetLiteB1, 1280, {}),
 (efficientnet_lite.EfficientNetLiteB2, 1280, {}),
 (efficientnet_lite.EfficientNetLiteB3, 1280, {}),
 (efficientnet_lite.EfficientNetLiteB4, 1280, {}),
 """
 
 
-class EfficientNetLiteTest(ModelsTest, tf.test.TestCase, parameterized.TestCase):
+class EfficientNetLiteTest(
+    ModelsTest, tf.test.TestCase, parameterized.TestCase
+):
     @parameterized.parameters(*MODEL_LIST)
     def test_application_base(self, app, _, args):
         super()._test_application_base(app, _, args)
 
     @parameterized.parameters(*MODEL_LIST)
     def test_application_with_rescaling(self, app, last_dim, args):
         super()._test_application_with_rescaling(app, last_dim, args)
```

## Comparing `keras_cv/models/efficientnet_v1.py` & `keras_cv/models/legacy/efficientnet_v1.py`

 * *Files 20% similar despite different names*

```diff
@@ -12,28 +12,29 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 """EfficientNet models for Keras.
 
 Reference:
-    - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](
-        https://arxiv.org/abs/1905.11946) (ICML 2019)
+    - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
+        (ICML 2019)
     - [Based on the original keras.applications EfficientNet](https://github.com/keras-team/keras/blob/master/keras/applications/efficientnet.py)
-"""
+"""  # noqa: E501
 
 import copy
 import math
 
 import tensorflow as tf
-from keras import backend
-from keras import layers
+from tensorflow import keras
+from tensorflow.keras import backend
+from tensorflow.keras import layers
 
-from keras_cv.models import utils
-from keras_cv.models.weights import parse_weights
+from keras_cv.models.legacy import utils
+from keras_cv.models.legacy.weights import parse_weights
 
 DEFAULT_BLOCKS_ARGS = [
     {
         "kernel_size": 3,
         "repeats": 1,
         "filters_in": 32,
         "filters_out": 16,
@@ -121,63 +122,56 @@
         "distribution": "uniform",
     },
 }
 
 BASE_DOCSTRING = """Instantiates the {name} architecture.
 
     Reference:
-    - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](
-        https://arxiv.org/abs/1905.11946) (ICML 2019)
+    - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
+        (ICML 2019)
 
-    This function returns a Keras image classification model.
+    This class represents a Keras image classification model.
 
     For image classification use cases, see
-    [this page for detailed examples](
-    https://keras.io/api/applications/#usage-examples-for-image-classification-models).
+    [this page for detailed examples](https://keras.io/api/applications/#usage-examples-for-image-classification-models).
 
     For transfer learning use cases, make sure to read the
-    [guide to transfer learning & fine-tuning](
-    https://keras.io/guides/transfer_learning/).
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-                    inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: Whether to include the fully-connected
-            layer at the top of the network.
-        weights: One of `None` (random initialization),
-                or the path to the weights file to be loaded.
-        input_shape: Optional shape tuple.
-            It should have exactly 3 inputs channels.
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
-        pooling: Optional pooling mode for feature extraction
-            when `include_top` is `False`. Defaults to None.
-            - `None` means that the output of the model will be
-                the 4D tensor output of the
-                last convolutional layer.
-            - `avg` means that global average pooling
-                will be applied to the output of the
-                last convolutional layer, and thus
-                the output of the model will be a 2D tensor.
-            - `max` means that global max pooling will
-                be applied.
-        classes: Optional number of classes to classify images
-            into, only to be specified if `include_top` is True, and
-            if no `weights` argument is specified. Defaults to None.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
-            Defaults to 'softmax'.
-            When loading pretrained weights, `classifier_activation` can only
-            be `None` or `"softmax"`.
+        include_rescaling: bool, whether to rescale the inputs. If set to
+            True, inputs will be passed through a `Rescaling(1/255.0)` layer.
+        include_top: bool, Whether to include the fully-connected layer at the
+            top of the network.
+        weights: One of `None` (random initialization), or the path to the
+            weights file to be loaded.
+        input_shape: tuple, Optional shape tuple. It should have exactly 3
+            inputs channels.
+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to
+            use as image input for the model.
+        pooling: Optional pooling mode for feature extraction when `include_top`
+            is `False`, defaults to None.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional layer.
+            - `avg` means that global average pooling will be applied to the
+                output of the last convolutional layer, and thus the output of
+                the model will be a 2D tensor.
+            - `max` means that global max pooling will be applied.
+        num_classes: int, Optional number of classes to classify images into,
+            only to be specified if `include_top` is True, and if no `weights`
+            argument is specified, defaults to None.
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer. Defaults to 'softmax'. When loading pretrained weights,
+            `classifier_activation` can only be `None` or `"softmax"`.
 
     Returns:
         A `keras.Model` instance.
-"""
-
+"""  # noqa: E501
 
 BN_AXIS = 3
 
 
 def correct_pad(inputs, kernel_size):
     """Returns a tuple for zero-padding for 2D convolution with downsampling.
     Args:
@@ -197,553 +191,741 @@
     correct = (kernel_size[0] // 2, kernel_size[1] // 2)
     return (
         (correct[0] - adjust[0], correct[0]),
         (correct[1] - adjust[1], correct[1]),
     )
 
 
-def EfficientNetBlock(
+def apply_conv_bn(
+    x,
+    conv_type,
+    filters,
+    kernel_size,
+    strides=1,
+    padding="same",
+    use_bias=False,
+    kernel_initializer=CONV_KERNEL_INITIALIZER,
+    bn_norm=True,
     activation="swish",
-    drop_rate=0.0,
     name="",
+):
+    """
+    Represents Convolutional Block with optional Batch Normalization layer and
+    activation layer
+
+    Args:
+        x: Tensor
+        conv_type: str, Type of Conv layer to be used in block.
+            - 'normal': The Conv2D layer will be used.
+            - 'depth': The DepthWiseConv2D layer will be used.
+        filters: int, The filter size of the Conv layer. It should be `None`
+            when `conv_type` is set as `depth`
+        kernel_size: int (or) tuple, The kernel size of the Conv layer.
+        strides: int (or) tuple, The stride value of Conv layer.
+        padding: str (or) callable, The type of padding for Conv layer.
+        use_bias: bool, Boolean to use bias for Conv layer.
+        kernel_initializer: dict (or) str (or) callable, The kernel initializer
+            for Conv layer.
+        bn_norm: bool, Boolean to add BatchNormalization layer after Conv layer.
+        activation: str (or) callable, Activation to be applied on the output at
+            the end.
+        name: str, name of the block
+
+    Returns:
+        tf.Tensor
+    """
+    if conv_type == "normal":
+        if filters is None or kernel_size is None:
+            raise ValueError(
+                "The filter size and kernel size should be set for Conv2D "
+                "layer."
+            )
+        x = layers.Conv2D(
+            filters,
+            kernel_size,
+            strides=strides,
+            padding=padding,
+            use_bias=use_bias,
+            kernel_initializer=kernel_initializer,
+            name=name + "_conv",
+        )(x)
+    elif conv_type == "depth":
+        if filters is not None:
+            raise ValueError(
+                "Filter size shouldn't be set for DepthWiseConv2D layer."
+            )
+        if kernel_size is None or strides is None:
+            raise ValueError(
+                "The kernel size and strides should be set for DepthWiseConv2D "
+                "layer."
+            )
+        x = layers.DepthwiseConv2D(
+            kernel_size,
+            strides=strides,
+            padding=padding,
+            use_bias=use_bias,
+            depthwise_initializer=kernel_initializer,
+            name=name + "_dwconv",
+        )(x)
+    else:
+        raise ValueError(
+            "The 'conv_type' parameter should be set either to 'normal' or "
+            "'depth'"
+        )
+
+    if bn_norm:
+        x = layers.BatchNormalization(axis=BN_AXIS, name=name + "_bn")(x)
+    if activation is not None:
+        x = layers.Activation(activation, name=name + "_activation")(x)
+
+    return x
+
+
+def apply_efficientnet_block(
+    inputs,
     filters_in=32,
     filters_out=16,
     kernel_size=3,
     strides=1,
+    activation="swish",
     expand_ratio=1,
     se_ratio=0.0,
     id_skip=True,
+    drop_rate=0.0,
+    name="",
 ):
     """An inverted residual block.
 
     Args:
-        inputs: input tensor.
-        activation: activation function.
-        drop_rate: float between 0 and 1, fraction of the input units to drop.
-        name: string, block label.
+        inputs: Tensor, The input tensor of the block
         filters_in: integer, the number of input filters.
         filters_out: integer, the number of output filters.
         kernel_size: integer, the dimension of the convolution window.
         strides: integer, the stride of the convolution.
+        activation: activation function.
         expand_ratio: integer, scaling coefficient for the input filters.
         se_ratio: float between 0 and 1, fraction to squeeze the input filters.
         id_skip: boolean.
+        drop_rate: float between 0 and 1, fraction of the input units to drop.
+        name: string, block label.
 
     Returns:
-        output tensor for the block.
+        tf.Tensor
     """
-
-    # Expansion phase
-    def apply(inputs):
-        filters = filters_in * expand_ratio
-        if expand_ratio != 1:
-            x = layers.Conv2D(
-                filters,
-                1,
-                padding="same",
-                use_bias=False,
-                kernel_initializer=CONV_KERNEL_INITIALIZER,
-                name=name + "expand_conv",
-            )(inputs)
-            x = layers.BatchNormalization(axis=BN_AXIS, name=name + "expand_bn")(x)
-            x = layers.Activation(activation, name=name + "expand_activation")(x)
-        else:
-            x = inputs
-
-        # Depthwise Convolution
-        if strides == 2:
-            x = layers.ZeroPadding2D(
-                padding=correct_pad(x, kernel_size),
-                name=name + "dwconv_pad",
-            )(x)
-            conv_pad = "valid"
-        else:
-            conv_pad = "same"
-        x = layers.DepthwiseConv2D(
-            kernel_size,
-            strides=strides,
-            padding=conv_pad,
+    filters = filters_in * expand_ratio
+    if expand_ratio != 1:
+        x = apply_conv_bn(
+            x=inputs,
+            conv_type="normal",
+            filters=filters,
+            kernel_size=1,
+            padding="same",
             use_bias=False,
-            depthwise_initializer=CONV_KERNEL_INITIALIZER,
-            name=name + "dwconv",
+            kernel_initializer=CONV_KERNEL_INITIALIZER,
+            bn_norm=True,
+            activation=activation,
+            name=name + "_expand",
+        )
+    else:
+        x = inputs
+
+    # Depthwise Convolution
+    if strides == 2:
+        x = layers.ZeroPadding2D(
+            padding=correct_pad(x, kernel_size),
+            name=name + "_dwconv_pad",
         )(x)
-        x = layers.BatchNormalization(axis=BN_AXIS, name=name + "bn")(x)
-        x = layers.Activation(activation, name=name + "activation")(x)
+        conv_pad = "valid"
+    else:
+        conv_pad = "same"
 
-        # Squeeze and Excitation phase
-        if 0 < se_ratio <= 1:
-            filters_se = max(1, int(filters_in * se_ratio))
-            se = layers.GlobalAveragePooling2D(name=name + "se_squeeze")(x)
-            if BN_AXIS == 1:
-                se_shape = (filters, 1, 1)
-            else:
-                se_shape = (1, 1, filters)
-            se = layers.Reshape(se_shape, name=name + "se_reshape")(se)
-            se = layers.Conv2D(
-                filters_se,
-                1,
-                padding="same",
-                activation=activation,
-                kernel_initializer=CONV_KERNEL_INITIALIZER,
-                name=name + "se_reduce",
-            )(se)
-            se = layers.Conv2D(
-                filters,
-                1,
-                padding="same",
-                activation="sigmoid",
-                kernel_initializer=CONV_KERNEL_INITIALIZER,
-                name=name + "se_expand",
-            )(se)
-            x = layers.multiply([x, se], name=name + "se_excite")
+    x = apply_conv_bn(
+        x=x,
+        conv_type="depth",
+        filters=None,
+        kernel_size=kernel_size,
+        strides=strides,
+        padding=conv_pad,
+        use_bias=False,
+        kernel_initializer=CONV_KERNEL_INITIALIZER,
+        bn_norm=True,
+        activation=activation,
+        name=name,
+    )
 
-        # Output phase
-        x = layers.Conv2D(
-            filters_out,
+    # Squeeze and Excitation phase
+    if 0 < se_ratio <= 1:
+        filters_se = max(1, int(filters_in * se_ratio))
+        se = layers.GlobalAveragePooling2D(name=name + "_se_squeeze")(x)
+        if BN_AXIS == 1:
+            se_shape = (filters, 1, 1)
+        else:
+            se_shape = (1, 1, filters)
+        se = layers.Reshape(se_shape, name=name + "_se_reshape")(se)
+        se = layers.Conv2D(
+            filters_se,
             1,
             padding="same",
-            use_bias=False,
+            activation=activation,
             kernel_initializer=CONV_KERNEL_INITIALIZER,
-            name=name + "project_conv",
-        )(x)
-        x = layers.BatchNormalization(axis=BN_AXIS, name=name + "project_bn")(x)
-        if id_skip and strides == 1 and filters_in == filters_out:
-            if drop_rate > 0:
-                x = layers.Dropout(
-                    drop_rate, noise_shape=(None, 1, 1, 1), name=name + "drop"
-                )(x)
-            x = layers.add([x, inputs], name=name + "add")
-        return x
+            name=name + "_se_reduce",
+        )(se)
+        se = layers.Conv2D(
+            filters,
+            1,
+            padding="same",
+            activation="sigmoid",
+            kernel_initializer=CONV_KERNEL_INITIALIZER,
+            name=name + "_se_expand",
+        )(se)
+        x = layers.multiply([x, se], name=name + "_se_excite")
+
+    # Output phase
+    x = apply_conv_bn(
+        x=x,
+        conv_type="normal",
+        filters=filters_out,
+        kernel_size=1,
+        padding="same",
+        use_bias=False,
+        kernel_initializer=CONV_KERNEL_INITIALIZER,
+        bn_norm=True,
+        activation=None,
+        name=name + "_project",
+    )
 
-    return apply
+    if id_skip and strides == 1 and filters_in == filters_out:
+        if drop_rate > 0:
+            x = layers.Dropout(
+                drop_rate,
+                noise_shape=(None, 1, 1, 1),
+                name=name + "_drop",
+            )(x)
+        x = layers.add([x, inputs], name=name + "_add")
 
+    return x
 
-def EfficientNet(
-    include_rescaling,
-    include_top,
-    width_coefficient,
-    depth_coefficient,
-    default_size,
-    dropout_rate=0.2,
-    drop_connect_rate=0.2,
-    depth_divisor=8,
-    activation="swish",
-    blocks_args="default",
-    model_name="efficientnet",
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classes=None,
-    classifier_activation="softmax",
-):
-    """Instantiates the EfficientNet architecture using given scaling coefficients.
 
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class EfficientNet(keras.Model):
+    """This class represents a Keras EfficientNet architecture.
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-                inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: whether to include the fully-connected
-            layer at the top of the network.
+        include_rescaling: bool, whether to rescale the inputs. If set to
+            True, inputs will be passed through a `Rescaling(1/255.0)` layer.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network.
         width_coefficient: float, scaling coefficient for network width.
         depth_coefficient: float, scaling coefficient for network depth.
         default_size: integer, default input image size.
         dropout_rate: float, dropout rate before final classifier layer.
         drop_connect_rate: float, dropout rate at skip connections.
         depth_divisor: integer, a unit of network width.
         activation: activation function.
         blocks_args: list of dicts, parameters to construct block modules.
         model_name: string, model name.
-        weights: one of `None` (random initialization),
-            or the path to the weights file to be loaded.
-        input_shape: optional shape tuple,
-            It should have exactly 3 inputs channels.
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
-        pooling: optional pooling mode for feature extraction
-            when `include_top` is `False`.
-            - `None` means that the output of the model will be
-                the 4D tensor output of the
-                last convolutional layer.
-            - `avg` means that global average pooling
-                will be applied to the output of the
-                last convolutional layer, and thus
-                the output of the model will be a 2D tensor.
-            - `max` means that global max pooling will
-                be applied.
-        classes: optional number of classes to classify images
-            into, only to be specified if `include_top` is True, and
-            if no `weights` argument is specified.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
-
+        weights: one of `None` (random initialization), or the path to the
+            weights file to be loaded.
+        input_shape: optional shape tuple, it should have exactly 3 input
+            channels.
+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to
+            use as image input for the model.
+        pooling: optional pooling mode for feature extraction when `include_top`
+            is `False`.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional layer.
+            - `avg` means that global average pooling will be applied to the
+                output of the last convolutional layer, and thus the output of
+                the model will be a 2D tensor.
+            - `max` means that global max pooling will be applied.
+        num_classes: optional number of classes to classify images into,
+            only to be specified if `include_top` is True, and if no `weights`
+            argument is specified.
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
     Returns:
       A `keras.Model` instance.
-
     Raises:
-      ValueError: in case of invalid argument for `weights`,
-        or invalid input shape.
+      ValueError: in case of invalid argument for `weights`, or invalid input
+        shape.
       ValueError: if `classifier_activation` is not `softmax` or `None` when
         using a pretrained top layer.
     """
-    if blocks_args == "default":
-        blocks_args = DEFAULT_BLOCKS_ARGS
 
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either `None` or the path to the "
-            "weights file to be loaded. Weights file not found at location: {weights}"
-        )
+    def __init__(
+        self,
+        include_rescaling,
+        include_top,
+        width_coefficient,
+        depth_coefficient,
+        default_size,
+        dropout_rate=0.2,
+        drop_connect_rate=0.2,
+        depth_divisor=8,
+        activation="swish",
+        blocks_args="default",
+        model_name="efficientnet",
+        weights=None,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        pooling=None,
+        num_classes=None,
+        classifier_activation="softmax",
+        **kwargs,
+    ):
+        blocks_args_type = blocks_args
 
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, you should specify `classes`. "
-            f"Received: classes={classes}"
-        )
+        if blocks_args == "default":
+            blocks_args = DEFAULT_BLOCKS_ARGS
 
-    if include_top and pooling:
-        raise ValueError(
-            f"`pooling` must be `None` when `include_top=True`."
-            f"Received pooling={pooling} and include_top={include_top}. "
+        if weights and not tf.io.gfile.exists(weights):
+            raise ValueError(
+                "The `weights` argument should be either `None` or the path to "
+                "the weights file to be loaded. Weights file not found at "
+                f"location: {weights}"
+            )
+
+        if include_top and not num_classes:
+            raise ValueError(
+                "If `include_top` is True, you should specify `num_classes`. "
+                f"Received: num_classes={num_classes}"
+            )
+
+        if include_top and pooling:
+            raise ValueError(
+                f"`pooling` must be `None` when `include_top=True`."
+                f"Received pooling={pooling} and include_top={include_top}. "
+            )
+
+        img_input = utils.parse_model_inputs(input_shape, input_tensor)
+
+        # Build stem
+        x = img_input
+
+        if include_rescaling:
+            # Use common rescaling strategy across keras_cv
+            x = layers.Rescaling(1.0 / 255.0)(x)
+
+        x = layers.ZeroPadding2D(
+            padding=correct_pad(x, 3), name="stem_conv_pad"
+        )(x)
+
+        x = apply_conv_bn(
+            x=x,
+            conv_type="normal",
+            filters=EfficientNet.round_filters(
+                32, width_coefficient, depth_divisor
+            ),
+            kernel_size=3,
+            strides=2,
+            padding="valid",
+            use_bias=False,
+            kernel_initializer=CONV_KERNEL_INITIALIZER,
+            bn_norm=True,
+            activation=activation,
+            name="stem",
         )
 
-    img_input = utils.parse_model_inputs(input_shape, input_tensor)
+        # Build blocks
+        blocks_args = copy.deepcopy(blocks_args)
+
+        b = 0
+        blocks = float(
+            sum(
+                EfficientNet.round_repeats(args["repeats"], depth_coefficient)
+                for args in blocks_args
+            )
+        )
+        for i, args in enumerate(blocks_args):
+            assert args["repeats"] > 0
+            # Update block input and output filters based on depth multiplier.
+            args["filters_in"] = EfficientNet.round_filters(
+                args["filters_in"], width_coefficient, depth_divisor
+            )
+            args["filters_out"] = EfficientNet.round_filters(
+                args["filters_out"], width_coefficient, depth_divisor
+            )
+
+            for j in range(
+                EfficientNet.round_repeats(
+                    args.pop("repeats"), depth_coefficient
+                )
+            ):
+                # The first block needs to take care of stride and filter size
+                # increase.
+                if j > 0:
+                    args["strides"] = 1
+                    args["filters_in"] = args["filters_out"]
+                x = apply_efficientnet_block(
+                    inputs=x,
+                    activation=activation,
+                    drop_rate=drop_connect_rate * b / blocks,
+                    name="block{}{}".format(i + 1, chr(j + 97)),
+                    **args,
+                )
+                b += 1
+
+        # Build top
+        x = apply_conv_bn(
+            x=x,
+            conv_type="normal",
+            filters=self.round_filters(1280, width_coefficient, depth_divisor),
+            kernel_size=1,
+            padding="same",
+            use_bias=False,
+            kernel_initializer=CONV_KERNEL_INITIALIZER,
+            bn_norm=True,
+            activation=activation,
+            name="top",
+        )
 
-    def round_filters(filters, divisor=depth_divisor):
-        """Round number of filters based on depth multiplier."""
+        if include_top:
+            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+            if dropout_rate > 0:
+                x = layers.Dropout(dropout_rate, name="top_dropout")(x)
+            x = layers.Dense(
+                num_classes,
+                activation=classifier_activation,
+                kernel_initializer=DENSE_KERNEL_INITIALIZER,
+                name="predictions",
+            )(x)
+        else:
+            if pooling == "avg":
+                x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
+            elif pooling == "max":
+                x = layers.GlobalMaxPooling2D(name="max_pool")(x)
+
+        inputs = img_input
+
+        # Create model.
+        super().__init__(inputs=inputs, outputs=x, name=model_name, **kwargs)
+
+        # Load weights.
+        if weights is not None:
+            self.load_weights(weights)
+
+        self.include_rescaling = include_rescaling
+        self.include_top = include_top
+        self.width_coefficient = width_coefficient
+        self.depth_coefficient = depth_coefficient
+        self.default_size = default_size
+        self.dropout_rate = dropout_rate
+        self.drop_connect_rate = drop_connect_rate
+        self.depth_divisor = depth_divisor
+        self.activation = activation
+        self.blocks_args = blocks_args_type
+        self.input_tensor = input_tensor
+        self.pooling = pooling
+        self.num_classes = num_classes
+        self.classifier_activation = classifier_activation
+
+    @staticmethod
+    def round_filters(filters, width_coefficient, divisor):
+        """Round number of filters based on depth multiplier.
+        Args:
+            filters: int, number of filters for Conv layer
+            width_coefficient: float, denotes the scaling coefficient of network
+                width
+            divisor: int, a unit of network width
+
+        Returns:
+            int, new rounded filters value for Conv layer
+        """
         filters *= width_coefficient
-        new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)
+        new_filters = max(
+            divisor, int(filters + divisor / 2) // divisor * divisor
+        )
         # Make sure that round down does not go down by more than 10%.
         if new_filters < 0.9 * filters:
             new_filters += divisor
         return int(new_filters)
 
-    def round_repeats(repeats):
-        """Round number of repeats based on depth multiplier."""
+    @staticmethod
+    def round_repeats(repeats, depth_coefficient):
+        """Round number of repeats based on depth multiplier.
+        Args:
+            repeats: int, number of repeats of efficientnet block
+            depth_coefficient: float, denotes the scaling coefficient of network
+                depth
+
+        Returns:
+            int, rounded repeats
+        """
         return int(math.ceil(depth_coefficient * repeats))
 
-    # Build stem
-    x = img_input
-
-    if include_rescaling:
-        # Use common rescaling strategy across keras_cv
-        x = layers.Rescaling(1.0 / 255.0)(x)
-
-    x = layers.ZeroPadding2D(padding=correct_pad(x, 3), name="stem_conv_pad")(x)
-    x = layers.Conv2D(
-        round_filters(32),
-        3,
-        strides=2,
-        padding="valid",
-        use_bias=False,
-        kernel_initializer=CONV_KERNEL_INITIALIZER,
-        name="stem_conv",
-    )(x)
-    x = layers.BatchNormalization(axis=BN_AXIS, name="stem_bn")(x)
-    x = layers.Activation(activation, name="stem_activation")(x)
-
-    # Build blocks
-    blocks_args = copy.deepcopy(blocks_args)
-
-    b = 0
-    blocks = float(sum(round_repeats(args["repeats"]) for args in blocks_args))
-    for i, args in enumerate(blocks_args):
-        assert args["repeats"] > 0
-        # Update block input and output filters based on depth multiplier.
-        args["filters_in"] = round_filters(args["filters_in"])
-        args["filters_out"] = round_filters(args["filters_out"])
-
-        for j in range(round_repeats(args.pop("repeats"))):
-            # The first block needs to take care of stride and filter size
-            # increase.
-            if j > 0:
-                args["strides"] = 1
-                args["filters_in"] = args["filters_out"]
-            x = EfficientNetBlock(
-                activation,
-                drop_connect_rate * b / blocks,
-                name="block{}{}_".format(i + 1, chr(j + 97)),
-                **args,
-            )(x)
-            b += 1
-
-    # Build top
-    x = layers.Conv2D(
-        round_filters(1280),
-        1,
-        padding="same",
-        use_bias=False,
-        kernel_initializer=CONV_KERNEL_INITIALIZER,
-        name="top_conv",
-    )(x)
-    x = layers.BatchNormalization(axis=BN_AXIS, name="top_bn")(x)
-    x = layers.Activation(activation, name="top_activation")(x)
-    if include_top:
-        x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        if dropout_rate > 0:
-            x = layers.Dropout(dropout_rate, name="top_dropout")(x)
-        x = layers.Dense(
-            classes,
-            activation=classifier_activation,
-            kernel_initializer=DENSE_KERNEL_INITIALIZER,
-            name="predictions",
-        )(x)
-    else:
-        if pooling == "avg":
-            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        elif pooling == "max":
-            x = layers.GlobalMaxPooling2D(name="max_pool")(x)
-
-    inputs = img_input
-
-    # Create model.
-    model = tf.keras.Model(inputs, x, name=model_name)
-
-    # Load weights.
-    if weights is not None:
-        model.load_weights(weights)
-
-    return model
+    def get_config(self):
+        return {
+            "include_rescaling": self.include_rescaling,
+            "include_top": self.include_top,
+            "width_coefficient": self.width_coefficient,
+            "depth_coefficient": self.depth_coefficient,
+            "default_size": self.default_size,
+            "dropout_rate": self.dropout_rate,
+            "drop_connect_rate": self.drop_connect_rate,
+            "depth_divisor": self.depth_divisor,
+            "activation": self.activation,
+            "blocks_args": self.blocks_args,
+            "input_tensor": self.input_tensor,
+            "input_shape": self.input_shape[1:],
+            "model_name": self.name,
+            "pooling": self.pooling,
+            "num_classes": self.num_classes,
+            "classifier_activation": self.classifier_activation,
+            "trainable": self.trainable,
+        }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
 
 
 def EfficientNetB0(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
+    name="efficientnetb0",
     **kwargs,
 ):
     return EfficientNet(
         include_rescaling,
         include_top,
         width_coefficient=1.0,
         depth_coefficient=1.0,
         default_size=224,
         dropout_rate=0.2,
-        model_name="efficientnetb0",
+        model_name=name,
         weights=parse_weights(weights, include_top, "efficientnetb0"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetB1(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
+    name="efficientnetb1",
     **kwargs,
 ):
     return EfficientNet(
         include_rescaling,
         include_top,
         width_coefficient=1.0,
         depth_coefficient=1.1,
         default_size=240,
         dropout_rate=0.2,
-        model_name="efficientnetb1",
+        model_name=name,
         weights=parse_weights(weights, include_top, "efficientnetb1"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetB2(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
+    name="efficientnetb2",
     **kwargs,
 ):
     return EfficientNet(
         include_rescaling,
         include_top,
         width_coefficient=1.1,
         depth_coefficient=1.2,
         default_size=260,
         dropout_rate=0.3,
-        model_name="efficientnetb2",
+        model_name=name,
         weights=parse_weights(weights, include_top, "efficientnetb2"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetB3(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
+    name="efficientnetb3",
     **kwargs,
 ):
     return EfficientNet(
         include_rescaling,
         include_top,
         width_coefficient=1.2,
         depth_coefficient=1.4,
         default_size=300,
         dropout_rate=0.3,
-        model_name="efficientnetb3",
+        model_name=name,
         weights=parse_weights(weights, include_top, "efficientnetb3"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetB4(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
+    name="efficientnetb4",
     **kwargs,
 ):
     return EfficientNet(
         include_rescaling,
         include_top,
         width_coefficient=1.4,
         depth_coefficient=1.8,
         default_size=380,
         dropout_rate=0.4,
-        model_name="efficientnetb4",
+        model_name=name,
         weights=parse_weights(weights, include_top, "efficientnetb4"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetB5(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
+    name="efficientnetb5",
     **kwargs,
 ):
     return EfficientNet(
         include_rescaling,
         include_top,
         width_coefficient=1.6,
         depth_coefficient=2.2,
         default_size=456,
         dropout_rate=0.4,
-        model_name="efficientnetb5",
+        model_name=name,
         weights=parse_weights(weights, include_top, "efficientnetb5"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetB6(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
+    name="efficientnetb6",
     **kwargs,
 ):
     return EfficientNet(
         include_rescaling,
         include_top,
         width_coefficient=1.8,
         depth_coefficient=2.6,
         default_size=528,
         dropout_rate=0.5,
-        model_name="efficientnetb6",
+        model_name=name,
         weights=parse_weights(weights, include_top, "efficientnetb6"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def EfficientNetB7(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
     classifier_activation="softmax",
+    name="efficientnetb7",
     **kwargs,
 ):
     return EfficientNet(
         include_rescaling,
         include_top,
         width_coefficient=2.0,
         depth_coefficient=3.1,
         default_size=600,
         dropout_rate=0.5,
-        model_name="efficientnetb7",
+        model_name=name,
         weights=parse_weights(weights, include_top, "efficientnetb7"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 EfficientNetB0.__doc__ = BASE_DOCSTRING.format(name="EfficientNetB0")
 EfficientNetB1.__doc__ = BASE_DOCSTRING.format(name="EfficientNetB1")
```

## Comparing `keras_cv/models/efficientnet_v1_test.py` & `keras_cv/models/legacy/efficientnet_v1_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import efficientnet_v1
+from keras_cv.models.legacy import efficientnet_v1
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
     (efficientnet_v1.EfficientNetB0, 1280, {}),
 ]
```

## Comparing `keras_cv/models/efficientnet_v2_test.py` & `keras_cv/models/legacy/vgg19_test.py`

 * *Files 26% similar despite different names*

```diff
@@ -11,35 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import efficientnet_v2
+from keras_cv.models.legacy import vgg19
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
-    (efficientnet_v2.EfficientNetV2B0, 1280, {}),
+    (vgg19.VGG19, 512, {}),
 ]
 
-"""
-Below are other configurations that we omit from our CI but that can/should
-be tested manually when making changes to this model.
-(efficientnet_v2.EfficientNetV2B1, 1280, {}),
-(efficientnet_v2.EfficientNetV2B2, 1408, {}),
-(efficientnet_v2.EfficientNetV2B3, 1536, {}),
-(efficientnet_v2.EfficientNetV2S, 1280, {}),
-(efficientnet_v2.EfficientNetV2M, 1280, {}),
-(efficientnet_v2.EfficientNetV2L, 1280, {}),
-"""
 
-
-class EfficientNetV2Test(ModelsTest, tf.test.TestCase, parameterized.TestCase):
+class VGG19Test(ModelsTest, tf.test.TestCase, parameterized.TestCase):
     @parameterized.parameters(*MODEL_LIST)
     def test_application_base(self, app, _, args):
         super()._test_application_base(app, _, args)
 
     @parameterized.parameters(*MODEL_LIST)
     def test_application_with_rescaling(self, app, last_dim, args):
         super()._test_application_with_rescaling(app, last_dim, args)
```

## Comparing `keras_cv/models/mlp_mixer.py` & `keras_cv/models/legacy/mlp_mixer.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,22 +12,22 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """MLP Mixer models for KerasCV.
 
 Reference:
   - [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)
-"""
+"""  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import backend
 from tensorflow.keras import layers
 
-from keras_cv.models import utils
+from keras_cv.models.legacy import utils
 
 MODEL_CONFIGS = {
     "MLPMixerB16": {
         "patch_size": 16,
         "num_blocks": 12,
         "hidden_dim": 768,
         "tokens_mlp_dim": 384,
@@ -46,140 +46,131 @@
         "hidden_dim": 1024,
         "tokens_mlp_dim": 512,
         "channels_mlp_dim": 4096,
     },
 }
 
 BASE_DOCSTRING = """Instantiates the {name} architecture.
+
     Reference:
         - [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)
-    This function returns a Keras {name} model.
+
+    This class represents a Keras {name} model.
 
     For transfer learning use cases, make sure to read the [guide to transfer
         learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: whether to include the fully-connected layer at the top of the
-            network.  If provided, classes must be provided.
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True.
+        include_rescaling: bool, whether to rescale the inputs. If set to
+            True, inputs will be passed through a `Rescaling(1/255.0)` layer.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, num_classes must be provided.
+        num_classes: integer, optional number of classes to classify images
+            into. Only to be specified if `include_top` is True.
         weights: one of `None` (random initialization), a pretrained weight file
-            path, or a reference to pre-trained weights (e.g. 'imagenet/classification')
-            (see available pre-trained weights in weights.py)
+            path, or a reference to pre-trained weights (e.g.
+            'imagenet/classification')(see available pre-trained weights in
+            weights.py)
         input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+        input_tensor: optional Keras tensor (i.e., output of `layers.Input()`)
             to use as image input for the model.
         pooling: optional pooling mode for feature extraction
             when `include_top` is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
-            - `avg` means that global average pooling will be applied to the output
-                of the last convolutional block, and thus the output of the model will
-                be a 2D tensor.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
+            - `avg` means that global average pooling will be applied to the
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
             - `max` means that global max pooling will be applied.
-        name: (Optional) name to pass to the model.  Defaults to "{name}".
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
+        name: string, optional name to pass to the model, defaults to "{name}".
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
+
     Returns:
       A `keras.Model` instance.
-"""
+"""  # noqa: E501
 
 
-def MLPBlock(mlp_dim, name=None):
+def apply_mlp_block(x, mlp_dim, name=None):
     """An MLP block consisting of two linear layers with GELU activation in
     between.
 
     Args:
+      x: input tensor.
       mlp_dim: integer, the number of units to be present in the first layer.
       name: string, block label.
 
     Returns:
-      a function that takes an input Tensor representing an MLP block.
+      the updated input tensor.
     """
     if name is None:
         name = f"mlp_block_{backend.get_uid('mlp_block')}"
 
-    def apply(x):
-        y = layers.Dense(mlp_dim, name=f"{name}_dense_1")(x)
-        y = layers.Activation("gelu", name=f"{name}_gelu")(y)
-        return layers.Dense(x.shape[-1], name=f"{name}_dense_2")(y)
+    y = layers.Dense(mlp_dim, name=f"{name}_dense_1")(x)
+    y = layers.Activation("gelu", name=f"{name}_gelu")(y)
+    return layers.Dense(x.shape[-1], name=f"{name}_dense_2")(y)
 
-    return apply
 
-
-def MixerBlock(tokens_mlp_dim, channels_mlp_dim, name=None):
+def apply_mixer_block(x, tokens_mlp_dim, channels_mlp_dim, name=None):
     """A mixer block.
 
     Args:
+      x: input tensor.
       tokens_mlp_dim: integer, number of units to be present in the MLP block
         dealing with tokens.
       channels_mlp_dim: integer, number of units to be present in the MLP block
         dealing with channels.
       name: string, block label.
 
     Returns:
-      a function that takes an input Tensor representing an MLP block.
+      the updated input tensor.
     """
     if name is None:
         name = f"mixer_block_{backend.get_uid('mlp_block')}"
 
-    def apply(x):
-        y = layers.LayerNormalization()(x)
-        y = layers.Permute((2, 1))(y)
+    y = layers.LayerNormalization()(x)
+    y = layers.Permute((2, 1))(y)
 
-        y = MLPBlock(tokens_mlp_dim, name=f"{name}_token_mixing")(y)
-        y = layers.Permute((2, 1))(y)
-        x = layers.Add()([x, y])
+    y = apply_mlp_block(y, tokens_mlp_dim, name=f"{name}_token_mixing")
+    y = layers.Permute((2, 1))(y)
+    x = layers.Add()([x, y])
 
-        y = layers.LayerNormalization()(x)
-        y = MLPBlock(channels_mlp_dim, name=f"{name}_channel_mixing")(y)
-        return layers.Add()([x, y])
+    y = layers.LayerNormalization()(x)
+    y = apply_mlp_block(y, channels_mlp_dim, name=f"{name}_channel_mixing")
+    return layers.Add()([x, y])
 
-    return apply
 
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class MLPMixer(keras.Model):
 
-def MLPMixer(
-    input_shape,
-    patch_size,
-    num_blocks,
-    hidden_dim,
-    tokens_mlp_dim,
-    channels_mlp_dim,
-    include_rescaling,
-    include_top,
-    classes=None,
-    input_tensor=None,
-    weights=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name=None,
-    **kwargs,
-):
     """Instantiates the MLP Mixer architecture.
 
     Args:
       input_shape: tuple denoting the input shape, (224, 224, 3) for example.
       patch_size: integer denoting the size of the patches to be extracted
         from the inputs (16 for extracting 16x16 patches for example).
-      num_blocks: number of mixer blocks.
-      hidden_dim: dimension to which the patches will be linearly projected.
-      tokens_mlp_dim: dimension of the MLP block responsible for tokens.
-      channels_mlp_dim: dimension of the MLP block responsible for channels.
-      include_rescaling: whether or not to Rescale the inputs.
-        If set to True, inputs will be passed through a
-        `Rescaling(1/255.0)` layer.
-      include_top: whether to include the fully-connected
-        layer at the top of the network.  If provided, classes must be provided.
-      classes: optional number of classes to classify images
-        into, only to be specified if `include_top` is True.
-      weights: one of `None` (random initialization), or a pretrained
+      num_blocks: integer, number of mixer blocks.
+      hidden_dim: integer, dimension to which the patches will be linearly
+        projected.
+      tokens_mlp_dim: integer, dimension of the MLP block responsible for
+        tokens.
+      channels_mlp_dim: integer, dimension of the MLP block responsible for
+        channels.
+      include_rescaling: whether to rescale the inputs. If set to True,
+        inputs will be passed through a `Rescaling(1/255.0)` layer.
+      include_top: bool, whether to include the fully-connected
+        layer at the top of the network. If provided, num_classes must be
+        provided.
+      num_classes: integer, optional number of classes to classify images
+        into. Only to be specified if `include_top` is True.
+      weights: one of `None` (random initialization) or a pretrained
         weight file path.
-      input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+      input_tensor: optional Keras tensor (i.e., output of `layers.Input()`)
         to use as image input for the model.
       pooling: optional pooling mode for feature extraction
         when `include_top` is `False`.
         - `None` means that the output of the model will be
             the 4D tensor output of the
             last convolutional block.
         - `avg` means that global average pooling
@@ -189,172 +180,230 @@
         - `max` means that global max pooling will
             be applied.
       classifier_activation: A `str` or callable. The activation function to use
         on the "top" layer. Ignored unless `include_top=True`. Set
         `classifier_activation=None` to return the logits of the "top" layer.
         When loading pretrained weights, `classifier_activation` can only
         be `None` or `"softmax"`.
-      name: (Optional) name to pass to the model.  Defaults to "DenseNet".
+      name: string, optional name to pass to the model, defaults to "MLPMixer".
 
     Returns:
       A `keras.Model` instance.
     """
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either "
-            "`None` or the path to the weights file to be loaded. "
-            f"Weights file not found at location: {weights}"
-        )
-
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, "
-            "you should specify `classes`. "
-            f"Received: classes={classes}"
-        )
-
-    if not isinstance(input_shape, tuple):
-        raise ValueError("`input_shape` needs to be tuple.")
-
-    if len(input_shape) != 3:
-        raise ValueError(
-            "`input_shape` needs to contain dimensions for three"
-            " axes: height, width, and channel ((224, 224, 3) for example)."
-        )
-
-    if input_shape[0] != input_shape[1]:
-        raise ValueError("Non-uniform resolutions are not supported.")
-
-    if input_shape[0] % patch_size != 0:
-        raise ValueError("Input resolution should be divisible by the patch size.")
-
-    inputs = utils.parse_model_inputs(input_shape, input_tensor)
-
-    x = inputs
-    if include_rescaling:
-        x = layers.Rescaling(1 / 255.0)(x)
-
-    x = layers.Conv2D(
-        filters=hidden_dim,
-        kernel_size=(patch_size, patch_size),
-        strides=(patch_size, patch_size),
-        padding="valid",
-        name="patchify_and_projection",
-    )(x)
-    x = layers.Reshape((x.shape[1] * x.shape[2], x.shape[3]))(x)
-
-    for i in range(num_blocks):
-        x = MixerBlock(tokens_mlp_dim, channels_mlp_dim, name=f"mixer_block_{i}")(x)
-
-    x = layers.LayerNormalization()(x)
-
-    if include_top:
-        x = layers.GlobalAveragePooling1D(name="avg_pool")(x)
-        x = layers.Dense(classes, activation=classifier_activation, name="predictions")(
-            x
-        )
-
-    elif pooling == "avg":
-        x = layers.GlobalAveragePooling1D(name="avg_pool")(x)
-    elif pooling == "max":
-        x = layers.GlobalMaxPooling1D(name="max_pool")(x)
-
-    model = keras.Model(inputs, x, name=name, **kwargs)
-
-    if weights is not None:
-        model.load_weights(weights)
-    return model
+
+    def __init__(
+        self,
+        input_shape,
+        patch_size,
+        num_blocks,
+        hidden_dim,
+        tokens_mlp_dim,
+        channels_mlp_dim,
+        include_rescaling,
+        include_top,
+        num_classes=None,
+        input_tensor=None,
+        weights=None,
+        pooling=None,
+        classifier_activation="softmax",
+        name="MLPMixer",
+        **kwargs,
+    ):
+        if weights and not tf.io.gfile.exists(weights):
+            raise ValueError(
+                "The `weights` argument should be either "
+                "`None` or the path to the weights file to be loaded. "
+                f"Weights file not found at location: {weights}"
+            )
+
+        if include_top and not num_classes:
+            raise ValueError(
+                "If `include_top` is True, "
+                "you should specify `num_classes`. "
+                f"Received: num_classes={num_classes}"
+            )
+
+        if not isinstance(input_shape, tuple):
+            raise ValueError("`input_shape` needs to be tuple.")
+
+        if len(input_shape) != 3:
+            raise ValueError(
+                "`input_shape` needs to contain dimensions for three"
+                " axes: height, width, and channel ((224, 224, 3) for example)."
+            )
+
+        if input_shape[0] != input_shape[1]:
+            raise ValueError("Non-uniform resolutions are not supported.")
+
+        if input_shape[0] % patch_size != 0:
+            raise ValueError(
+                "Input resolution should be divisible by the patch size."
+            )
+
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
+
+        x = inputs
+        if include_rescaling:
+            x = layers.Rescaling(1 / 255.0)(x)
+
+        x = layers.Conv2D(
+            filters=hidden_dim,
+            kernel_size=(patch_size, patch_size),
+            strides=(patch_size, patch_size),
+            padding="valid",
+            name="patchify_and_projection",
+        )(x)
+        x = layers.Reshape((x.shape[1] * x.shape[2], x.shape[3]))(x)
+
+        for i in range(num_blocks):
+            x = apply_mixer_block(
+                x, tokens_mlp_dim, channels_mlp_dim, name=f"mixer_block_{i}"
+            )
+
+        x = layers.LayerNormalization()(x)
+
+        if include_top:
+            x = layers.GlobalAveragePooling1D(name="avg_pool")(x)
+            x = layers.Dense(
+                num_classes,
+                activation=classifier_activation,
+                name="predictions",
+            )(x)
+
+        elif pooling == "avg":
+            x = layers.GlobalAveragePooling1D(name="avg_pool")(x)
+        elif pooling == "max":
+            x = layers.GlobalMaxPooling1D(name="max_pool")(x)
+
+        super().__init__(inputs=inputs, outputs=x, name=name, **kwargs)
+
+        if weights is not None:
+            self.load_weights(weights)
+
+        self.patch_size = patch_size
+        self.num_blocks = num_blocks
+        self.hidden_dim = hidden_dim
+        self.tokens_mlp_dim = tokens_mlp_dim
+        self.channels_mlp_dim = channels_mlp_dim
+        self.include_rescaling = include_rescaling
+        self.include_top = include_top
+        self.num_classes = num_classes
+        self.input_tensor = input_tensor
+        self.pooling = pooling
+        self.classifier_activation = classifier_activation
+
+    def get_config(self):
+        return {
+            "input_shape": self.input_shape[1:],
+            "patch_size": self.patch_size,
+            "num_blocks": self.num_blocks,
+            "hidden_dim": self.hidden_dim,
+            "tokens_mlp_dim": self.tokens_mlp_dim,
+            "channels_mlp_dim": self.channels_mlp_dim,
+            "include_rescaling": self.include_rescaling,
+            "include_top": self.include_top,
+            "num_classes": self.num_classes,
+            "input_tensor": self.input_tensor,
+            "pooling": self.pooling,
+            "classifier_activation": self.classifier_activation,
+            "name": self.name,
+            "trainable": self.trainable,
+        }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
 
 
 def MLPMixerB16(
     input_shape,
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     input_tensor=None,
     weights=None,
     pooling=None,
-    name="mlp_mixer_b16",
+    name="MLPMixerB16",
     **kwargs,
 ):
     """Instantiates the MLPMixerB16 architecture."""
 
     return MLPMixer(
         input_shape=input_shape,
         patch_size=MODEL_CONFIGS["MLPMixerB16"]["patch_size"],
         num_blocks=MODEL_CONFIGS["MLPMixerB16"]["num_blocks"],
         hidden_dim=MODEL_CONFIGS["MLPMixerB16"]["hidden_dim"],
         tokens_mlp_dim=MODEL_CONFIGS["MLPMixerB16"]["tokens_mlp_dim"],
         channels_mlp_dim=MODEL_CONFIGS["MLPMixerB16"]["channels_mlp_dim"],
         include_rescaling=include_rescaling,
         include_top=include_top,
-        classes=classes,
+        num_classes=num_classes,
         input_tensor=input_tensor,
         weights=weights,
         pooling=pooling,
         name=name,
         **kwargs,
     )
 
 
 def MLPMixerB32(
     input_shape,
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     input_tensor=None,
     weights=None,
     pooling=None,
-    name="mlp_mixer_b32",
+    name="MLPMixerB32",
     **kwargs,
 ):
     """Instantiates the MLPMixerB32 architecture."""
     return MLPMixer(
         input_shape=input_shape,
         patch_size=MODEL_CONFIGS["MLPMixerB32"]["patch_size"],
         num_blocks=MODEL_CONFIGS["MLPMixerB32"]["num_blocks"],
         hidden_dim=MODEL_CONFIGS["MLPMixerB32"]["hidden_dim"],
         tokens_mlp_dim=MODEL_CONFIGS["MLPMixerB32"]["tokens_mlp_dim"],
         channels_mlp_dim=MODEL_CONFIGS["MLPMixerB32"]["channels_mlp_dim"],
         include_rescaling=include_rescaling,
         include_top=include_top,
-        classes=classes,
+        num_classes=num_classes,
         input_tensor=input_tensor,
         weights=weights,
         pooling=pooling,
         name=name,
         **kwargs,
     )
 
 
 def MLPMixerL16(
     input_shape,
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     input_tensor=None,
     weights=None,
     pooling=None,
-    name="mlp_mixer_l16",
+    name="MLPMixerL16",
     **kwargs,
 ):
     """Instantiates the MLPMixerL16 architecture."""
     return MLPMixer(
         input_shape=input_shape,
         patch_size=MODEL_CONFIGS["MLPMixerL16"]["patch_size"],
         num_blocks=MODEL_CONFIGS["MLPMixerL16"]["num_blocks"],
         hidden_dim=MODEL_CONFIGS["MLPMixerL16"]["hidden_dim"],
         tokens_mlp_dim=MODEL_CONFIGS["MLPMixerL16"]["tokens_mlp_dim"],
         channels_mlp_dim=MODEL_CONFIGS["MLPMixerL16"]["channels_mlp_dim"],
         include_rescaling=include_rescaling,
         include_top=include_top,
-        classes=classes,
+        num_classes=num_classes,
         input_tensor=input_tensor,
         weights=weights,
         pooling=pooling,
         name=name,
         **kwargs,
     )
```

## Comparing `keras_cv/models/mlp_mixer_test.py` & `keras_cv/models/legacy/mlp_mixer_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import mlp_mixer
+from keras_cv.models.legacy import mlp_mixer
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
     (
         mlp_mixer.MLPMixerB16,
         768,
```

## Comparing `keras_cv/models/mobilenet_v3.py` & `keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py`

 * *Files 19% similar despite different names*

```diff
@@ -7,550 +7,613 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+"""ResNet models for Keras.
+Reference:
+  - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
+  - [Based on the original keras.applications ResNet](https://github.com/keras-team/keras/blob/master/keras/applications/resnet_v2.py)
+"""  # noqa: E501
 
-"""MobileNet v3 models for KerasCV.
+import copy
 
-References:
-    - [Searching for MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf) (ICCV 2019)
-    - [Based on the original keras.applications MobileNetv3](https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet_v3.py)
-"""
-
-import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import backend
 from tensorflow.keras import layers
-from tensorflow.keras.utils import custom_object_scope
 
-from keras_cv import layers as cv_layers
 from keras_cv.models import utils
-
-channel_axis = -1
-
-BASE_DOCSTRING = """Instantiates the {name} architecture.
-
-    References:
-        - [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)
-        - [Based on the Original keras.applications MobileNetv3](https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet_v3.py)
-
-    This function returns a Keras {name} model.
-
-    For transfer learning use cases, make sure to read the [guide to transfer
-        learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+from keras_cv.models.backbones.backbone import Backbone
+from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone_presets import (
+    backbone_presets,
+)
+from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone_presets import (
+    backbone_presets_with_weights,
+)
+from keras_cv.utils.python_utils import classproperty
+
+BN_AXIS = 3
+BN_EPSILON = 1.001e-5
+
+
+def apply_basic_block(
+    x,
+    filters,
+    kernel_size=3,
+    stride=1,
+    dilation=1,
+    conv_shortcut=False,
+    name=None,
+):
+    """A basic residual block (v2).
 
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(scale=1 / 255)`
-            layer, defaults to True.
-        include_top: whether to include the fully-connected layer at the top of the
-            network.  If provided, `classes` must be provided.
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True, and if no `weights` argument is
-            specified.
-        weights: one of `None` (random initialization), or a pretrained weight file
-            path.
-        input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
-        pooling: optional pooling mode for feature extraction
-            when `include_top` is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
-            - `avg` means that global average pooling will be applied to the output
-                of the last convolutional block, and thus the output of the model will
-                be a 2D tensor.
-            - `max` means that global max pooling will be applied.
-        alpha: controls the width of the network. This is known as the
-            depth multiplier in the MobileNetV3 paper, but the name is kept for
-            consistency with MobileNetV1 in Keras.
-            - If `alpha` < 1.0, proportionally decreases the number
-                of filters in each layer.
-            - If `alpha` > 1.0, proportionally increases the number
-                of filters in each layer.
-            - If `alpha` = 1, default number of filters from the paper
-                are used at each layer.
-        minimalistic: in addition to large and small models this module also
-            contains so-called minimalistic models, these models have the same
-            per-layer dimensions characteristic as MobilenetV3 however, they don't
-            utilize any of the advanced blocks (squeeze-and-excite units, hard-swish,
-            and 5x5 convolutions). While these models are less efficient on CPU, they
-            are much more performant on GPU/DSP.
-        dropout_rate: a float between 0 and 1 denoting the fraction of input units to
-            drop, defaults to 0.2.
-        classifier_activation: the activation function to use, defaults to softmax.
-        name: (Optional) name to pass to the model. Defaults to "{name}".
+        x: input tensor.
+        filters: int, filters of the basic layer.
+        kernel_size: int, kernel size of the bottleneck layer, defaults to 3.
+        stride: int, stride of the first layer, defaults to 1.
+        dilation: int, the dilation rate to use for dilated convolution.
+            Defaults to 1.
+        conv_shortcut: bool, uses convolution shortcut if `True`. If `False`
+            (default), uses identity or pooling shortcut, based on stride.
+        name: string, optional prefix for the layer names used in the block.
 
     Returns:
-        A `keras.Model` instance.
-"""
-
+      Output tensor for the residual block.
+    """
 
-def depth(x, divisor=8, min_value=None):
-    """Ensure that all layers have a channel number that is divisble by the `divisor`.
+    if name is None:
+        name = f"v2_basic_block_{backend.get_uid('v2_basic_block')}"
 
-    Args:
-        x: input value.
-        divisor: integer, the value by which a channel number should be divisble,
-            defaults to 8.
-        min_value: float, minimum value for the new tensor.
+    use_preactivation = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_use_preactivation_bn"
+    )(x)
 
-    Returns:
-        the updated value of the input.
-    """
+    use_preactivation = layers.Activation(
+        "relu", name=name + "_use_preactivation_relu"
+    )(use_preactivation)
+
+    s = stride if dilation == 1 else 1
+    if conv_shortcut:
+        shortcut = layers.Conv2D(filters, 1, strides=s, name=name + "_0_conv")(
+            use_preactivation
+        )
+    else:
+        shortcut = (
+            layers.MaxPooling2D(
+                1, strides=stride, name=name + "_0_max_pooling"
+            )(x)
+            if s > 1
+            else x
+        )
 
-    if min_value is None:
-        min_value = divisor
+    x = layers.Conv2D(
+        filters,
+        kernel_size,
+        padding="SAME",
+        strides=1,
+        use_bias=False,
+        name=name + "_1_conv",
+    )(use_preactivation)
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_1_bn"
+    )(x)
+    x = layers.Activation("relu", name=name + "_1_relu")(x)
 
-    new_x = max(min_value, int(x + divisor / 2) // divisor * divisor)
+    x = layers.Conv2D(
+        filters,
+        kernel_size,
+        strides=s,
+        padding="same",
+        dilation_rate=dilation,
+        use_bias=False,
+        name=name + "_2_conv",
+    )(x)
 
-    # make sure that round down does not go down by more than 10%.
-    if new_x < 0.9 * x:
-        new_x += divisor
-    return new_x
+    x = layers.Add(name=name + "_out")([shortcut, x])
+    return x
 
 
-def HardSigmoid(name=None):
-    """The Hard Sigmoid function.
+def apply_block(
+    x,
+    filters,
+    kernel_size=3,
+    stride=1,
+    dilation=1,
+    conv_shortcut=False,
+    name=None,
+):
+    """A residual block (v2).
 
     Args:
-        name: string, layer label.
+        x: input tensor.
+        filters: int, filters of the basic layer.
+        kernel_size: int, kernel size of the bottleneck layer, defaults to 3.
+        stride: int, stride of the first layer, defaults to 1.
+        dilation: int, the dilation rate to use for dilated convolution.
+            Defaults to 1.
+        conv_shortcut: bool, uses convolution shortcut if `True`. If `False`
+            (default), uses identity or pooling shortcut, based on stride.
+        name: string, optional prefix for the layer names used in the block.
 
     Returns:
-        a function that takes an input Tensor representing a HardSigmoid layer.
+      Output tensor for the residual block.
     """
     if name is None:
-        name = f"hard_sigmoid_{backend.get_uid('hard_sigmoid')}"
+        name = f"v2_block_{backend.get_uid('v2_block')}"
 
-    activation = layers.ReLU(6.0)
+    use_preactivation = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_use_preactivation_bn"
+    )(x)
 
-    def apply(x):
-        return activation(x + 3.0) * (1.0 / 6.0)
+    use_preactivation = layers.Activation(
+        "relu", name=name + "_use_preactivation_relu"
+    )(use_preactivation)
+
+    s = stride if dilation == 1 else 1
+    if conv_shortcut:
+        shortcut = layers.Conv2D(
+            4 * filters,
+            1,
+            strides=s,
+            name=name + "_0_conv",
+        )(use_preactivation)
+    else:
+        shortcut = (
+            layers.MaxPooling2D(
+                1, strides=stride, name=name + "_0_max_pooling"
+            )(x)
+            if s > 1
+            else x
+        )
 
-    return apply
+    x = layers.Conv2D(
+        filters, 1, strides=1, use_bias=False, name=name + "_1_conv"
+    )(use_preactivation)
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_1_bn"
+    )(x)
+    x = layers.Activation("relu", name=name + "_1_relu")(x)
 
+    x = layers.Conv2D(
+        filters,
+        kernel_size,
+        strides=s,
+        use_bias=False,
+        padding="same",
+        dilation_rate=dilation,
+        name=name + "_2_conv",
+    )(x)
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_2_bn"
+    )(x)
+    x = layers.Activation("relu", name=name + "_2_relu")(x)
 
-def HardSwish(name=None):
-    """The Hard Swish function.
+    x = layers.Conv2D(4 * filters, 1, name=name + "_3_conv")(x)
+    x = layers.Add(name=name + "_out")([shortcut, x])
+    return x
+
+
+def apply_stack(
+    x,
+    filters,
+    blocks,
+    stride=2,
+    dilations=1,
+    name=None,
+    block_type="block",
+    first_shortcut=True,
+):
+    """A set of stacked blocks.
 
     Args:
-        name: string, layer label.
+        x: input tensor.
+        filters: int, filters of the layer in a block.
+        blocks: int, blocks in the stacked blocks.
+        stride: int, stride of the first layer in the first block, defaults
+            to 2.
+        dilations: int, the dilation rate to use for dilated convolution.
+            Defaults to 1.
+        name: string, optional prefix for the layer names used in the block.
+        block_type: string, one of "basic_block" or "block". The block type to
+            stack. Use "basic_block" for ResNet18 and ResNet34.
+        first_shortcut: bool. Use convolution shortcut if `True` (default),
+            otherwise uses identity or pooling shortcut, based on stride.
 
     Returns:
-        a function that takes an input Tensor representing a HardSwish layer.
+        Output tensor for the stacked blocks.
     """
-    if name is None:
-        name = f"hard_swish_{backend.get_uid('hard_swish')}"
 
-    hard_sigmoid = HardSigmoid()
-    multiply_layer = layers.Multiply()
+    if name is None:
+        name = "v2_stack"
 
-    def apply(x):
-        return multiply_layer([x, hard_sigmoid(x)])
+    if block_type == "basic_block":
+        block_fn = apply_basic_block
+    elif block_type == "block":
+        block_fn = apply_block
+    else:
+        raise ValueError(
+            """`block_type` must be either "basic_block" or "block". """
+            f"Received block_type={block_type}."
+        )
 
-    return apply
+    x = block_fn(
+        x, filters, conv_shortcut=first_shortcut, name=name + "_block1"
+    )
+    for i in range(2, blocks):
+        x = block_fn(
+            x, filters, dilation=dilations, name=name + "_block" + str(i)
+        )
+    x = block_fn(
+        x,
+        filters,
+        stride=stride,
+        dilation=dilations,
+        name=name + "_block" + str(blocks),
+    )
+    return x
 
 
-def InvertedResBlock(
-    expansion, filters, kernel_size, stride, se_ratio, activation, block_id, name=None
-):
-    """An Inverted Residual Block.
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class ResNetV2Backbone(Backbone):
+    """Instantiates the ResNetV2 architecture.
 
-    Args:
-        expansion: integer, the expansion ratio, multiplied with infilters to get the
-            minimum value passed to depth.
-        filters: integer, number of filters for convolution layer.
-        kernel_size: integer, the kernel size for DpethWise Convolutions.
-        stride: integer, the stride length for DpethWise Convolutions.
-        se_ratio: float, ratio for bottleneck filters. Number of bottleneck
-            filters = filters * se_ratio.
-        activation: the activation layer to use.
-        block_id: integer, a unique identification if you want to use expanded
-            convolutions.
-        name: string, layer label.
+    Reference:
+        - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
 
-    Returns:
-        a function that takes an input Tensor representing a InvertedResBlock.
-    """
-    if name is None:
-        name = f"inverted_res_block_{backend.get_uid('inverted_res_block')}"
+    The difference in Resnet and ResNetV2 rests in the structure of their
+    individual building blocks. In ResNetV2, the batch normalization and
+    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
+    the batch normalization and ReLU activation are applied after the
+    convolution layers.
 
-    def apply(x):
-        shortcut = x
-        prefix = "expanded_conv/"
-        infilters = backend.int_shape(x)[channel_axis]
-
-        if block_id:
-            prefix = f"expanded_conv_{block_id}"
-
-            x = layers.Conv2D(
-                depth(infilters * expansion),
-                kernel_size=1,
-                padding="same",
-                use_bias=False,
-                name=prefix + "expand",
-            )(x)
-            x = layers.BatchNormalization(
-                axis=channel_axis,
-                epsilon=1e-3,
-                momentum=0.999,
-                name=prefix + "expand/BatchNorm",
-            )(x)
-            x = activation(x)
+    For transfer learning use cases, make sure to read the
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
-        x = layers.DepthwiseConv2D(
-            kernel_size,
-            strides=stride,
-            padding="same" if stride == 1 else "valid",
-            use_bias=False,
-            name=prefix + "depthwise",
-        )(x)
-        x = layers.BatchNormalization(
-            axis=channel_axis,
-            epsilon=1e-3,
-            momentum=0.999,
-            name=prefix + "depthwise/BatchNorm",
-        )(x)
-        x = activation(x)
+    Args:
+        stackwise_filters: list of ints, number of filters for each stack in
+            the model.
+        stackwise_blocks: list of ints, number of blocks for each stack in the
+            model.
+        stackwise_strides: list of ints, stride for each stack in the model.
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
+        stackwise_dilations: list of ints, dilation for each stack in the
+            model. If `None` (default), dilation will not be used.
+        input_shape: optional shape tuple, defaults to (None, None, 3).
+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+            to use as image input for the model.
+        block_type: string, one of "basic_block" or "block". The block type to
+            stack. Use "basic_block" for smaller models like ResNet18 and
+            ResNet34.
+
+    Examples:
+    ```python
+    input_data = tf.ones(shape=(8, 224, 224, 3))
+
+    # Pretrained backbone
+    model = keras_cv.models.ResNetV2Backbone.from_preset("resnet50_v2_imagenet")
+    output = model(input_data)
+
+    # Randomly initialized backbone with a custom config
+    model = ResNetV2Backbone(
+        stackwise_filters=[64, 128, 256, 512],
+        stackwise_blocks=[2, 2, 2, 2],
+        stackwise_strides=[1, 2, 2, 2],
+        include_rescaling=False,
+    )
+    output = model(input_data)
+    ```
+    """  # noqa: E501
+
+    def __init__(
+        self,
+        *,
+        stackwise_filters,
+        stackwise_blocks,
+        stackwise_strides,
+        include_rescaling,
+        stackwise_dilations=None,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        block_type="block",
+        **kwargs,
+    ):
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
+        x = inputs
 
-        if se_ratio:
-            with custom_object_scope({"hard_sigmoid": HardSigmoid()}):
-                x = cv_layers.SqueezeAndExcite2D(
-                    filters=depth(infilters * expansion),
-                    ratio=se_ratio,
-                    squeeze_activation="relu",
-                    excite_activation="hard_sigmoid",
-                )(x)
+        if include_rescaling:
+            x = layers.Rescaling(1 / 255.0)(x)
 
         x = layers.Conv2D(
-            filters,
-            kernel_size=1,
+            64,
+            7,
+            strides=2,
+            use_bias=True,
             padding="same",
-            use_bias=False,
-            name=prefix + "project",
+            name="conv1_conv",
         )(x)
+
+        x = layers.MaxPooling2D(
+            3, strides=2, padding="same", name="pool1_pool"
+        )(x)
+
+        num_stacks = len(stackwise_filters)
+        if stackwise_dilations is None:
+            stackwise_dilations = [1] * num_stacks
+
+        pyramid_level_inputs = {}
+        for stack_index in range(num_stacks):
+            x = apply_stack(
+                x,
+                filters=stackwise_filters[stack_index],
+                blocks=stackwise_blocks[stack_index],
+                stride=stackwise_strides[stack_index],
+                dilations=stackwise_dilations[stack_index],
+                block_type=block_type,
+                first_shortcut=(block_type == "block" or stack_index > 0),
+                name=f"v2_stack_{stack_index}",
+            )
+            pyramid_level_inputs[stack_index + 2] = x.node.layer.name
+
         x = layers.BatchNormalization(
-            axis=channel_axis,
-            epsilon=1e-3,
-            momentum=0.999,
-            name=prefix + "project/BatchNorm",
+            axis=BN_AXIS, epsilon=BN_EPSILON, name="post_bn"
         )(x)
+        x = layers.Activation("relu", name="post_relu")(x)
 
-        if stride == 1 and infilters == filters:
-            x = layers.Add(name=prefix + "Add")([shortcut, x])
+        # Create model.
+        super().__init__(inputs=inputs, outputs=x, **kwargs)
 
-        return x
+        # All references to `self` below this line
+        self.pyramid_level_inputs = pyramid_level_inputs
+        self.stackwise_filters = stackwise_filters
+        self.stackwise_blocks = stackwise_blocks
+        self.stackwise_strides = stackwise_strides
+        self.include_rescaling = include_rescaling
+        self.stackwise_dilations = stackwise_dilations
+        self.input_tensor = input_tensor
+        self.block_type = block_type
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                "stackwise_filters": self.stackwise_filters,
+                "stackwise_blocks": self.stackwise_blocks,
+                "stackwise_strides": self.stackwise_strides,
+                "include_rescaling": self.include_rescaling,
+                # Remove batch dimension from `input_shape`
+                "input_shape": self.input_shape[1:],
+                "stackwise_dilations": self.stackwise_dilations,
+                "input_tensor": self.input_tensor,
+                "block_type": self.block_type,
+            }
+        )
+        return config
 
-    return apply
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return copy.deepcopy(backbone_presets)
 
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return copy.deepcopy(backbone_presets_with_weights)
 
-def MobileNetV3(
-    stack_fn,
-    last_point_ch,
-    include_rescaling,
-    include_top,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    alpha=1.0,
-    minimalistic=True,
-    dropout_rate=0.2,
-    classifier_activation="softmax",
-    name="MobileNetV3",
-    **kwargs,
-):
-    """Instantiates the MobileNetV3 architecture.
 
-    References:
-        - [Searching for MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf) (ICCV 2019)
-        - [Based on the Original keras.applications MobileNetv3](https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet_v3.py)
+ALIAS_DOCSTRING = """ResNetV2Backbone model with {num_layers} layers.
+
+    Reference:
+        - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
 
-    This function returns a Keras MobileNetV3 model.
+    The difference in ResNet and ResNetV2 rests in the structure of their
+    individual building blocks. In ResNetV2, the batch normalization and
+    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
+    the batch normalization and ReLU activation are applied after the
+    convolution layers.
 
-    For transfer learning use cases, make sure to read the [guide to transfer
-        learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+    For transfer learning use cases, make sure to read the
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
     Args:
-        stack_fn: a function that returns tensors passed through Inverted
-            Residual Blocks.
-        last_point_ch: the number of filters for the convolution layer.
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(scale=1 / 255)`
-            layer, defaults to True.
-        include_top: whether to include the fully-connected layer at the top of the
-            network.  If provided, `classes` must be provided.
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True, and if no `weights` argument is
-            specified.
-        weights: one of `None` (random initialization), or a pretrained weight file
-            path.
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
         input_shape: optional shape tuple, defaults to (None, None, 3).
         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
             to use as image input for the model.
-        pooling: optional pooling mode for feature extraction
-            when `include_top` is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
-            - `avg` means that global average pooling will be applied to the output
-                of the last convolutional block, and thus the output of the model will
-                be a 2D tensor.
-            - `max` means that global max pooling will be applied.
-        alpha: controls the width of the network. This is known as the
-            depth multiplier in the MobileNetV3 paper, but the name is kept for
-            consistency with MobileNetV1 in Keras.
-            - If `alpha` < 1.0, proportionally decreases the number
-                of filters in each layer.
-            - If `alpha` > 1.0, proportionally increases the number
-                of filters in each layer.
-            - If `alpha` = 1, default number of filters from the paper
-                are used at each layer.
-        minimalistic: in addition to large and small models this module also
-            contains so-called minimalistic models, these models have the same
-            per-layer dimensions characteristic as MobilenetV3 however, they don't
-            utilize any of the advanced blocks (squeeze-and-excite units, hard-swish,
-            and 5x5 convolutions). While these models are less efficient on CPU, they
-            are much more performant on GPU/DSP.
-        dropout_rate: a float between 0 and 1 denoting the fraction of input units to
-            drop, defaults to 0.2.
-        classifier_activation: the activation function to use, defaults to softmax.
-
-        name: (Optional) name to pass to the model. Defaults to "MobileNetV3".
-
-    Returns:
-        A `keras.Model` instance.
 
-    Raises:
-        ValueError: if `weights` represents an invalid path to weights file and is not
-            None.
-        ValueError: if `include_top` is True and `classes` is not specified.
-    """
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either "
-            "`None` or the path to the weights file to be loaded. "
-            f"Weights file not found at location: {weights}"
+    Examples:
+    ```python
+    input_data = tf.ones(shape=(8, 224, 224, 3))
+
+    # Randomly initialized backbone
+    model = ResNet{num_layers}V2Backbone()
+    output = model(input_data)
+    ```
+"""  # noqa: E501
+
+
+class ResNet18V2Backbone(ResNetV2Backbone):
+    def __new__(
+        cls,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        **kwargs,
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
         )
+        return ResNetV2Backbone.from_preset("resnet18_v2", **kwargs)
 
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, "
-            "you should specify `classes`. "
-            f"Received: classes={classes}"
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {}
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
+
+class ResNet34V2Backbone(ResNetV2Backbone):
+    def __new__(
+        cls,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        **kwargs,
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
         )
+        return ResNetV2Backbone.from_preset("resnet34_v2", **kwargs)
 
-    if minimalistic:
-        kernel = 3
-        activation = layers.ReLU()
-        se_ratio = None
-    else:
-        kernel = 5
-        activation = HardSwish()
-        se_ratio = 0.25
-
-    inputs = utils.parse_model_inputs(input_shape, input_tensor)
-
-    x = inputs
-
-    if include_rescaling:
-        x = layers.Rescaling(scale=1 / 255)(x)
-
-    x = layers.Conv2D(
-        16,
-        kernel_size=3,
-        strides=(2, 2),
-        padding="same",
-        use_bias=False,
-        name="Conv",
-    )(x)
-    x = layers.BatchNormalization(
-        axis=channel_axis, epsilon=1e-3, momentum=0.999, name="Conv/BatchNorm"
-    )(x)
-    x = activation(x)
-
-    x = stack_fn(x, kernel, activation, se_ratio)
-
-    last_conv_ch = depth(backend.int_shape(x)[channel_axis] * 6)
-
-    # if the width multiplier is greater than 1 we
-    # increase the number of output channels
-    if alpha > 1.0:
-        last_point_ch = depth(last_point_ch * alpha)
-    x = layers.Conv2D(
-        last_conv_ch,
-        kernel_size=1,
-        padding="same",
-        use_bias=False,
-        name="Conv_1",
-    )(x)
-    x = layers.BatchNormalization(
-        axis=channel_axis, epsilon=1e-3, momentum=0.999, name="Conv_1/BatchNorm"
-    )(x)
-    x = activation(x)
-    if include_top:
-        x = layers.GlobalAveragePooling2D(keepdims=True)(x)
-        x = layers.Conv2D(
-            last_point_ch,
-            kernel_size=1,
-            padding="same",
-            use_bias=True,
-            name="Conv_2",
-        )(x)
-        x = activation(x)
-
-        if dropout_rate > 0:
-            x = layers.Dropout(dropout_rate)(x)
-        x = layers.Conv2D(classes, kernel_size=1, padding="same", name="Logits")(x)
-        x = layers.Flatten()(x)
-        x = layers.Activation(activation=classifier_activation, name="Predictions")(x)
-    elif pooling == "avg":
-        x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-    elif pooling == "max":
-        x = layers.GlobalMaxPooling2D(name="max_pool")(x)
-
-    model = keras.Model(inputs, x, name=name, **kwargs)
-
-    if weights is not None:
-        model.load_weights(weights)
-    return model
-
-
-def MobileNetV3Small(
-    *,
-    include_rescaling,
-    include_top,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    alpha=1.0,
-    minimalistic=False,
-    dropout_rate=0.2,
-    classifier_activation="softmax",
-    name="MobileNetV3Small",
-    **kwargs,
-):
-    def stack_fn(x, kernel, activation, se_ratio):
-        x = InvertedResBlock(1, depth(16 * alpha), 3, 2, se_ratio, layers.ReLU(), 0)(x)
-        x = InvertedResBlock(
-            72.0 / 16, depth(24 * alpha), 3, 2, None, layers.ReLU(), 1
-        )(x)
-        x = InvertedResBlock(
-            88.0 / 24, depth(24 * alpha), 3, 1, None, layers.ReLU(), 2
-        )(x)
-        x = InvertedResBlock(4, depth(40 * alpha), kernel, 2, se_ratio, activation, 3)(
-            x
-        )
-        x = InvertedResBlock(6, depth(40 * alpha), kernel, 1, se_ratio, activation, 4)(
-            x
-        )
-        x = InvertedResBlock(6, depth(40 * alpha), kernel, 1, se_ratio, activation, 5)(
-            x
-        )
-        x = InvertedResBlock(3, depth(48 * alpha), kernel, 1, se_ratio, activation, 6)(
-            x
-        )
-        x = InvertedResBlock(3, depth(48 * alpha), kernel, 1, se_ratio, activation, 7)(
-            x
-        )
-        x = InvertedResBlock(6, depth(96 * alpha), kernel, 2, se_ratio, activation, 8)(
-            x
-        )
-        x = InvertedResBlock(6, depth(96 * alpha), kernel, 1, se_ratio, activation, 9)(
-            x
-        )
-        x = InvertedResBlock(6, depth(96 * alpha), kernel, 1, se_ratio, activation, 10)(
-            x
-        )
-        return x
-
-    return MobileNetV3(
-        stack_fn=stack_fn,
-        last_point_ch=1024,
-        include_rescaling=include_rescaling,
-        include_top=include_top,
-        classes=classes,
-        weights=weights,
-        input_shape=input_shape,
-        input_tensor=input_tensor,
-        pooling=pooling,
-        alpha=alpha,
-        minimalistic=minimalistic,
-        dropout_rate=dropout_rate,
-        classifier_activation=classifier_activation,
-        name=name,
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {}
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
+
+class ResNet50V2Backbone(ResNetV2Backbone):
+    def __new__(
+        cls,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
         **kwargs,
-    )
-
-
-def MobileNetV3Large(
-    *,
-    include_rescaling,
-    include_top,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    alpha=1.0,
-    minimalistic=False,
-    dropout_rate=0.2,
-    classifier_activation="softmax",
-    name="MobileNetV3Large",
-    **kwargs,
-):
-    def stack_fn(x, kernel, activation, se_ratio):
-        x = InvertedResBlock(1, depth(16 * alpha), 3, 1, None, layers.ReLU(), 0)(x)
-        x = InvertedResBlock(4, depth(24 * alpha), 3, 2, None, layers.ReLU(), 1)(x)
-        x = InvertedResBlock(3, depth(24 * alpha), 3, 1, None, layers.ReLU(), 2)(x)
-        x = InvertedResBlock(
-            3, depth(40 * alpha), kernel, 2, se_ratio, layers.ReLU(), 3
-        )(x)
-        x = InvertedResBlock(
-            3, depth(40 * alpha), kernel, 1, se_ratio, layers.ReLU(), 4
-        )(x)
-        x = InvertedResBlock(
-            3, depth(40 * alpha), kernel, 1, se_ratio, layers.ReLU(), 5
-        )(x)
-        x = InvertedResBlock(6, depth(80 * alpha), 3, 2, None, activation, 6)(x)
-        x = InvertedResBlock(2.5, depth(80 * alpha), 3, 1, None, activation, 7)(x)
-        x = InvertedResBlock(2.3, depth(80 * alpha), 3, 1, None, activation, 8)(x)
-        x = InvertedResBlock(2.3, depth(80 * alpha), 3, 1, None, activation, 9)(x)
-        x = InvertedResBlock(6, depth(112 * alpha), 3, 1, se_ratio, activation, 10)(x)
-        x = InvertedResBlock(6, depth(112 * alpha), 3, 1, se_ratio, activation, 11)(x)
-        x = InvertedResBlock(
-            6, depth(160 * alpha), kernel, 2, se_ratio, activation, 12
-        )(x)
-        x = InvertedResBlock(
-            6, depth(160 * alpha), kernel, 1, se_ratio, activation, 13
-        )(x)
-        x = InvertedResBlock(
-            6, depth(160 * alpha), kernel, 1, se_ratio, activation, 14
-        )(x)
-        return x
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
+        )
+        return ResNetV2Backbone.from_preset("resnet50_v2", **kwargs)
 
-    return MobileNetV3(
-        stack_fn=stack_fn,
-        last_point_ch=1280,
-        include_rescaling=include_rescaling,
-        include_top=include_top,
-        classes=classes,
-        weights=weights,
-        input_shape=input_shape,
-        input_tensor=input_tensor,
-        pooling=pooling,
-        alpha=alpha,
-        minimalistic=minimalistic,
-        dropout_rate=dropout_rate,
-        classifier_activation=classifier_activation,
-        name=name,
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {
+            "resnet50_v2_imagenet": copy.deepcopy(
+                backbone_presets["resnet50_v2_imagenet"]
+            ),
+        }
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return cls.presets
+
+
+class ResNet101V2Backbone(ResNetV2Backbone):
+    def __new__(
+        cls,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
         **kwargs,
-    )
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
+        )
+        return ResNetV2Backbone.from_preset("resnet101_v2", **kwargs)
 
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {}
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
+
+class ResNet152V2Backbone(ResNetV2Backbone):
+    def __new__(
+        cls,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        **kwargs,
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
+        )
+        return ResNetV2Backbone.from_preset("resnet152_v2", **kwargs)
 
-setattr(MobileNetV3Large, "__doc__", BASE_DOCSTRING.format(name="MobileNetV3Large"))
-setattr(MobileNetV3Small, "__doc__", BASE_DOCSTRING.format(name="MobileNetV3Small"))
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {}
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
+
+setattr(
+    ResNet18V2Backbone,
+    "__doc__",
+    ALIAS_DOCSTRING.format(num_layers=18),
+)
+setattr(
+    ResNet34V2Backbone,
+    "__doc__",
+    ALIAS_DOCSTRING.format(num_layers=34),
+)
+setattr(
+    ResNet50V2Backbone,
+    "__doc__",
+    ALIAS_DOCSTRING.format(num_layers=50),
+)
+setattr(
+    ResNet101V2Backbone,
+    "__doc__",
+    ALIAS_DOCSTRING.format(num_layers=101),
+)
+setattr(
+    ResNet152V2Backbone,
+    "__doc__",
+    ALIAS_DOCSTRING.format(num_layers=152),
+)
```

## Comparing `keras_cv/models/models_test.py` & `keras_cv/models/legacy/models_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,33 +9,37 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Integration tests for KerasCV models."""
 
+import os
+
 import pytest
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import backend
 
 
 class ModelsTest:
     def assertShapeEqual(self, shape1, shape2):
         self.assertEqual(tf.TensorShape(shape1), tf.TensorShape(shape2))
 
     @pytest.fixture(autouse=True)
     def cleanup_global_session(self):
         # Code before yield runs before the test
         yield
-        tf.keras.backend.clear_session()
+        keras.backend.clear_session()
 
     def _test_application_base(self, app, _, args):
         # Can be instantiated with default arguments
-        model = app(include_top=True, classes=1000, include_rescaling=False, **args)
+        model = app(
+            include_top=True, num_classes=1000, include_rescaling=False, **args
+        )
 
         # Can be serialized and deserialized
         config = model.get_config()
         reconstructed_model = model.__class__.from_config(config)
         self.assertEqual(len(model.weights), len(reconstructed_model.weights))
 
         # There is no rescaling layer bcause include_rescaling=False
@@ -43,15 +47,17 @@
             model.get_layer(name="rescaling")
 
     def _test_application_with_rescaling(self, app, last_dim, args):
         model = app(include_rescaling=True, include_top=False, **args)
         self.assertIsNotNone(model.get_layer(name="rescaling"))
 
     def _test_application_pooling(self, app, last_dim, args):
-        model = app(include_rescaling=False, include_top=False, pooling="avg", **args)
+        model = app(
+            include_rescaling=False, include_top=False, pooling="avg", **args
+        )
 
         self.assertShapeEqual(model.output_shape, (None, last_dim))
 
     def _test_application_variable_input_channels(self, app, last_dim, args):
         # Make a local copy of args because we modify them in the test
         args = dict(args)
 
@@ -152,10 +158,22 @@
         x = backbone(x)
 
         backbone_output = backbone.get_layer(index=-1).output
 
         model = keras.Model(inputs=inputs, outputs=[backbone_output])
         model.compile()
 
+    def _test_model_serialization(self, app, _, args, save_format, filename):
+        model = app(include_rescaling=True, include_top=False, **args)
+        input_batch = tf.ones(shape=(16, 224, 224, 3))
+        model_output = model(input_batch)
+        save_path = os.path.join(self.get_temp_dir(), filename)
+        model.save(save_path, save_format=save_format)
+        restored_model = keras.models.load_model(save_path)
+
+        # Check that output matches.
+        restored_output = restored_model(input_batch)
+        self.assertAllClose(model_output, restored_output)
+
 
 if __name__ == "__main__":
     tf.test.main()
```

## Comparing `keras_cv/models/regnet.py` & `keras_cv/models/legacy/regnet.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,26 +9,26 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """RegNet models for KerasCV.
 References:
-    - [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)
-    (CVPR 2020)
+    - [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678) (CVPR 2020)
     - [Based on the Original keras.applications RegNet](https://github.com/keras-team/keras/blob/master/keras/applications/regnet.py)
-"""
+"""  # noqa: E501
 
 import tensorflow as tf
-from keras import backend
-from keras import layers
+from tensorflow import keras
+from tensorflow.keras import backend
+from tensorflow.keras import layers
 
 from keras_cv.layers import SqueezeAndExcite2D
-from keras_cv.models import utils
-from keras_cv.models.weights import parse_weights
+from keras_cv.models.legacy import utils
+from keras_cv.models.legacy.weights import parse_weights
 
 # The widths and depths are deduced from a quantized linear function. For
 # more information, please refer to "Designing Network Design Spaces" by
 # Radosavovic et al.
 
 # BatchNorm momentum and epsilon values taken from original implementation.
 
@@ -199,45 +199,47 @@
         "widths": [232, 696, 1392, 3712],
         "group_width": 232,
         "default_size": 224,
         "block_type": "Y",
     },
 }
 
-BASE_DOCSTRING = """Instantiates the {name} architecture.
+BASE_DOCSTRING = """This class represents the {name} architecture.
 
   Reference:
     - [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)
     (CVPR 2020)
 
-  For image classification use cases, see [this page for detailed examples](https://keras.io/api/applications/#usage-examples-for-image-classification-models).
+  For image classification use cases, see
+  [this page for detailed examples](https://keras.io/api/applications/#usage-examples-for-image-classification-models).
 
-  For transfer learning use cases, make sure to read the [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+  For transfer learning use cases, make sure to read the
+  [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
 
   The naming of models is as follows: `RegNet<block_type><flops>` where
   `block_type` is one of `(X, Y)` and `flops` signifies hundred million
   floating point operations. For example RegNetY064 corresponds to RegNet with
   Y block and 6.4 giga flops (64 hundred million flops).
 
   Args:
-    include_rescaling: whether or not to Rescale the inputs.If set to True,
+    include_rescaling: whether to rescale the inputs. If set to True,
         inputs will be passed through a `Rescaling(1/255.0)` layer.
     include_top: Whether to include the fully-connected
         layer at the top of the network.
-    classes: Optional number of classes to classify images
+    num_classes: Optional number of classes to classify images
         into, only to be specified if `include_top` is True.
     weights: One of `None` (random initialization), or the path to the weights
-          file to be loaded. Defaults to `None`.
+          file to be loaded, defaults to `None`.
     input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)
         to use as image input for the model.
     input_shape: Optional shape tuple, defaults to (None, None, 3).
         It should have exactly 3 inputs channels.
     pooling: Optional pooling mode for feature extraction
-        when `include_top` is `False`. Defaults to None.
+        when `include_top` is `False`, defaults to None.
         - `None` means that the output of the model will be
             the 4D tensor output of the
             last convolutional layer.
         - `avg` means that global average pooling
             will be applied to the output of the
             last convolutional layer, and thus
             the output of the model will be a 2D tensor.
@@ -248,565 +250,593 @@
         `classifier_activation=None` to return the logits of the "top" layer.
         Defaults to `"softmax"`.
         When loading pretrained weights, `classifier_activation` can only
         be `None` or `"softmax"`.
 
   Returns:
     A `keras.Model` instance.
-"""
+"""  # noqa: E501
+
+
+def apply_conv2d_bn(
+    x,
+    filters,
+    kernel_size,
+    strides=1,
+    use_bias=False,
+    groups=1,
+    padding="valid",
+    kernel_initializer="he_normal",
+    batch_norm=True,
+    activation="relu",
+    name="",
+):
+    x = layers.Conv2D(
+        filters,
+        kernel_size,
+        strides=strides,
+        groups=groups,
+        use_bias=use_bias,
+        padding=padding,
+        kernel_initializer=kernel_initializer,
+        name=name,
+    )(x)
+
+    if batch_norm:
+        x = layers.BatchNormalization(
+            momentum=0.9, epsilon=1e-5, name=name + "_bn"
+        )(x)
+
+    if activation is not None:
+        x = layers.Activation(activation, name=name + f"_{activation}")(x)
 
+    return x
 
-def Stem(name=None):
+
+def apply_stem(x, name=None):
     """Implementation of RegNet stem.
 
     (Common to all model variants)
     Args:
+      x: Tensor, input tensor to the stem
       name: name prefix
 
     Returns:
       Output tensor of the Stem
     """
     if name is None:
         name = "stem" + str(backend.get_uid("stem"))
 
-    def apply(x):
-        x = layers.Conv2D(
-            32,
-            (3, 3),
-            strides=2,
-            use_bias=False,
-            padding="same",
-            kernel_initializer="he_normal",
-            name=name + "_stem_conv",
-        )(x)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_stem_bn"
-        )(x)
-        x = layers.ReLU(name=name + "_stem_relu")(x)
-        return x
+    x = apply_conv2d_bn(
+        x=x,
+        filters=32,
+        kernel_size=(3, 3),
+        strides=2,
+        padding="same",
+        name=name + "_stem_conv",
+    )
 
-    return apply
+    return x
 
 
-def XBlock(filters_in, filters_out, group_width, stride=1, name=None):
+def apply_x_block(
+    inputs, filters_in, filters_out, group_width, stride=1, name=None
+):
     """Implementation of X Block.
     References:
         - [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)
 
     Args:
-      filters_in: filters in the input tensor
-      filters_out: filters in the output tensor
-      group_width: group width
-      stride: stride
-      name: name prefix
+      inputs: Tensor, input tensor to the block
+      filters_in: int, filters in the input tensor
+      filters_out: int, filters in the output tensor
+      group_width: int, group width
+      stride: int (or) tuple, stride of Conv layer
+      name: str, name prefix
     Returns:
       Output tensor of the block
     """
     if name is None:
         name = str(backend.get_uid("xblock"))
 
-    def apply(inputs):
-        if filters_in != filters_out and stride == 1:
-            raise ValueError(
-                f"Input filters({filters_in}) and output "
-                f"filters({filters_out}) "
-                f"are not equal for stride {stride}. Input and output filters "
-                f"must be equal for stride={stride}."
-            )
-
-        # Declare layers
-        groups = filters_out // group_width
-
-        if stride != 1:
-            skip = layers.Conv2D(
-                filters_out,
-                (1, 1),
-                strides=stride,
-                use_bias=False,
-                kernel_initializer="he_normal",
-                name=name + "_skip_1x1",
-            )(inputs)
-            skip = layers.BatchNormalization(
-                momentum=0.9, epsilon=1e-5, name=name + "_skip_bn"
-            )(skip)
-        else:
-            skip = inputs
+    if filters_in != filters_out and stride == 1:
+        raise ValueError(
+            f"Input filters({filters_in}) and output "
+            f"filters({filters_out}) "
+            f"are not equal for stride {stride}. Input and output filters "
+            f"must be equal for stride={stride}."
+        )
 
-        # Build block
-        # conv_1x1_1
-        x = layers.Conv2D(
-            filters_out,
-            (1, 1),
-            use_bias=False,
-            kernel_initializer="he_normal",
-            name=name + "_conv_1x1_1",
-        )(inputs)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_conv_1x1_1_bn"
-        )(x)
-        x = layers.ReLU(name=name + "_conv_1x1_1_relu")(x)
+    # Declare layers
+    groups = filters_out // group_width
 
-        # conv_3x3
-        x = layers.Conv2D(
-            filters_out,
-            (3, 3),
-            use_bias=False,
+    if stride != 1:
+        skip = apply_conv2d_bn(
+            x=inputs,
+            filters=filters_out,
+            kernel_size=(1, 1),
             strides=stride,
-            groups=groups,
-            padding="same",
-            kernel_initializer="he_normal",
-            name=name + "_conv_3x3",
-        )(x)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_conv_3x3_bn"
-        )(x)
-        x = layers.ReLU(name=name + "_conv_3x3_relu")(x)
+            activation=None,
+            name=name + "_skip_1x1",
+        )
+    else:
+        skip = inputs
 
-        # conv_1x1_2
-        x = layers.Conv2D(
-            filters_out,
-            (1, 1),
-            use_bias=False,
-            kernel_initializer="he_normal",
-            name=name + "_conv_1x1_2",
-        )(x)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_conv_1x1_2_bn"
-        )(x)
+    # Build block
+    # conv_1x1_1
+    x = apply_conv2d_bn(
+        x=inputs,
+        filters=filters_out,
+        kernel_size=(1, 1),
+        name=name + "_conv_1x1_1",
+    )
 
-        x = layers.ReLU(name=name + "_exit_relu")(x + skip)
+    # conv_3x3
+    x = apply_conv2d_bn(
+        x=x,
+        filters=filters_out,
+        kernel_size=(3, 3),
+        strides=stride,
+        groups=groups,
+        padding="same",
+        name=name + "_conv_3x3",
+    )
 
-        return x
+    # conv_1x1_2
+    x = apply_conv2d_bn(
+        x=x,
+        filters=filters_out,
+        kernel_size=(1, 1),
+        activation=None,
+        name=name + "_conv_1x1_2",
+    )
+
+    x = layers.Activation("relu", name=name + "_exit_relu")(x + skip)
 
-    return apply
+    return x
 
 
-def YBlock(
+def apply_y_block(
+    inputs,
     filters_in,
     filters_out,
     group_width,
     stride=1,
     squeeze_excite_ratio=0.25,
     name=None,
 ):
     """Implementation of Y Block.
     References:
         - [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)
 
     Args:
-      filters_in: filters in the input tensor
-      filters_out: filters in the output tensor
-      group_width: group width
-      stride: stride
-      squeeze_excite_ratio: expansion ration for Squeeze and Excite block
-      name: name prefix
+      inputs: Tensor, input tensor to the block
+      filters_in: int, filters in the input tensor
+      filters_out: int, filters in the output tensor
+      group_width: int, group width
+      stride: int (or) tuple, stride of Conv layer
+      squeeze_excite_ratio: float, expansion ratio for Squeeze and Excite block
+      name: str, name prefix
     Returns:
       Output tensor of the block
     """
     if name is None:
         name = str(backend.get_uid("yblock"))
 
-    def apply(inputs):
-        if filters_in != filters_out and stride == 1:
-            raise ValueError(
-                f"Input filters({filters_in}) and output "
-                f"filters({filters_out}) "
-                f"are not equal for stride {stride}. Input and output filters "
-                f"must be equal for stride={stride}."
-            )
-
-        groups = filters_out // group_width
-
-        if stride != 1:
-            skip = layers.Conv2D(
-                filters_out,
-                (1, 1),
-                strides=stride,
-                use_bias=False,
-                kernel_initializer="he_normal",
-                name=name + "_skip_1x1",
-            )(inputs)
-            skip = layers.BatchNormalization(
-                momentum=0.9, epsilon=1e-5, name=name + "_skip_bn"
-            )(skip)
-        else:
-            skip = inputs
+    if filters_in != filters_out and stride == 1:
+        raise ValueError(
+            f"Input filters({filters_in}) and output "
+            f"filters({filters_out}) "
+            f"are not equal for stride {stride}. Input and output filters "
+            f"must be equal for stride={stride}."
+        )
 
-        # Build block
-        # conv_1x1_1
-        x = layers.Conv2D(
-            filters_out,
-            (1, 1),
-            use_bias=False,
-            kernel_initializer="he_normal",
-            name=name + "_conv_1x1_1",
-        )(inputs)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_conv_1x1_1_bn"
-        )(x)
-        x = layers.ReLU(name=name + "_conv_1x1_1_relu")(x)
+    groups = filters_out // group_width
 
-        # conv_3x3
-        x = layers.Conv2D(
-            filters_out,
-            (3, 3),
-            use_bias=False,
+    if stride != 1:
+        skip = apply_conv2d_bn(
+            x=inputs,
+            filters=filters_out,
+            kernel_size=(1, 1),
             strides=stride,
-            groups=groups,
-            padding="same",
-            kernel_initializer="he_normal",
-            name=name + "_conv_3x3",
-        )(x)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_conv_3x3_bn"
-        )(x)
-        x = layers.ReLU(name=name + "_conv_3x3_relu")(x)
-
-        # Squeeze-Excitation block
-        x = SqueezeAndExcite2D(filters_out, ratio=squeeze_excite_ratio, name=name)(x)
-
-        # conv_1x1_2
-        x = layers.Conv2D(
-            filters_out,
-            (1, 1),
-            use_bias=False,
-            kernel_initializer="he_normal",
-            name=name + "_conv_1x1_2",
-        )(x)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_conv_1x1_2_bn"
-        )(x)
+            activation=None,
+            name=name + "_skip_1x1",
+        )
+    else:
+        skip = inputs
 
-        x = layers.ReLU(name=name + "_exit_relu")(x + skip)
+    # Build block
+    # conv_1x1_1
+    x = apply_conv2d_bn(
+        x=inputs,
+        filters=filters_out,
+        kernel_size=(1, 1),
+        name=name + "_conv_1x1_1",
+    )
+
+    # conv_3x3
+    x = apply_conv2d_bn(
+        x=x,
+        filters=filters_out,
+        kernel_size=(3, 3),
+        strides=stride,
+        groups=groups,
+        padding="same",
+        name=name + "_conv_3x3",
+    )
+
+    # Squeeze-Excitation block
+    x = SqueezeAndExcite2D(
+        filters_out,
+        bottleneck_filters=filters_out * squeeze_excite_ratio,
+        name=name,
+    )(x)
+
+    # conv_1x1_2
+    x = apply_conv2d_bn(
+        x=x,
+        filters=filters_out,
+        kernel_size=(1, 1),
+        activation=None,
+        name=name + "_conv_1x1_2",
+    )
 
-        return x
+    x = layers.Activation("relu", name=name + "_exit_relu")(x + skip)
 
-    return apply
+    return x
 
 
-def ZBlock(
+def apply_z_block(
+    inputs,
     filters_in,
     filters_out,
     group_width,
     stride=1,
     squeeze_excite_ratio=0.25,
     bottleneck_ratio=0.25,
     name=None,
 ):
     """Implementation of Z block.
 
     References:
         - [Fast and Accurate Model Scaling](https://arxiv.org/abs/2103.06877).
 
     Args:
-      filters_in: filters in the input tensor
-      filters_out: filters in the output tensor
-      group_width: group width
-      stride: stride
-      squeeze_excite_ratio: expansion ration for Squeeze and Excite block
-      bottleneck_ratio: inverted bottleneck ratio
-      name: name prefix
+      inputs: Tensor, input tensor to the block
+      filters_in: int, filters in the input tensor
+      filters_out: int, filters in the output tensor
+      group_width: int, group width
+      stride: int (or) tuple, stride
+      squeeze_excite_ratio: float, expansion ration for Squeeze and Excite block
+      bottleneck_ratio: float, inverted bottleneck ratio
+      name: str, name prefix
     Returns:
       Output tensor of the block
     """
     if name is None:
         name = str(backend.get_uid("zblock"))
 
-    def apply(inputs):
-        if filters_in != filters_out and stride == 1:
-            raise ValueError(
-                f"Input filters({filters_in}) and output filters({filters_out})"
-                f"are not equal for stride {stride}. Input and output filters "
-                f"must be equal for stride={stride}."
-            )
-
-        groups = filters_out // group_width
-
-        inv_btlneck_filters = int(filters_out / bottleneck_ratio)
-
-        # Build block
-        # conv_1x1_1
-        x = layers.Conv2D(
-            inv_btlneck_filters,
-            (1, 1),
-            use_bias=False,
-            kernel_initializer="he_normal",
-            name=name + "_conv_1x1_1",
-        )(inputs)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_conv_1x1_1_bn"
-        )(x)
-        x = tf.nn.silu(x)
-
-        # conv_3x3
-        x = layers.Conv2D(
-            inv_btlneck_filters,
-            (3, 3),
-            use_bias=False,
-            strides=stride,
-            groups=groups,
-            padding="same",
-            kernel_initializer="he_normal",
-            name=name + "_conv_3x3",
-        )(x)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_conv_3x3_bn"
-        )(x)
-        x = tf.nn.silu(x)
-
-        # Squeeze-Excitation block
-        x = SqueezeAndExcite2D(
-            inv_btlneck_filters, ratio=squeeze_excite_ratio, name=name
+    if filters_in != filters_out and stride == 1:
+        raise ValueError(
+            f"Input filters({filters_in}) and output filters({filters_out})"
+            f"are not equal for stride {stride}. Input and output filters "
+            f"must be equal for stride={stride}."
         )
 
-        # conv_1x1_2
-        x = layers.Conv2D(
-            filters_out,
-            (1, 1),
-            use_bias=False,
-            kernel_initializer="he_normal",
-            name=name + "_conv_1x1_2",
-        )(x)
-        x = layers.BatchNormalization(
-            momentum=0.9, epsilon=1e-5, name=name + "_conv_1x1_2_bn"
-        )(x)
+    groups = filters_out // group_width
 
-        if stride != 1:
-            return x
-        else:
-            return x + inputs
+    inv_btlneck_filters = int(filters_out / bottleneck_ratio)
+
+    # Build block
+    # conv_1x1_1
+    x = apply_conv2d_bn(
+        x=inputs,
+        filters=inv_btlneck_filters,
+        kernel_size=(1, 1),
+        name=name + "_conv_1x1_1",
+        activation="silu",
+    )
+
+    # conv_3x3
+    x = apply_conv2d_bn(
+        x=x,
+        filters=inv_btlneck_filters,
+        kernel_size=(3, 3),
+        strides=stride,
+        groups=groups,
+        padding="same",
+        name=name + "_conv_3x3",
+        activation="silu",
+    )
+
+    # Squeeze-Excitation block
+    x = SqueezeAndExcite2D(
+        inv_btlneck_filters,
+        bottleneck_filter=inv_btlneck_filters * squeeze_excite_ratio,
+        name=name,
+    )(x)
+
+    # conv_1x1_2
+    x = apply_conv2d_bn(
+        x=x,
+        filters=filters_out,
+        kernel_size=(1, 1),
+        activation=None,
+        name=name + "_conv_1x1_2",
+    )
 
-    return apply
+    if stride != 1:
+        return x
+    else:
+        return x + inputs
 
 
-def Stage(block_type, depth, group_width, filters_in, filters_out, name=None):
+def apply_stage(
+    x, block_type, depth, group_width, filters_in, filters_out, name=None
+):
     """Implementation of Stage in RegNet.
 
     Args:
+      x: Tensor, input tensor to the stage
       block_type: must be one of "X", "Y", "Z"
-      depth: depth of stage, number of blocks to use
-      group_width: group width of all blocks in  this stage
-      filters_in: input filters to this stage
-      filters_out: output filters from this stage
-      name: name prefix
+      depth: int, depth of stage, number of blocks to use
+      group_width: int, group width of all blocks in  this stage
+      filters_in: int, input filters to this stage
+      filters_out: int, output filters from this stage
+      name: str, name prefix
 
     Returns:
-      Output tensor of Stage
+      Output tensor of the block
     """
     if name is None:
         name = str(backend.get_uid("stage"))
 
-    def apply(inputs):
-        x = inputs
-        if block_type == "X":
-            x = XBlock(
-                filters_in,
+    if block_type == "X":
+        x = apply_x_block(
+            x,
+            filters_in,
+            filters_out,
+            group_width,
+            stride=2,
+            name=f"{name}_XBlock_0",
+        )
+        for i in range(1, depth):
+            x = apply_x_block(
+                x,
+                filters_out,
                 filters_out,
                 group_width,
-                stride=2,
-                name=f"{name}_XBlock_0",
-            )(x)
-            for i in range(1, depth):
-                x = XBlock(
-                    filters_out,
-                    filters_out,
-                    group_width,
-                    name=f"{name}_XBlock_{i}",
-                )(x)
-        elif block_type == "Y":
-            x = YBlock(
-                filters_in,
+                name=f"{name}_XBlock_{i}",
+            )
+    elif block_type == "Y":
+        x = apply_y_block(
+            x,
+            filters_in,
+            filters_out,
+            group_width,
+            stride=2,
+            name=name + "_YBlock_0",
+        )
+        for i in range(1, depth):
+            x = apply_y_block(
+                x,
+                filters_out,
                 filters_out,
                 group_width,
-                stride=2,
-                name=name + "_YBlock_0",
-            )(x)
-            for i in range(1, depth):
-                x = YBlock(
-                    filters_out,
-                    filters_out,
-                    group_width,
-                    name=f"{name}_YBlock_{i}",
-                )(x)
-        elif block_type == "Z":
-            x = ZBlock(
-                filters_in,
+                name=f"{name}_YBlock_{i}",
+            )
+    elif block_type == "Z":
+        x = apply_z_block(
+            x,
+            filters_in,
+            filters_out,
+            group_width,
+            stride=2,
+            name=f"{name}_ZBlock_0",
+        )
+        for i in range(1, depth):
+            x = apply_z_block(
+                x,
+                filters_out,
                 filters_out,
                 group_width,
-                stride=2,
-                name=f"{name}_ZBlock_0",
-            )(x)
-            for i in range(1, depth):
-                x = ZBlock(
-                    filters_out,
-                    filters_out,
-                    group_width,
-                    name=f"{name}_ZBlock_{i}",
-                )(x)
-        else:
-            raise NotImplementedError(
-                f"Block type `{block_type}` not recognized."
-                f"block_type must be one of (`X`, `Y`, `Z`). "
+                name=f"{name}_ZBlock_{i}",
             )
-        return x
-
-    return apply
+    else:
+        raise NotImplementedError(
+            f"Block type `{block_type}` not recognized."
+            f"block_type must be one of (`X`, `Y`, `Z`). "
+        )
+    return x
 
 
-def Head(classes=None, name=None, activation=None):
+def apply_head(x, num_classes=None, name=None, activation=None):
     """Implementation of classification head of RegNet.
 
     Args:
-      classes: number of classes for Dense layer
-      name: name prefix
+      x: Tensor, input to the head block
+      num_classes: int, number of classes for Dense layer
+      name: str, name prefix
 
     Returns:
       Output logits tensor.
     """
     if name is None:
         name = str(backend.get_uid("head"))
 
-    def apply(x):
-        x = layers.GlobalAveragePooling2D(name=name + "_head_gap")(x)
-        x = layers.Dense(classes, name=name + "head_dense", activation=activation)(x)
-        return x
-
-    return apply
+    x = layers.GlobalAveragePooling2D(name=name + "_head_gap")(x)
+    x = layers.Dense(
+        num_classes, name=name + "head_dense", activation=activation
+    )(x)
+    return x
 
 
-def RegNet(
-    depths,
-    widths,
-    group_width,
-    block_type,
-    include_rescaling,
-    include_top,
-    classes=None,
-    model_name="regnet",
-    weights=None,
-    input_tensor=None,
-    input_shape=(None, None, 3),
-    pooling=None,
-    classifier_activation="softmax",
-    **kwargs,
-):
-    """Instantiates RegNet architecture given specific configuration.
-
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class RegNet(keras.Model):
+    """
+    This class represents the architecture of RegNet
     Args:
-        depths: An iterable containing depths for each individual stages.
-        widths: An iterable containing output channel width of each individual
+        depths: iterable, Contains depths for each individual stages.
+        widths: iterable, Contains output channel width of each individual
             stages
-        group_width: Number of channels to be used in each group. See grouped
-            convolutions for more information.
+        group_width: int, Number of channels to be used in each group. See
+            grouped convolutions for more information.
         block_type: Must be one of `{"X", "Y", "Z"}`. For more details see the
             papers "Designing network design spaces" and "Fast and Accurate
             Model Scaling"
-        default_size: Default input image size.
-        model_name: An optional name for the model.
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: Whether to include the fully-connected
+        default_size: tuple (or) list, default input image size.
+        model_name: str, An optional name for the model.
+        include_rescaling: bool, whether to rescale the inputs. If set to
+            True, inputs will be passed through a `Rescaling(1/255.0)` layer.
+        include_top: bool, Whether to include the fully-connected
             layer at the top of the network.
-        classes: Optional number of classes to classify images
+        num_classes: int, Optional number of classes to classify images
             into, only to be specified if `include_top` is True, and
             if no `weights` argument is specified.
-        weights: One of `None` (random initialization), or the path to the
-            weights file to be loaded. Defaults to `None`.
-        input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
+        weights: str, One of `None` (random initialization), or the path to the
+            weights file to be loaded, defaults to `None`.
+        input_tensor: Tensor, Optional Keras tensor (i.e. output of
+            `layers.Input()`) to use as image input for the model.
         input_shape: Optional shape tuple, defaults to (None, None, 3).
             It should have exactly 3 inputs channels.
         pooling: Optional pooling mode for feature extraction
-            when `include_top` is `False`. Defaults to None.
+            when `include_top` is `False`, defaults to None.
             - `None` means that the output of the model will be
                 the 4D tensor output of the
                 last convolutional layer.
             - `avg` means that global average pooling
                 will be applied to the output of the
                 last convolutional layer, and thus
                 the output of the model will be a 2D tensor.
             - `max` means that global max pooling will
                 be applied.
         classifier_activation: A `str` or callable. The activation function to
             use on the "top" layer. Ignored unless `include_top=True`. Set
             `classifier_activation=None` to return the logits of the "top"
             layer. Defaults to `"softmax"`.
-
-    Returns:
-      A `keras.Model` instance.
     """
-    if not (weights is None or tf.io.gfile.exists(weights)):
-        raise ValueError(
-            "The `weights` argument should be either "
-            "`None` (random initialization) "
-            "or the path to the weights file to be loaded."
-        )
 
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, you should specify `classes`. "
-            f"Received: classes={classes}"
-        )
+    def __init__(
+        self,
+        depths,
+        widths,
+        group_width,
+        block_type,
+        include_rescaling,
+        include_top,
+        num_classes=None,
+        model_name="regnet",
+        weights=None,
+        input_tensor=None,
+        input_shape=(None, None, 3),
+        pooling=None,
+        classifier_activation="softmax",
+        **kwargs,
+    ):
+        if not (weights is None or tf.io.gfile.exists(weights)):
+            raise ValueError(
+                "The `weights` argument should be either "
+                "`None` (random initialization) "
+                "or the path to the weights file to be loaded."
+            )
 
-    if include_top and pooling:
-        raise ValueError(
-            f"`pooling` must be `None` when `include_top=True`."
-            f"Received pooling={pooling} and include_top={include_top}. "
-        )
+        if include_top and not num_classes:
+            raise ValueError(
+                "If `include_top` is True, you should specify `num_classes`. "
+                f"Received: num_classes={num_classes}"
+            )
 
-    img_input = utils.parse_model_inputs(input_shape, input_tensor)
-    x = img_input
+        if include_top and pooling:
+            raise ValueError(
+                f"`pooling` must be `None` when `include_top=True`."
+                f"Received pooling={pooling} and include_top={include_top}. "
+            )
 
-    if include_rescaling:
-        x = layers.Rescaling(scale=1.0 / 255.0)(x)
-    x = Stem(name=model_name)(x)
+        img_input = utils.parse_model_inputs(input_shape, input_tensor)
+        x = img_input
 
-    in_channels = x.shape[-1]  # Output from Stem
+        if include_rescaling:
+            x = layers.Rescaling(scale=1.0 / 255.0)(x)
+        x = apply_stem(x, name=model_name)
 
-    NUM_STAGES = 4
+        in_channels = x.shape[-1]  # Output from Stem
 
-    for stage_index in range(NUM_STAGES):
-        depth = depths[stage_index]
-        out_channels = widths[stage_index]
+        NUM_STAGES = 4
 
-        x = Stage(
-            block_type,
-            depth,
-            group_width,
-            in_channels,
-            out_channels,
-            name=model_name + "_Stage_" + str(stage_index),
-        )(x)
-        in_channels = out_channels
+        for stage_index in range(NUM_STAGES):
+            depth = depths[stage_index]
+            out_channels = widths[stage_index]
 
-    if include_top:
-        x = Head(classes=classes, activation=classifier_activation)(x)
-    else:
-        if pooling == "avg":
-            x = layers.GlobalAveragePooling2D()(x)
-        elif pooling == "max":
-            x = layers.GlobalMaxPooling2D()(x)
-
-    model = tf.keras.Model(inputs=img_input, outputs=x, name=model_name, **kwargs)
-
-    # Load weights.
-    if weights is not None:
-        model.load_weights(weights)
+            x = apply_stage(
+                x,
+                block_type,
+                depth,
+                group_width,
+                in_channels,
+                out_channels,
+                name=model_name + "_Stage_" + str(stage_index),
+            )
+            in_channels = out_channels
 
-    return model
+        if include_top:
+            x = apply_head(
+                x, num_classes=num_classes, activation=classifier_activation
+            )
+        else:
+            if pooling == "avg":
+                x = layers.GlobalAveragePooling2D()(x)
+            elif pooling == "max":
+                x = layers.GlobalMaxPooling2D()(x)
+
+        super().__init__(inputs=img_input, outputs=x, name=model_name, **kwargs)
+
+        # Load weights.
+        if weights is not None:
+            self.load_weights(weights)
+
+        self.depths = depths
+        self.widths = widths
+        self.group_width = group_width
+        self.block_type = block_type
+        self.include_rescaling = include_rescaling
+        self.include_top = include_top
+        self.num_classes = num_classes
+        self.model_name = model_name
+        self.input_tensor = input_tensor
+        self.pooling = pooling
+        self.classifier_activation = classifier_activation
+
+    def get_config(self):
+        return {
+            "depths": self.depths,
+            "widths": self.widths,
+            "group_width": self.group_width,
+            "block_type": self.block_type,
+            "include_rescaling": self.include_rescaling,
+            "include_top": self.include_top,
+            "num_classes": self.num_classes,
+            "model_name": self.model_name,
+            "input_tensor": self.input_tensor,
+            "input_shape": self.input_shape[1:],
+            "pooling": self.pooling,
+            "classifier_activation": self.classifier_activation,
+            "trainable": self.trainable,
+        }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
 
 
 # Instantiating variants
-
-
 def RegNetX002(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx002",
     classifier_activation="softmax",
     **kwargs,
@@ -819,25 +849,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx002"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX004(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx004",
     classifier_activation="softmax",
     **kwargs,
@@ -850,25 +880,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx004"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX006(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx006",
     classifier_activation="softmax",
     **kwargs,
@@ -881,25 +911,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx006"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX008(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx008",
     classifier_activation="softmax",
     **kwargs,
@@ -912,25 +942,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx008"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX016(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx016",
     classifier_activation="softmax",
     **kwargs,
@@ -943,25 +973,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx016"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX032(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx032",
     classifier_activation="softmax",
     **kwargs,
@@ -974,25 +1004,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx032"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX040(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx040",
     classifier_activation="softmax",
     **kwargs,
@@ -1005,25 +1035,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx040"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX064(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx064",
     classifier_activation="softmax",
     **kwargs,
@@ -1036,25 +1066,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx064"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX080(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx080",
     classifier_activation="softmax",
     **kwargs,
@@ -1067,25 +1097,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx080"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX120(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx120",
     classifier_activation="softmax",
     **kwargs,
@@ -1098,25 +1128,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx120"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX160(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx160",
     classifier_activation="softmax",
     **kwargs,
@@ -1129,25 +1159,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx160"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetX320(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnetx320",
     classifier_activation="softmax",
     **kwargs,
@@ -1160,25 +1190,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnetx320"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY002(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety002",
     classifier_activation="softmax",
     **kwargs,
@@ -1191,25 +1221,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety002"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY004(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety004",
     classifier_activation="softmax",
     **kwargs,
@@ -1222,25 +1252,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety004"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY006(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety006",
     classifier_activation="softmax",
     **kwargs,
@@ -1253,25 +1283,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety006"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY008(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety008",
     classifier_activation="softmax",
     **kwargs,
@@ -1284,25 +1314,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety008"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY016(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety016",
     classifier_activation="softmax",
     **kwargs,
@@ -1315,25 +1345,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety016"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY032(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety032",
     classifier_activation="softmax",
     **kwargs,
@@ -1346,25 +1376,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety032"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY040(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety040",
     classifier_activation="softmax",
     **kwargs,
@@ -1377,25 +1407,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety040"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY064(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety064",
     classifier_activation="softmax",
     **kwargs,
@@ -1408,25 +1438,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety064"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY080(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety080",
     classifier_activation="softmax",
     **kwargs,
@@ -1439,25 +1469,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety080"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY120(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety120",
     classifier_activation="softmax",
     **kwargs,
@@ -1470,25 +1500,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety120"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY160(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety160",
     classifier_activation="softmax",
     **kwargs,
@@ -1501,25 +1531,25 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety160"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 def RegNetY320(
     *,
     include_rescaling,
     include_top,
-    classes=None,
+    num_classes=None,
     weights=None,
     input_tensor=None,
     input_shape=(None, None, 3),
     pooling=None,
     model_name="regnety320",
     classifier_activation="softmax",
     **kwargs,
@@ -1532,15 +1562,15 @@
         model_name=model_name,
         include_top=include_top,
         include_rescaling=include_rescaling,
         weights=parse_weights(weights, include_top, "regnety320"),
         input_tensor=input_tensor,
         input_shape=input_shape,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         classifier_activation=classifier_activation,
         **kwargs,
     )
 
 
 RegNetX002.__doc__ = BASE_DOCSTRING.format(name="RegNetX002")
 RegNetX004.__doc__ = BASE_DOCSTRING.format(name="RegNetX004")
```

## Comparing `keras_cv/models/regnetx_test.py` & `keras_cv/models/legacy/regnetx_test.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import regnet
+from keras_cv.models.legacy import regnet
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
     (regnet.RegNetX002, 368, {}),
 ]
```

## Comparing `keras_cv/models/regnety_test.py` & `keras_cv/models/legacy/regnety_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import regnet
+from keras_cv.models.legacy import regnet
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
     (regnet.RegNetY002, 368, {}),
 ]
```

## Comparing `keras_cv/models/resnet_v1.py` & `keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,530 +1,551 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """ResNet models for KerasCV.
 Reference:
-  - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (CVPR 2015)
-  - [Based on the original keras.applications ResNet](https://github.com/keras-team/keras/blob/master/keras/applications/resnet.py)
+  - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
+    (CVPR 2015)
+  - [Based on the original keras.applications ResNet](https://github.com/keras-team/keras/blob/master/keras/applications/resnet.py)  # noqa: E501
 """
 
-import types
+import copy
 
-import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras import backend
 from tensorflow.keras import layers
 
 from keras_cv.models import utils
-from keras_cv.models.weights import parse_weights
-
-MODEL_CONFIGS = {
-    "ResNet18": {
-        "stackwise_filters": [64, 128, 256, 512],
-        "stackwise_blocks": [2, 2, 2, 2],
-        "stackwise_strides": [1, 2, 2, 2],
-    },
-    "ResNet34": {
-        "stackwise_filters": [64, 128, 256, 512],
-        "stackwise_blocks": [3, 4, 6, 3],
-        "stackwise_strides": [1, 2, 2, 2],
-    },
-    "ResNet50": {
-        "stackwise_filters": [64, 128, 256, 512],
-        "stackwise_blocks": [3, 4, 6, 3],
-        "stackwise_strides": [1, 2, 2, 2],
-    },
-    "ResNet101": {
-        "stackwise_filters": [64, 128, 256, 512],
-        "stackwise_blocks": [3, 4, 23, 3],
-        "stackwise_strides": [1, 2, 2, 2],
-    },
-    "ResNet152": {
-        "stackwise_filters": [64, 128, 256, 512],
-        "stackwise_blocks": [3, 8, 36, 3],
-        "stackwise_strides": [1, 2, 2, 2],
-    },
-}
+from keras_cv.models.backbones.backbone import Backbone
+from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone_presets import (
+    backbone_presets,
+)
+from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone_presets import (
+    backbone_presets_with_weights,
+)
+from keras_cv.utils.python_utils import classproperty
 
 BN_AXIS = 3
+BN_EPSILON = 1.001e-5
 
-BASE_DOCSTRING = """Instantiates the {name} architecture.
-    Reference:
-        - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
-        - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
-    This function returns a Keras {name} model.
 
-    The difference in Resnet and ResNetV2 rests in the structure of their
-    individual building blocks. In ResNetV2, the batch normalization and
-    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
-    the batch normalization and ReLU activation are applied after the
-    convolution layers.
+def apply_basic_block(
+    x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None
+):
+    """A basic residual block (v1).
 
-    For transfer learning use cases, make sure to read the [guide to transfer
-        learning & fine-tuning](https://keras.io/guides/transfer_learning/).
     Args:
-        include_rescaling: whether or not to Rescale the inputs.If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-        include_top: whether to include the fully-connected layer at the top of the
-            network.  If provided, classes must be provided.
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True.
-        weights: one of `None` (random initialization), or a pretrained weight file
-            path.
-        input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
-        pooling: optional pooling mode for feature extraction
-            when `include_top` is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
-            - `avg` means that global average pooling will be applied to the output
-                of the last convolutional block, and thus the output of the model will
-                be a 2D tensor.
-            - `max` means that global max pooling will be applied.
-        name: (Optional) name to pass to the model.  Defaults to "{name}".
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
-    Returns:
-      A `keras.Model` instance.
-"""
+        x: input tensor.
+        filters: int, filters of the basic layer.
+        kernel_size: int, kernel size of the bottleneck layer, defaults to 3.
+        stride: int, stride of the first layer, defaults to 1.
+        conv_shortcut: bool, uses convolution shortcut if `True`. If `False`
+            (default), uses identity or pooling shortcut, based on stride.
+        name: string, optional prefix for the layer names used in the block.
 
-
-def BasicBlock(filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):
-    """A basic residual block.
-    Args:
-      x: input tensor.
-      filters: integer, filters of the basic layer.
-      kernel_size: default 3, kernel size of the basic layer.
-      stride: default 1, stride of the first layer.
-      conv_shortcut: default True, use convolution shortcut if True,
-          otherwise identity shortcut.
-      name: string, block label.
     Returns:
       Output tensor for the residual block.
     """
+
     if name is None:
         name = f"v1_basic_block_{backend.get_uid('v1_basic_block_')}"
 
-    def apply(x):
-        if conv_shortcut:
-            shortcut = layers.Conv2D(
-                filters, 1, strides=stride, use_bias=False, name=name + "_0_conv"
-            )(x)
-            shortcut = layers.BatchNormalization(
-                axis=BN_AXIS, epsilon=1.001e-5, name=name + "_0_bn"
-            )(shortcut)
-        else:
-            shortcut = x
-
-        x = layers.Conv2D(
+    if conv_shortcut:
+        shortcut = layers.Conv2D(
             filters,
-            kernel_size,
-            padding="SAME",
+            1,
             strides=stride,
             use_bias=False,
-            name=name + "_1_conv",
-        )(x)
-        x = layers.BatchNormalization(
-            axis=BN_AXIS, epsilon=1.001e-5, name=name + "_1_bn"
+            name=name + "_0_conv",
         )(x)
-        x = layers.Activation("relu", name=name + "_1_relu")(x)
+        shortcut = layers.BatchNormalization(
+            axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_0_bn"
+        )(shortcut)
+    else:
+        shortcut = x
 
-        x = layers.Conv2D(
-            filters, kernel_size, padding="SAME", use_bias=False, name=name + "_2_conv"
-        )(x)
-        x = layers.BatchNormalization(
-            axis=BN_AXIS, epsilon=1.001e-5, name=name + "_2_bn"
-        )(x)
+    x = layers.Conv2D(
+        filters,
+        kernel_size,
+        padding="SAME",
+        strides=stride,
+        use_bias=False,
+        name=name + "_1_conv",
+    )(x)
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_1_bn"
+    )(x)
+    x = layers.Activation("relu", name=name + "_1_relu")(x)
+
+    x = layers.Conv2D(
+        filters,
+        kernel_size,
+        padding="SAME",
+        use_bias=False,
+        name=name + "_2_conv",
+    )(x)
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_2_bn"
+    )(x)
 
-        x = layers.Add(name=name + "_add")([shortcut, x])
-        x = layers.Activation("relu", name=name + "_out")(x)
-        return x
+    x = layers.Add(name=name + "_add")([shortcut, x])
+    x = layers.Activation("relu", name=name + "_out")(x)
+    return x
 
-    return apply
 
+def apply_block(
+    x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None
+):
+    """A residual block (v1).
 
-def Block(filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):
-    """A residual block.
     Args:
-      x: input tensor.
-      filters: integer, filters of the bottleneck layer.
-      kernel_size: default 3, kernel size of the bottleneck layer.
-      stride: default 1, stride of the first layer.
-      conv_shortcut: default True, use convolution shortcut if True,
-          otherwise identity shortcut.
-      name: string, block label.
+        x: input tensor.
+        filters: int, filters of the basic layer.
+        kernel_size: int, kernel size of the bottleneck layer, defaults to 3.
+        stride: int, stride of the first layer, defaults to 1.
+        conv_shortcut: bool, uses convolution shortcut if `True`. If `False`
+            (default), uses identity or pooling shortcut, based on stride.
+        name: string, optional prefix for the layer names used in the block.
+
     Returns:
       Output tensor for the residual block.
     """
+
     if name is None:
         name = f"v1_block_{backend.get_uid('v1_block')}"
 
-    def apply(x):
-        if conv_shortcut:
-            shortcut = layers.Conv2D(
-                4 * filters, 1, strides=stride, use_bias=False, name=name + "_0_conv"
-            )(x)
-            shortcut = layers.BatchNormalization(
-                axis=BN_AXIS, epsilon=1.001e-5, name=name + "_0_bn"
-            )(shortcut)
-        else:
-            shortcut = x
-
-        x = layers.Conv2D(
-            filters, 1, strides=stride, use_bias=False, name=name + "_1_conv"
-        )(x)
-        x = layers.BatchNormalization(
-            axis=BN_AXIS, epsilon=1.001e-5, name=name + "_1_bn"
+    if conv_shortcut:
+        shortcut = layers.Conv2D(
+            4 * filters,
+            1,
+            strides=stride,
+            use_bias=False,
+            name=name + "_0_conv",
         )(x)
-        x = layers.Activation("relu", name=name + "_1_relu")(x)
+        shortcut = layers.BatchNormalization(
+            axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_0_bn"
+        )(shortcut)
+    else:
+        shortcut = x
 
-        x = layers.Conv2D(
-            filters, kernel_size, padding="SAME", use_bias=False, name=name + "_2_conv"
-        )(x)
-        x = layers.BatchNormalization(
-            axis=BN_AXIS, epsilon=1.001e-5, name=name + "_2_bn"
-        )(x)
-        x = layers.Activation("relu", name=name + "_2_relu")(x)
+    x = layers.Conv2D(
+        filters, 1, strides=stride, use_bias=False, name=name + "_1_conv"
+    )(x)
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_1_bn"
+    )(x)
+    x = layers.Activation("relu", name=name + "_1_relu")(x)
 
-        x = layers.Conv2D(4 * filters, 1, use_bias=False, name=name + "_3_conv")(x)
-        x = layers.BatchNormalization(
-            axis=BN_AXIS, epsilon=1.001e-5, name=name + "_3_bn"
-        )(x)
+    x = layers.Conv2D(
+        filters,
+        kernel_size,
+        padding="SAME",
+        use_bias=False,
+        name=name + "_2_conv",
+    )(x)
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_2_bn"
+    )(x)
+    x = layers.Activation("relu", name=name + "_2_relu")(x)
 
-        x = layers.Add(name=name + "_add")([shortcut, x])
-        x = layers.Activation("relu", name=name + "_out")(x)
-        return x
+    x = layers.Conv2D(4 * filters, 1, use_bias=False, name=name + "_3_conv")(x)
+    x = layers.BatchNormalization(
+        axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_3_bn"
+    )(x)
 
-    return apply
+    x = layers.Add(name=name + "_add")([shortcut, x])
+    x = layers.Activation("relu", name=name + "_out")(x)
+    return x
 
 
-def Stack(filters, blocks, stride=2, name=None, block_fn=Block, first_shortcut=True):
+def apply_stack(
+    x,
+    filters,
+    blocks,
+    stride=2,
+    name=None,
+    block_type="block",
+    first_shortcut=True,
+):
     """A set of stacked residual blocks.
+
     Args:
-      filters: integer, filters of the layers in a block.
-      blocks: integer, blocks in the stacked blocks.
-      stride1: default 2, stride of the first layer in the first block.
-      name: string, stack label.
-      block_fn: callable, `Block` or `BasicBlock`, the block function to stack.
-      first_shortcut: default True, use convolution shortcut if True,
-          otherwise identity shortcut.
+        x: input tensor.
+        filters: int, filters of the layer in a block.
+        blocks: int, blocks in the stacked blocks.
+        stride: int, stride of the first layer in the first block, defaults to
+            2.
+        name: string, optional prefix for the layer names used in the block.
+        block_type: string, one of "basic_block" or "block". The block type to
+              stack. Use "basic_block" for ResNet18 and ResNet34.
+        first_shortcut: bool. Use convolution shortcut if `True` (default),
+              otherwise uses identity or pooling shortcut, based on stride.
+
     Returns:
-      Output tensor for the stacked blocks.
+        Output tensor for the stacked blocks.
     """
+
     if name is None:
-        name = f"v1_stack_{backend.get_uid('v1_stack')}"
+        name = "v1_stack"
+
+    if block_type == "basic_block":
+        block_fn = apply_basic_block
+    elif block_type == "block":
+        block_fn = apply_block
+    else:
+        raise ValueError(
+            """`block_type` must be either "basic_block" or "block". """
+            f"Received block_type={block_type}."
+        )
 
-    def apply(x):
+    x = block_fn(
+        x,
+        filters,
+        stride=stride,
+        name=name + "_block1",
+        conv_shortcut=first_shortcut,
+    )
+    for i in range(2, blocks + 1):
         x = block_fn(
-            filters, stride=stride, name=name + "_block1", conv_shortcut=first_shortcut
-        )(x)
-        for i in range(2, blocks + 1):
-            x = block_fn(filters, conv_shortcut=False, name=name + "_block" + str(i))(x)
-        return x
-
-    return apply
-
-
-def ResNet(
-    stackwise_filters,
-    stackwise_blocks,
-    stackwise_strides,
-    include_rescaling,
-    include_top,
-    name="ResNet",
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classes=None,
-    classifier_activation="softmax",
-    block_fn=Block,
-    **kwargs,
-):
+            x, filters, conv_shortcut=False, name=name + "_block" + str(i)
+        )
+    return x
+
+
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class ResNetBackbone(Backbone):
     """Instantiates the ResNet architecture.
 
+    Reference:
+        - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
+
+    The difference in ResNetV1 and ResNetV2 rests in the structure of their
+    individual building blocks. In ResNetV2, the batch normalization and
+    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
+    the batch normalization and ReLU activation are applied after the
+    convolution layers.
+
+    For transfer learning use cases, make sure to read the
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+
     Args:
-        stackwise_filters: number of filters for each stack in the model.
-        stackwise_blocks: number of blocks for each stack in the model.
-        stackwise_strides: stride for each stack in the model.
-        include_rescaling: whether or not to Rescale the inputs. If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-            name: string, model name.
-        include_top: whether to include the fully-connected
-            layer at the top of the network.
-        weights: one of `None` (random initialization),
-            or the path to the weights file to be loaded.
+        stackwise_filters: list of ints, number of filters for each stack in
+            the model.
+        stackwise_blocks: list of ints, number of blocks for each stack in the
+            model.
+        stackwise_strides: list of ints, stride for each stack in the model.
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
         input_shape: optional shape tuple, defaults to (None, None, 3).
         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
             to use as image input for the model.
-        pooling: optional pooling mode for feature extraction
-            when `include_top` is `False`.
-            - `None` means that the output of the model will be
-                the 4D tensor output of the
-                last convolutional layer.
-            - `avg` means that global average pooling
-                will be applied to the output of the
-                last convolutional layer, and thus
-                the output of the model will be a 2D tensor.
-            - `max` means that global max pooling will
-                be applied.
-        classes: optional number of classes to classify images
-            into, only to be specified if `include_top` is True.
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
-        block_fn: callable, `Block` or `BasicBlock`, the block function to stack.
-            Use 'basic_block' for ResNet18 and ResNet34.
-        **kwargs: Pass-through keyword arguments to `tf.keras.Model`.
+        block_type: string, one of "basic_block" or "block". The block type to
+            stack. Use "basic_block" for ResNet18 and ResNet34.
 
-    Returns:
-      A `keras.Model` instance.
-    """
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either `None` or the path to the "
-            f"weights file to be loaded. Weights file not found at location: {weights}"
-        )
+    Examples:
+    ```python
+    input_data = tf.ones(shape=(8, 224, 224, 3))
+
+    # Pretrained backbone
+    model = keras_cv.models.ResNetBackbone.from_preset("resnet50_imagenet")
+    output = model(input_data)
+
+    # Randomly initialized backbone with a custom config
+    model = ResNetBackbone(
+        stackwise_filters=[64, 128, 256, 512],
+        stackwise_blocks=[2, 2, 2, 2],
+        stackwise_strides=[1, 2, 2, 2],
+        include_rescaling=False,
+    )
+    output = model(input_data)
+    ```
+    """  # noqa: E501
+
+    def __init__(
+        self,
+        *,
+        stackwise_filters,
+        stackwise_blocks,
+        stackwise_strides,
+        include_rescaling,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        block_type="block",
+        **kwargs,
+    ):
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
+        x = inputs
 
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, you should specify `classes`. "
-            f"Received: classes={classes}"
-        )
+        if include_rescaling:
+            x = layers.Rescaling(1 / 255.0)(x)
 
-    if include_top and pooling:
-        raise ValueError(
-            f"`pooling` must be `None` when `include_top=True`."
-            f"Received pooling={pooling} and include_top={include_top}. "
-        )
+        x = layers.Conv2D(
+            64, 7, strides=2, use_bias=False, padding="same", name="conv1_conv"
+        )(x)
 
-    inputs = utils.parse_model_inputs(input_shape, input_tensor)
-    x = inputs
+        x = layers.BatchNormalization(
+            axis=BN_AXIS, epsilon=BN_EPSILON, name="conv1_bn"
+        )(x)
+        x = layers.Activation("relu", name="conv1_relu")(x)
 
-    if include_rescaling:
-        x = layers.Rescaling(1 / 255.0)(x)
+        x = layers.MaxPooling2D(
+            3, strides=2, padding="same", name="pool1_pool"
+        )(x)
 
-    x = layers.Conv2D(
-        64, 7, strides=2, use_bias=False, padding="same", name="conv1_conv"
-    )(x)
+        num_stacks = len(stackwise_filters)
+
+        pyramid_level_inputs = {}
+        for stack_index in range(num_stacks):
+            x = apply_stack(
+                x,
+                filters=stackwise_filters[stack_index],
+                blocks=stackwise_blocks[stack_index],
+                stride=stackwise_strides[stack_index],
+                block_type=block_type,
+                first_shortcut=(block_type == "block" or stack_index > 0),
+                name=f"v2_stack_{stack_index}",
+            )
+            pyramid_level_inputs[stack_index + 2] = x.node.layer.name
+
+        # Create model.
+        super().__init__(inputs=inputs, outputs=x, **kwargs)
+
+        # All references to `self` below this line
+        self.pyramid_level_inputs = pyramid_level_inputs
+        self.stackwise_filters = stackwise_filters
+        self.stackwise_blocks = stackwise_blocks
+        self.stackwise_strides = stackwise_strides
+        self.include_rescaling = include_rescaling
+        self.input_tensor = input_tensor
+        self.block_type = block_type
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                "stackwise_filters": self.stackwise_filters,
+                "stackwise_blocks": self.stackwise_blocks,
+                "stackwise_strides": self.stackwise_strides,
+                "include_rescaling": self.include_rescaling,
+                # Remove batch dimension from `input_shape`
+                "input_shape": self.input_shape[1:],
+                "input_tensor": self.input_tensor,
+                "block_type": self.block_type,
+            }
+        )
+        return config
 
-    x = layers.BatchNormalization(axis=BN_AXIS, epsilon=1.001e-5, name="conv1_bn")(x)
-    x = layers.Activation("relu", name="conv1_relu")(x)
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return copy.deepcopy(backbone_presets)
 
-    x = layers.MaxPooling2D(3, strides=2, padding="same", name="pool1_pool")(x)
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return copy.deepcopy(backbone_presets_with_weights)
 
-    num_stacks = len(stackwise_filters)
 
-    stack_level_outputs = {}
-    for stack_index in range(num_stacks):
-        x = Stack(
-            filters=stackwise_filters[stack_index],
-            blocks=stackwise_blocks[stack_index],
-            stride=stackwise_strides[stack_index],
-            block_fn=block_fn,
-            first_shortcut=block_fn == Block or stack_index > 0,
-        )(x)
-        stack_level_outputs[stack_index + 2] = x
+ALIAS_DOCSTRING = """ResNetBackbone (V1) model with {num_layers} layers.
 
-    if include_top:
-        x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        x = layers.Dense(classes, activation=classifier_activation, name="predictions")(
-            x
-        )
-    else:
-        if pooling == "avg":
-            x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
-        elif pooling == "max":
-            x = layers.GlobalMaxPooling2D(name="max_pool")(x)
-
-    # Create model.
-    model = tf.keras.Model(inputs, x, name=name, **kwargs)
-
-    if weights is not None:
-        model.load_weights(weights)
-
-    # Set this private attribute for recreate backbone model with outputs at each of the
-    # resolution level.
-    model._backbone_level_outputs = stack_level_outputs
-
-    # Bind the `to_backbone_model` method to the application model.
-    model.as_backbone = types.MethodType(utils.as_backbone, model)
-
-    return model
-
-
-def ResNet18(
-    *,
-    include_rescaling,
-    include_top,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name="resnet18",
-    **kwargs,
-):
-    """Instantiates the ResNet18 architecture."""
+    Reference:
+        - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
 
-    return ResNet(
-        stackwise_filters=MODEL_CONFIGS["ResNet18"]["stackwise_filters"],
-        stackwise_blocks=MODEL_CONFIGS["ResNet18"]["stackwise_blocks"],
-        stackwise_strides=MODEL_CONFIGS["ResNet18"]["stackwise_strides"],
-        include_rescaling=include_rescaling,
-        include_top=include_top,
-        name=name,
-        weights=weights,
-        input_shape=input_shape,
-        input_tensor=input_tensor,
-        pooling=pooling,
-        classes=classes,
-        classifier_activation=classifier_activation,
-        block_fn=BasicBlock,
-        **kwargs,
-    )
+    The difference in ResNetV1 and ResNetV2 rests in the structure of their
+    individual building blocks. In ResNetV2, the batch normalization and
+    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
+    the batch normalization and ReLU activation are applied after the
+    convolution layers.
 
+    For transfer learning use cases, make sure to read the
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
-def ResNet34(
-    *,
-    include_rescaling,
-    include_top,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name="resnet34",
-    **kwargs,
-):
-    """Instantiates the ResNet34 architecture."""
+    Args:
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
+        input_shape: optional shape tuple, defaults to (None, None, 3).
+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+            to use as image input for the model.
 
-    return ResNet(
-        stackwise_filters=MODEL_CONFIGS["ResNet34"]["stackwise_filters"],
-        stackwise_blocks=MODEL_CONFIGS["ResNet34"]["stackwise_blocks"],
-        stackwise_strides=MODEL_CONFIGS["ResNet34"]["stackwise_strides"],
-        include_rescaling=include_rescaling,
-        include_top=include_top,
-        name=name,
-        weights=weights,
-        input_shape=input_shape,
-        input_tensor=input_tensor,
-        pooling=pooling,
-        classes=classes,
-        classifier_activation=classifier_activation,
-        block_fn=BasicBlock,
+    Examples:
+    ```python
+    input_data = tf.ones(shape=(8, 224, 224, 3))
+
+    # Randomly initialized backbone
+    model = ResNet{num_layers}Backbone()
+    output = model(input_data)
+    ```
+"""  # noqa: E501
+
+
+class ResNet18Backbone(ResNetBackbone):
+    def __new__(
+        cls,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
         **kwargs,
-    )
-
-
-def ResNet50(
-    *,
-    include_rescaling,
-    include_top,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name="resnet50",
-    **kwargs,
-):
-    """Instantiates the ResNet50 architecture."""
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
+        )
+        return ResNetBackbone.from_preset("resnet18", **kwargs)
 
-    return ResNet(
-        stackwise_filters=MODEL_CONFIGS["ResNet50"]["stackwise_filters"],
-        stackwise_blocks=MODEL_CONFIGS["ResNet50"]["stackwise_blocks"],
-        stackwise_strides=MODEL_CONFIGS["ResNet50"]["stackwise_strides"],
-        include_rescaling=include_rescaling,
-        include_top=include_top,
-        name=name,
-        weights=parse_weights(weights, include_top, "resnet50"),
-        input_shape=input_shape,
-        input_tensor=input_tensor,
-        pooling=pooling,
-        classes=classes,
-        classifier_activation=classifier_activation,
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {}
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
+
+class ResNet34Backbone(ResNetBackbone):
+    def __new__(
+        cls,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
         **kwargs,
-    )
-
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
+        )
+        return ResNetBackbone.from_preset("resnet34", **kwargs)
 
-def ResNet101(
-    *,
-    include_rescaling,
-    include_top,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name="resnet101",
-    **kwargs,
-):
-    """Instantiates the ResNet101 architecture."""
-    return ResNet(
-        stackwise_filters=MODEL_CONFIGS["ResNet101"]["stackwise_filters"],
-        stackwise_blocks=MODEL_CONFIGS["ResNet101"]["stackwise_blocks"],
-        stackwise_strides=MODEL_CONFIGS["ResNet101"]["stackwise_strides"],
-        name=name,
-        include_rescaling=include_rescaling,
-        include_top=include_top,
-        weights=weights,
-        input_shape=input_shape,
-        input_tensor=input_tensor,
-        pooling=pooling,
-        classes=classes,
-        classifier_activation=classifier_activation,
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {}
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
+
+class ResNet50Backbone(ResNetBackbone):
+    def __new__(
+        cls,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
         **kwargs,
-    )
-
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
+        )
+        return ResNetBackbone.from_preset("resnet50", **kwargs)
 
-def ResNet152(
-    *,
-    include_rescaling,
-    include_top,
-    classes=None,
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classifier_activation="softmax",
-    name="resnet152",
-    **kwargs,
-):
-    """Instantiates the ResNet152 architecture."""
-    return ResNet(
-        stackwise_filters=MODEL_CONFIGS["ResNet152"]["stackwise_filters"],
-        stackwise_blocks=MODEL_CONFIGS["ResNet152"]["stackwise_blocks"],
-        stackwise_strides=MODEL_CONFIGS["ResNet152"]["stackwise_strides"],
-        include_rescaling=include_rescaling,
-        include_top=include_top,
-        name=name,
-        weights=weights,
-        input_shape=input_shape,
-        input_tensor=input_tensor,
-        pooling=pooling,
-        classes=classes,
-        classifier_activation=classifier_activation,
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {
+            "resnet50_imagenet": copy.deepcopy(
+                backbone_presets["resnet50_imagenet"]
+            ),
+        }
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return cls.presets
+
+
+class ResNet101Backbone(ResNetBackbone):
+    def __new__(
+        cls,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
         **kwargs,
-    )
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
+        )
+        return ResNetBackbone.from_preset("resnet101", **kwargs)
 
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {}
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
+
+class ResNet152Backbone(ResNetBackbone):
+    def __new__(
+        self,
+        include_rescaling=True,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        **kwargs,
+    ):
+        # Pack args in kwargs
+        kwargs.update(
+            {
+                "include_rescaling": include_rescaling,
+                "input_shape": input_shape,
+                "input_tensor": input_tensor,
+            }
+        )
+        return ResNetBackbone.from_preset("resnet152", **kwargs)
 
-setattr(ResNet18, "__doc__", BASE_DOCSTRING.format(name="ResNet18"))
-setattr(ResNet34, "__doc__", BASE_DOCSTRING.format(name="ResNet34"))
-setattr(ResNet50, "__doc__", BASE_DOCSTRING.format(name="ResNet50"))
-setattr(ResNet101, "__doc__", BASE_DOCSTRING.format(name="ResNet101"))
-setattr(ResNet152, "__doc__", BASE_DOCSTRING.format(name="ResNet152"))
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return {}
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
+
+setattr(ResNet18Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=18))
+setattr(ResNet34Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=34))
+setattr(ResNet50Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=50))
+setattr(ResNet101Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=101))
+setattr(ResNet152Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=152))
```

## Comparing `keras_cv/models/resnet_v1_test.py` & `keras_cv/models/legacy/vit_test.py`

 * *Files 21% similar despite different names*

```diff
@@ -11,33 +11,38 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models import resnet_v1
+from keras_cv.models.legacy import vit
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
-    (resnet_v1.ResNet18, 512, {}),
+    (vit.ViTS16, 384, {"input_shape": (224, 224, 3)}),
 ]
 
 """
 Below are other configurations that we omit from our CI but that can/should
 be tested manually when making changes to this model.
-(resnet_v1.ResNet34, 512, {}),
-(resnet_v1.ResNet50, 2048, {}),
-(resnet_v1.ResNet101, 2048, {}),
-(resnet_v1.ResNet152, 2048, {}),
+(vit.ViTTiny16, 192, {"input_shape": (224, 224, 3)}),
+(vit.ViTB16, 768, {"input_shape": (224, 224, 3)}),
+(vit.ViTL16, 1024, {"input_shape": (224, 224, 3)}),
+(vit.ViTH16, 1280, {"input_shape": (224, 224, 3)}),
+(vit.ViTTiny32, 192, {"input_shape": (224, 224, 3)}),
+(vit.ViTS32, 384, {"input_shape": (224, 224, 3)}),
+(vit.ViTB32, 768, {"input_shape": (224, 224, 3)}),
+(vit.ViTL32, 1024, {"input_shape": (224, 224, 3)}),
+(vit.ViTH32, 1280, {"input_shape": (224, 224, 3)}),
 """
 
 
-class ResNetV1Test(ModelsTest, tf.test.TestCase, parameterized.TestCase):
+class ViTTest(ModelsTest, tf.test.TestCase, parameterized.TestCase):
     @parameterized.parameters(*MODEL_LIST)
     def test_application_base(self, app, _, args):
         super()._test_application_base(app, _, args)
 
     @parameterized.parameters(*MODEL_LIST)
     def test_application_with_rescaling(self, app, last_dim, args):
         super()._test_application_with_rescaling(app, last_dim, args)
```

## Comparing `keras_cv/models/vit.py` & `keras_cv/models/legacy/vit.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,26 +9,28 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """ViT (Vision Transformer) models for Keras.
 Reference:
-  - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2) (ICLR 2021)
-  - [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270) (CoRR 2021)
-"""
+  - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2)
+    (ICLR 2021)
+  - [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270)
+    (CoRR 2021)
+"""  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 
 from keras_cv.layers import TransformerEncoder
 from keras_cv.layers.vit_layers import PatchingAndEmbedding
-from keras_cv.models import utils
-from keras_cv.models.weights import parse_weights
+from keras_cv.models.legacy import utils
+from keras_cv.models.legacy.weights import parse_weights
 
 MODEL_CONFIGS = {
     "ViTTiny16": {
         "patch_size": 16,
         "transformer_layer_num": 12,
         "project_dim": 192,
         "mlp_dim": 768,
@@ -117,97 +119,84 @@
         "mlp_dropout": 0.1,
         "attention_dropout": 0.0,
     },
 }
 
 BASE_DOCSTRING = """Instantiates the {name} architecture.
     Reference:
-        - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2) (ICLR 2021)
+        - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2)
+        (ICLR 2021)
     This function returns a Keras {name} model.
 
-    The naming convention of ViT models follows: ViTSize_Patch-size (i.e. ViTS16).
+    The naming convention of ViT models follows: ViTSize_Patch-size
+        (i.e. ViTS16).
     The following sizes were released in the original paper:
         - S (Small)
         - B (Base)
         - L (Large)
     But subsequent work from the same authors introduced:
         - Ti (Tiny)
         - H (Huge)
 
-    The parameter configurations for all of these sizes, at patch sizes 16 and 32 are made available, following the naming convention
-    laid out above.
+    The parameter configurations for all of these sizes, at patch sizes 16 and
+    32 are made available, following the naming convention laid out above.
 
-    For transfer learning use cases, make sure to read the [guide to transfer
-        learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+    For transfer learning use cases, make sure to read the
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
     Args:
-        include_rescaling: whether or not to Rescale the inputs. If set to True,
-            inputs will be passed through a `Rescaling(scale=1./255.0)` layer. Note that ViTs
-            expect an input range of `[0..1]` if rescaling isn't used. Regardless of whether
-            you supply `[0..1]` or the input is rescaled to `[0..1]`, the inputs will further be
-            rescaled to `[-1..1]`.
-        include_top: whether to include the fully-connected layer at the top of the
-            network.  If provided, classes must be provided.
-        classes: optional number of classes to classify images into, only to be
-            specified if `include_top` is True.
+        include_rescaling: bool, whether to rescale the inputs. If set to
+            True, inputs will be passed through a `Rescaling(scale=1./255.0)`
+            layer. Note that ViTs expect an input range of `[0..1]` if rescaling
+            isn't used. Regardless of whether you supply `[0..1]` or the input
+            is rescaled to `[0..1]`, the inputs will further be rescaled to
+            `[-1..1]`.
+        include_top: bool, whether to include the fully-connected layer at the
+            top of the network. If provided, num_classes must be provided.
+        num_classes: optional int, number of classes to classify images into,
+            only to be specified if `include_top` is True.
         weights: one of `None` (random initialization), a pretrained weight file
-            path, or a reference to pre-trained weights (e.g. 'imagenet/classification')
-            (see available pre-trained weights in weights.py). Note that the 'imagenet'
-            weights only work on an input shape of (224, 224, 3) due to the input shape dependent
+            path, or a reference to pre-trained weights
+            (e.g. 'imagenet/classification') (see available pre-trained weights
+            in weights.py). Note that the 'imagenet' weights only work on an
+            input shape of (224, 224, 3) due to the input shape dependent
             patching and flattening logic.
         input_shape: optional shape tuple, defaults to (None, None, 3).
         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
             to use as image input for the model.
         pooling: optional pooling mode for feature extraction
             when `include_top` is `False`.
-            - `None` means that the output of the model will be the 4D tensor output
-                of the last convolutional block.
-            - `avg` means that global average pooling will be applied to the output
-                of the last convolutional block, and thus the output of the model will
-                be a 2D tensor.
+            - `None` means that the output of the model will be the 4D tensor
+                output of the last convolutional block.
+            - `avg` means that global average pooling will be applied to the
+                output of the last convolutional block, and thus the output of
+                the model will be a 2D tensor.
             - `max` means that global max pooling will be applied.
             - `token_pooling`, default, means that the token at the start of the
                 sequences is used instead of regular pooling.
-        name: (Optional) name to pass to the model.  Defaults to "{name}".
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
+        name: (Optional) name to pass to the model, defaults to "{name}".
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
     Returns:
       A `keras.Model` instance.
-"""
+"""  # noqa: E501
 
 
-def ViT(
-    include_rescaling,
-    include_top,
-    name="ViT",
-    weights=None,
-    input_shape=(None, None, 3),
-    input_tensor=None,
-    pooling=None,
-    classes=None,
-    patch_size=None,
-    transformer_layer_num=None,
-    num_heads=None,
-    mlp_dropout=None,
-    attention_dropout=None,
-    activation=None,
-    project_dim=None,
-    mlp_dim=None,
-    classifier_activation="softmax",
-    **kwargs,
-):
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class ViT(keras.Model):
     """Instantiates the ViT architecture.
 
     Args:
         mlp_dim: the dimensionality of the hidden Dense layer in the transformer
             MLP head
-        include_rescaling: whether or not to Rescale the inputs. If set to True,
-            inputs will be passed through a `Rescaling(1/255.0)` layer.
-            name: string, model name.
-        include_top: whether to include the fully-connected
+        include_rescaling: bool, whether to rescale the inputs. If set to
+            True, inputs will be passed through a `Rescaling(1/255.0)` layer.
+        name: string, model name.
+        include_top: bool, whether to include the fully-connected
             layer at the top of the network.
         weights: one of `None` (random initialization),
             or the path to the weights file to be loaded.
         input_shape: optional shape tuple, defaults to (None, None, 3).
         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
             to use as image input for the model.
         pooling: optional pooling mode for feature extraction
@@ -219,127 +208,191 @@
                 will be applied to the output of the
                 last convolutional layer, and thus
                 the output of the model will be a 2D tensor.
             - `max` means that global max pooling will
                 be applied.
             - `token_pooling`, default, means that the token at the start of the
                 sequences is used instead of regular pooling.
-        classes: optional number of classes to classify images
+        num_classes: optional number of classes to classify images
             into, only to be specified if `include_top` is True.
                     mlp_dim:
-        project_dim: the latent dimensionality to be projected into in the output
-            of each stacked transformer encoder
-        activation: the activation function to use in the first `layers.Dense` layer
-            in the MLP head of the transformer encoder
+        project_dim: the latent dimensionality to be projected into in the
+            output of each stacked transformer encoder
+        activation: the activation function to use in the first `layers.Dense`
+            layer in the MLP head of the transformer encoder
         attention_dropout: the dropout rate to apply to the `MultiHeadAttention`
             in each transformer encoder
         mlp_dropout: the dropout rate to apply between `layers.Dense` layers
             in the MLP head of the transformer encoder
         num_heads: the number of heads to use in the `MultiHeadAttention` layer
             of each transformer encoder
         transformer_layer_num: the number of transformer encoder layers to stack
             in the Vision Transformer
         patch_size: the patch size to be supplied to the Patching layer to turn
             input images into a flattened sequence of patches
-        classifier_activation: A `str` or callable. The activation function to use
-            on the "top" layer. Ignored unless `include_top=True`. Set
-            `classifier_activation=None` to return the logits of the "top" layer.
-        **kwargs: Pass-through keyword arguments to `tf.keras.Model`.
-
-    Returns:
-      A `keras.Model` instance.
+        classifier_activation: A `str` or callable. The activation function to
+            use on the "top" layer. Ignored unless `include_top=True`. Set
+            `classifier_activation=None` to return the logits of the "top"
+            layer.
+        **kwargs: Pass-through keyword arguments to `keras.Model`.
     """
 
-    if weights and not tf.io.gfile.exists(weights):
-        raise ValueError(
-            "The `weights` argument should be either `None` or the path to the "
-            "weights file to be loaded. Weights file not found at location: {weights}"
-        )
-
-    if include_top and not classes:
-        raise ValueError(
-            "If `include_top` is True, you should specify `classes`. "
-            f"Received: classes={classes}"
-        )
-
-    if include_top and pooling:
-        raise ValueError(
-            f"`pooling` must be `None` when `include_top=True`."
-            f"Received pooling={pooling} and include_top={include_top}. "
+    def __init__(
+        self,
+        include_rescaling,
+        include_top,
+        weights=None,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        pooling=None,
+        num_classes=None,
+        patch_size=None,
+        transformer_layer_num=None,
+        num_heads=None,
+        mlp_dropout=None,
+        attention_dropout=None,
+        activation=None,
+        project_dim=None,
+        mlp_dim=None,
+        classifier_activation="softmax",
+        **kwargs,
+    ):
+        if weights and not tf.io.gfile.exists(weights):
+            raise ValueError(
+                "The `weights` argument should be either `None` or the path "
+                "to the weights file to be loaded. Weights file not found at "
+                "location: {weights}"
+            )
+
+        if include_top and not num_classes:
+            raise ValueError(
+                "If `include_top` is True, you should specify `num_classes`. "
+                f"Received: num_classes={num_classes}"
+            )
+
+        if include_top and pooling:
+            raise ValueError(
+                f"`pooling` must be `None` when `include_top=True`."
+                f"Received pooling={pooling} and include_top={include_top}. "
+            )
+
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
+        x = inputs
+
+        if include_rescaling:
+            x = layers.Rescaling(1.0 / 255.0, name="rescaling")(x)
+
+        # The previous layer rescales [0..255] to [0..1] if applicable
+        # This one rescales [0..1] to [-1..1] since ViTs expect [-1..1]
+        x = layers.Rescaling(scale=1.0 / 0.5, offset=-1.0, name="rescaling_2")(
+            x
         )
 
-    inputs = utils.parse_model_inputs(input_shape, input_tensor)
-    x = inputs
-
-    if include_rescaling:
-        x = layers.Rescaling(1.0 / 255.0, name="rescaling")(x)
-
-    # The previous layer rescales [0..255] to [0..1] if applicable
-    # This one rescales [0..1] to [-1..1] since ViTs expect [-1..1]
-    x = layers.Rescaling(scale=1.0 / 0.5, offset=-1.0, name="rescaling_2")(x)
-
-    encoded_patches = PatchingAndEmbedding(project_dim, patch_size)(x)
-    encoded_patches = layers.Dropout(mlp_dropout)(encoded_patches)
-
-    for _ in range(transformer_layer_num):
-        encoded_patches = TransformerEncoder(
-            project_dim=project_dim,
-            mlp_dim=mlp_dim,
-            num_heads=num_heads,
-            mlp_dropout=mlp_dropout,
-            attention_dropout=attention_dropout,
-            activation=activation,
-        )(encoded_patches)
-
-    output = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
-
-    if include_top:
-        output = layers.Lambda(lambda rep: rep[:, 0])(output)
-        output = layers.Dense(classes, activation=classifier_activation)(output)
-
-    elif pooling == "token_pooling":
-        output = layers.Lambda(lambda rep: rep[:, 0])(output)
-    elif pooling == "avg":
-        output = layers.GlobalAveragePooling1D()(output)
-
-    model = keras.Model(inputs=inputs, outputs=output)
-
-    if weights is not None:
-        model.load_weights(weights)
+        encoded_patches = PatchingAndEmbedding(project_dim, patch_size)(x)
+        encoded_patches = layers.Dropout(mlp_dropout)(encoded_patches)
 
-    return model
+        for _ in range(transformer_layer_num):
+            encoded_patches = TransformerEncoder(
+                project_dim=project_dim,
+                mlp_dim=mlp_dim,
+                num_heads=num_heads,
+                mlp_dropout=mlp_dropout,
+                attention_dropout=attention_dropout,
+                activation=activation,
+            )(encoded_patches)
+
+        output = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
+
+        if include_top:
+            output = output[:, 0]
+            output = layers.Dense(
+                num_classes, activation=classifier_activation
+            )(output)
+
+        elif pooling == "token_pooling":
+            output = output[:, 0]
+        elif pooling == "avg":
+            output = layers.GlobalAveragePooling1D()(output)
+
+        # Create model.
+        super().__init__(inputs=inputs, outputs=output, **kwargs)
+
+        if weights is not None:
+            self.load_weights(weights)
+
+        self.include_rescaling = include_rescaling
+        self.include_top = include_top
+        self.input_tensor = input_tensor
+        self.pooling = pooling
+        self.num_classes = num_classes
+        self.patch_size = patch_size
+        self.transformer_layer_num = transformer_layer_num
+        self.num_heads = num_heads
+        self.mlp_dropout = mlp_dropout
+        self.attention_dropout = attention_dropout
+        self.activation = activation
+        self.project_dim = project_dim
+        self.mlp_dim = mlp_dim
+        self.classifier_activation = classifier_activation
+
+    def get_config(self):
+        return {
+            "include_rescaling": self.include_rescaling,
+            "include_top": self.include_top,
+            "name": self.name,
+            "input_shape": self.input_shape[1:],
+            "input_tensor": self.input_tensor,
+            "pooling": self.pooling,
+            "num_classes": self.num_classes,
+            "patch_size": self.patch_size,
+            "transformer_layer_num": self.transformer_layer_num,
+            "num_heads": self.num_heads,
+            "mlp_dropout": self.mlp_dropout,
+            "attention_dropout": self.attention_dropout,
+            "activation": self.activation,
+            "project_dim": self.project_dim,
+            "mlp_dim": self.mlp_dim,
+            "classifier_activation": self.classifier_activation,
+            "trainable": self.trainable,
+        }
+
+    @classmethod
+    def from_config(cls, config):
+        return cls(**config)
 
 
 def ViTTiny16(
     *,
     include_rescaling,
     include_top,
     name="ViTTiny16",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTTiny16 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=parse_weights(weights, include_top, "vittiny16"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTTiny16"]["patch_size"],
-        transformer_layer_num=MODEL_CONFIGS["ViTTiny16"]["transformer_layer_num"],
+        transformer_layer_num=MODEL_CONFIGS["ViTTiny16"][
+            "transformer_layer_num"
+        ],
         project_dim=MODEL_CONFIGS["ViTTiny16"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTTiny16"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTTiny16"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTTiny16"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTTiny16"]["attention_dropout"],
         activation=activation,
         classifier_activation=classifier_activation,
@@ -352,30 +405,30 @@
     include_rescaling,
     include_top,
     name="ViTS16",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTS16 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=parse_weights(weights, include_top, "vits16"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTS16"]["patch_size"],
         transformer_layer_num=MODEL_CONFIGS["ViTB32"]["transformer_layer_num"],
         project_dim=MODEL_CONFIGS["ViTS16"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTS16"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTS16"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTS16"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTS16"]["attention_dropout"],
@@ -390,30 +443,30 @@
     include_rescaling,
     include_top,
     name="ViTB16",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTB16 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=parse_weights(weights, include_top, "vitb16"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTB16"]["patch_size"],
         transformer_layer_num=MODEL_CONFIGS["ViTB16"]["transformer_layer_num"],
         project_dim=MODEL_CONFIGS["ViTB16"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTB16"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTB16"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTB16"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTB16"]["attention_dropout"],
@@ -428,30 +481,30 @@
     include_rescaling,
     include_top,
     name="ViTL16",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTL16 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=parse_weights(weights, include_top, "vitl16"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTL16"]["patch_size"],
         transformer_layer_num=MODEL_CONFIGS["ViTL16"]["transformer_layer_num"],
         project_dim=MODEL_CONFIGS["ViTL16"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTL16"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTL16"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTL16"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTL16"]["attention_dropout"],
@@ -466,30 +519,30 @@
     include_rescaling,
     include_top,
     name="ViTH16",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTH16 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=weights,
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTH16"]["patch_size"],
         transformer_layer_num=MODEL_CONFIGS["ViTH16"]["transformer_layer_num"],
         project_dim=MODEL_CONFIGS["ViTH16"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTH16"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTH16"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTH16"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTH16"]["attention_dropout"],
@@ -504,32 +557,34 @@
     include_rescaling,
     include_top,
     name="ViTTiny32",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTTiny32 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=weights,
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTTiny32"]["patch_size"],
-        transformer_layer_num=MODEL_CONFIGS["ViTTiny32"]["transformer_layer_num"],
+        transformer_layer_num=MODEL_CONFIGS["ViTTiny32"][
+            "transformer_layer_num"
+        ],
         project_dim=MODEL_CONFIGS["ViTTiny32"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTTiny32"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTTiny32"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTTiny32"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTTiny32"]["attention_dropout"],
         activation=activation,
         classifier_activation=classifier_activation,
@@ -542,30 +597,30 @@
     include_rescaling,
     include_top,
     name="ViTS32",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTS32 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=parse_weights(weights, include_top, "vits32"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTS32"]["patch_size"],
         transformer_layer_num=MODEL_CONFIGS["ViTS32"]["transformer_layer_num"],
         project_dim=MODEL_CONFIGS["ViTS32"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTS32"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTS32"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTS32"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTS32"]["attention_dropout"],
@@ -580,30 +635,30 @@
     include_rescaling,
     include_top,
     name="ViTB32",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTB32 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=parse_weights(weights, include_top, "vitb32"),
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTB32"]["patch_size"],
         transformer_layer_num=MODEL_CONFIGS["ViTB32"]["transformer_layer_num"],
         project_dim=MODEL_CONFIGS["ViTB32"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTB32"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTB32"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTB32"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTB32"]["attention_dropout"],
@@ -618,30 +673,30 @@
     include_rescaling,
     include_top,
     name="ViTL32",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTL32 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=weights,
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTL32"]["patch_size"],
         transformer_layer_num=MODEL_CONFIGS["ViTL32"]["transformer_layer_num"],
         project_dim=MODEL_CONFIGS["ViTL32"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTL32"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTL32"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTL32"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTL32"]["attention_dropout"],
@@ -656,30 +711,30 @@
     include_rescaling,
     include_top,
     name="ViTH32",
     weights=None,
     input_shape=(None, None, 3),
     input_tensor=None,
     pooling=None,
-    classes=None,
-    activation=tf.keras.activations.gelu,
+    num_classes=None,
+    activation=keras.activations.gelu,
     classifier_activation="softmax",
     **kwargs,
 ):
     """Instantiates the ViTH32 architecture."""
 
     return ViT(
         include_rescaling,
         include_top,
         name=name,
         weights=weights,
         input_shape=input_shape,
         input_tensor=input_tensor,
         pooling=pooling,
-        classes=classes,
+        num_classes=num_classes,
         patch_size=MODEL_CONFIGS["ViTH32"]["patch_size"],
         transformer_layer_num=MODEL_CONFIGS["ViTH32"]["transformer_layer_num"],
         project_dim=MODEL_CONFIGS["ViTH32"]["project_dim"],
         mlp_dim=MODEL_CONFIGS["ViTH32"]["mlp_dim"],
         num_heads=MODEL_CONFIGS["ViTH32"]["num_heads"],
         mlp_dropout=MODEL_CONFIGS["ViTH32"]["mlp_dropout"],
         attention_dropout=MODEL_CONFIGS["ViTH32"]["attention_dropout"],
```

## Comparing `keras_cv/models/__internal__/darknet_utils.py` & `keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -8,43 +8,44 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""DarkNet model utils for KerasCV.
+"""CSPDarkNet model utils for KerasCV.
 Reference:
   - [YoloV3 Paper](https://arxiv.org/abs/1804.02767)
   - [YoloV3 implementation](https://github.com/ultralytics/yolov3)
 """
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import backend
 from tensorflow.keras import layers
 
 
 def DarknetConvBlock(
     filters, kernel_size, strides, use_bias=False, activation="silu", name=None
 ):
-    """The basic conv block used in Darknet. Applies Conv2D followed by a BatchNorm.
+    """The basic conv block used in Darknet. Applies Conv2D followed by a
+    BatchNorm.
 
     Args:
-        filters: Integer, the dimensionality of the output space (i.e. the number of
-            output filters in the convolution).
-        kernel_size: An integer or tuple/list of 2 integers, specifying the height
-            and width of the 2D convolution window. Can be a single integer to specify
-            the same value both dimensions.
-        strides: An integer or tuple/list of 2 integers, specifying the strides of
-            the convolution along the height and width. Can be a single integer to
-            the same value both dimensions.
+        filters: Integer, the dimensionality of the output space (i.e. the
+            number of output filters in the convolution).
+        kernel_size: An integer or tuple/list of 2 integers, specifying the
+            height and width of the 2D convolution window. Can be a single
+            integer to specify the same value both dimensions.
+        strides: An integer or tuple/list of 2 integers, specifying the strides
+            of the convolution along the height and width. Can be a single
+            integer to the same value both dimensions.
         use_bias: Boolean, whether the layer uses a bias vector.
-        activation: the activation applied after the BatchNorm layer. One of "silu",
-            "relu" or "leaky_relu". Defaults to "silu".
+        activation: the activation applied after the BatchNorm layer. One of
+            "silu", "relu" or "leaky_relu", defaults to "silu".
         name: the prefix for the layer names used in the block.
     """
 
     if name is None:
         name = f"conv_block{backend.get_uid('conv_block')}"
 
     model_layers = [
@@ -69,16 +70,16 @@
     return keras.Sequential(model_layers, name=None)
 
 
 def ResidualBlocks(filters, num_blocks, name=None):
     """A residual block used in DarkNet models, repeated `num_blocks` times.
 
     Args:
-        filters: Integer, the dimensionality of the output spaces (i.e. the number of
-            output filters in used the blocks).
+        filters: Integer, the dimensionality of the output spaces (i.e. the
+            number of output filters in used the blocks).
         num_blocks: number of times the residual connections are repeated
         name: the prefix for the layer names used in the block.
 
     Returns:
         a function that takes an input Tensor representing a ResidualBlock.
     """
 
@@ -119,31 +120,37 @@
 
         return x
 
     return apply
 
 
 def SpatialPyramidPoolingBottleneck(
-    filters, hidden_filters=None, kernel_sizes=(5, 9, 13), activation="silu", name=None
+    filters,
+    hidden_filters=None,
+    kernel_sizes=(5, 9, 13),
+    activation="silu",
+    name=None,
 ):
     """Spatial pyramid pooling layer used in YOLOv3-SPP
 
     Args:
-        filters: Integer, the dimensionality of the output spaces (i.e. the number of
-            output filters in used the blocks).
-        hidden_filters: Integer, the dimensionality of the intermediate bottleneck space
-            (i.e. the number of output filters in the bottleneck convolution). If None,
-            it will be equal to filters. Defaults to None.
-        kernel_sizes: A list or tuple representing all the pool sizes used for the
-            pooling layers. Defaults to (5, 9, 13).
-        activation: Activation for the conv layers. Defaults to "silu".
+        filters: Integer, the dimensionality of the output spaces (i.e. the
+            number of output filters in used the blocks).
+        hidden_filters: Integer, the dimensionality of the intermediate
+            bottleneck space (i.e. the number of output filters in the
+            bottleneck convolution). If None, it will be equal to filters.
+            Defaults to None.
+        kernel_sizes: A list or tuple representing all the pool sizes used for
+            the pooling layers, defaults to (5, 9, 13).
+        activation: Activation for the conv layers, defaults to "silu".
         name: the prefix for the layer names used in the block.
 
     Returns:
-        a function that takes an input Tensor representing an SpatialPyramidPoolingBottleneck.
+        a function that takes an input Tensor representing an
+        SpatialPyramidPoolingBottleneck.
     """
     if name is None:
         name = f"spp{backend.get_uid('spp')}"
 
     if hidden_filters is None:
         hidden_filters = filters
 
@@ -183,66 +190,71 @@
 
 def DarknetConvBlockDepthwise(
     filters, kernel_size, strides, activation="silu", name=None
 ):
     """The depthwise conv block used in CSPDarknet.
 
     Args:
-        filters: Integer, the dimensionality of the output space (i.e. the number of
-            output filters in the final convolution).
-        kernel_size: An integer or tuple/list of 2 integers, specifying the height
-            and width of the 2D convolution window. Can be a single integer to specify
-            the same value both dimensions.
-        strides: An integer or tuple/list of 2 integers, specifying the strides of
-            the convolution along the height and width. Can be a single integer to
-            the same value both dimensions.
+        filters: Integer, the dimensionality of the output space (i.e. the
+            number of output filters in the final convolution).
+        kernel_size: An integer or tuple/list of 2 integers, specifying the
+            height and width of the 2D convolution window. Can be a single
+            integer to specify the same value both dimensions.
+        strides: An integer or tuple/list of 2 integers, specifying the strides
+            of the convolution along the height and width. Can be a single
+            integer to the same value both dimensions.
         activation: the activation applied after the final layer. One of "silu",
-            "relu" or "leaky_relu". Defaults to "silu".
+            "relu" or "leaky_relu", defaults to "silu".
         name: the prefix for the layer names used in the block.
 
     """
 
     if name is None:
         name = f"conv_block{backend.get_uid('conv_block')}"
 
     model_layers = [
-        layers.DepthwiseConv2D(kernel_size, strides, padding="same", use_bias=False),
+        layers.DepthwiseConv2D(
+            kernel_size, strides, padding="same", use_bias=False
+        ),
         layers.BatchNormalization(),
     ]
 
     if activation == "silu":
         model_layers.append(layers.Lambda(lambda x: keras.activations.swish(x)))
     elif activation == "relu":
         model_layers.append(layers.ReLU())
     elif activation == "leaky_relu":
         model_layers.append(layers.LeakyReLU(0.1))
 
     model_layers.append(
-        DarknetConvBlock(filters, kernel_size=1, strides=1, activation=activation)
+        DarknetConvBlock(
+            filters, kernel_size=1, strides=1, activation=activation
+        )
     )
 
     return keras.Sequential(model_layers, name=name)
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class CrossStagePartial(layers.Layer):
     """A block used in Cross Stage Partial Darknet.
 
     Args:
-        filters: Integer, the dimensionality of the output space (i.e. the number of
-            output filters in the final convolution).
-        num_bottlenecks: an integer representing the number of blocks added in the
-            layer bottleneck.
+        filters: Integer, the dimensionality of the output space (i.e. the
+            number of output filters in the final convolution).
+        num_bottlenecks: an integer representing the number of blocks added in
+            the layer bottleneck.
         residual: a boolean representing whether the value tensor before the
-            bottleneck should be added to the output of the bottleneck as a residual.
-            Defaults to True.
-        use_depthwise: a boolean value used to decide whether a depthwise conv block
-            should be used over a regular darknet block. Defaults to False
+            bottleneck should be added to the output of the bottleneck as a
+            residual, defaults to True.
+        use_depthwise: a boolean value used to decide whether a depthwise conv
+            block should be used over a regular darknet block, defaults to
+            False.
         activation: the activation applied after the final layer. One of "silu",
-            "relu" or "leaky_relu". Defaults to "silu".
+            "relu" or "leaky_relu", defaults to "silu".
     """
 
     def __init__(
         self,
         filters,
         num_bottlenecks,
         residual=True,
@@ -254,15 +266,17 @@
         self.filters = filters
         self.num_bottlenecks = num_bottlenecks
         self.residual = residual
         self.use_depthwise = use_depthwise
         self.activation = activation
 
         hidden_channels = filters // 2
-        ConvBlock = DarknetConvBlockDepthwise if use_depthwise else DarknetConvBlock
+        ConvBlock = (
+            DarknetConvBlockDepthwise if use_depthwise else DarknetConvBlock
+        )
 
         self.darknet_conv1 = DarknetConvBlock(
             hidden_channels,
             kernel_size=1,
             strides=1,
             activation=activation,
         )
@@ -327,26 +341,27 @@
             "activation": self.activation,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
 
 
 def Focus(name=None):
-    """A block used in CSPDarknet to focus information into channels of the image.
+    """A block used in CSPDarknet to focus information into channels of the
+    image.
 
-    If the dimensions of a batch input is (batch_size, width, height, channels), this
-    layer converts the image into size (batch_size, width/2, height/2, 4*channels).
-    See [the original discussion on YoloV5 Focus Layer](https://github.com/ultralytics/yolov5/discussions/3181).
+    If the dimensions of a batch input is (batch_size, width, height, channels),
+    this layer converts the image into size (batch_size, width/2, height/2,
+    4*channels). See [the original discussion on YoloV5 Focus Layer](https://github.com/ultralytics/yolov5/discussions/3181).
 
     Args:
         name: the name for the lambda layer used in the block.
 
     Returns:
         a function that takes an input Tensor representing a Focus layer.
-    """
+    """  # noqa: E501
 
     def apply(x):
         return layers.Lambda(
             lambda x: tf.concat(
                 [
                     x[..., ::2, ::2, :],
                     x[..., 1::2, ::2, :],
```

## Comparing `keras_cv/models/object_detection/faster_rcnn.py` & `keras_cv/models/object_detection/retinanet/retinanet.py`

 * *Files 21% similar despite different names*

```diff
@@ -8,576 +8,575 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import copy
+
+import numpy as np
 import tensorflow as tf
-from absl import logging
+from tensorflow import keras
 
 import keras_cv
 from keras_cv import bounding_box
 from keras_cv import layers as cv_layers
 from keras_cv.bounding_box.converters import _decode_deltas_to_boxes
-from keras_cv.bounding_box.utils import _clip_boxes
-from keras_cv.layers.object_detection.anchor_generator import AnchorGenerator
-from keras_cv.layers.object_detection.box_matcher import BoxMatcher
-from keras_cv.layers.object_detection.roi_align import _ROIAligner
-from keras_cv.layers.object_detection.roi_generator import ROIGenerator
-from keras_cv.layers.object_detection.roi_sampler import _ROISampler
-from keras_cv.layers.object_detection.rpn_label_encoder import _RpnLabelEncoder
+from keras_cv.models.backbones.backbone_presets import backbone_presets
+from keras_cv.models.backbones.backbone_presets import (
+    backbone_presets_with_weights,
+)
 from keras_cv.models.object_detection import predict_utils
 from keras_cv.models.object_detection.__internal__ import unpack_input
+from keras_cv.models.object_detection.retinanet import FeaturePyramid
+from keras_cv.models.object_detection.retinanet import PredictionHead
+from keras_cv.models.object_detection.retinanet import RetinaNetLabelEncoder
+from keras_cv.models.object_detection.retinanet.retinanet_presets import (
+    retinanet_presets,
+)
+from keras_cv.models.task import Task
+from keras_cv.utils.python_utils import classproperty
+from keras_cv.utils.train import get_feature_extractor
 
 BOX_VARIANCE = [0.1, 0.1, 0.2, 0.2]
 
 
-class FeaturePyramid(tf.keras.layers.Layer):
-    """Builds the Feature Pyramid with the feature maps from the backbone."""
-
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-        self.conv_c2_1x1 = tf.keras.layers.Conv2D(256, 1, 1, "same")
-        self.conv_c3_1x1 = tf.keras.layers.Conv2D(256, 1, 1, "same")
-        self.conv_c4_1x1 = tf.keras.layers.Conv2D(256, 1, 1, "same")
-        self.conv_c5_1x1 = tf.keras.layers.Conv2D(256, 1, 1, "same")
-
-        self.conv_c2_3x3 = tf.keras.layers.Conv2D(256, 3, 1, "same")
-        self.conv_c3_3x3 = tf.keras.layers.Conv2D(256, 3, 1, "same")
-        self.conv_c4_3x3 = tf.keras.layers.Conv2D(256, 3, 1, "same")
-        self.conv_c5_3x3 = tf.keras.layers.Conv2D(256, 3, 1, "same")
-        self.conv_c6_3x3 = tf.keras.layers.Conv2D(256, 3, 1, "same")
-        self.conv_c6_pool = tf.keras.layers.MaxPool2D()
-        self.upsample_2x = tf.keras.layers.UpSampling2D(2)
-
-    def call(self, inputs, training=None):
-        c2_output = inputs[2]
-        c3_output = inputs[3]
-        c4_output = inputs[4]
-        c5_output = inputs[5]
-
-        c6_output = self.conv_c6_pool(c5_output)
-        p6_output = c6_output
-        p5_output = self.conv_c5_1x1(c5_output)
-        p4_output = self.conv_c4_1x1(c4_output)
-        p3_output = self.conv_c3_1x1(c3_output)
-        p2_output = self.conv_c2_1x1(c2_output)
-
-        p4_output = p4_output + self.upsample_2x(p5_output)
-        p3_output = p3_output + self.upsample_2x(p4_output)
-        p2_output = p2_output + self.upsample_2x(p3_output)
-
-        p6_output = self.conv_c6_3x3(p6_output)
-        p5_output = self.conv_c5_3x3(p5_output)
-        p4_output = self.conv_c4_3x3(p4_output)
-        p3_output = self.conv_c3_3x3(p3_output)
-        p2_output = self.conv_c2_3x3(p2_output)
-
-        return {2: p2_output, 3: p3_output, 4: p4_output, 5: p5_output, 6: p6_output}
-
-    def get_config(self):
-        config = {}
-        base_config = super().get_config()
-        return dict(list(base_config.items()) + list(config.items()))
-
-
-class RPNHead(tf.keras.layers.Layer):
-    def __init__(
-        self,
-        num_anchors_per_location=3,
-        **kwargs,
-    ):
-        super().__init__(**kwargs)
-        self.num_anchors = num_anchors_per_location
-
-    def build(self, input_shape):
-        if isinstance(input_shape, (dict, list, tuple)):
-            input_shape = tf.nest.flatten(input_shape)
-            input_shape = input_shape[0]
-        filters = input_shape[-1]
-        self.conv = tf.keras.layers.Conv2D(
-            filters=filters,
-            kernel_size=3,
-            strides=1,
-            padding="same",
-            activation="relu",
-            kernel_initializer="truncated_normal",
-        )
-        self.objectness_logits = tf.keras.layers.Conv2D(
-            filters=self.num_anchors * 1,
-            kernel_size=1,
-            strides=1,
-            padding="same",
-            kernel_initializer="truncated_normal",
-        )
-        self.anchor_deltas = tf.keras.layers.Conv2D(
-            filters=self.num_anchors * 4,
-            kernel_size=1,
-            strides=1,
-            padding="same",
-            kernel_initializer="truncated_normal",
-        )
-
-    def call(self, feature_map, training=None):
-        def call_single_level(f_map):
-            batch_size = f_map.get_shape().as_list()[0] or tf.shape(f_map)[0]
-            # [BS, H, W, C]
-            t = self.conv(f_map)
-            # [BS, H, W, K]
-            rpn_scores = self.objectness_logits(t)
-            # [BS, H, W, K * 4]
-            rpn_boxes = self.anchor_deltas(t)
-            # [BS, H*W*K, 4]
-            rpn_boxes = tf.reshape(rpn_boxes, [batch_size, -1, 4])
-            # [BS, H*W*K, 1]
-            rpn_scores = tf.reshape(rpn_scores, [batch_size, -1, 1])
-            return rpn_boxes, rpn_scores
-
-        if not isinstance(feature_map, (dict, list, tuple)):
-            return call_single_level(feature_map)
-        elif isinstance(feature_map, (list, tuple)):
-            rpn_boxes = []
-            rpn_scores = []
-            for f_map in feature_map:
-                rpn_box, rpn_score = call_single_level(f_map)
-                rpn_boxes.append(rpn_box)
-                rpn_scores.append(rpn_score)
-            return rpn_boxes, rpn_scores
-        else:
-            rpn_boxes = {}
-            rpn_scores = {}
-            for lvl, f_map in feature_map.items():
-                rpn_box, rpn_score = call_single_level(f_map)
-                rpn_boxes[lvl] = rpn_box
-                rpn_scores[lvl] = rpn_score
-            return rpn_boxes, rpn_scores
-
-    def get_config(self):
-        config = {
-            "num_anchors_per_location": self.num_anchors,
-        }
-        base_config = super().get_config()
-        return dict(list(base_config.items()) + list(config.items()))
-
-
-# class agnostic regression
-class RCNNHead(tf.keras.layers.Layer):
-    def __init__(
-        self,
-        classes,
-        conv_dims=[],
-        fc_dims=[1024, 1024],
-        **kwargs,
-    ):
-        super().__init__(**kwargs)
-        self.num_classes = classes
-        self.conv_dims = conv_dims
-        self.fc_dims = fc_dims
-        self.convs = []
-        for conv_dim in conv_dims:
-            layer = tf.keras.layers.Conv2D(
-                filters=conv_dim,
-                kernel_size=3,
-                strides=1,
-                padding="same",
-                activation="relu",
-            )
-            self.convs.append(layer)
-        self.fcs = []
-        for fc_dim in fc_dims:
-            layer = tf.keras.layers.Dense(units=fc_dim, activation="relu")
-            self.fcs.append(layer)
-        self.box_pred = tf.keras.layers.Dense(units=4)
-        self.cls_score = tf.keras.layers.Dense(units=classes + 1, activation="softmax")
-
-    def call(self, feature_map, training=None):
-        x = feature_map
-        for conv in self.convs:
-            x = conv(x)
-        for fc in self.fcs:
-            x = fc(x)
-        rcnn_boxes = self.box_pred(x)
-        rcnn_scores = self.cls_score(x)
-        return rcnn_boxes, rcnn_scores
-
-    def get_config(self):
-        config = {
-            "classes": self.num_classes,
-            "conv_dims": self.conv_dims,
-            "fc_dims": self.fc_dims,
-        }
-        base_config = super().get_config()
-        return dict(list(base_config.items()) + list(config.items()))
-
-
-# TODO(tanzheny): add more configurations
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
-class FasterRCNN(tf.keras.Model):
-    """A Keras model implementing the FasterRCNN architecture.
+# TODO(jbischof): Generalize `FeaturePyramid` class to allow for any P-levels
+#  and add `feature_pyramid_levels` param.
+@keras.utils.register_keras_serializable(package="keras_cv")
+class RetinaNet(Task):
+    """A Keras model implementing the RetinaNet meta-architecture.
+
+    Implements the RetinaNet architecture for object detection. The constructor
+    requires `num_classes`, `bounding_box_format`, and a backbone. Optionally,
+    a custom label encoder, and prediction decoder may be provided.
 
-    Implements the FasterRCNN architecture for object detection.  The constructor
-    requires `classes`, `bounding_box_format` and a `backbone`.
-
-    References:
-        - [FasterRCNN](https://arxiv.org/pdf/1506.01497.pdf)
-
-    Usage:
+    Examples:
     ```python
-    retina_net = keras_cv.models.FasterRCNN(
-        classes=20,
+    images = tf.ones(shape=(1, 512, 512, 3))
+    labels = {
+        "boxes": [
+            [
+                [0, 0, 100, 100],
+                [100, 100, 200, 200],
+                [300, 300, 100, 100],
+            ]
+        ],
+        "classes": [[1, 1, 1]],
+    }
+    model = keras_cv.models.RetinaNet(
+        num_classes=20,
         bounding_box_format="xywh",
-        backbone=None,
+        backbone=keras_cv.models.ResNet50Backbone.from_preset(
+            "resnet50_imagenet"
+        )
     )
+
+    # Evaluate model
+    model(images)
+
+    # Train model
+    model.compile(
+        classification_loss='focal',
+        box_loss='smoothl1',
+        optimizer=tf.optimizers.SGD(global_clipnorm=10.0),
+        jit_compile=False,
+    )
+    model.fit(images, labels)
     ```
 
     Args:
-        classes: the number of classes in your dataset excluding the background
-            class.  Classes should be represented by integers in the range
-            [0, classes).
-        bounding_box_format: The format of bounding boxes of model output. Refer
+        num_classes: the number of classes in your dataset excluding the
+            background class. Classes should be represented by integers in the
+            range [0, num_classes).
+        bounding_box_format: The format of bounding boxes of input dataset.
+            Refer
             [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
             for more details on supported bounding box formats.
-        backbone: a `tf.keras.Model` custom backbone model. For now, only a backbone
-            with per level dict output is supported, for example, ResNet50 with FPN, which
-            uses the last conv block from stage 2 to stage 6 and add a max pooling at
-            stage 7. Defaults to `keras_cv.models.ResNet50`.
-        anchor_generator: (Optional) a `keras_cv.layers.AnchorGeneratot`. It is used
-            in the model to match ground truth boxes and labels with anchors, or with
-            region proposals. By default it uses the sizes and ratios from the paper,
-            that is optimized for image size between [640, 800]. The users should pass
-            their own anchor generator if the input image size differs from paper.
-            For now, only anchor generator with per level dict output is supported,
-        label_encoder: (Optional) a keras.Layer that accepts an anchors Tensor, a
-            bounding box Tensor and a bounding box class Tensor to its `call()` method,
-            and returns RetinaNet training targets. It returns box and class targets as
-            well as sample weights.
-        rpn_head: (Optional) a `keras.layers.Layer` that takes input feature map and
-            returns a box delta prediction (in reference to anchors) and binary prediction
-            (foreground vs background) with per level dict output is supported. By default
-            it uses the rpn head from paper, which is 3x3 conv followed by 1 box regressor
-            and 1 binary classifier.
-        rcnn_head: (Optional) a `keras.layers.Layer` that takes input feature map and
-            returns a box delta prediction (in reference to rois) and multi-class prediction
-            (all foreground classes + one background class). By default it uses the rcnn head
-            from paper, which is 2 FC layer with 1024 dimension, 1 box regressor and 1
-            softmax classifier.
-        prediction_decoder: (Optional) a `keras.layers.Layer` that takes input box prediction and
-            softmaxed score prediction, and returns NMSed box prediction, NMSed softmaxed
-            score prediction, NMSed class prediction, and NMSed valid detection.
-    """
+        backbone: `keras.Model`. Must implement the `pyramid_level_inputs`
+            property with keys 3, 4, and 5 and layer names as values. A somewhat
+            sensible backbone to use in many cases is the:
+            `keras_cv.models.ResNetBackbone.from_preset("resnet50_imagenet")`
+        anchor_generator: (Optional) a `keras_cv.layers.AnchorGenerator`. If
+            provided, the anchor generator will be passed to both the
+            `label_encoder` and the `prediction_decoder`. Only to be used when
+            both `label_encoder` and `prediction_decoder` are both `None`.
+            Defaults to an anchor generator with the parameterization:
+            `strides=[2**i for i in range(3, 8)]`,
+            `scales=[2**x for x in [0, 1 / 3, 2 / 3]]`,
+            `sizes=[32.0, 64.0, 128.0, 256.0, 512.0]`,
+            and `aspect_ratios=[0.5, 1.0, 2.0]`.
+        label_encoder: (Optional) a keras.Layer that accepts an image Tensor, a
+            bounding box Tensor and a bounding box class Tensor to its `call()`
+            method, and returns RetinaNet training targets. By default, a
+            KerasCV standard `RetinaNetLabelEncoder` is created and used.
+            Results of this object's `call()` method are passed to the `loss`
+            object for `box_loss` and `classification_loss` the `y_true`
+            argument.
+        prediction_decoder: (Optional)  A `keras.layers.Layer` that is
+            responsible for transforming RetinaNet predictions into usable
+            bounding box Tensors. If not provided, a default is provided. The
+            default `prediction_decoder` layer is a
+            `keras_cv.layers.MultiClassNonMaxSuppression` layer, which uses
+            a Non-Max Suppression for box pruning.
+        classification_head: (Optional) A `keras.Layer` that performs
+            classification of the bounding boxes. If not provided, a simple
+            ConvNet with 3 layers will be used.
+        box_head: (Optional) A `keras.Layer` that performs regression of the
+            bounding boxes. If not provided, a simple ConvNet with 3 layers
+            will be used.
+    """  # noqa: E501
 
     def __init__(
         self,
-        classes,
+        backbone,
+        num_classes,
         bounding_box_format,
-        backbone=None,
         anchor_generator=None,
         label_encoder=None,
-        feature_pyramid=None,
-        rpn_head=None,
-        rcnn_head=None,
         prediction_decoder=None,
+        classification_head=None,
+        box_head=None,
         **kwargs,
     ):
-        self.bounding_box_format = bounding_box_format
-        super().__init__(**kwargs)
-        scales = [2**x for x in [0]]
-        aspect_ratios = [0.5, 1.0, 2.0]
-        self.anchor_generator = anchor_generator or AnchorGenerator(
-            bounding_box_format="yxyx",
-            sizes={2: 32.0, 3: 64.0, 4: 128.0, 5: 256.0, 6: 512.0},
-            scales=scales,
-            aspect_ratios=aspect_ratios,
-            strides={i: 2**i for i in range(2, 7)},
-            clip_boxes=True,
+        if anchor_generator is not None and (
+            prediction_decoder or label_encoder
+        ):
+            raise ValueError(
+                "`anchor_generator` is only to be provided when "
+                "both `label_encoder` and `prediction_decoder` are both "
+                f"`None`. Received `anchor_generator={anchor_generator}` "
+                f"`label_encoder={label_encoder}`, "
+                f"`prediction_decoder={prediction_decoder}`. To customize the "
+                "behavior of the anchor_generator inside of a custom "
+                "`label_encoder` or custom `prediction_decoder` you should "
+                "provide both to `RetinaNet`, and ensure that the "
+                "`anchor_generator` provided to both is identical"
+            )
+        anchor_generator = (
+            anchor_generator
+            or RetinaNet.default_anchor_generator(bounding_box_format)
+        )
+        label_encoder = label_encoder or RetinaNetLabelEncoder(
+            bounding_box_format=bounding_box_format,
+            anchor_generator=anchor_generator,
+            box_variance=BOX_VARIANCE,
         )
-        self.rpn_head = rpn_head or RPNHead(
-            num_anchors_per_location=len(scales) * len(aspect_ratios)
+
+        extractor_levels = [3, 4, 5]
+        extractor_layer_names = [
+            backbone.pyramid_level_inputs[i] for i in extractor_levels
+        ]
+        feature_extractor = get_feature_extractor(
+            backbone, extractor_layer_names, extractor_levels
+        )
+        feature_pyramid = FeaturePyramid()
+
+        prior_probability = keras.initializers.Constant(
+            -np.log((1 - 0.01) / 0.01)
+        )
+        classification_head = classification_head or PredictionHead(
+            output_filters=9 * num_classes,
+            bias_initializer=prior_probability,
+        )
+        box_head = box_head or PredictionHead(
+            output_filters=9 * 4, bias_initializer=keras.initializers.Zeros()
+        )
+
+        # Begin construction of forward pass
+        images = keras.layers.Input(feature_extractor.input_shape[1:])
+
+        backbone_outputs = feature_extractor(images)
+        features = feature_pyramid(backbone_outputs)
+
+        batch_size = tf.shape(images)[0]
+        cls_pred = []
+        box_pred = []
+        for feature in features:
+            box_pred.append(tf.reshape(box_head(feature), [batch_size, -1, 4]))
+            cls_pred.append(
+                tf.reshape(
+                    classification_head(feature),
+                    [batch_size, -1, num_classes],
+                )
+            )
+
+        cls_pred = keras.layers.Concatenate(axis=1, name="classification")(
+            cls_pred
         )
-        self.roi_generator = ROIGenerator(
-            bounding_box_format="yxyx",
-            nms_score_threshold_train=float("-inf"),
-            nms_score_threshold_test=float("-inf"),
-        )
-        self.box_matcher = BoxMatcher(thresholds=[0.0, 0.5], match_values=[-2, -1, 1])
-        self.roi_sampler = _ROISampler(
-            bounding_box_format="yxyx",
-            roi_matcher=self.box_matcher,
-            background_class=classes,
-            num_sampled_rois=512,
-        )
-        self.roi_pooler = _ROIAligner(bounding_box_format="yxyx")
-        self.rcnn_head = rcnn_head or RCNNHead(classes)
-        self.backbone = (
-            backbone
-            or keras_cv.models.ResNet50(
-                include_top=False, include_rescaling=True
-            ).as_backbone()
-        )
-        self.feature_pyramid = FeaturePyramid()
-        self.rpn_labeler = label_encoder or _RpnLabelEncoder(
-            anchor_format="yxyx",
-            ground_truth_box_format="yxyx",
-            positive_threshold=0.7,
-            negative_threshold=0.3,
-            samples_per_image=256,
-            positive_fraction=0.5,
-            box_variance=BOX_VARIANCE,
+        box_pred = keras.layers.Concatenate(axis=1, name="box")(box_pred)
+        # box_pred is always in "center_yxhw" delta-encoded no matter what
+        # format you pass in.
+
+        inputs = images
+        outputs = {"box": box_pred, "classification": cls_pred}
+
+        super().__init__(
+            inputs=inputs,
+            outputs=outputs,
+            **kwargs,
         )
+        self.label_encoder = label_encoder
+        self.anchor_generator = anchor_generator
+        self.bounding_box_format = bounding_box_format
+        self.num_classes = num_classes
+        self.backbone = backbone
+
+        self.feature_extractor = feature_extractor
         self._prediction_decoder = (
             prediction_decoder
             or cv_layers.MultiClassNonMaxSuppression(
                 bounding_box_format=bounding_box_format,
-                from_logits=False,
-                max_detections_per_class=10,
-                max_detections=10,
+                from_logits=True,
             )
         )
 
-    def _call_rpn(self, images, anchors, training=None):
-        image_shape = tf.shape(images[0])
-        feature_map = self.backbone(images, training=training)
-        feature_map = self.feature_pyramid(feature_map, training=training)
-        # [BS, num_anchors, 4], [BS, num_anchors, 1]
-        rpn_boxes, rpn_scores = self.rpn_head(feature_map, training=training)
-        # the decoded format is center_xywh, convert to yxyx
-        decoded_rpn_boxes = _decode_deltas_to_boxes(
-            anchors=anchors,
-            boxes_delta=rpn_boxes,
-            anchor_format="yxyx",
-            box_format="yxyx",
-            variance=BOX_VARIANCE,
+        self.feature_pyramid = feature_pyramid
+        self.classification_head = classification_head
+        self.box_head = box_head
+        self.build(backbone.input_shape)
+
+    def make_predict_function(self, force=False):
+        return predict_utils.make_predict_function(self, force=force)
+
+    @property
+    def prediction_decoder(self):
+        return self._prediction_decoder
+
+    @prediction_decoder.setter
+    def prediction_decoder(self, prediction_decoder):
+        if prediction_decoder.bounding_box_format != self.bounding_box_format:
+            raise ValueError(
+                "Expected `prediction_decoder` and RetinaNet to "
+                "use the same `bounding_box_format`, but got "
+                "`prediction_decoder.bounding_box_format="
+                f"{prediction_decoder.bounding_box_format}`, and "
+                "`self.bounding_box_format="
+                f"{self.bounding_box_format}`."
+            )
+        self._prediction_decoder = prediction_decoder
+        self.make_predict_function(force=True)
+        self.make_train_function(force=True)
+        self.make_test_function(force=True)
+
+    @staticmethod
+    def default_anchor_generator(bounding_box_format):
+        strides = [2**i for i in range(3, 8)]
+        scales = [2**x for x in [0, 1 / 3, 2 / 3]]
+        sizes = [32.0, 64.0, 128.0, 256.0, 512.0]
+        aspect_ratios = [0.5, 1.0, 2.0]
+        return cv_layers.AnchorGenerator(
+            bounding_box_format=bounding_box_format,
+            sizes=sizes,
+            aspect_ratios=aspect_ratios,
+            scales=scales,
+            strides=strides,
+            clip_boxes=True,
         )
-        rois, _ = self.roi_generator(decoded_rpn_boxes, rpn_scores, training=training)
-        rois = _clip_boxes(rois, "yxyx", image_shape)
-        rpn_boxes = tf.concat(tf.nest.flatten(rpn_boxes), axis=1)
-        rpn_scores = tf.concat(tf.nest.flatten(rpn_scores), axis=1)
-        return rois, feature_map, rpn_boxes, rpn_scores
-
-    def _call_rcnn(self, rois, feature_map, training=None):
-        feature_map = self.roi_pooler(feature_map, rois)
-        # [BS, H*W*K, pool_shape*C]
-        feature_map = tf.reshape(
-            feature_map, tf.concat([tf.shape(rois)[:2], [-1]], axis=0)
-        )
-        # [BS, H*W*K, 4], [BS, H*W*K, num_classes + 1]
-        rcnn_box_pred, rcnn_cls_pred = self.rcnn_head(feature_map, training=training)
-        return rcnn_box_pred, rcnn_cls_pred
 
-    def call(self, images, training=None):
+    def decode_predictions(self, predictions, images):
+        box_pred, cls_pred = predictions["box"], predictions["classification"]
+        # box_pred is on "center_yxhw" format, convert to target format.
         image_shape = tf.shape(images[0])
         anchors = self.anchor_generator(image_shape=image_shape)
-        rois, feature_map, _, _ = self._call_rpn(images, anchors, training=training)
-        box_pred, cls_pred = self._call_rcnn(rois, feature_map, training=training)
-        if not training:
-            # box_pred is on "center_yxhw" format, convert to target format.
-            box_pred = _decode_deltas_to_boxes(
-                anchors=rois,
-                boxes_delta=box_pred,
-                anchor_format="yxyx",
-                box_format=self.bounding_box_format,
-                variance=[0.1, 0.1, 0.2, 0.2],
-            )
+        anchors = tf.concat(tf.nest.flatten(anchors), axis=0)
 
-        return box_pred, cls_pred
+        box_pred = _decode_deltas_to_boxes(
+            anchors=anchors,
+            boxes_delta=box_pred,
+            anchor_format=self.anchor_generator.bounding_box_format,
+            box_format=self.bounding_box_format,
+            variance=BOX_VARIANCE,
+            image_shape=image_shape,
+        )
+        # box_pred is now in "self.bounding_box_format" format
+        box_pred = bounding_box.convert_format(
+            box_pred,
+            source=self.bounding_box_format,
+            target=self.prediction_decoder.bounding_box_format,
+            image_shape=image_shape,
+        )
+        y_pred = self.prediction_decoder(
+            box_pred, cls_pred, image_shape=image_shape
+        )
+        y_pred["boxes"] = bounding_box.convert_format(
+            y_pred["boxes"],
+            source=self.prediction_decoder.bounding_box_format,
+            target=self.bounding_box_format,
+            image_shape=image_shape,
+        )
+        return y_pred
 
-    # TODO(tanzhenyu): Support compile with metrics.
     def compile(
         self,
         box_loss=None,
         classification_loss=None,
-        rpn_box_loss=None,
-        rpn_classification_loss=None,
         weight_decay=0.0001,
         loss=None,
+        metrics=None,
         **kwargs,
     ):
-        # TODO(tanzhenyu): Add metrics support once COCOMap issue is addressed.
-        # https://github.com/keras-team/keras-cv/issues/915
-        if "metrics" in kwargs.keys():
-            raise ValueError(
-                "`FasterRCNN` does not currently support the use of "
-                "`metrics` due to performance and distribution concerns. "
-                "Please use the `PyCOCOCallback` to evaluate COCO metrics."
-            )
+        """compiles the RetinaNet.
+
+        compile() mirrors the standard Keras compile() method, but has a few key
+        distinctions. Primarily, all metrics must support bounding boxes, and
+        two losses must be provided: `box_loss` and `classification_loss`.
+
+        Args:
+            box_loss: a Keras loss to use for box offset regression.
+                Preconfigured losses are provided when the string "huber" or
+                "smoothl1" are passed.
+            classification_loss: a Keras loss to use for box classification.
+                A preconfigured `FocalLoss` is provided when the string "focal"
+                is passed.
+            weight_decay: a float for variable weight decay.
+            metrics: KerasCV object detection metrics that accept decoded
+                bounding boxes as their inputs. Examples of this metric type
+                are `keras_cv.metrics.BoxRecall()` and
+                `keras_cv.metrics.BoxMeanAveragePrecision()`. When `metrics` are
+                included in the call to `compile()`, the RetinaNet will perform
+                non-max suppression decoding during the forward pass. By
+                default, the RetinaNet uses a
+                `keras_cv.layers.MultiClassNonMaxSuppression()` layer to
+                perform decoding. This behavior can be customized by passing in
+                a `prediction_decoder` to the constructor or by modifying the
+                `prediction_decoder` attribute on the model. It should be noted
+                that the default non-max suppression operation does not have
+                TPU support, and thus when training on TPU metrics must be
+                evaluated in a `keras.utils.SidecarEvaluator` or a
+                `keras.callbacks.Callback`.
+            kwargs: most other `keras.Model.compile()` arguments are supported
+                and propagated to the `keras.Model` class.
+        """
         if loss is not None:
             raise ValueError(
-                "`FasterRCNN` does not accept a `loss` to `compile()`. "
+                "`RetinaNet` does not accept a `loss` to `compile()`. "
                 "Instead, please pass `box_loss` and `classification_loss`. "
                 "`loss` will be ignored during training."
             )
-        box_loss = _validate_and_get_loss(box_loss, "box_loss")
-        classification_loss = _validate_and_get_loss(
-            classification_loss, "classification_loss"
-        )
-        rpn_box_loss = _validate_and_get_loss(rpn_box_loss, "rpn_box_loss")
-        if rpn_classification_loss == "BinaryCrossentropy":
-            rpn_classification_loss = tf.keras.losses.BinaryCrossentropy(
-                from_logits=True, reduction=tf.keras.losses.Reduction.SUM
-            )
-        rpn_classification_loss = _validate_and_get_loss(
-            rpn_classification_loss, "rpn_cls_loss"
-        )
-        if not rpn_classification_loss.from_logits:
-            raise ValueError(
-                "`rpn_classification_loss` must come with `from_logits`=True"
-            )
+        box_loss = _parse_box_loss(box_loss)
+        classification_loss = _parse_classification_loss(classification_loss)
+
+        if hasattr(classification_loss, "from_logits"):
+            if not classification_loss.from_logits:
+                raise ValueError(
+                    "RetinaNet.compile() expects `from_logits` to be True for "
+                    "`classification_loss`. Got "
+                    "`classification_loss.from_logits="
+                    f"{classification_loss.from_logits}`"
+                )
+        if hasattr(box_loss, "bounding_box_format"):
+            if box_loss.bounding_box_format != self.bounding_box_format:
+                raise ValueError(
+                    "Wrong `bounding_box_format` passed to `box_loss` in "
+                    "`RetinaNet.compile()`. Got "
+                    "`box_loss.bounding_box_format="
+                    f"{box_loss.bounding_box_format}`, want "
+                    "`box_loss.bounding_box_format="
+                    f"{self.bounding_box_format}`"
+                )
 
-        self.rpn_box_loss = rpn_box_loss
-        self.rpn_cls_loss = rpn_classification_loss
         self.box_loss = box_loss
-        self.cls_loss = classification_loss
+        self.classification_loss = classification_loss
         self.weight_decay = weight_decay
         losses = {
             "box": self.box_loss,
-            "classification": self.cls_loss,
-            "rpn_box": self.rpn_box_loss,
-            "rpn_classification": self.rpn_cls_loss,
+            "classification": self.classification_loss,
         }
+        self._has_user_metrics = metrics is not None and len(metrics) != 0
+        self._user_metrics = metrics
         super().compile(loss=losses, **kwargs)
 
-    def compute_loss(self, images, boxes, classes, training):
-        image_shape = tf.shape(images[0])
-        local_batch = images.get_shape().as_list()[0]
-        if tf.distribute.has_strategy():
-            num_sync = tf.distribute.get_strategy().num_replicas_in_sync
-        else:
-            num_sync = 1
-        global_batch = local_batch * num_sync
-        anchors = self.anchor_generator(image_shape=image_shape)
-        (
-            rpn_box_targets,
-            rpn_box_weights,
-            rpn_cls_targets,
-            rpn_cls_weights,
-        ) = self.rpn_labeler(
-            tf.concat(tf.nest.flatten(anchors), axis=0), boxes, classes
-        )
-        rpn_box_weights /= self.rpn_labeler.samples_per_image * global_batch * 0.25
-        rpn_cls_weights /= self.rpn_labeler.samples_per_image * global_batch
-        rois, feature_map, rpn_box_pred, rpn_cls_pred = self._call_rpn(
-            images, anchors, training=training
-        )
-        rois = tf.stop_gradient(rois)
-        rois, box_targets, box_weights, cls_targets, cls_weights = self.roi_sampler(
-            rois, boxes, classes
-        )
-        box_weights /= self.roi_sampler.num_sampled_rois * global_batch * 0.25
-        cls_weights /= self.roi_sampler.num_sampled_rois * global_batch
-        box_pred, cls_pred = self._call_rcnn(rois, feature_map, training=training)
+    def compute_loss(self, x, box_pred, cls_pred, boxes, classes):
+        if boxes.shape[-1] != 4:
+            raise ValueError(
+                "boxes should have shape (None, None, 4). Got "
+                f"boxes.shape={tuple(boxes.shape)}"
+            )
+
+        if box_pred.shape[-1] != 4:
+            raise ValueError(
+                "box_pred should have shape (None, None, 4). Got "
+                f"box_pred.shape={tuple(box_pred.shape)}. Does your model's "
+                "`num_classes` parameter match your losses `num_classes` "
+                "parameter?"
+            )
+        if cls_pred.shape[-1] != self.num_classes:
+            raise ValueError(
+                "cls_pred should have shape (None, None, 4). Got "
+                f"cls_pred.shape={tuple(cls_pred.shape)}. Does your model's "
+                "`num_classes` parameter match your losses `num_classes` "
+                "parameter?"
+            )
+
+        cls_labels = tf.one_hot(
+            tf.cast(classes, dtype=tf.int32),
+            depth=self.num_classes,
+            dtype=tf.float32,
+        )
+
+        positive_mask = tf.cast(tf.greater(classes, -1.0), dtype=tf.float32)
+        normalizer = tf.reduce_sum(positive_mask)
+        cls_weights = tf.cast(
+            tf.math.not_equal(classes, -2.0), dtype=tf.float32
+        )
+        cls_weights /= normalizer
+        box_weights = positive_mask / normalizer
         y_true = {
-            "rpn_box": rpn_box_targets,
-            "rpn_classification": rpn_cls_targets,
-            "box": box_targets,
-            "classification": cls_targets,
+            "box": boxes,
+            "classification": cls_labels,
         }
         y_pred = {
-            "rpn_box": rpn_box_pred,
-            "rpn_classification": rpn_cls_pred,
             "box": box_pred,
             "classification": cls_pred,
         }
-        weights = {
-            "rpn_box": rpn_box_weights,
-            "rpn_classification": rpn_cls_weights,
+        sample_weights = {
             "box": box_weights,
             "classification": cls_weights,
         }
         return super().compute_loss(
-            x=images, y=y_true, y_pred=y_pred, sample_weight=weights
+            x=x, y=y_true, y_pred=y_pred, sample_weight=sample_weights
         )
 
     def train_step(self, data):
-        images, y = unpack_input(data)
-
-        boxes = y["boxes"]
-        if len(y["classes"].shape) != 2:
-            raise ValueError(
-                "Expected 'classes' to be a tf.Tensor of rank 2. "
-                f"Got y['classes'].shape={y['classes'].shape}."
-            )
-        # TODO(tanzhenyu): remove this hack and perform broadcasting elsewhere
-        classes = tf.expand_dims(y["classes"], axis=-1)
+        x, y = unpack_input(data)
+        y_for_label_encoder = bounding_box.convert_format(
+            y,
+            source=self.bounding_box_format,
+            target=self.label_encoder.bounding_box_format,
+            images=x,
+        )
+        boxes, classes = self.label_encoder(x, y_for_label_encoder)
+        # boxes are now in `center_yxhw`. This is always the case in training
         with tf.GradientTape() as tape:
-            total_loss = self.compute_loss(images, boxes, classes, training=True)
+            outputs = self(x, training=True)
+            box_pred, cls_pred = outputs["box"], outputs["classification"]
+            total_loss = self.compute_loss(
+                x, box_pred, cls_pred, boxes, classes
+            )
+
             reg_losses = []
             if self.weight_decay:
                 for var in self.trainable_variables:
                     if "bn" not in var.name:
-                        reg_losses.append(self.weight_decay * tf.nn.l2_loss(var))
+                        reg_losses.append(
+                            self.weight_decay * tf.nn.l2_loss(var)
+                        )
                 l2_loss = tf.math.add_n(reg_losses)
             total_loss += l2_loss
-        self.optimizer.minimize(total_loss, self.trainable_variables, tape=tape)
-        return self.compute_metrics(images, {}, {}, sample_weight={})
-
-    def test_step(self, data):
-        images, y = unpack_input(data)
-
-        boxes = y["boxes"]
-        if len(y["classes"].shape) != 2:
-            raise ValueError(
-                "Expected 'classes' to be a tf.Tensor of rank 2. "
-                f"Got y['classes'].shape={y['classes'].shape}."
-            )
-        classes = tf.expand_dims(y["classes"], axis=-1)
-        self.compute_loss(images, boxes, classes, training=False)
-        return self.compute_metrics(images, {}, {}, sample_weight={})
+        # Training specific code
+        trainable_vars = self.trainable_variables
+        gradients = tape.gradient(total_loss, trainable_vars)
+        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
 
-    def make_predict_function(self, force=False):
-        return predict_utils.make_predict_function(self, force=force)
+        if not self._has_user_metrics:
+            return super().compute_metrics(x, {}, {}, sample_weight={})
 
-    @property
-    def prediction_decoder(self):
-        return self._prediction_decoder
+        y_pred = self.decode_predictions(outputs, x)
+        return self.compute_metrics(x, y, y_pred, sample_weight=None)
 
-    @prediction_decoder.setter
-    def prediction_decoder(self, prediction_decoder):
-        self._prediction_decoder = prediction_decoder
-        self.make_predict_function(force=True)
-
-    def decode_predictions(self, predictions, images):
-        # no-op if default decoder is used.
-        box_pred, scores_pred = predictions
-        box_pred = bounding_box.convert_format(
-            box_pred,
+    def test_step(self, data):
+        x, y = unpack_input(data)
+        y_for_label_encoder = bounding_box.convert_format(
+            y,
             source=self.bounding_box_format,
-            target=self.prediction_decoder.bounding_box_format,
-            images=images,
+            target=self.label_encoder.bounding_box_format,
+            images=x,
         )
-        y_pred = self.prediction_decoder(box_pred, scores_pred[..., :-1])
-        box_pred = bounding_box.convert_format(
-            y_pred["boxes"],
-            source=self.prediction_decoder.bounding_box_format,
+        boxes, classes = self.label_encoder(x, y_for_label_encoder)
+        boxes = bounding_box.convert_format(
+            boxes,
+            source=self.label_encoder.bounding_box_format,
             target=self.bounding_box_format,
-            images=images,
+            images=x,
         )
-        y_pred["boxes"] = box_pred
-        return y_pred
+
+        outputs = self(x, training=False)
+        box_pred, cls_pred = outputs["box"], outputs["classification"]
+        _ = self.compute_loss(x, box_pred, cls_pred, boxes, classes)
+
+        if not self._has_user_metrics:
+            return super().compute_metrics(x, {}, {}, sample_weight={})
+        y_pred = self.decode_predictions(outputs, x)
+        return self.compute_metrics(x, y, y_pred, sample_weight=None)
+
+    def compute_metrics(self, x, y, y_pred, sample_weight):
+        metrics = {}
+        metrics.update(super().compute_metrics(x, {}, {}, sample_weight={}))
+
+        for metric in self._user_metrics:
+            metric.update_state(y, y_pred, sample_weight=sample_weight)
+
+        for metric in self._user_metrics:
+            result = metric.result()
+            if isinstance(result, dict):
+                metrics.update(result)
+            else:
+                metrics[metric.name] = result
+        return metrics
 
     def get_config(self):
         return {
-            "classes": self.classes,
+            "num_classes": self.num_classes,
             "bounding_box_format": self.bounding_box_format,
-            "backbone": self.backbone,
-            "anchor_generator": self.anchor_generator,
-            "label_encoder": self.rpn_labeler,
+            "backbone": keras.utils.serialize_keras_object(self.backbone),
+            "label_encoder": self.label_encoder,
             "prediction_decoder": self._prediction_decoder,
-            "feature_pyramid": self.feature_pyramid,
-            "rpn_head": self.rpn_head,
-            "rcnn_head": self.rcnn_head,
+            "classification_head": self.classification_head,
+            "box_head": self.box_head,
         }
 
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return copy.deepcopy({**backbone_presets, **retinanet_presets})
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return copy.deepcopy(
+            {**backbone_presets_with_weights, **retinanet_presets}
+        )
+
+    @classproperty
+    def backbone_presets(cls):
+        """Dictionary of preset names and configurations of compatible
+        backbones."""
+        return copy.deepcopy(backbone_presets)
+
+
+def _parse_box_loss(loss):
+    if not isinstance(loss, str):
+        # support arbitrary callables
+        return loss
+
+    # case insensitive comparison
+    if loss.lower() == "smoothl1":
+        return keras_cv.losses.SmoothL1Loss(
+            l1_cutoff=1.0, reduction=keras.losses.Reduction.SUM
+        )
+    if loss.lower() == "huber":
+        return keras.losses.Huber(reduction=keras.losses.Reduction.SUM)
+
+    raise ValueError(
+        "Expected `box_loss` to be either a Keras Loss, "
+        f"callable, or the string 'SmoothL1'. Got loss={loss}."
+    )
+
 
-def _validate_and_get_loss(loss, loss_name):
-    if isinstance(loss, str):
-        loss = tf.keras.losses.get(loss)
-    if loss is None or not isinstance(loss, tf.keras.losses.Loss):
-        raise ValueError(
-            f"FasterRCNN only accepts `tf.keras.losses.Loss` for {loss_name}, got {loss}"
-        )
-    if loss.reduction != tf.keras.losses.Reduction.SUM:
-        logging.info(
-            f"FasterRCNN only accepts `SUM` reduction, got {loss.reduction}, automatically converted."
+def _parse_classification_loss(loss):
+    if not isinstance(loss, str):
+        # support arbitrary callables
+        return loss
+
+    # case insensitive comparison
+    if loss.lower() == "focal":
+        return keras_cv.losses.FocalLoss(
+            from_logits=True, reduction=keras.losses.Reduction.SUM
         )
-        loss.reduction = tf.keras.losses.Reduction.SUM
-    return loss
+
+    raise ValueError(
+        "Expected `classification_loss` to be either a Keras Loss, "
+        f"callable, or the string 'Focal'. Got loss={loss}."
+    )
```

## Comparing `keras_cv/models/object_detection/retina_net/__init__.py` & `keras_cv/models/backbones/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
```

## Comparing `keras_cv/models/object_detection/retina_net/retina_net_test.py` & `keras_cv/models/object_detection/retinanet/retinanet_test.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,223 +8,213 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import copy
 import os
 
 import pytest
 import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras import optimizers
 
 import keras_cv
-from keras_cv.models.object_detection.__test_utils__ import _create_bounding_box_dataset
+from keras_cv.models.object_detection.__test_utils__ import (
+    _create_bounding_box_dataset,
+)
 
 
 class RetinaNetTest(tf.test.TestCase):
     @pytest.fixture(autouse=True)
     def cleanup_global_session(self):
         # Code before yield runs before the test
         tf.config.set_soft_device_placement(False)
         yield
-        # Reset soft device placement to not interfere with other unit test files
+        # Reset soft device placement to not interfere with other unit test
+        # files
         tf.config.set_soft_device_placement(True)
-        tf.keras.backend.clear_session()
+        keras.backend.clear_session()
 
-    def test_retina_net_construction(self):
-        retina_net = keras_cv.models.RetinaNet(
-            classes=20,
+    def test_retinanet_construction(self):
+        retinanet = keras_cv.models.RetinaNet(
+            num_classes=20,
             bounding_box_format="xywh",
-            backbone=self.build_backbone(),
+            backbone=keras_cv.models.ResNet50V2Backbone(),
         )
-        retina_net.compile(
+        retinanet.compile(
             classification_loss="focal",
             box_loss="smoothl1",
             optimizer="adam",
         )
 
         # TODO(lukewood) uncomment when using keras_cv.models.ResNet50
-        # self.assertIsNotNone(retina_net.backbone.get_layer(name="rescaling"))
+        # self.assertIsNotNone(retinanet.backbone.get_layer(name="rescaling"))
         # TODO(lukewood): test compile with the FocalLoss class
 
     @pytest.mark.skipif(
         "INTEGRATION" not in os.environ or os.environ["INTEGRATION"] != "true",
         reason="Takes a long time to run, only runs when INTEGRATION "
-        "environment variable is set.  To run the test please run: \n"
+        "environment variable is set. To run the test please run: \n"
         "`INTEGRATION=true pytest keras_cv/",
     )
-    def test_retina_net_call(self):
-        retina_net = keras_cv.models.RetinaNet(
-            classes=20,
+    def test_retinanet_call(self):
+        retinanet = keras_cv.models.RetinaNet(
+            num_classes=20,
             bounding_box_format="xywh",
-            backbone=self.build_backbone(),
+            backbone=keras_cv.models.ResNet50V2Backbone(),
         )
         images = tf.random.uniform((2, 512, 512, 3))
-        _ = retina_net(images)
-        _ = retina_net.predict(images)
+        _ = retinanet(images)
+        _ = retinanet.predict(images)
 
     def test_wrong_logits(self):
-        retina_net = keras_cv.models.RetinaNet(
-            classes=2,
+        retinanet = keras_cv.models.RetinaNet(
+            num_classes=2,
             bounding_box_format="xywh",
-            backbone=self.build_backbone(),
+            backbone=keras_cv.models.ResNet50V2Backbone(),
         )
 
         with self.assertRaisesRegex(
             ValueError,
             "from_logits",
         ):
-            retina_net.compile(
+            retinanet.compile(
                 optimizer=optimizers.SGD(learning_rate=0.25),
                 classification_loss=keras_cv.losses.FocalLoss(
                     from_logits=False, reduction="none"
                 ),
-                box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0, reduction="none"),
+                box_loss=keras_cv.losses.SmoothL1Loss(
+                    l1_cutoff=1.0, reduction="none"
+                ),
             )
 
     def test_no_metrics(self):
-        retina_net = keras_cv.models.RetinaNet(
-            classes=2,
+        retinanet = keras_cv.models.RetinaNet(
+            num_classes=2,
             bounding_box_format="xywh",
-            backbone=self.build_backbone(),
+            backbone=keras_cv.models.ResNet50V2Backbone(),
         )
 
-        retina_net.compile(
+        retinanet.compile(
             optimizer=optimizers.SGD(learning_rate=0.25),
             classification_loss=keras_cv.losses.FocalLoss(
                 from_logits=True, reduction="none"
             ),
-            box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0, reduction="none"),
+            box_loss=keras_cv.losses.SmoothL1Loss(
+                l1_cutoff=1.0, reduction="none"
+            ),
         )
 
     def test_weights_contained_in_trainable_variables(self):
         bounding_box_format = "xywh"
-        retina_net = keras_cv.models.RetinaNet(
-            classes=1,
+        retinanet = keras_cv.models.RetinaNet(
+            num_classes=2,
             bounding_box_format=bounding_box_format,
-            backbone=self.build_backbone(),
+            backbone=keras_cv.models.ResNet50V2Backbone(),
         )
-        retina_net.backbone.trainable = False
-        retina_net.compile(
+        retinanet.backbone.trainable = False
+        retinanet.compile(
             optimizer=optimizers.Adam(),
             classification_loss=keras_cv.losses.FocalLoss(
                 from_logits=True, reduction="none"
             ),
-            box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0, reduction="none"),
+            box_loss=keras_cv.losses.SmoothL1Loss(
+                l1_cutoff=1.0, reduction="none"
+            ),
         )
         xs, ys = _create_bounding_box_dataset(bounding_box_format)
 
         # call once
-        _ = retina_net(xs)
-        variable_names = [x.name for x in retina_net.trainable_variables]
+        _ = retinanet(xs)
+        variable_names = [x.name for x in retinanet.trainable_variables]
         # classification_head
-        self.assertIn("RetinaNet/prediction_head/conv2d_8/kernel:0", variable_names)
+        self.assertIn("prediction_head/conv2d_8/kernel:0", variable_names)
         # box_head
-        self.assertIn("RetinaNet/prediction_head_1/conv2d_12/kernel:0", variable_names)
+        self.assertIn("prediction_head_1/conv2d_12/kernel:0", variable_names)
 
     def test_weights_change(self):
         bounding_box_format = "xywh"
-        retina_net = keras_cv.models.RetinaNet(
-            classes=1,
+        retinanet = keras_cv.models.RetinaNet(
+            num_classes=2,
             bounding_box_format=bounding_box_format,
-            backbone=self.build_backbone(),
+            backbone=keras_cv.models.ResNet50V2Backbone(),
         )
 
-        retina_net.compile(
+        retinanet.compile(
             optimizer=optimizers.Adam(),
             classification_loss=keras_cv.losses.FocalLoss(
                 from_logits=True, reduction="none"
             ),
-            box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0, reduction="none"),
+            box_loss=keras_cv.losses.SmoothL1Loss(
+                l1_cutoff=1.0, reduction="none"
+            ),
         )
         xs, ys = _create_bounding_box_dataset(bounding_box_format)
 
         # call once
-        _ = retina_net(xs)
-        original_fpn_weights = retina_net.feature_pyramid.get_weights()
-        original_box_head_weights = retina_net.box_head.get_weights()
+        _ = retinanet(xs)
+        original_fpn_weights = retinanet.feature_pyramid.get_weights()
+        original_box_head_weights = retinanet.box_head.get_weights()
         original_classification_head_weights = (
-            retina_net.classification_head.get_weights()
+            retinanet.classification_head.get_weights()
         )
 
-        retina_net.fit(x=xs, y=ys, epochs=1)
-        fpn_after_fit = retina_net.feature_pyramid.get_weights()
-        box_head_after_fit_weights = retina_net.box_head.get_weights()
+        retinanet.fit(x=xs, y=ys, epochs=1)
+        fpn_after_fit = retinanet.feature_pyramid.get_weights()
+        box_head_after_fit_weights = retinanet.box_head.get_weights()
         classification_head_after_fit_weights = (
-            retina_net.classification_head.get_weights()
+            retinanet.classification_head.get_weights()
         )
 
         for w1, w2 in zip(
-            original_classification_head_weights, classification_head_after_fit_weights
+            original_classification_head_weights,
+            classification_head_after_fit_weights,
         ):
             self.assertNotAllClose(w1, w2)
 
-        for w1, w2 in zip(original_box_head_weights, box_head_after_fit_weights):
+        for w1, w2 in zip(
+            original_box_head_weights, box_head_after_fit_weights
+        ):
             self.assertNotAllClose(w1, w2)
 
         for w1, w2 in zip(original_fpn_weights, fpn_after_fit):
             self.assertNotAllClose(w1, w2)
 
-    # TODO(lukewood): configure for other coordinate systems.
-    @pytest.mark.skipif(
-        "INTEGRATION" not in os.environ or os.environ["INTEGRATION"] != "true",
-        reason="Takes a long time to run, only runs when INTEGRATION "
-        "environment variable is set.  To run the test please run: \n"
-        "`INTEGRATION=true pytest "
-        "keras_cv/models/object_detection/retina_net/retina_net_test.py -k "
-        "test_fit_coco_metrics -s`",
-    )
-    def test_fit_coco_metrics(self):
-        bounding_box_format = "xywh"
-        retina_net = keras_cv.models.RetinaNet(
-            classes=1,
-            bounding_box_format=bounding_box_format,
-            backbone=self.build_backbone(),
-        )
-
-        retina_net.compile(
-            optimizer=optimizers.Adam(),
-            classification_loss=keras_cv.losses.FocalLoss(
-                from_logits=True, reduction="none"
-            ),
-            box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0, reduction="none"),
+    def test_serialization(self):
+        # TODO(haifengj): Reuse test code from
+        # ModelTest._test_model_serialization.
+        model = keras_cv.models.RetinaNet(
+            num_classes=20,
+            bounding_box_format="xywh",
+            backbone=keras_cv.models.ResNet50V2Backbone(),
         )
-
-        xs, ys = _create_bounding_box_dataset(bounding_box_format)
-        retina_net.fit(x=xs, y=ys, epochs=1)
-        _ = retina_net.predict(xs)
-
-    @pytest.mark.skipif(
-        "INTEGRATION" not in os.environ or os.environ["INTEGRATION"] != "true",
-        reason="Takes a long time to run, only runs when INTEGRATION "
-        "environment variable is set.  To run the test please run: \n"
-        "`INTEGRATION=true pytest keras_cv/",
-    )
-    def test_retina_net_with_dictionary_input_format(self):
-        retina_net = keras_cv.models.RetinaNet(
-            classes=20,
+        serialized_1 = keras.utils.serialize_keras_object(model)
+        restored = keras.utils.deserialize_keras_object(
+            copy.deepcopy(serialized_1)
+        )
+        serialized_2 = keras.utils.serialize_keras_object(restored)
+        self.assertEqual(serialized_1, serialized_2)
+
+
+@pytest.mark.large
+class RetinaNetSmokeTest(tf.test.TestCase):
+    def test_backbone_preset_weight_loading(self):
+        # Check that backbone preset weights loaded correctly
+        # TODO(lukewood): need to forward pass test once proper weights are
+        # implemented
+        keras_cv.models.RetinaNet.from_preset(
+            "resnet50_v2_imagenet",
+            num_classes=20,
             bounding_box_format="xywh",
-            backbone=self.build_backbone(),
         )
 
-        images, boxes = _create_bounding_box_dataset("xywh")
-        dataset = tf.data.Dataset.from_tensor_slices(
-            {"images": images, "bounding_boxes": boxes}
-        ).batch(5, drop_remainder=True)
-
-        retina_net.compile(
-            optimizer=optimizers.Adam(),
-            classification_loss=keras_cv.losses.FocalLoss(
-                from_logits=True, reduction="none"
-            ),
-            box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0, reduction="none"),
+    def test_full_preset_weight_loading(self):
+        # Check that backbone preset weights loaded correctly
+        # TODO(lukewood): need to forward pass test once proper weights are
+        # implemented
+        keras_cv.models.RetinaNet.from_preset(
+            "retinanet_resnet50_pascalvoc",
+            bounding_box_format="xywh",
         )
-
-        retina_net.fit(dataset, epochs=1)
-        retina_net.evaluate(dataset)
-
-    def build_backbone(self):
-        return keras_cv.models.ResNet50(
-            include_top=False, include_rescaling=False
-        ).as_backbone()
```

## Comparing `keras_cv/models/object_detection/retina_net/__internal__/__init__.py` & `keras_cv/models/backbones/csp_darknet/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
```

## Comparing `keras_cv/models/object_detection/retina_net/__internal__/layers/__init__.py` & `keras_cv/models/object_detection/retinanet/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -7,14 +7,16 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-from keras_cv.models.object_detection.retina_net.__internal__.layers.feature_pyramid import (
+from keras_cv.models.object_detection.retinanet.feature_pyramid import (
     FeaturePyramid,
 )
-from keras_cv.models.object_detection.retina_net.__internal__.layers.prediction_head import (
+from keras_cv.models.object_detection.retinanet.prediction_head import (
     PredictionHead,
 )
+from keras_cv.models.object_detection.retinanet.retinanet_label_encoder import (  # noqa: E501
+    RetinaNetLabelEncoder,
+)
```

## Comparing `keras_cv/models/object_detection/retina_net/__internal__/layers/feature_pyramid.py` & `keras_cv/models/object_detection/retinanet/feature_pyramid.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from tensorflow import keras
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class FeaturePyramid(keras.layers.Layer):
     """Builds the Feature Pyramid with the feature maps from the backbone."""
 
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, "same")
         self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, "same")
```

## Comparing `keras_cv/models/object_detection/retina_net/__internal__/layers/prediction_head.py` & `keras_cv/models/object_detection/retinanet/prediction_head.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,63 +8,78 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras import layers
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class PredictionHead(layers.Layer):
     """The class/box predictions head.
 
     Arguments:
       output_filters: Number of convolution filters in the final layer.
       bias_initializer: Bias Initializer for the final convolution layer.
 
     Returns:
       A function representing either the classification
         or the box regression head depending on `output_filters`.
     """
 
-    def __init__(self, output_filters, bias_initializer, num_conv_layers=3, **kwargs):
+    def __init__(
+        self, output_filters, bias_initializer, num_conv_layers=3, **kwargs
+    ):
         super().__init__(**kwargs)
         self.output_filters = output_filters
         self.bias_initializer = bias_initializer
         self.num_conv_layers = num_conv_layers
 
         self.conv_layers = [
             layers.Conv2D(
                 256,
                 kernel_size=3,
                 padding="same",
-                kernel_initializer=tf.keras.initializers.Orthogonal(),
+                kernel_initializer=keras.initializers.Orthogonal(),
                 activation="relu",
             )
             for _ in range(num_conv_layers)
         ]
         self.prediction_layer = layers.Conv2D(
             self.output_filters,
             kernel_size=3,
             strides=1,
             padding="same",
-            kernel_initializer=tf.keras.initializers.Orthogonal(),
+            kernel_initializer=keras.initializers.Orthogonal(),
             bias_initializer=self.bias_initializer,
         )
 
     def call(self, x, training=False):
         for layer in self.conv_layers:
             x = layer(x, training=training)
         x = self.prediction_layer(x, training=training)
         return x
 
     def get_config(self):
         config = {
-            "bias_initializer": self.bias_initializer,
+            "bias_initializer": keras.initializers.serialize(
+                self.bias_initializer
+            ),
             "output_filters": self.output_filters,
             "num_conv_layers": self.num_conv_layers,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
+
+    @classmethod
+    def from_config(cls, config):
+        config.update(
+            {
+                "bias_initializer": keras.initializers.deserialize(
+                    config["bias_initializer"]
+                )
+            }
+        )
+        return super().from_config(config)
```

## Comparing `keras_cv/models/segmentation/__init__.py` & `keras_cv/models/backbones/efficientnet_v2/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,15 +1,13 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-from keras_cv.models.segmentation.deeplab import DeepLabV3
```

## Comparing `keras_cv/models/segmentation/deeplab.py` & `keras_cv/models/legacy/segmentation/deeplab.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,105 +13,107 @@
 # limitations under the License.
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 
 from keras_cv.layers.spatial_pyramid import SpatialPyramidPooling
-from keras_cv.models import utils
-from keras_cv.models.weights import parse_weights
+from keras_cv.models.legacy import utils
+from keras_cv.models.legacy.weights import parse_weights
 
 
 @keras.utils.register_keras_serializable(package="keras_cv")
 class DeepLabV3(keras.Model):
-    """
-    A segmentation model based on the DeepLab v3.
+    # TODO: add a code example in the docstring.
+    """A segmentation model based on DeepLab v3.
 
     Args:
-        classes: int, the number of classes for the detection model. Note that
-            the classes doesn't contain the background class, and the classes
-            from the data should be represented by integers with range
-            [0, classes).
-        backbone: an optional backbone network for the model. Should be a KerasCV model.
-        weights: weights for the complete DeepLabV3 model. one of `None` (random
+        num_classes: int, the number of classes for the detection model. Note
+            that the num_classes doesn't contain the background class, and the
+            classes from the data should be represented by integers with range
+            [0, num_classes).
+        backbone: Optional backbone network for the model. Should be a KerasCV
+            model.
+        weights: Weights for the complete DeepLabV3 model. one of `None` (random
             initialization), a pretrained weight file path, or a reference to
-            pre-trained weights (e.g. 'imagenet/classification' or 'voc/segmentation') (see available
-            pre-trained weights in weights.py)
-        spatial_pyramid_pooling: also known as Atrous Spatial Pyramid Pooling (ASPP).
-            Performs spatial pooling on different spatial levels in the pyramid, with
-            dilation.
-        segmentation_head: an optional `tf.keras.Layer` that predict the segmentation
+            pre-trained weights (e.g. 'imagenet/classification' or
+            'voc/segmentation') (see available pre-trained weights in
+            weights.py)
+        spatial_pyramid_pooling: Also known as Atrous Spatial Pyramid Pooling
+            (ASPP). Performs spatial pooling on different spatial levels in the
+            pyramid, with dilation.
+        segmentation_head: Optional `keras.Layer` that predict the segmentation
             mask based on feature from backbone and feature from decoder.
     """
 
-    def build(self, input_shape):
-        height = input_shape[1]
-        width = input_shape[2]
-        feature_map_shape = self.backbone.compute_output_shape(input_shape)
-        self.up_layer = tf.keras.layers.UpSampling2D(
-            size=(height // feature_map_shape[1], width // feature_map_shape[2]),
-            interpolation="bilinear",
-        )
-
     def __init__(
         self,
-        classes,
+        num_classes,
         backbone,
         spatial_pyramid_pooling=None,
         segmentation_head=None,
         segmentation_head_activation="softmax",
+        weight_decay=0.0001,
         input_shape=(None, None, 3),
         input_tensor=None,
         weights=None,
         **kwargs,
     ):
-        if not isinstance(backbone, tf.keras.layers.Layer):
+        if not isinstance(backbone, keras.layers.Layer):
             raise ValueError(
-                "Backbone need to be a `tf.keras.layers.Layer`, " f"received {backbone}"
+                "Argument `backbone` must be a `keras.layers.Layer` instance. "
+                f"Received instead backbone={backbone} (of type "
+                f"{type(backbone)})."
             )
 
         if weights and not tf.io.gfile.exists(
             parse_weights(weights, True, "deeplabv3")
         ):
             raise ValueError(
-                "The `weights` argument should be either `None` or the path to the "
-                "weights file to be loaded. Weights file not found at location: {weights}"
+                "The `weights` argument should be either `None` or the path "
+                "to the weights file to be loaded. Weights file not found at "
+                "location: {weights}"
             )
 
         inputs = utils.parse_model_inputs(input_shape, input_tensor)
 
         if input_shape[0] is None and input_shape[1] is None:
             input_shape = backbone.input_shape[1:]
             inputs = layers.Input(tensor=input_tensor, shape=input_shape)
 
         if input_shape[0] is None and input_shape[1] is None:
             raise ValueError(
-                "Input shapes for both the backbone and DeepLabV3 are `None`."
+                "Input shapes for both the backbone and DeepLabV3 cannot be "
+                "`None`. Received: input_shape={input_shape} and "
+                "backbone.input_shape={backbone.input_shape[1:]}"
             )
 
         height = input_shape[0]
         width = input_shape[1]
 
-        x = inputs
-
-        feature_map = backbone(x)
+        feature_map = backbone(inputs)
         if spatial_pyramid_pooling is None:
-            spatial_pyramid_pooling = SpatialPyramidPooling(dilation_rates=[6, 12, 18])
+            spatial_pyramid_pooling = SpatialPyramidPooling(
+                dilation_rates=[6, 12, 18]
+            )
 
         output = spatial_pyramid_pooling(feature_map)
-        output = tf.keras.layers.UpSampling2D(
-            size=(height // feature_map.shape[1], width // feature_map.shape[2]),
+        output = keras.layers.UpSampling2D(
+            size=(
+                height // feature_map.shape[1],
+                width // feature_map.shape[2],
+            ),
             interpolation="bilinear",
         )(output)
 
         if segmentation_head is None:
             segmentation_head = SegmentationHead(
-                classes=classes,
+                num_classes=num_classes,
                 name="segmentation_head",
-                convs=1,
+                convolutions=1,
                 dropout=0.2,
                 kernel_size=1,
                 activation=segmentation_head_activation,
             )
 
         # Segmentation head expects a multiple-level output dictionary
         output = segmentation_head({1: output})
@@ -126,187 +128,198 @@
             **kwargs,
         )
 
         if weights is not None:
             self.load_weights(parse_weights(weights, True, "deeplabv3"))
 
         # All references to `self` below this line
-        self.classes = classes
+        self.num_classes = num_classes
         self.backbone = backbone
         self.spatial_pyramid_pooling = spatial_pyramid_pooling
         self.segmentation_head = segmentation_head
         self.segmentation_head_activation = segmentation_head_activation
-
-    # TODO(tanzhenyu): consolidate how regularization should be applied to KerasCV.
-    def compile(self, weight_decay=0.0001, **kwargs):
         self.weight_decay = weight_decay
-        super().compile(**kwargs)
+
+    def build(self, input_shape):
+        height = input_shape[1]
+        width = input_shape[2]
+        feature_map_shape = self.backbone.compute_output_shape(input_shape)
+        self.up_layer = keras.layers.UpSampling2D(
+            size=(
+                height // feature_map_shape[1],
+                width // feature_map_shape[2],
+            ),
+            interpolation="bilinear",
+        )
 
     def train_step(self, data):
-        images, y_true, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)
+        images, y_true, sample_weight = keras.utils.unpack_x_y_sample_weight(
+            data
+        )
         with tf.GradientTape() as tape:
             y_pred = self(images, training=True)
-            total_loss = self.compute_loss(images, y_true, y_pred, sample_weight)
+            total_loss = self.compute_loss(
+                images, y_true, y_pred, sample_weight
+            )
             reg_losses = []
             if self.weight_decay:
                 for var in self.trainable_variables:
                     if "bn" not in var.name:
-                        reg_losses.append(self.weight_decay * tf.nn.l2_loss(var))
+                        reg_losses.append(
+                            self.weight_decay * tf.nn.l2_loss(var)
+                        )
                 l2_loss = tf.math.add_n(reg_losses)
                 total_loss += l2_loss
         self.optimizer.minimize(total_loss, self.trainable_variables, tape=tape)
-        return self.compute_metrics(images, y_true, y_pred, sample_weight=sample_weight)
+        return self.compute_metrics(
+            images, y_true, y_pred, sample_weight=sample_weight
+        )
 
     def get_config(self):
         return {
-            "classes": self.classes,
+            "num_classes": self.num_classes,
             "backbone": self.backbone,
             "spatial_pyramid_pooling": self.spatial_pyramid_pooling,
             "segmentation_head": self.segmentation_head,
             "segmentation_head_activation": self.segmentation_head_activation,
+            "weight_decay": self.weight_decay,
         }
 
 
-@tf.keras.utils.register_keras_serializable(package="keras_cv")
+@keras.utils.register_keras_serializable(package="keras_cv")
 class SegmentationHead(layers.Layer):
     """Prediction head for the segmentation model
 
     The head will take the output from decoder (eg FPN or ASPP), and produce a
     segmentation mask (pixel level classifications) as the output for the model.
 
     Args:
-        classes: int, the number of output classes for the prediction. This should
-            include all the classes (eg background) for the model to predict.
-        convs: int, the number of conv2D layers that are stacked before the final
-            classification layer. Default to 2.
-        filters: int, the number of filter/channels for the the conv2D layers. Default
-            to 256.
-        activations: str or 'tf.keras.activations', activation functions between the
-            conv2D layers and the final classification layer. Default to 'relu'
-        output_scale_factor: int, or a pair of ints, for upsample the output mask.
-            This is useful to scale the output mask back to same size as the input
-            image. When single int is provided, the mask will be scaled with same
-            ratio on both width and height. When a pair of ints are provided, they will
-            be parsed as (height_factor, width_factor). Default to None, which means
-            no resize will happen to the output mask tensor.
-        kernel_size: default 3; the kernel_size to be used in each of the `convs` blocks
-        use_bias: default False; whether to use bias or not in each of the `convs` blocks
-                Defaults to none since the blocks use `BatchNormalization` after each conv, rendering
-                bias obsolete
-        activation: default 'softmax', the activation to apply in the classification
-            layer (output of the head)
+        num_classes: int, number of output classes for the prediction. This
+            should include all the classes (e.g. background) for the model to
+            predict.
+        convolutions: int, number of `Conv2D` layers that are stacked before the
+            final classification layer, defaults to 2.
+        filters: int, number of filter/channels for the conv2D layers.
+            Defaults to 256.
+        activations: str or function, activation functions between the conv2D
+            layers and the final classification layer, defaults to `"relu"`.
+        output_scale_factor: int, or a pair of ints. Factor for upsampling the
+            output mask. This is useful to scale the output mask back to same
+            size as the input image. When single int is provided, the mask will
+            be scaled with same ratio on both width and height. When a pair of
+            ints are provided, they will be parsed as `(height_factor,
+            width_factor)`. Defaults to `None`, which means no resize will
+            happen to the output mask tensor.
+        kernel_size: int, the kernel size to be used in each of the
+            convolutional blocks, defaults to 3.
+        use_bias: boolean, whether to use bias or not in each of the
+            convolutional blocks, defaults to False since the blocks use
+            `BatchNormalization` after each convolution, rendering bias
+            obsolete.
+        activation: str or function, activation to apply in the classification
+            layer (output of the head), defaults to `"softmax"`.
+
+    Examples:
 
-    Sample code
     ```python
     # Mimic a FPN output dict
     p3 = tf.ones([2, 32, 32, 3])
     p4 = tf.ones([2, 16, 16, 3])
     p5 = tf.ones([2, 8, 8, 3])
     inputs = {3: p3, 4: p4, 5: p5}
 
-    head = SegmentationHead(classes=11)
+    head = SegmentationHead(num_classes=11)
 
     output = head(inputs)
-    # output tensor has shape [2, 32, 32, 11]. It has the same resolution as the p3.
+    # output tensor has shape [2, 32, 32, 11]. It has the same resolution as
+    the p3.
     ```
     """
 
     def __init__(
         self,
-        classes,
-        convs=2,
+        num_classes,
+        convolutions=2,
         filters=256,
         activations="relu",
         dropout=0.0,
         kernel_size=3,
         activation="softmax",
         use_bias=False,
         **kwargs,
     ):
-        """
-        Args:
-            classes: the number of possible classes for the segmentation map
-            convs: default 2; the number of conv blocks to use in the head (conv2d-batch_norm-activation blocks)
-            filters: default 256; the number of filters in each Conv2D layer
-            activations: default 'relu'; the activation to apply in conv blocks
-            dropout: default 0.0; the dropout to apply between each conv block
-            kernel_size: default 3; the kernel_size to be used in each of the `convs` blocks
-            use_bias: default False; whether to use bias or not in each of the `convs` blocks
-                Defaults to none since the blocks use `BatchNormalization` after each conv, rendering
-                bias obsolete
-            activation: default 'softmax', the activation to apply in the classification
-                layer (output of the head)
-            **kwargs:
-        """
         super().__init__(**kwargs)
-        self.classes = classes
-        self.convs = convs
+        self.num_classes = num_classes
+        self.convolutions = convolutions
         self.filters = filters
         self.activations = activations
         self.dropout = dropout
         self.kernel_size = kernel_size
         self.use_bias = use_bias
         self.activation = activation
 
         self._conv_layers = []
         self._bn_layers = []
-        for i in range(self.convs):
+        for i in range(self.convolutions):
             conv_name = "segmentation_head_conv_{}".format(i)
             self._conv_layers.append(
-                tf.keras.layers.Conv2D(
+                keras.layers.Conv2D(
                     name=conv_name,
                     filters=self.filters,
                     kernel_size=self.kernel_size,
                     padding="same",
                     use_bias=self.use_bias,
                 )
             )
             norm_name = "segmentation_head_norm_{}".format(i)
-            self._bn_layers.append(tf.keras.layers.BatchNormalization(name=norm_name))
+            self._bn_layers.append(
+                keras.layers.BatchNormalization(name=norm_name)
+            )
 
-        self._classification_layer = tf.keras.layers.Conv2D(
+        self._classification_layer = keras.layers.Conv2D(
             name="segmentation_output",
-            filters=self.classes,
+            filters=self.num_classes,
             kernel_size=1,
             use_bias=False,
             padding="same",
             activation=self.activation,
-            # Force the dtype of the classification head to float32 to avoid the NAN loss
-            # issue when used with mixed precision API.
+            # Force the dtype of the classification head to float32 to avoid the
+            # NAN loss issue when used with mixed precision API.
             dtype=tf.float32,
         )
 
-        self.dropout_layer = tf.keras.layers.Dropout(self.dropout)
+        self.dropout_layer = keras.layers.Dropout(self.dropout)
 
     def call(self, inputs):
         """Forward path for the segmentation head.
 
-        For now, it accepts the output from the decoder only, which is a dict with int
-        key and tensor as value (level-> processed feature output). The head will use the
-        lowest level of feature output as the input for the head.
+        For now, it accepts the output from the decoder only, which is a dict
+        with int key and tensor as value (level-> processed feature output). The
+        head will use the lowest level of feature output as the input for the
+        head.
         """
         if not isinstance(inputs, dict):
-            raise ValueError(f"Expect the inputs to be a dict, but received {inputs}")
+            raise ValueError(
+                f"Expect inputs to be a dict. Received instead inputs={inputs}"
+            )
 
         lowest_level = next(iter(sorted(inputs)))
         x = inputs[lowest_level]
         for conv_layer, bn_layer in zip(self._conv_layers, self._bn_layers):
             x = conv_layer(x)
             x = bn_layer(x)
-            x = tf.keras.activations.get(self.activations)(x)
+            x = keras.activations.get(self.activations)(x)
             if self.dropout:
                 x = self.dropout_layer(x)
-
-        x = self._classification_layer(x)
-        return x
+        return self._classification_layer(x)
 
     def get_config(self):
         config = {
-            "classes": self.classes,
-            "convs": self.convs,
+            "num_classes": self.num_classes,
+            "convolutions": self.convolutions,
             "filters": self.filters,
             "activations": self.activations,
             "dropout": self.dropout,
             "kernel_size": self.kernel_size,
             "use_bias": self.use_bias,
             "activation": self.activation,
         }
```

## Comparing `keras_cv/models/segmentation/deeplab_test.py` & `keras_cv/models/legacy/segmentation/deeplab_test.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,106 +8,105 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import os
-
 import pytest
 import tensorflow as tf
 import tensorflow_datasets as tfds
+from tensorflow import keras
 
-from keras_cv import models
-from keras_cv.models import segmentation
+from keras_cv.models import ResNet50V2Backbone
+from keras_cv.models.legacy import segmentation
 
 
 class DeeplabTest(tf.test.TestCase):
     def test_deeplab_model_construction_with_preconfigured_setting(self):
-        backbone = models.ResNet50V2(
-            include_rescaling=True, include_top=False, input_shape=[64, 64, 3]
+        backbone = ResNet50V2Backbone(
+            input_shape=[64, 64, 3],
         )
-        model = segmentation.DeepLabV3(classes=11, backbone=backbone)
+        model = segmentation.DeepLabV3(num_classes=11, backbone=backbone)
         input_image = tf.random.uniform(shape=[1, 64, 64, 3])
         output = model(input_image, training=True)
 
         self.assertEquals(output["output"].shape, [1, 64, 64, 11])
 
     def test_greyscale_input(self):
-        backbone = models.ResNet50V2(
-            include_rescaling=True, include_top=False, input_shape=[64, 64, 1]
+        backbone = ResNet50V2Backbone(
+            input_shape=[64, 64, 1],
         )
-        model = segmentation.DeepLabV3(classes=11, backbone=backbone)
+        model = segmentation.DeepLabV3(num_classes=11, backbone=backbone)
         input_image = tf.random.uniform(shape=[1, 64, 64, 1])
         output = model(input_image, training=True)
 
         self.assertEquals(output["output"].shape, [1, 64, 64, 11])
 
     def test_missing_input_shapes(self):
         with self.assertRaisesRegex(
-            ValueError, "Input shapes for both the backbone and DeepLabV3 are `None`."
+            ValueError,
+            "Input shapes for both the backbone and DeepLabV3 "
+            "cannot be `None`.",
         ):
-            backbone = models.ResNet50V2(include_rescaling=True, include_top=False)
-            segmentation.DeepLabV3(classes=11, backbone=backbone)
+            backbone = ResNet50V2Backbone()
+            segmentation.DeepLabV3(num_classes=11, backbone=backbone)
 
     def test_deeplab_model_with_components(self):
-        backbone = models.ResNet50V2(
-            include_rescaling=True, include_top=False, input_shape=[64, 64, 3]
+        backbone = ResNet50V2Backbone(
+            input_shape=[64, 64, 3],
         )
         model = segmentation.DeepLabV3(
-            classes=11,
+            num_classes=11,
             backbone=backbone,
         )
 
         input_image = tf.random.uniform(shape=[1, 64, 64, 3])
         output = model(input_image, training=True)
 
         self.assertEquals(output["output"].shape, [1, 64, 64, 11])
 
     def test_mixed_precision(self):
-        tf.keras.mixed_precision.set_global_policy("mixed_float16")
-        backbone = models.ResNet50V2(
-            include_rescaling=True, include_top=False, input_shape=[64, 64, 3]
+        keras.mixed_precision.set_global_policy("mixed_float16")
+        backbone = ResNet50V2Backbone(
+            input_shape=[64, 64, 3],
         )
         model = segmentation.DeepLabV3(
-            classes=11,
+            num_classes=11,
             backbone=backbone,
         )
         input_image = tf.random.uniform(shape=[1, 64, 64, 3])
         output = model(input_image, training=True)
 
         self.assertEquals(output["output"].dtype, tf.float32)
+        keras.mixed_precision.set_global_policy("float32")
 
     def test_invalid_backbone_model(self):
         with self.assertRaisesRegex(
-            ValueError, "Backbone need to be a `tf.keras.layers.Layer`"
+            ValueError, "Argument `backbone` must be a `keras.layers.Layer`"
         ):
             segmentation.DeepLabV3(
-                classes=11,
+                num_classes=11,
                 backbone=tf.Module(),
             )
 
-    @pytest.mark.skipif(
-        "REGRESSION" not in os.environ or os.environ["REGRESSION"] != "true",
-        reason="Takes a long time to run, only runs when REGRESSION "
-        "environment variable is set.  To run the test please run: \n"
-        "`REGRESSION=true pytest keras_cv/",
-    )
+    @pytest.mark.extra_large
     def test_model_train(self):
-        backbone = models.ResNet50V2(
-            include_rescaling=True, include_top=False, input_shape=[384, 384, 3]
+        backbone = ResNet50V2Backbone(
+            input_shape=[384, 384, 3],
         )
-        model = segmentation.DeepLabV3(classes=1, backbone=backbone)
+        model = segmentation.DeepLabV3(num_classes=1, backbone=backbone)
 
         gcs_data_pattern = "gs://caltech_birds2011_mask/0.1.1/*.tfrecord*"
         features = tfds.features.FeaturesDict(
             {
                 "bbox": tfds.features.BBoxFeature(),
-                "image": tfds.features.Image(shape=(None, None, 3), dtype=tf.uint8),
+                "image": tfds.features.Image(
+                    shape=(None, None, 3), dtype=tf.uint8
+                ),
                 "image/filename": tfds.features.Text(),
                 "label": tfds.features.ClassLabel(num_classes=200),
                 "label_name": tfds.features.Text(),
                 "segmentation_mask": tfds.features.Image(
                     shape=(None, None, 1), dtype=tf.uint8
                 ),
             }
@@ -121,19 +120,21 @@
         ds = ds.with_options(ignore_order)
         ds = ds.map(features.deserialize_example, num_parallel_calls=AUTO)
 
         target_size = [384, 384]
         output_res = [96, 96]
         num_images = 11788
 
-        image_resizing = tf.keras.layers.Resizing(target_size[1], target_size[0])
-        labels_resizing = tf.keras.layers.Resizing(output_res[1], output_res[0])
+        image_resizing = keras.layers.Resizing(target_size[1], target_size[0])
+        labels_resizing = keras.layers.Resizing(output_res[1], output_res[0])
 
         def resize_images_and_masks(data):
-            image = tf.image.convert_image_dtype(data["image"], dtype=tf.float32)
+            image = tf.image.convert_image_dtype(
+                data["image"], dtype=tf.float32
+            )
             data["image"] = image_resizing(image)
             # WARNING: assumes processing unbatched
             mask = data["segmentation_mask"]
             mask = tf.image.convert_image_dtype(mask, dtype=tf.float32)
             data["segmentation_mask"] = labels_resizing(mask)
             return data
 
@@ -149,18 +150,20 @@
             tf.data.experimental.dense_to_ragged_batch(batch_size)
         )
         training_dataset = training_dataset.repeat()
 
         epochs = 1
         model.compile(
             optimizer="adam",
-            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
+            loss=keras.losses.BinaryCrossentropy(from_logits=True),
             metrics=["accuracy"],
         )
 
         model.fit(
-            training_dataset, epochs=epochs, steps_per_epoch=num_images // batch_size
+            training_dataset,
+            epochs=epochs,
+            steps_per_epoch=num_images // batch_size,
         )
 
 
 if __name__ == "__main__":
     tf.test.main()
```

## Comparing `keras_cv/ops/point_cloud.py` & `keras_cv/point_cloud/point_cloud.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,39 +40,40 @@
     if points.shape.rank == 2 and boxes.shape.rank == 2:
         return custom_ops.ops.kcv_within_box(points, boxes)
     elif points.shape.rank == 3 and boxes.shape.rank == 3:
         num_samples = points.get_shape().as_list()[0]
         results = []
         for i in range(num_samples):
             results.append(
-                custom_ops.ops.kcv_within_box(points[i], boxes[i])[tf.newaxis, ...]
+                custom_ops.ops.kcv_within_box(points[i], boxes[i])[
+                    tf.newaxis, ...
+                ]
             )
         return tf.concat(results, axis=0)
     else:
         raise ValueError(
-            "is_within_box3d_v2 are expecting inputs point clouds and bounding boxes to "
-            "be rank 2D (Point, Feature) or 3D (Frame, Point, Feature) tensors. Got shape: {} and {}".format(
-                points.shape, boxes.shape
-            )
+            "is_within_box3d_v2 are expecting inputs point clouds and bounding "
+            "boxes to be rank 2D (Point, Feature) or 3D (Frame, Point, Feature)"
+            " tensors. Got shape: {} and {}".format(points.shape, boxes.shape)
         )
 
 
 def group_points_by_boxes(points, boxes):
     """Checks if 3d points are within 3d bounding boxes.
     Currently only xyz format is supported.
     This function assumes that bounding boxes DO NOT overlap with each other.
 
     Args:
       points: [..., num_points, 3] float32 Tensor for 3d points in xyz format.
       boxes: [..., num_boxes, 7] float32 Tensor for 3d boxes in [x, y, z, dx,
         dy, dz, phi].
 
     Returns:
-      boolean Ragged Tensor of shape [..., num_boxes, ragged_points] for each box, all
-      the point indices that belong to the box.
+      boolean Ragged Tensor of shape [..., num_boxes, ragged_points] for each
+      box, all the point indices that belong to the box.
 
     """
     num_boxes = boxes.get_shape().as_list()[-2] or tf.shape(boxes)[-2]
     # [..., num_points]
     box_indices = within_box3d_index(points, boxes)
     num_points = points.get_shape().as_list()[-2] or tf.shape(points)[-2]
     point_indices = tf.range(num_points, dtype=tf.int32)
@@ -119,14 +120,35 @@
     """
     res = tf.greater_equal(within_box3d_index(points, boxes), 0)
     if keepdims:
         res = res[..., tf.newaxis]
     return res
 
 
+def is_within_any_box3d_v3(points, boxes, keepdims=False):
+    """Checks if 3d points are within 3d bounding boxes.
+    Currently only xyz format is supported.
+
+    Args:
+      points: [..., num_points, 3] float32 Tensor for 3d points in xyz format.
+      boxes: [..., num_boxes, 7] float32 Tensor for 3d boxes in [x, y, z, dx,
+        dy, dz, phi].
+      keepdims: boolean. If true, retains reduced dimensions with length 1.
+
+    Returns:
+      boolean Tensor of shape [..., num_points] indicating whether
+      the point belongs to the box.
+
+    """
+    res = custom_ops.ops.kcv_within_any_box(points, boxes)
+    if keepdims:
+        res = res[..., tf.newaxis]
+    return res
+
+
 def get_rank(tensor):
     return tensor.shape.ndims or tf.rank(tensor)
 
 
 def wrap_angle_radians(angle_radians, min_val=-np.pi, max_val=np.pi):
     """Wrap the value of `angles_radians` to the range [min_val, max_val]."""
     max_min_diff = max_val - min_val
@@ -201,15 +223,16 @@
 
 def _center_xyzWHD_to_corner_xyz(boxes):
     """convert from center format to corner format.
     Args:
       boxes: [..., num_boxes, 7] float32 Tensor for 3d boxes in [x, y, z, dx,
         dy, dz, phi].
     Returns:
-      corners: [..., num_boxes, 8, 3] float32 Tensor for 3d corners in [x, y, z].
+      corners: [..., num_boxes, 8, 3] float32 Tensor for 3d corners in
+        [x, y, z].
     """
     # relative corners w.r.t to origin point
     # this will return all corners in top-down counter clockwise instead of
     # only left top and bottom right.
     rel_corners = tf.constant(
         [
             [0.5, 0.5, 0.5],  # top
@@ -293,15 +316,16 @@
       boxes: a float Tensor of [..., 4, 2] of boxes. The last coordinates
         are the four corners of the box and (x, y). The corners must be given in
         counter-clockwise order.
     """
     boxes_roll = tf.roll(boxes, shift=1, axis=-2)
     det = (
         tf.reduce_sum(
-            boxes[..., 0] * boxes_roll[..., 1] - boxes[..., 1] * boxes_roll[..., 0],
+            boxes[..., 0] * boxes_roll[..., 1]
+            - boxes[..., 1] * boxes_roll[..., 0],
             axis=-1,
             keepdims=True,
         )
         / 2.0
     )
     return tf.abs(det)
 
@@ -323,18 +347,20 @@
         boxes[..., 0, :],
         boxes[..., 1, :],
         boxes[..., 2, :],
         boxes[..., 3, :],
     )
     is_inside = tf.math.logical_and(
         tf.math.logical_and(
-            _is_on_lefthand_side(points, v1, v2), _is_on_lefthand_side(points, v2, v3)
+            _is_on_lefthand_side(points, v1, v2),
+            _is_on_lefthand_side(points, v2, v3),
         ),
         tf.math.logical_and(
-            _is_on_lefthand_side(points, v3, v4), _is_on_lefthand_side(points, v4, v1)
+            _is_on_lefthand_side(points, v3, v4),
+            _is_on_lefthand_side(points, v4, v1),
         ),
     )
     valid_area = tf.greater(_box_area(boxes), 0)
     is_inside = tf.math.logical_and(is_inside, valid_area)
     # swap the last two dimensions
     is_inside = tf.einsum("...ij->...ji", tf.cast(is_inside, tf.int32))
     return tf.cast(is_inside, tf.bool)
@@ -409,16 +435,16 @@
     pose should contain 6 floating point values:
       translate_x, translate_y, translate_z: The translation to apply.
       yaw, roll, pitch: The rotation angles in radians.
 
     Args:
       points: Float shape [..., 3]: Points to transform to new coordinates.
       pose: Float shape [6]: [translate_x, translate_y, translate_z, yaw, roll,
-        pitch]. The pose in the frame that 'points' comes from, and the definition
-        of the rotation and translation angles to apply to points.
+        pitch]. The pose in the frame that 'points' comes from, and the
+        definition of the rotation and translation angles to apply to points.
     Returns:
     'points' transformed to the coordinates defined by 'pose'.
     """
     translate_x = pose[0]
     translate_y = pose[1]
     translate_z = pose[2]
 
@@ -429,67 +455,73 @@
     # Compose the rotations along the three axes.
     #
     # Note: Yaw->Z, Roll->X, Pitch->Y.
     yaw, roll, pitch = pose[3], pose[4], pose[5]
     rotation_matrix = _get_3d_rotation_matrix(yaw, roll, pitch)
     # Finally, rotate the points about the pose's origin according to the
     # rotation matrix.
-    rotated_points = tf.einsum("...i,...ij->...j", translated_points, rotation_matrix)
+    rotated_points = tf.einsum(
+        "...i,...ij->...j", translated_points, rotation_matrix
+    )
     return rotated_points
 
 
 def spherical_coordinate_transform(points):
     """Converts points from xyz coordinates to spherical coordinates.
     https://en.wikipedia.org/wiki/Spherical_coordinate_system#Coordinate_system_conversions
     for definitions of the transformations.
     Args:
-      points_xyz: A floating point tensor with shape [..., 3], where the inner 3
+      points: A floating point tensor with shape [..., 3], where the inner 3
         dimensions correspond to xyz coordinates.
     Returns:
       A floating point tensor with the same shape [..., 3], where the inner
       dimensions correspond to (dist, theta, phi), where phi corresponds to
-      azimuth/yaw (rotation around z), and theta corresponds to pitch/inclination
-      (rotation around y).
+      azimuth/yaw (rotation around z), and theta corresponds to
+      pitch/inclination (rotation around y).
     """
     dist = tf.sqrt(tf.reduce_sum(tf.square(points), axis=-1))
     theta = tf.acos(points[..., 2] / tf.maximum(dist, 1e-7))
     # Note: tf.atan2 takes in (y, x).
     phi = tf.atan2(points[..., 1], points[..., 0])
     return tf.stack([dist, theta, phi], axis=-1)
 
 
 def within_a_frustum(points, center, r_distance, theta_width, phi_width):
     """Check if 3d points are within a 3d frustum.
-    https://en.wikipedia.org/wiki/Spherical_coordinate_system for definitions of r, theta, and phi.
-    https://en.wikipedia.org/wiki/Viewing_frustum for defination of a viewing frustum. Here, we
-    use a conical shaped frustum (https://mathworld.wolfram.com/ConicalFrustum.html).
-    Currently only xyz format is supported.
+    https://en.wikipedia.org/wiki/Spherical_coordinate_system for definitions of
+    r, theta, and phi. https://en.wikipedia.org/wiki/Viewing_frustum for
+    definition of a viewing frustum. Here, we use a conical shaped frustum
+    (https://mathworld.wolfram.com/ConicalFrustum.html). Currently, only xyz
+    format is supported.
 
     Args:
       points: [num_points, 3] float32 Tensor for 3d points in xyz format.
       center: [3, ] float32 Tensor for the frustum center in xyz format.
       r_distance: A float scalar sets the starting distance of a frustum.
       theta_width: A float scalar sets the theta width of a frustum.
       phi_width: A float scalar sets the phi width of a frustum.
 
     Returns:
       boolean Tensor of shape [num_points] indicating whether
       points are within the frustum.
 
     """
-    r, theta, phi = tf.unstack(spherical_coordinate_transform(points[:, :3]), axis=-1)
+    r, theta, phi = tf.unstack(
+        spherical_coordinate_transform(points[:, :3]), axis=-1
+    )
 
     _, center_theta, center_phi = tf.unstack(
         spherical_coordinate_transform(center[tf.newaxis, :]), axis=-1
     )
 
     theta_half_width = theta_width / 2.0
     phi_half_width = phi_width / 2.0
 
-    # Points within theta and phi width and further than r distance are selected.
+    # Points within theta and phi width and
+    # further than r distance are selected.
     in_theta_width = (theta < (center_theta + theta_half_width)) & (
         theta > (center_theta - theta_half_width)
     )
 
     in_phi_width = (phi < (center_phi + phi_half_width)) & (
         phi > (center_phi - phi_half_width)
     )
```

## Comparing `keras_cv/ops/point_cloud_test.py` & `keras_cv/point_cloud/point_cloud_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -15,34 +15,37 @@
 import os
 
 import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv import ops
+from keras_cv import point_cloud
 
 
 class AngleTest(tf.test.TestCase):
     def test_wrap_angle_radians(self):
         self.assertAllClose(
-            -np.pi + 0.1, ops.point_cloud.wrap_angle_radians(np.pi + 0.1)
+            -np.pi + 0.1, point_cloud.wrap_angle_radians(np.pi + 0.1)
         )
-        self.assertAllClose(0.0, ops.point_cloud.wrap_angle_radians(2 * np.pi))
+        self.assertAllClose(0.0, point_cloud.wrap_angle_radians(2 * np.pi))
 
 
 class Boxes3DTestCase(tf.test.TestCase, parameterized.TestCase):
     def test_convert_center_to_corners(self):
         boxes = tf.constant(
             [
                 [[1, 2, 3, 4, 3, 6, 0], [1, 2, 3, 4, 3, 6, 0]],
-                [[1, 2, 3, 4, 3, 6, np.pi / 2.0], [1, 2, 3, 4, 3, 6, np.pi / 2.0]],
+                [
+                    [1, 2, 3, 4, 3, 6, np.pi / 2.0],
+                    [1, 2, 3, 4, 3, 6, np.pi / 2.0],
+                ],
             ]
         )
-        corners = ops._center_xyzWHD_to_corner_xyz(boxes)
+        corners = point_cloud._center_xyzWHD_to_corner_xyz(boxes)
         self.assertEqual((2, 2, 8, 3), corners.shape)
         for i in [0, 1]:
             self.assertAllClose(-1, np.min(corners[0, i, :, 0]))
             self.assertAllClose(3, np.max(corners[0, i, :, 0]))
             self.assertAllClose(0.5, np.min(corners[0, i, :, 1]))
             self.assertAllClose(3.5, np.max(corners[0, i, :, 1]))
             self.assertAllClose(0, np.min(corners[0, i, :, 2]))
@@ -71,15 +74,15 @@
                 [-0.5, 1.5],
                 [-0.5, 0.5],
                 [1.0, 1.0],
                 [0.5, 0.5],
             ],
             dtype=tf.float32,
         )
-        is_inside = ops.is_within_box2d(points, boxes)
+        is_inside = point_cloud.is_within_box2d(points, boxes)
         expected = [[False]] * 8 + [[True]] * 2
         self.assertAllEqual(expected, is_inside)
 
     def test_within_zero_box2d(self):
         bbox = tf.constant(
             [[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]], dtype=tf.float32
         )
@@ -94,27 +97,27 @@
                 [-0.5, 1.5],
                 [-0.5, 0.5],
                 [1.0, 1.0],
                 [0.5, 0.5],
             ],
             dtype=tf.float32,
         )
-        is_inside = ops.is_within_box2d(points, bbox)
+        is_inside = point_cloud.is_within_box2d(points, bbox)
         expected = [[False]] * 10
         self.assertAllEqual(expected, is_inside)
 
     def test_is_on_lefthand_side(self):
         v1 = tf.constant([[0.0, 0.0]], dtype=tf.float32)
         v2 = tf.constant([[1.0, 0.0]], dtype=tf.float32)
         p = tf.constant([[0.5, 0.5], [-1.0, -3], [-1.0, 1.0]], dtype=tf.float32)
-        res = ops._is_on_lefthand_side(p, v1, v2)
+        res = point_cloud._is_on_lefthand_side(p, v1, v2)
         self.assertAllEqual([[True, False, True]], res)
-        res = ops._is_on_lefthand_side(v1, v1, v2)
+        res = point_cloud._is_on_lefthand_side(v1, v1, v2)
         self.assertAllEqual([[True]], res)
-        res = ops._is_on_lefthand_side(v2, v1, v2)
+        res = point_cloud._is_on_lefthand_side(v2, v1, v2)
         self.assertAllEqual([[True]], res)
 
     @parameterized.named_parameters(
         ("without_rotation", 0.0),
         ("with_rotation_1_rad", 1.0),
         ("with_rotation_2_rad", 2.0),
         ("with_rotation_3_rad", 3.0),
@@ -134,15 +137,15 @@
             rotation_matrix = tf.reshape(
                 [tf.cos(theta), -tf.sin(theta), tf.sin(theta), tf.cos(theta)],
                 shape=(2, 2),
             )
             return tf.matmul(bbox, rotation_matrix)
 
         rotated_bboxes = _rotate(boxes, angle)
-        res = ops._box_area(rotated_bboxes)
+        res = point_cloud._box_area(rotated_bboxes)
         self.assertAllClose(expected, res)
 
     def test_within_box3d(self):
         num_points, num_boxes = 19, 4
         # rotate the first box by pi / 2 so dim_x and dim_y are swapped.
         # The last box is a cube rotated by 45 degrees.
         bboxes = tf.constant(
@@ -201,24 +204,26 @@
                 [False, False, False, False],
             ]
         )
         assert points.shape[0] == num_points
         assert bboxes.shape[0] == num_boxes
         assert expected_is_inside.shape[0] == num_points
         assert expected_is_inside.shape[1] == num_boxes
-        is_inside = ops.is_within_box3d(points, bboxes)
+        is_inside = point_cloud.is_within_box3d(points, bboxes)
         self.assertAllEqual([num_points, num_boxes], is_inside.shape)
         self.assertAllEqual(expected_is_inside, is_inside)
         # Add a batch dimension to the data and see that it still works
         # as expected.
         batch_size = 3
         points = tf.tile(points[tf.newaxis, ...], [batch_size, 1, 1])
         bboxes = tf.tile(bboxes[tf.newaxis, ...], [batch_size, 1, 1])
-        is_inside = ops.is_within_box3d(points, bboxes)
-        self.assertAllEqual([batch_size, num_points, num_boxes], is_inside.shape)
+        is_inside = point_cloud.is_within_box3d(points, bboxes)
+        self.assertAllEqual(
+            [batch_size, num_points, num_boxes], is_inside.shape
+        )
         for batch_idx in range(batch_size):
             self.assertAllEqual(expected_is_inside, is_inside[batch_idx])
 
     def testCoordinateTransform(self):
         # This is a validated test case from a real scene.
         #
         # A single point [1, 1, 3].
@@ -240,27 +245,29 @@
                 -3.10496902,
                 0.03288471,
                 0.00115049,
             ],
             dtype=tf.float32,
         )
 
-        result = ops.coordinate_transform(replicated_points, pose)
+        result = point_cloud.coordinate_transform(replicated_points, pose)
 
-        # We expect the point to be translated close to the car, and then rotated
-        # mostly around the x-axis.
-        # the result is device dependent, skip or ignore this test locally if it fails.
+        # We expect the point to be translated close to the car, and then
+        # rotated mostly around the x-axis. The result is device dependent, skip
+        # or ignore this test locally if it fails.
         expected = np.tile([[[-8.184512, -0.13086952, -0.04200769]]], [2, 4, 1])
 
         self.assertAllClose(expected, result)
 
     def testSphericalCoordinatesTransform(self):
         np_xyz = np.random.randn(5, 6, 3)
         points = tf.constant(np_xyz, dtype=tf.float32)
-        spherical_coordinates = ops.spherical_coordinate_transform(points)
+        spherical_coordinates = point_cloud.spherical_coordinate_transform(
+            points
+        )
 
         # Convert coordinates back to xyz to verify.
         dist = spherical_coordinates[..., 0]
         theta = spherical_coordinates[..., 1]
         phi = spherical_coordinates[..., 2]
 
         x = dist * np.sin(theta) * np.cos(phi)
@@ -268,15 +275,16 @@
         z = dist * np.cos(theta)
 
         self.assertAllClose(x, np_xyz[..., 0])
         self.assertAllClose(y, np_xyz[..., 1])
         self.assertAllClose(z, np_xyz[..., 2])
 
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def test_group_points(self):
         # rotate the first box by pi / 2 so dim_x and dim_y are swapped.
         # The last box is a cube rotated by 45 degrees.
         with tf.device("cpu:0"):
             bboxes = tf.constant(
@@ -304,38 +312,44 @@
                     [0.4, 0.3, 0.1],  # box 2 (below z)
                     [0.4, 0.3, 0.29],  # box 2 (above z)
                     [0.4, 0.3, 0.0],  # box 2 (too far z)
                     [0.4, 0.3, 0.4],  # box 2 (too far z)
                     [5.0, 7.0, 8.0],  # none
                     [1.0, 5.0, 3.6],  # box0, box1
                     [-11.6, -10.0, -10.0],  # box3 (rotated corner point).
-                    [-11.4, -11.4, -10.0],  # not in box3, would be if not rotated.
+                    [
+                        -11.4,
+                        -11.4,
+                        -10.0,
+                    ],  # not in box3, would be if not rotated.
                 ],
                 dtype=tf.float32,
             )
-            res = ops.group_points_by_boxes(points, bboxes)
+            res = point_cloud.group_points_by_boxes(points, bboxes)
             expected_result = tf.ragged.constant(
                 [[0, 1, 2], [5, 6, 7, 16], [10, 11, 12], [17]]
             )
             self.assertAllClose(expected_result.flat_values, res.flat_values)
 
     def testWithinAFrustum(self):
         center = tf.constant([1.0, 1.0, 1.0])
-        points = tf.constant([[0.0, 0.0, 0.0], [1.0, 2.0, 1.0], [1.0, 0.0, 1.0]])
+        points = tf.constant(
+            [[0.0, 0.0, 0.0], [1.0, 2.0, 1.0], [1.0, 0.0, 1.0]]
+        )
 
-        point_mask = ops.within_a_frustum(
+        point_mask = point_cloud.within_a_frustum(
             points, center, r_distance=1.0, theta_width=1.0, phi_width=1.0
         )
         target_point_mask = tf.constant([False, True, False])
         self.assertAllClose(point_mask, target_point_mask)
 
-        point_mask = ops.within_a_frustum(
+        point_mask = point_cloud.within_a_frustum(
             points, center, r_distance=1.0, theta_width=3.14, phi_width=3.14
         )
         target_point_mask = tf.constant([False, True, True])
         self.assertAllClose(point_mask, target_point_mask)
 
-        point_mask = ops.within_a_frustum(
+        point_mask = point_cloud.within_a_frustum(
             points, center, r_distance=3.0, theta_width=1.0, phi_width=1.0
         )
         target_point_mask = tf.constant([False, False, False])
         self.assertAllClose(point_mask, target_point_mask)
```

## Comparing `keras_cv/ops/within_box_3d_test.py` & `keras_cv/point_cloud/within_box_3d_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -29,39 +29,47 @@
 def get_points_boxes():
     points = tf.random.uniform(
         shape=[num_points, 2], minval=0, maxval=box_dimension, dtype=tf.float32
     )
     points_z = 5.0 * tf.ones(shape=[num_points, 1], dtype=tf.float32)
     points = tf.concat([points, points_z], axis=-1)
     boxes_x = tf.random.uniform(
-        shape=[num_boxes, 1], minval=0, maxval=box_dimension - 1.0, dtype=tf.float32
+        shape=[num_boxes, 1],
+        minval=0,
+        maxval=box_dimension - 1.0,
+        dtype=tf.float32,
     )
     boxes_y = tf.random.uniform(
-        shape=[num_boxes, 1], minval=0, maxval=box_dimension - 1.0, dtype=tf.float32
+        shape=[num_boxes, 1],
+        minval=0,
+        maxval=box_dimension - 1.0,
+        dtype=tf.float32,
     )
     boxes_dx = tf.random.uniform(
         shape=[num_boxes, 1], minval=0, maxval=5.0, dtype=tf.float32
     )
-    boxes_dx = tf.math.minimum(10 - boxes_x, boxes_dx)
+    boxes_dx = tf.math.minimum(box_dimension - boxes_x, boxes_dx)
     boxes_dy = tf.random.uniform(
         shape=[num_boxes, 1], minval=0, maxval=5.0, dtype=tf.float32
     )
-    boxes_dy = tf.math.minimum(10 - boxes_y, boxes_dy)
+    boxes_dy = tf.math.minimum(box_dimension - boxes_y, boxes_dy)
     boxes_z = 5.0 * tf.ones([num_boxes, 1], dtype=tf.float32)
     boxes_dz = 3.0 * tf.ones([num_boxes, 1], dtype=tf.float32)
     boxes_angle = tf.zeros([num_boxes, 1], dtype=tf.float32)
     boxes = tf.concat(
-        [boxes_x, boxes_y, boxes_z, boxes_dx, boxes_dy, boxes_dz, boxes_angle], axis=-1
+        [boxes_x, boxes_y, boxes_z, boxes_dx, boxes_dy, boxes_dz, boxes_angle],
+        axis=-1,
     )
     return points, boxes
 
 
 class WithinBox3DTest(tf.test.TestCase):
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def test_unbatched_unrotated(self):
         boxes = np.array(
             [
                 [0, 0, 0, 4, 4, 4, 0],
                 [5, 5, 5, 1, 1, 1, 0],
@@ -77,19 +85,20 @@
                 [2.01, 0, 0],
                 # this point belongs to 2nd box
                 [5.5, 5.5, 5.5],
                 # this point doesn't belong to 2nd box
                 [5.6, 5.5, 5.5],
             ]
         ).astype("float32")
-        res = keras_cv.ops.within_box3d_index(points, boxes)
+        res = keras_cv.point_cloud.within_box3d_index(points, boxes)
         self.assertAllEqual([0, 0, -1, 0, -1, 1, -1], res)
 
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def test_unbatched_rotated(self):
         # a box rotated with 45 degree, the intersection with x and y axis
         # is [2*sqrt(2), 0] and [0, 2*sqrt(2)]
         boxes = np.array(
             [
@@ -103,19 +112,20 @@
                 # this point has z value larger than box top z
                 [0, 0, 2.1],
                 [2.82, 0, 0],
                 # this point has x value larger than rotated box
                 [2.83, 0, 0],
             ]
         ).astype("float32")
-        res = keras_cv.ops.within_box3d_index(points, boxes)
+        res = keras_cv.point_cloud.within_box3d_index(points, boxes)
         self.assertAllClose([0, 0, -1, 0, -1], res)
 
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def test_batched_unrotated(self):
         boxes = np.array(
             [
                 [[0, 0, 0, 4, 4, 4, 0]],
                 [[5, 5, 5, 1, 1, 1, 0]],
@@ -134,21 +144,22 @@
                     [5.5, 5.5, 5.5],
                     # this point doesn't belong to 2nd box
                     [5.6, 5.5, 5.5],
                 ]
             ]
             * 2
         ).astype("float32")
-        res = keras_cv.ops.within_box3d_index(points, boxes)
+        res = keras_cv.point_cloud.within_box3d_index(points, boxes)
         self.assertAllEqual(
             [[0, 0, -1, 0, -1, -1, -1], [-1, -1, -1, -1, -1, 0, -1]], res
         )
 
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def test_batched_rotated(self):
         # a box rotated with 45 degree, the intersection with x and y axis
         # is [2*sqrt(2), 0] and [0, 2*sqrt(2)]
         boxes = np.array(
             [
@@ -166,35 +177,48 @@
                     [2.82, 0, 0],
                     # this point has x value larger than rotated box
                     [2.83, 0, 0],
                 ]
             ]
             * 2
         ).astype("float32")
-        res = keras_cv.ops.within_box3d_index(points, boxes)
+        res = keras_cv.point_cloud.within_box3d_index(points, boxes)
         self.assertAllEqual([[0, 0, -1, 0, -1], [-1, -1, -1, -1, -1]], res)
 
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
     def test_many_points(self):
         points, boxes = get_points_boxes()
 
         for _ in range(5):
-            res = keras_cv.ops.within_box3d_index(points, boxes)
+            res = keras_cv.point_cloud.within_box3d_index(points, boxes)
             self.assertAllClose(res.shape, points.shape[:1])
 
     @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ or os.environ["TEST_CUSTOM_OPS"] != "true",
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
         reason="Requires binaries compiled from source",
     )
+    @pytest.mark.extra_large
     def test_equal(self):
         for _ in range(10000):
             with tf.device("cpu:0"):
-                box_center = tf.random.uniform(shape=[1, 3], minval=-10.0, maxval=10.0)
-                box_dim = tf.random.uniform(shape=[1, 3], minval=0.1, maxval=10.0)
+                box_center = tf.random.uniform(
+                    shape=[1, 3], minval=-10.0, maxval=10.0
+                )
+                box_dim = tf.random.uniform(
+                    shape=[1, 3], minval=0.1, maxval=10.0
+                )
                 boxes = tf.concat([box_center, box_dim, [[0.0]]], axis=-1)
                 points = tf.random.normal([32, 3])
-                res = keras_cv.ops.is_within_any_box3d(points, boxes)
-                res_v2 = keras_cv.ops.is_within_any_box3d_v2(points, boxes)
+                res = keras_cv.point_cloud.is_within_any_box3d(points, boxes)
+                res_v2 = keras_cv.point_cloud.is_within_any_box3d_v2(
+                    points, boxes
+                )
+                res_v3 = keras_cv.point_cloud.is_within_any_box3d_v3(
+                    points, boxes
+                )
                 self.assertAllEqual(res, res_v2)
+                self.assertAllEqual(res, res_v3)
```

## Comparing `keras_cv-0.4.2.dist-info/LICENSE` & `keras_cv-0.5.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `keras_cv-0.4.2.dist-info/METADATA` & `keras_cv-0.5.0.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: keras-cv
-Version: 0.4.2
+Version: 0.5.0
 Summary: Industry-strength computer Vision extensions for Keras.
 Home-page: https://github.com/keras-team/keras-cv
 Author: Keras team
 Author-email: keras-cv@google.com
 License: Apache License 2.0
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3.7
@@ -26,115 +26,137 @@
 Requires-Dist: matplotlib ; extra == 'examples'
 Provides-Extra: tests
 Requires-Dist: flake8 ; extra == 'tests'
 Requires-Dist: isort ; extra == 'tests'
 Requires-Dist: black[jupyter] ; extra == 'tests'
 Requires-Dist: pytest ; extra == 'tests'
 Requires-Dist: pycocotools ; extra == 'tests'
+Requires-Dist: tensorflow ; extra == 'tests'
 
 # KerasCV
 
 [![](https://github.com/keras-team/keras-cv/workflows/Tests/badge.svg?branch=master)](https://github.com/keras-team/keras-cv/actions?query=workflow%3ATests+branch%3Amaster)
 ![Downloads](https://img.shields.io/pypi/dm/keras-cv.svg)
 ![Python](https://img.shields.io/badge/python-v3.7.0+-success.svg)
 ![Tensorflow](https://img.shields.io/badge/tensorflow-v2.9.0+-success.svg)
 [![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/keras-team/keras-cv/issues)
 
 KerasCV is a library of modular computer vision oriented Keras components.
 These components include models, layers, metrics, losses, callbacks, and utility
 functions.
 
+<img style="width: 440px; max-width: 90%;" src="https://storage.googleapis.com/keras-cv/guides/keras-cv-augmentations.gif">
+
 KerasCV's primary goal is to provide a coherent, elegant, and pleasant API to train state of the art computer vision models.
 Users should be able to train state of the art models using only `Keras`, `KerasCV`, and TensorFlow core (i.e. `tf.data`) components.
 
 KerasCV can be understood as a horizontal extension of the Keras API: the components are new first-party
 Keras objects (layers, metrics, etc.) that are too specialized to be added to core Keras. They receive the same level of polish and backwards compatibility guarantees as the core Keras API, and they are maintained by the Keras team.
 
 Our APIs assist in common computer vision tasks such as data-augmentation, classification, object detection, image generation, and more.
 Applied computer vision engineers can leverage KerasCV to quickly assemble production-grade, state-of-the-art training and inference pipelines for all of these common tasks.
 
 In addition to API consistency, KerasCV components aim to be mixed-precision compatible, QAT compatible, XLA compilable, and TPU compatible.
 We also aim to provide generic model optimization tools for deployment on devices such as onboard GPUs, mobile, and edge chips.
 
-
-To learn more about the future project direction, please check the [roadmap](.github/ROADMAP.md).
-
 ## Quick Links
+- [List of available models and presets](https://keras.io/api/keras_cv/models/)
+- [Developer Guides](https://keras.io/guides/keras_cv/)
 - [Contributing Guide](.github/CONTRIBUTING.md)
 - [Call for Contributions](https://github.com/keras-team/keras-cv/issues?q=is%3Aopen+is%3Aissue+label%3Acontribution-welcome)
-- [Roadmap](.github/ROADMAP.md)
 - [API Design Guidelines](.github/API_DESIGN.md)
 
-## Quickstart
+## Installation
 
-Create a preprocessing pipeline:
+To install the latest official release:
 
-```python
-import keras_cv
-import tensorflow as tf
-from tensorflow import keras
-import tensorflow_datasets as tfds
+```
+pip install keras-cv tensorflow --upgrade
+```
 
-augmenter = keras_cv.layers.Augmenter(
-  layers=[
-      keras_cv.layers.RandomFlip(),
-      keras_cv.layers.RandAugment(value_range=(0, 255)),
-      keras_cv.layers.CutMix(),
-      keras_cv.layers.MixUp()
-    ]
-)
+To install the latest unreleased changes to the library, we recommend using
+pip to install directly from the master branch on github:
 
-def augment_data(images, labels):
-  labels = tf.one_hot(labels, 3)
-  inputs = {"images": images, "labels": labels}
-  outputs = augmenter(inputs)
-  return outputs['images'], outputs['labels']
+```
+pip install git+https://github.com/keras-team/keras-cv.git tensorflow --upgrade
 ```
 
-Augment a `tf.data.Dataset`:
+## Quickstart
 
 ```python
-dataset = tfds.load('rock_paper_scissors', as_supervised=True, split='train')
-dataset = dataset.batch(64)
-dataset = dataset.map(augment_data, num_parallel_calls=tf.data.AUTOTUNE)
-```
+import tensorflow as tf
+from tensorflow import keras
+import keras_cv
+import tensorflow_datasets as tfds
 
-Create a model:
+# Create a preprocessing pipeline with augmentations
+BATCH_SIZE = 16
+NUM_CLASSES = 3
+augmenter = keras.Sequential(
+    [
+        keras_cv.layers.RandomFlip(),
+        keras_cv.layers.RandAugment(value_range=(0, 255)),
+        keras_cv.layers.CutMix(),
+    ]
+)
 
-```python
-densenet = keras_cv.models.DenseNet121(
-  include_rescaling=True,
-  include_top=True,
-  classes=3
+def preprocess_data(images, labels, augment=False):
+    labels = tf.one_hot(labels, NUM_CLASSES)
+    inputs = {"images": images, "labels": labels}
+    outputs = augmenter(inputs) if augment else inputs
+    return outputs['images'], outputs['labels']
+
+train_dataset, test_dataset = tfds.load(
+    'rock_paper_scissors',
+    as_supervised=True,
+    split=['train', 'test'],
 )
-densenet.compile(
-  loss='categorical_crossentropy',
-  optimizer='adam',
-  metrics=['accuracy']
+train_dataset = train_dataset.batch(BATCH_SIZE).map(
+    lambda x, y: preprocess_data(x, y, augment=True),
+        num_parallel_calls=tf.data.AUTOTUNE).prefetch(
+            tf.data.AUTOTUNE)
+test_dataset = test_dataset.batch(BATCH_SIZE).map(
+    preprocess_data, num_parallel_calls=tf.data.AUTOTUNE).prefetch(
+        tf.data.AUTOTUNE)
+
+# Create a model using a pretrained backbone
+backbone = keras_cv.models.EfficientNetV2Backbone.from_preset(
+    "efficientnetv2_b0_imagenet"
+)
+model = keras_cv.models.ImageClassifier(
+    backbone=backbone,
+    num_classes=NUM_CLASSES,
+    activation="softmax",
+)
+model.compile(
+    loss='categorical_crossentropy',
+    optimizer=keras.optimizers.Adam(learning_rate=1e-5),
+    metrics=['accuracy']
 )
-```
-
-Train your model:
 
-```python
-densenet.fit(dataset)
+# Train your model
+model.fit(
+    train_dataset,
+    validation_data=test_dataset,
+    epochs=8,
+)
 ```
 
 ## Contributors
 If you'd like to contribute, please see our [contributing guide](.github/CONTRIBUTING.md).
 
 To find an issue to tackle, please check our [call for contributions](.github/CALL_FOR_CONTRIBUTIONS.md).
 
 We would like to leverage/outsource the Keras community not only for bug reporting,
 but also for active development for feature delivery. To achieve this, here is the predefined
 process for how to contribute to this repository:
 
-1) Contributors are always welcome to help us fix an issue, add tests, better documentation.  
+1) Contributors are always welcome to help us fix an issue, add tests, better documentation. 
 2) If contributors would like to create a backbone, we usually require a pre-trained weight set
-with the model for one dataset as the first PR, and a training script as a follow-up. The training script will preferrably help us reproduce the results claimed from paper. The backbone should be generic but the training script can contain paper specific parameters such as learning rate schedules and weight decays. The training script will be used to produce leaderboard results.  
+with the model for one dataset as the first PR, and a training script as a follow-up. The training script will preferrably help us reproduce the results claimed from paper. The backbone should be generic but the training script can contain paper specific parameters such as learning rate schedules and weight decays. The training script will be used to produce leaderboard results. 
 Exceptions apply to large transformer-based models which are difficult to train. If this is the case,
 contributors should let us know so the team can help in training the model or providing GCP resources.
 3) If contributors would like to create a meta arch, please try to be aligned with our roadmap and create a PR for design review to make sure the meta arch is modular.
 4) If contributors would like to create a new input formatting which is not in our roadmap for the next 6 months, e.g., keypoint, please create an issue and ask for a sponsor.
 5) If contributors would like to support a new task which is not in our roadmap for the next 6 months, e.g., 3D reconstruction, please create an issue and ask for a sponsor.
 
 Thank you to all of our wonderful contributors!
@@ -152,36 +174,36 @@
 Performance metrics for the provided pre-trained weights can be found
 in the training history for each documented task.
 An example of this can be found in the ImageNet classification training
 [history for backbone models](examples/training/classification/imagenet/training_history.json).
 All results are reproducible using the training scripts in this repository.
 
 Historically, many models have been trained on image datasets rescaled via manually
-crafted normalization schemes.  
+crafted normalization schemes. 
 The most common variant of manually crafted normalization scheme is subtraction of the
 imagenet mean pixel followed by standard deviation normalization based on the imagenet
 pixel standard deviation.
 This scheme is an artifact of the days of manual feature engineering, but is no longer
 required to score state of the art scores using modern deep learning architectures.
 Due to this, KerasCV is standardized to operate on images that have been rescaled using
 a simple `1/255` rescaling layer.
 This can be seen in all KerasCV training pipelines and code examples.
 
 ## Custom Ops
-Note that in some the 3D Object Detection layers, custom TF ops are used. The
+Note that in some of the 3D Object Detection layers, custom TF ops are used. The
 binaries for these ops are not shipped in our PyPi package in order to keep our
 wheels pure-Python.
 
 If you'd like to use these custom ops, you can install from source using the
 instructions below.
 
 ### Installing KerasCV with Custom Ops from Source
 
 Installing custom ops from source requires the [Bazel](https://bazel.build/) build
-system (version >= 5.4.0).  Steps to install Bazel can be [found here](https://github.com/keras-team/keras/blob/v2.11.0/.devcontainer/Dockerfile#L21-L23).
+system (version >= 5.4.0). Steps to install Bazel can be [found here](https://github.com/keras-team/keras/blob/v2.11.0/.devcontainer/Dockerfile#L21-L23).
 
 ```
 git clone https://github.com/keras-team/keras-cv.git
 cd keras-cv
 
 python3 build_deps/configure.py
 
@@ -210,12 +232,12 @@
 
 If KerasCV helps your research, we appreciate your citations.
 Here is the BibTeX entry:
 
 ```bibtex
 @misc{wood2022kerascv,
   title={KerasCV},
-  author={Wood, Luke and Tan, Zhenyu and Stenbit, Ian and Zhu, Scott and Chollet, Fran\c{c}ois and others},
+  author={Wood, Luke and Tan, Zhenyu and Stenbit, Ian and Bischof, Jonathan and Zhu, Scott and Chollet, Fran\c{c}ois and others},
   year={2022},
   howpublished={\url{https://github.com/keras-team/keras-cv}},
 }
 ```
```

## Comparing `keras_cv-0.4.2.dist-info/RECORD` & `keras_cv-0.5.0.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,352 +1,400 @@
-keras_cv/__init__.py,sha256=4ad3Po562fNLbB28NhFKKn-p0mQh0VscYpyW14ytmXc,1113
-keras_cv/version_check.py,sha256=SbSdtWej6Y7kAdcRDXqq0DfYUgSugmSAmLuLCwvv9B4,1144
-keras_cv/version_check_test.py,sha256=SXdZwGAlNS4JLyMaFkqxjdGhwDlEAnZ3WE6lvfbBtk4,1353
-keras_cv/bounding_box/__init__.py,sha256=lfD6VmedGujjv2lNVLCbpYwuIUxYpktpIEkTbbi_s60,1540
-keras_cv/bounding_box/converters.py,sha256=2V9vOjEpWHBglGdQxXaKW3xvRvm-LdWbbxQsCZfxhs8,18119
-keras_cv/bounding_box/converters_test.py,sha256=VofHc-37_qWc9j8HH0aDGvESI5ran9ohP0zoEYoeKdE,6148
-keras_cv/bounding_box/formats.py,sha256=Z6Bn9--Si1aTEiFqdy20eKwJeIv--__G5Z4ePvWJLVg,4078
-keras_cv/bounding_box/iou.py,sha256=r55VxksSsby8WeKR4svz3s_UuCG7iRbB58NopQAPunk,6124
-keras_cv/bounding_box/iou_test.py,sha256=QqbQ4_zYtES1qwonfTCtlpzIRCvry1CiONaQOIBxxV4,5814
-keras_cv/bounding_box/mask_invalid_detections.py,sha256=zEfjxe1qnWablD87rxVmD9yG0R8ejxtLTLQ2aUtDo3s,3490
-keras_cv/bounding_box/mask_invalid_detections_test.py,sha256=TOsLQW4znf7G1KDIllPCOfL6qa0MP20n_F6pssOkako,2906
-keras_cv/bounding_box/to_dense.py,sha256=RJZdMV4mnfAmkbzZZQdrhV5THqw-CF83QhjgLFCc9tU,2866
+keras_cv/__init__.py,sha256=nC27SQ3bDpMketN8Ais4afv-oWdEtD8igF1plTACZxo,1182
+keras_cv/conftest.py,sha256=S4niR-IK05iFT_hWO9FuvmXX3FXyV2O1FwRv0DcpsXE,2271
+keras_cv/version_check.py,sha256=LqMks3U07vEn9lmEZOoBvTgRl7_Nee-6rt_bkUP8VKM,1159
+keras_cv/version_check_test.py,sha256=rV0HuMJEAQOoE8embKlWKMLFMB8dY4WbYBxFdJ-jbjU,1362
+keras_cv/bounding_box/__init__.py,sha256=ZSWjUIwDIAlQf6uKq5H-XUD1U8JwJDSZb0HN-vSdpag,1611
+keras_cv/bounding_box/converters.py,sha256=zdOwi1MtCv8E6WbXgnBRghxLDvJWCCrFZXrhSMFcCdI,18483
+keras_cv/bounding_box/converters_test.py,sha256=vhV7ip5QMgsL27zu5GWsSvRgv6z1jlRV0FyZ9mYD4Oo,7134
+keras_cv/bounding_box/ensure_tensor.py,sha256=6d7lbfv4-PKeY4yfpvphMClNdw2g49WohBGekzXXoHk,909
+keras_cv/bounding_box/ensure_tensor_test.py,sha256=y_8eepWMJ5Y2QRHOD8J3FDDT9hZoMTlDf_5Znehumvo,1443
+keras_cv/bounding_box/formats.py,sha256=ClUExYPKQ9pEnPMhgmc48sC0PDpPnqHkzPq_GqlNuWs,4035
+keras_cv/bounding_box/iou.py,sha256=x9udflg92_5F5GDUM6FOpEUTYMSSFv1kHDFpVe2kEZY,6412
+keras_cv/bounding_box/iou_test.py,sha256=H4_6OzNVEQ-pbRestVmGds85R8mFZOS1Z9S0wj7Krsc,6130
+keras_cv/bounding_box/mask_invalid_detections.py,sha256=-kCZNPvMCWhCUSxF2GGSLxOfgzwDxxqCeLffgiWhGwE,3751
+keras_cv/bounding_box/mask_invalid_detections_test.py,sha256=tMexLag8SQ5BaWY8Ti8lJJYrnay0ovkTun0xc2dA06U,3901
+keras_cv/bounding_box/to_dense.py,sha256=GrDNFiDdJGIzJUIMOS0zjZFkfKf1J_XUjfwgxqo-PBE,3204
 keras_cv/bounding_box/to_dense_test.py,sha256=wjUtUn8AhyjRAHHpBLt3wFmwpXvy2CFc6qhL_orOXr4,1147
-keras_cv/bounding_box/to_ragged.py,sha256=jtFF5IUaaLD51jRgyaQcySIDMDDchGsx7lPp0577geQ,2586
-keras_cv/bounding_box/to_ragged_test.py,sha256=kYtyGzmmDEqApTyfJC0EBF-47lbgkkduAmXwdRoJUbc,2081
-keras_cv/bounding_box/utils.py,sha256=txxVndaSdNv8_fzw0sAyqXNp-4j6lnGt_kxEme39epc,7061
-keras_cv/bounding_box/utils_test.py,sha256=detLu6ZSglu-_LNut90C73DTA5PR75FfL1GeLg0-ipM,5509
-keras_cv/bounding_box/validate_format.py,sha256=ThsIiVTGnQspZQTI_YqOC5QtA3D-PQiD_uuMpbD3-LQ,3411
-keras_cv/bounding_box/validate_format_test.py,sha256=kwK9FR7wPg7GQgyULs9X5r9KOPpTisCRjwJQAsXnbhY,1568
+keras_cv/bounding_box/to_ragged.py,sha256=w0idLsXjuySRvxtIdjuonfn1fAvyeSAGZJ0yZNcGieA,3014
+keras_cv/bounding_box/to_ragged_test.py,sha256=w5P8uupWyEcdGt8TG-OeTCc_UhJQPMHrlc-rKWabVxs,2713
+keras_cv/bounding_box/utils.py,sha256=JXbLVd1Qhqu6uJ-Pj9Ffx65q68yZi26M40T2gzAVSDM,7178
+keras_cv/bounding_box/utils_test.py,sha256=oqOp4QULsLwwO0F4bpZmc1QqIhi3N8qZgRWpwlfJqCI,5569
+keras_cv/bounding_box/validate_format.py,sha256=IrdFqLw5a-ENw3g1LTO5V-VFY30M1VA7ZKg-vaB0viM,3479
+keras_cv/bounding_box/validate_format_test.py,sha256=ruHJk2_ioqZnymkCBf8xCZPv07GlZ7IxP0B2oh6Q1O8,1581
 keras_cv/bounding_box_3d/__init__.py,sha256=hokR_WW7-_7UBm4ebdib6MjQpuDHoe1TU-fHFfi4ZPE,652
-keras_cv/bounding_box_3d/formats.py,sha256=IwjSiBfBNdW3Wk1dUQWMAWST9wYE1iaFD8Zz2lVCqRY,1605
-keras_cv/callbacks/__init__.py,sha256=PFupyWhyIKrEN4cUAaYAGGVtAeEiEK4lvzHINjSZ_Dc,1016
-keras_cv/callbacks/pycoco_callback.py,sha256=rQt7S3uvsuoM08_x1ACo19qB3I6PUgLx_gIjcDxaRuo,4571
-keras_cv/callbacks/pycoco_callback_test.py,sha256=DTSIazTXWHNMyukYIbn0PRoy54sfkgEtPGK9IpvP6Xo,3186
-keras_cv/callbacks/waymo_evaluation_callback.py,sha256=fDe-qTqhHo7xpJOt19EbYuGc3frlAeieXOOxcHw0TsM,4831
-keras_cv/callbacks/waymo_evaluation_callback_test.py,sha256=XYw4ly4izjkHqFq0FCTMhwt4lf22S4VLwztFy0iaakA,2645
-keras_cv/core/__init__.py,sha256=dKPKXZlBiEWDx10VdZoPvsLjt4yOLx4Id_S7Zz308r0,909
+keras_cv/bounding_box_3d/formats.py,sha256=_VLtI5w8ksveB-eJ4l1Dselw1C0fCF1fA4-myYbt28Y,1609
+keras_cv/callbacks/__init__.py,sha256=GaURZvzP51cgtturjtiGm0Kuov_0NsLWAMDvT7_ECB0,727
+keras_cv/callbacks/pycoco_callback.py,sha256=JWt8UcdesSoNSY9ZvS6NZqTBBvd2pPr1vjh5H9xgxqo,4693
+keras_cv/callbacks/pycoco_callback_test.py,sha256=SjiKTIhjkuzeC1FKAOPOJvNvIY3X7UbUuUYmEAK7kyU,3263
+keras_cv/callbacks/waymo_evaluation_callback.py,sha256=3nwLcbEwCBFkrs-FoURCRibivNHowyWi3OECDK2S4PI,6912
+keras_cv/callbacks/waymo_evaluation_callback_test.py,sha256=A9eEyqeETWt_FIwSel43VRGWmGntxNfM7m4FNdtxOrY,3413
+keras_cv/core/__init__.py,sha256=CKYvfvZOXfcuH9XwYxt1XD-aR316P6mfXuYJSkHoAww,936
 keras_cv/core/factor_sampler/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/core/factor_sampler/constant_factor_sampler.py,sha256=YumVX_IKs6x3LGCbS-iFJjFcbHgLCZWH2EsC2QT7vug,1555
+keras_cv/core/factor_sampler/constant_factor_sampler.py,sha256=yyn__-MmJAqVTJeExFvY6NVkSij0fO--bLVsp1hqvf8,1667
 keras_cv/core/factor_sampler/constant_factor_sampler_test.py,sha256=2BzWrYJXlQZLT6OJwqb52qtQOXHA8srLGvqBcPhu3gI,964
-keras_cv/core/factor_sampler/factor_sampler.py,sha256=ekQbMZoiO_UCq67fafrygU_rNJqcQE5WPr5YpQXzoO4,1335
-keras_cv/core/factor_sampler/normal_factor_sampler.py,sha256=_V77CRGGoVALrVbpZTFjX8PiT_2_F751BGLY4oyLkbE,2393
-keras_cv/core/factor_sampler/normal_factor_sampler_test_.py,sha256=6QSjOVN4-zUe2Fb2kSi0mupa-93EYcrrHIpif7BfjZ0,1076
-keras_cv/core/factor_sampler/uniform_factor_sampler.py,sha256=im8uxZf-rhAGXYmS0b34mcm69A8D4qUGGjulaTm-3v4,2014
+keras_cv/core/factor_sampler/factor_sampler.py,sha256=PYBNGRE1SxHKZ6AdgP5TrxyAxc0pQmIIVfkfNFDQ1Bo,1343
+keras_cv/core/factor_sampler/normal_factor_sampler.py,sha256=wzBfrG2aXszrR7bob0cPQaDbZ8vLNLwMXLiT8vYQz6c,2501
+keras_cv/core/factor_sampler/normal_factor_sampler_test_.py,sha256=cQqqe87wUlglJn5K1YwYAeF9TiyDTJw0q1fcNJKPsXU,1120
+keras_cv/core/factor_sampler/uniform_factor_sampler.py,sha256=KGYFd4pCSGlA4yOU1xlfwQ0rYj6gn5cZeryXKgq_pqE,2183
 keras_cv/core/factor_sampler/uniform_factor_sampler_test.py,sha256=OMlxuiBWKcODLvYZUaRXgsn9R6qccw5_-3rA5RF3epc,1026
 keras_cv/custom_ops/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/datasets/__init__.py,sha256=oNKhcaJac7rrFtur5fCmh8Vo3KzF8s-SnZZ1cCOuFo4,625
 keras_cv/datasets/imagenet/__init__.py,sha256=X2ZFXZYF8VtYzfbDRwr0uQGiDfkNDuRDiHsjX9pLgeA,633
-keras_cv/datasets/imagenet/load.py,sha256=qkoJinPB2GrE5x6RKLiGDk4myE9oLxTlYr0_ik98w0c,4391
+keras_cv/datasets/imagenet/load.py,sha256=wyIyaBzEO_Gh72yKdOR4BhiKsOFEG9lhM7ykbZ4VtNE,4439
 keras_cv/datasets/pascal_voc/__init__.py,sha256=Cc37KmVJB0wxRb63vQa017x9u3G_5SXqv_YWlJs3of0,635
-keras_cv/datasets/pascal_voc/load.py,sha256=xFpC5-CbpDtY0Hl6EkT0iXICWrYyjXm5ESCQXvpHAoE,3702
-keras_cv/datasets/pascal_voc/segmentation.py,sha256=CPiW6S7RZu36nEA4hg1WhkK2H6IMw5vNlf4BcLiTcCg,18489
-keras_cv/datasets/pascal_voc/segmentation_test.py,sha256=JP7yyXt3CzACldYeVZmgzY9obQNINEMsLaM5nRERoeg,11789
-keras_cv/datasets/waymo/__init__.py,sha256=AQv8a16Z9l_2kBJYF2CmZRSS0x5yBsIytk4zB6_aEdI,926
-keras_cv/datasets/waymo/load.py,sha256=tIXYiZo-36zLeUFIOdDxuEK5G27Wdi8xJifhYUuGEdg,2767
-keras_cv/datasets/waymo/load_test.py,sha256=xZo5neRNOGLQ8tqoH_mYqKKTWabICmrTx-VVOv6Be84,2098
+keras_cv/datasets/pascal_voc/load.py,sha256=mZywRZankqaOcGudNL_J-NCCJ_h17ibuHaEHBOEXfFA,3642
+keras_cv/datasets/pascal_voc/segmentation.py,sha256=tdhjOfWItn4hjb3kclnf5cYAatPsUfyjcc5oLGGMFco,18789
+keras_cv/datasets/pascal_voc/segmentation_test.py,sha256=8ymPaN1alV_L8Hmt5_VNGTOuLec0F_gbi-jRZ-nbc7M,12179
+keras_cv/datasets/waymo/__init__.py,sha256=NEsHt4F6ds54pdXVnyyAKaWyRfEgH4_pQWurjjUQq2A,1103
+keras_cv/datasets/waymo/load.py,sha256=W4ap6acHH-9Ft4RSoGbKwDN58esKb2rkV5Nhemb_E7E,2935
+keras_cv/datasets/waymo/load_test.py,sha256=wBEmsPxj-QGovW1-UoIiGAGZah8lsTU2LHURkWd-wsU,2114
 keras_cv/datasets/waymo/struct.py,sha256=5x_AQt9cG73e2V-E3LvaJ_OU4q44gU7DyE3b7JmOzQU,1980
-keras_cv/datasets/waymo/transformer.py,sha256=C8W0VenGwjPeymnI-BGRwMQMpM2OTcxvWM7bcDn7mM0,25514
-keras_cv/datasets/waymo/transformer_test.py,sha256=HShQGhgRR1gL0eu7hoirnUip_dJM4rLIVyJboyfdZMI,5163
+keras_cv/datasets/waymo/transformer.py,sha256=AH-BHltxSrPtqrySEq1akTEIV8X5mAIGtO4hVetUNJk,27253
+keras_cv/datasets/waymo/transformer_test.py,sha256=tmTtyO_B5ra3RFB6stssf2wTOatOmaceaeK2QcEXTb0,6947
 keras_cv/keypoint/__init__.py,sha256=r64NpD6b0BT0CS_hmpr_60Ah74pXgyYANss1x2bO2r0,783
-keras_cv/keypoint/converters.py,sha256=k0-XJ-O6ZSagbbcmu8NrrQCpE3QSWU3lsnJcg_ZsI6M,6945
-keras_cv/keypoint/converters_test.py,sha256=xDaxxVgIODQuaN4fW7W2_6BINslpRI35jvQGCsWjcjA,5121
-keras_cv/keypoint/formats.py,sha256=1Ub40ccsbV1dQKq8E_dBQSx9T9u7_NzhqYMgvJxNp8M,1726
+keras_cv/keypoint/converters.py,sha256=VKklqsoBwwtEmfwqOHBe0DCZc55orzbC1-xvC6V-YGk,6955
+keras_cv/keypoint/converters_test.py,sha256=qpgRbTtaH5oUe18sj9itOR1_mmU-4jcyXUnUDKMdZjc,5181
+keras_cv/keypoint/formats.py,sha256=b7vVRK9ePdC9lMeLNOlIRg_lnq8anuDsHu6o8Lq8XJU,1725
 keras_cv/keypoint/utils.py,sha256=H5SCMC0WWm8rclaAJr7pIHzitQuljbZ8FxCR4GYfLu8,1597
-keras_cv/keypoint/utils_test.py,sha256=V8UnnTRfwEZf6kNuEr3pcZ9bpCX47zRlPzTECAi8rjM,1940
-keras_cv/layers/__init__.py,sha256=DTbJF1mf0V69bc4p6-vzhPvXMzHYh6JFwJruuAtZu6E,5897
-keras_cv/layers/feature_pyramid.py,sha256=qJqx8SXrwh93nedW7oKJSP5tWBvpT-mOGf1wRGUEy_E,8720
-keras_cv/layers/feature_pyramid_test.py,sha256=Rn_Gp067ZR99VMgdhC6d3P7pChd1XBz7bwlqyk87T2Q,4880
-keras_cv/layers/fusedmbconv.py,sha256=k-VAYo1lhaTFI0LjGxqwyzAi_HmHk_EoqlGeECp_9I8,7953
-keras_cv/layers/fusedmbconv_test.py,sha256=y85TvkSMqrwo8rDE6r523fI--FDsOHBmF7GZutVBvfU,2214
-keras_cv/layers/mbconv.py,sha256=Q1GKW4p1YJWTlRjCjFPGqvNdwzoa1iaJfYNLnlaSoSk,8165
-keras_cv/layers/mbconv_test.py,sha256=GCu8p7N1kZeo4qy5fI_qF-lZHrN5uifS62vjx0IgP7Q,2179
-keras_cv/layers/serialization_test.py,sha256=MEnD-ksTc6V8zJ6pxcH1t6GFM4FlJ7dn64j8heIu3n0,12304
-keras_cv/layers/spatial_pyramid.py,sha256=6e4FHA1gPhkNy1Z8F_rHH9EfycAUwSpaoWChIj0WAH8,6304
-keras_cv/layers/spatial_pyramid_test.py,sha256=2oqT68vZ0YCTyEGznaUWw8S32n29DlpU9q9tyc1vOyA,1264
-keras_cv/layers/transformer_encoder.py,sha256=Ofnn5qTvyx5A9TnKtg4mOcdQ4Pb-FSCJJkhMJkR1L7E,4977
-keras_cv/layers/transformer_encoder_test.py,sha256=nYYZpCUfG4Qg3o3wPrELrrgnFO6CsBodh18XurFTMXk,2105
-keras_cv/layers/vit_layers.py,sha256=SfyZwMMsewbWPOKMPkBMVDt9O9XSsL5nwvGOsdeSCg0,7570
-keras_cv/layers/vit_layers_test.py,sha256=04JPOyN9CVFd__3w1Y-oEWxx_e5K_tw0ne6eG8Lb3cE,2793
+keras_cv/keypoint/utils_test.py,sha256=_pcpWKS6zI5rj_vxM8PsaLRy7R2n5JA11hE4aaan25E,2000
+keras_cv/layers/__init__.py,sha256=gpmQu4OcLwLHAJFLkBO23eBnkMom6m9oKbfhWwiPhNI,6057
+keras_cv/layers/feature_pyramid.py,sha256=KcNUCNRTcJwjXVbbvUUN4EY3oPQ4pZZbZ9hEMdnZqo4,8828
+keras_cv/layers/feature_pyramid_test.py,sha256=BwO_c3GFiITgxsdUJ4xN-EelrQkmoof-pvrOV-7XUGI,5125
+keras_cv/layers/fusedmbconv.py,sha256=MqLvyOR5wjs4kdlkWvPzdgFd1PogH5z2xJwh80xVDpk,8076
+keras_cv/layers/fusedmbconv_test.py,sha256=aEfM3bhsqI2CY3TrxJOpYliCMxGnQSly7wz3_vNmpCg,2273
+keras_cv/layers/mbconv.py,sha256=6YbzUBk19egc7L4qcTSgezhYJ5mhWmz3ON761irZKfM,8349
+keras_cv/layers/mbconv_test.py,sha256=gx0ylTG9BL1kopIh8EgIYpOILNQ8ck4Amp-skYKVzi0,2216
+keras_cv/layers/serialization_test.py,sha256=41RwgyRNRa6NN3vImEmLzJOLr9GB7ypvYVnSQqYeaAQ,12344
+keras_cv/layers/spatial_pyramid.py,sha256=moO2kwHzMORqv8KL25-dNStPXCKc6rqm0EsJLVreq3E,6281
+keras_cv/layers/spatial_pyramid_test.py,sha256=pcCIG967qGmE5VI7hfrln59VRCzk7-WySFpXU4zfrQU,1290
+keras_cv/layers/transformer_encoder.py,sha256=VG_j7yNYWfy0V8noJAXHOSiIGhW2FQ62g5az42Jk8aw,5249
+keras_cv/layers/transformer_encoder_test.py,sha256=OoZF6xLbZSrQC7L0COMyiXcow8pVFX256o6DsjvB2U4,2135
+keras_cv/layers/vit_layers.py,sha256=eDe99utjMz9CrZYtWmAVwj6u3Q1BKqwgjmRvbW2VQT8,7723
+keras_cv/layers/vit_layers_test.py,sha256=ykwjmxfMuCiJAYZf46W_0oQt-wEgQ8LRhWSYnQ30zvQ,2886
 keras_cv/layers/object_detection/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/layers/object_detection/anchor_generator.py,sha256=ZQtP_VX7ISEJX90lnKM9oPmniJK27Xw8Oq1AhWvs5D0,11151
+keras_cv/layers/object_detection/anchor_generator.py,sha256=5Ezt0HR8TcZ82RoPsxswqUsPJDDqf4RGE1t-cnsUX7s,11340
 keras_cv/layers/object_detection/anchor_generator_test.py,sha256=KfqFHUU3zpmfIT9LCsxXbw_wN58GELkbWvrbKi_blWw,6318
-keras_cv/layers/object_detection/box_matcher.py,sha256=7rWM46xfO-MR_zjhFVPhfAgUrxbJOjo-78huMISKo8k,11167
-keras_cv/layers/object_detection/box_matcher_test.py,sha256=FsMq4MpcC_mjEBqrSYZP-J5otv9dnJB1CwN3dESiYT0,4873
-keras_cv/layers/object_detection/multi_class_non_max_suppression.py,sha256=icFU5mkI4o-l42AFFWKERqkCiIuip6zGdNYFvFnhK3M,4839
-keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py,sha256=naQvziGUNGM20Gr0xVGXRCSSB7PJCrCZc13B7eP38MI,2636
-keras_cv/layers/object_detection/retina_net_label_encoder.py,sha256=tI2wuW9u-KSydMmmhieyd_KnroNdNWLuFBOL291mpN0,9481
-keras_cv/layers/object_detection/retina_net_label_encoder_test.py,sha256=-kXvuoOhIr8ngyYJo0xjMREuixrC1OppFXK0Ce3CcoM,4500
-keras_cv/layers/object_detection/roi_align.py,sha256=rGMzEohRhmDMiiHrLK1uO2aQEfrwyBGyyKH_haxc_Z0,15263
-keras_cv/layers/object_detection/roi_generator.py,sha256=ObkVvEwykEgHeonDXhila6ZuNClKXVeg8vRfHMcFap0,9674
-keras_cv/layers/object_detection/roi_generator_test.py,sha256=VHt9kACaiXH41d3vwLTC7_6ioluSSXz1HK5i4FQXF_k,8183
-keras_cv/layers/object_detection/roi_pool.py,sha256=dQp7-Yu6scu24fkBEFJNRm5k3gxwtC-Xo66ShrCCJBw,6170
-keras_cv/layers/object_detection/roi_pool_test.py,sha256=eu0UK0W9jaTipo1mw6FXcBheoJqEVkUYnfPHI6GRYOY,9223
-keras_cv/layers/object_detection/roi_sampler.py,sha256=9J3QaZ8llBZKXQ8i2sgxL0likNJmH5Oc9ZGsqQ2Hul8,8870
-keras_cv/layers/object_detection/roi_sampler_test.py,sha256=NHjvu_uOPVTTeBfW6djYGj0Q16JOHGw1NsOtAcO7tes,11080
-keras_cv/layers/object_detection/rpn_label_encoder.py,sha256=AD42gN_KnHQhCv6nh-FHSxon2IJIe93qNakEfa4Ruuo,8992
-keras_cv/layers/object_detection/rpn_label_encoder_test.py,sha256=M0ogjMkTnd5pVTqdVynOAwmkTU7qwssXDLLjyXnUiIw,5473
-keras_cv/layers/object_detection/sampling.py,sha256=hYueevuBB_qcW6ntJdRP3907DEcz0E85uBj4SV7OLgQ,3391
-keras_cv/layers/object_detection/sampling_test.py,sha256=UzpwFyA-K3ry30Zs2OPP8-JFtMwDokjSQhmanIaDcns,5894
+keras_cv/layers/object_detection/box_matcher.py,sha256=OYNhkXpRr5SDMbD-MMI936RhO-v11sIMT4CYmSN-QkM,11483
+keras_cv/layers/object_detection/box_matcher_test.py,sha256=jR2m1KFh_9g875-qksmjZdsghlJSR7ppw3slJhLXq68,4939
+keras_cv/layers/object_detection/multi_class_non_max_suppression.py,sha256=_QoL0dolU5xv0J2Zz1_UdlD308m7ptXWq7nnwwQgdzE,5140
+keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py,sha256=yW5cuqoJ4xzhWfElEYiGD-XE6aRg5aLHDeR7eAiWV8U,1610
+keras_cv/layers/object_detection/roi_align.py,sha256=BWYhDlFcccUmciUcik1nbB3ekj1UEyHRyPedFZy1bHM,15804
+keras_cv/layers/object_detection/roi_generator.py,sha256=SiVVMlrBtCuj55oBr9zkJWeYXX04p8kug4RwMWubjp4,9833
+keras_cv/layers/object_detection/roi_generator_test.py,sha256=gGwxAoefwEziGq3KYL4hYv_oV9qPRmcM6OQXCX4Omkk,9256
+keras_cv/layers/object_detection/roi_pool.py,sha256=cIYIH76gWvGtDKgmZ8NqUA4l-Sw58V9Xktp11wkN9gM,6427
+keras_cv/layers/object_detection/roi_pool_test.py,sha256=E4lKzwcuOG8y_2a6EykriLJG6Fngkk_X4GIJjdeFHfc,9712
+keras_cv/layers/object_detection/roi_sampler.py,sha256=w_0TA2Vtlgx-CGUIiEZRgg1zyn4sfiak9tUj29s43_M,9064
+keras_cv/layers/object_detection/roi_sampler_test.py,sha256=gj0H4nYgWFj1xpAn9mBPllJ4PWYGCveig5BsziGh3Bw,11644
+keras_cv/layers/object_detection/rpn_label_encoder.py,sha256=AnMNPwSr_v9tPC5rfHO4--U24gL5BDZRS22kyGX9bpc,9272
+keras_cv/layers/object_detection/rpn_label_encoder_test.py,sha256=wH2PrfMfhp2HZ3f2QI-tmlmykC8schjLfsb2k6IcoIU,5631
+keras_cv/layers/object_detection/sampling.py,sha256=YCckkFqLp71zp85JZ1DZHGtqho8W7hE2fg7IYvmovsE,3426
+keras_cv/layers/object_detection/sampling_test.py,sha256=NoDiwuAm2WqPX4l_qPtPkVw4lM4sjGNC0XleNQ9LoWo,7277
 keras_cv/layers/object_detection_3d/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/layers/object_detection_3d/center_net_label_encoder.py,sha256=X_FwA8fNWyYVQdIlexGR3AihC72xXoiMbbsaysOCNAQ,15985
-keras_cv/layers/object_detection_3d/center_net_label_encoder_test.py,sha256=Y-R0Vm5qZ_HA7Ir_M5esJxYL5s1494GoDG7l0Rw5CWA,4237
-keras_cv/layers/object_detection_3d/heatmap_decoder.py,sha256=roirtljtp7KhDnRLCNtOo2PfCVZM41D--xK1tZpO610,7983
-keras_cv/layers/object_detection_3d/voxel_utils.py,sha256=xJpQhZvkQeC7I3XcA8eGbEQ2xiZx3iMAkR5YSyZu2d8,9897
-keras_cv/layers/object_detection_3d/voxel_utils_test.py,sha256=p12izqi89-C6f57O-O8_eLnkAxKSTvTNoxvaTW6VlT0,2788
-keras_cv/layers/object_detection_3d/voxelization.py,sha256=0jl6DL6cUVA7cg-JVyVz5TjOZkX3gLR14yP-DLFz95I,8972
-keras_cv/layers/object_detection_3d/voxelization_test.py,sha256=jEPMMX-Oa7OY0l8nmvgX_kpPxkafM1XDDBLEUd8gMTM,4017
-keras_cv/layers/preprocessing/__init__.py,sha256=ggYxKepdaz2ba-Xae73WOpr1YhAuc5WAqBXqcO4zr9s,3724
-keras_cv/layers/preprocessing/aug_mix.py,sha256=ejB-tzx_YIZa0JeGGizuP2-ZPI1Q4wy4uw-BbEhapHc,12250
+keras_cv/layers/object_detection_3d/centernet_label_encoder.py,sha256=LbKJrnMXTc28yqDrRhqmNHLoZL5vTVIXHlZXO9uh2qk,16374
+keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py,sha256=MVzXwchPiaQCL_3RHfwtkpOXu7ChndiqBwbGCd4Coqw,4863
+keras_cv/layers/object_detection_3d/heatmap_decoder.py,sha256=tCaA5MEmKvhERcrtVrDc0iVZvL73Y5_1ZxK2hmr18eY,8107
+keras_cv/layers/object_detection_3d/voxel_utils.py,sha256=AM1czJfqLqaXrRjHy8XROHJcXKAA-ZfTRPtoZBAHsEc,10029
+keras_cv/layers/object_detection_3d/voxel_utils_test.py,sha256=bFR-xIGe2fda3klY13bcz4ZJtLyT_T_ita1BzZjWf9k,2841
+keras_cv/layers/object_detection_3d/voxelization.py,sha256=iYTadgC2h3yNqnwg-fpa7G62kG5rIfKLSerzP1Bccv8,9159
+keras_cv/layers/object_detection_3d/voxelization_test.py,sha256=9b9MO1RhUZXZ_1-WLcGbgahL8XLYEslLpsZvFtmdkf8,4114
+keras_cv/layers/preprocessing/__init__.py,sha256=YDXIE8StGdSaEPkRgkNKbZBv7mGZAK3gyZijgpIvE0Q,4003
+keras_cv/layers/preprocessing/aug_mix.py,sha256=TkkcX2Hnmp7SeHVT_q5dlchMZM38a_-Z3J8IVOZ9eMc,12754
 keras_cv/layers/preprocessing/aug_mix_test.py,sha256=ZXY3L4eYZKesaDV4aRa5zYP9bf49rmuEntBggpnADvo,2205
-keras_cv/layers/preprocessing/augmenter.py,sha256=JE-Dqd-mDMKWBwMv8o3PNxYZOTVFRu2-y96F5yMfvYg,1286
-keras_cv/layers/preprocessing/augmenter_test.py,sha256=vhv-YQX3Roa1eXeGMyUJGpA7lZdOVOziU1a9CPX7PNU,1903
-keras_cv/layers/preprocessing/auto_contrast.py,sha256=pGm82CQY-ClKqmpZmvm5yV6ZNK3QPRSvbnXWKOokrLk,3070
-keras_cv/layers/preprocessing/auto_contrast_test.py,sha256=tK1E87DRu03QFxwry8mAD9eNHNOS2jkg-0Zzv7lPeD0,3318
-keras_cv/layers/preprocessing/base_image_augmentation_layer.py,sha256=1z0sHz9Hr5e6LuSLhvL4B7-vFE5NtDD23jP9tmRdosY,20102
-keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py,sha256=r7oDtO-7y_ocqPW1Ius2L7R2R9z8M15XuzW9Ffgul4I,10713
-keras_cv/layers/preprocessing/channel_shuffle.py,sha256=5AH4q-NMBxtnjVXXzZ5wP8Ay7YvSxgARPhHUjwH1iks,3507
-keras_cv/layers/preprocessing/channel_shuffle_test.py,sha256=aUbUHm5S05ZY-DGMqUyxYfWBaX8oSHdEZSHsKDhvSh4,2995
-keras_cv/layers/preprocessing/cut_mix.py,sha256=U4mIC97-m3OQmHHuEKD-O3n6Js2vZmE-BN33olgLDQo,5935
-keras_cv/layers/preprocessing/cut_mix_test.py,sha256=D-WntP3UKX4dNHQADljPDZteCa3mw9lQUu715pgesh0,4980
-keras_cv/layers/preprocessing/equalization.py,sha256=p650iaU0QL8Si6BAEcDg7n0H5WWtkyh7HftJSHlY6k0,4884
-keras_cv/layers/preprocessing/equalization_test.py,sha256=6Na8wzaSzt8lUVybqJs5-ek7KW1QIOTQhuAAX1OultY,2422
-keras_cv/layers/preprocessing/fourier_mix.py,sha256=rqlblLSDWAuRAX1EDdkz1wq1qlE9uKfW3khIWHklizw,7759
-keras_cv/layers/preprocessing/fourier_mix_test.py,sha256=NGlDvX7E6Gk1D00UjN9BtOuJ9dm94_lZo-N1iZNWPuM,3387
-keras_cv/layers/preprocessing/grayscale.py,sha256=lPAfF-1sLDZny_7o-06Hz2UJAlhaoIhqP2VbuUg7N0U,3874
-keras_cv/layers/preprocessing/grayscale_test.py,sha256=kkmrsKuVsdn0WX_qplFYn78ThjIqBQXA8IOCYpFH0EY,2852
-keras_cv/layers/preprocessing/grid_mask.py,sha256=JYQaY1IpTuJf3NavlRFdHmzFcrn9dwSd1TNU1tvSUfE,9660
-keras_cv/layers/preprocessing/grid_mask_test.py,sha256=i9mHkvq6MYqwOePI3snHXpxncpssuMk-jTxdnHEQL4g,3771
-keras_cv/layers/preprocessing/jittered_resize.py,sha256=6Nz78HIGEvJcXHGQWVCo2gFmAOa5OA5KnY7okROmc0Y,9419
-keras_cv/layers/preprocessing/jittered_resize_test.py,sha256=MFApYelMXwNW0f6OKRG6AGV9w1eEhFGTmXfBYBDhvZs,6511
-keras_cv/layers/preprocessing/maybe_apply.py,sha256=vzyiIEVuBWTs7l9Ygr184M47Qnvs65vCJREY4JFaDdE,4970
-keras_cv/layers/preprocessing/maybe_apply_test.py,sha256=SAxTeFOTBVwuJi3sxlwEvf6HYjN0y3w4EogJXV8g-LI,4607
-keras_cv/layers/preprocessing/mix_up.py,sha256=5hhIYCUnOXE3Yhp4bic4GzNj5iURFzsv1tR3BI1ZGh0,5974
-keras_cv/layers/preprocessing/mix_up_test.py,sha256=D69vuZ0N1OoGXGz8giqeRxxHWKO_xO-K_FDH3DZaZLc,4436
-keras_cv/layers/preprocessing/mosaic.py,sha256=EMexmHlPKLuLiVxQvYOIitydl93DnL8EQ9bo_LHotus,12955
-keras_cv/layers/preprocessing/mosaic_test.py,sha256=y6ACI7SqY1esOFSf71MPIMqR33WpGlz-H10yxHhcT88,3581
-keras_cv/layers/preprocessing/posterization.py,sha256=cvFyQPZCwsqmWWZYexxDFsHsdLo0-K0wweb9UqxwMxc,4171
-keras_cv/layers/preprocessing/posterization_test.py,sha256=oIKk5sKSgdU-JfByQ-NhTMfNgt8avI6IIxYMlfY38Nk,3746
-keras_cv/layers/preprocessing/ragged_image_test.py,sha256=F8XQQJlt2U_qmUTh1asN1shQ2ohloo6MopK9NNPFfSU,4709
-keras_cv/layers/preprocessing/rand_augment.py,sha256=qN-ZrwPXgsLkLEFHXPIIcqduk3ZSOJDijPzufcIJxl0,10563
-keras_cv/layers/preprocessing/rand_augment_test.py,sha256=bqgbDej9LpPLwBLH134GM2Aso_FyT844C-swaKg-ohs,3544
-keras_cv/layers/preprocessing/random_aspect_ratio.py,sha256=PGEWplt7iapumOQ_YY9AhxTyiTpYSZAV1vNNpT9zevc,4567
-keras_cv/layers/preprocessing/random_aspect_ratio_test.py,sha256=xmY_7wtINHa88KskmbBbFq2fIT9necV9dl92qazGFZc,2620
-keras_cv/layers/preprocessing/random_augmentation_pipeline.py,sha256=CIAKxvIw8cbVB-GjEJWT4q_bWwXw_XZNUPaJ2uI9wtA,4391
-keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py,sha256=15d7Fhe2sVDA3sS9VdrsvddF2zPtERZNhLuiwNpfy2k,3224
-keras_cv/layers/preprocessing/random_brightness.py,sha256=r92ucAjB-GuSwgBJE6-aH-JplSm-Y8l7PB90_-RzG-U,4516
-keras_cv/layers/preprocessing/random_brightness_test.py,sha256=-QjRoJP4Jl1xn2CGh4HCYCelblyh-RkwXVJvkhFhXIE,3205
-keras_cv/layers/preprocessing/random_channel_shift.py,sha256=tRDNXiGN-BbWQePKDk8dTuWRDmdff_A4fbeOIC8lR50,4353
-keras_cv/layers/preprocessing/random_channel_shift_test.py,sha256=2v0nobVEA-UlZ8yV6Ew_B6Ia04zOH6T2whPMwBHjryQ,4133
-keras_cv/layers/preprocessing/random_choice.py,sha256=_vwNpaRp4vbV76O2qtBG_Sj8LhDNNYNt8wn7LxvTsgg,3852
-keras_cv/layers/preprocessing/random_choice_test.py,sha256=BGkPPocYc7KwLoEHZE4MOTLRlovwTzwAnICCtzHg1gU,2489
-keras_cv/layers/preprocessing/random_color_degeneration.py,sha256=ySBYinBhK9mqFFU_0PBVSTIyj4mTkWrlKqD6EUZH24c,3110
-keras_cv/layers/preprocessing/random_color_degeneration_test.py,sha256=sfqDqRjJVEZ9JaXnJtAL-DQidMeVToFgXEo0oJHaFgM,2638
-keras_cv/layers/preprocessing/random_color_jitter.py,sha256=Aoz8rBbgBw1Hc_OoovQjZwXdRJ0LbjzwVzb9QYIMluk,6568
-keras_cv/layers/preprocessing/random_color_jitter_test.py,sha256=ZSpRF70vDYLaK3Ldt_tFSNQpQ20PCuvS3YarQhRpFk4,4302
-keras_cv/layers/preprocessing/random_contrast.py,sha256=csKmxoMrJCyYG0mo5HPM1qDgX3zA93J1dQCAqEq07po,3737
-keras_cv/layers/preprocessing/random_contrast_test.py,sha256=kPfrgZVo9MikEyMUqAJpih77eof-2xUD8PDJRcMsiTE,1957
-keras_cv/layers/preprocessing/random_crop.py,sha256=ujPZc836_JOhdE8LU7-e6CvZiJTCdbV_cbWgbgoJ7mI,7260
-keras_cv/layers/preprocessing/random_crop_and_resize.py,sha256=ZPHo-uoTDQIkpNh0ehxNHvh8SAgpxkXiUg3gNdj0n0A,11232
-keras_cv/layers/preprocessing/random_crop_and_resize_test.py,sha256=Nky7ylu0Z2FCylFvz27cWoh8u6cEv3CEIZE5b_YKONE,10527
-keras_cv/layers/preprocessing/random_crop_test.py,sha256=5eKiVCnGwv_Ep8npug02usJcmKgvJtDs8-g7l1T108M,6382
-keras_cv/layers/preprocessing/random_cutout.py,sha256=ARpWFaDhX7ed4d0mCrMmQBLFklrwr__BAHDKBt3KapQ,7007
-keras_cv/layers/preprocessing/random_cutout_test.py,sha256=1fEEFfT6Rxb5IZEvCh2BrpRKvg_VT1mOLIqWSwObSUk,4904
-keras_cv/layers/preprocessing/random_flip.py,sha256=sB63u3OjUeg3CKp_dW99fjZf411jh77JD1RX5a0Ugwg,7325
-keras_cv/layers/preprocessing/random_flip_test.py,sha256=KDifJrcuOEwjG52iQCce2EnnaM_XlproS9Gt-MFa25Q,9249
-keras_cv/layers/preprocessing/random_gaussian_blur.py,sha256=3YLpVUAfzgWNDg0d9iZpbswpLAolQhCUJ3wcONAGHQE,4490
-keras_cv/layers/preprocessing/random_gaussian_blur_test.py,sha256=UgunSN4ubVPuUO3q_wdfV2LmY94Nvq8guvYqtS2K8TA,3157
-keras_cv/layers/preprocessing/random_hue.py,sha256=VfXIrz7qf-74rwDZkgwUFiTX4IIoKXSHq1Rq9z9gPy0,4142
-keras_cv/layers/preprocessing/random_hue_test.py,sha256=sU5meRa1KMXPJOb1ppSyvbXAvaYYSIZu0eZNvSG0NhE,4023
-keras_cv/layers/preprocessing/random_jpeg_quality.py,sha256=GHXRjepunVYUapKqI7bKvypExwQw2Ta0xLIigSZi0FI,2922
+keras_cv/layers/preprocessing/auto_contrast.py,sha256=8TOYm11omxHoR6cdfVj0YjeIbdxLZYgkAY2uJUAe_p0,3509
+keras_cv/layers/preprocessing/auto_contrast_test.py,sha256=h9q40em7oa9MVH-J05MOYM9-9oXmCnJGQ9T_cwYfCa0,3331
+keras_cv/layers/preprocessing/base_image_augmentation_layer.py,sha256=_zz2ltzbTw555AkwRXcCMU24QBNiHdHD9ICLZIKni5E,20504
+keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py,sha256=FFcoKKdpAkgBNWYwvLHg-v6wTfZhyJFhtJeF9amUXEI,10953
+keras_cv/layers/preprocessing/channel_shuffle.py,sha256=Vl8do16_OhmFfNQht7-wT6sYxEjLBDLazvcFZdOPOOU,4559
+keras_cv/layers/preprocessing/channel_shuffle_test.py,sha256=Or2CYMmxNoJ2HWUMt7IanqmcHtQrfYCcljL6nwm-MXc,4065
+keras_cv/layers/preprocessing/cut_mix.py,sha256=d41F7OQPTwShcZIHxYq0lsq7eAhj8SPzDzDou87ldMs,5992
+keras_cv/layers/preprocessing/cut_mix_test.py,sha256=UuC7ZXugY5gFOF3iuWExf_AxgYsLELqamw9wlcdZsgU,5040
+keras_cv/layers/preprocessing/equalization.py,sha256=Go5iZ-2BqOhsNegljii_bH096HYUyKK4qQ0FDgyyOr0,4929
+keras_cv/layers/preprocessing/equalization_test.py,sha256=0MhbiO8wp_30YlkSoxdsZf7wawh20DVO3qx8zQqVyf8,2446
+keras_cv/layers/preprocessing/fourier_mix.py,sha256=Mg8nQymcoa_h5tCToKhwxyJZnW1qnIQ2Y-fw-KAS1QM,8025
+keras_cv/layers/preprocessing/fourier_mix_test.py,sha256=2D6YCdpq120gB9uHkRItdN4v5jYgNbfN7699NtV7Jug,3447
+keras_cv/layers/preprocessing/grayscale.py,sha256=mM8jwsRqxsDb016u7LaOImUU823udd59xMu6Byj3GAc,3841
+keras_cv/layers/preprocessing/grayscale_test.py,sha256=JTVymTD9mBBh5Zu-Akkf9Pck9Rg-2kPwXcMmdRjN4Kc,2820
+keras_cv/layers/preprocessing/grid_mask.py,sha256=qeyFEi9_mb6YFHayr8MJoLxRKpQaJHqPD9xRvPpXj6o,9733
+keras_cv/layers/preprocessing/grid_mask_test.py,sha256=RFsl45u6Yi_nG4lorqDQz6FJahRF1YzaZOcrXBsJmHU,3823
+keras_cv/layers/preprocessing/jittered_resize.py,sha256=R_FVyRid5n-DNh6KNbpkiMapdTpcMMKHIlI2uLfySWE,11349
+keras_cv/layers/preprocessing/jittered_resize_test.py,sha256=1ahFxImZ_XDBnYT7v3BNa9VTlankWdsSgbLpH6enlEo,7360
+keras_cv/layers/preprocessing/maybe_apply.py,sha256=ZuolFxkYzDReMNjBQrkiBGLLmyAuawgfRQmijdgmDvc,5019
+keras_cv/layers/preprocessing/maybe_apply_test.py,sha256=PUiV4DITtMM1zVohrkaGIPuHCZQZTTwf9MDHoWW3rSs,4634
+keras_cv/layers/preprocessing/mix_up.py,sha256=rGyP0c0by3iHA1kEu9s56uaJNfGE4OLXffe7B4B4rco,6100
+keras_cv/layers/preprocessing/mix_up_test.py,sha256=AStUGw7CmFgQviVAOz5Bva4oijJL-jTZRlloijhUh4o,4581
+keras_cv/layers/preprocessing/mosaic.py,sha256=u4ms7qWnSQ_a-Ms9KwlfVXQ84V-oxhBsEQFKd8ltijg,13424
+keras_cv/layers/preprocessing/mosaic_test.py,sha256=TbxJbwGqMkp3O_ilSuJNH1_gQC2-s7lj3HamHz41vX4,3726
+keras_cv/layers/preprocessing/posterization.py,sha256=z_i8-SeHd20hppTrWlzrtggPfgf8145ZGkVDJnaczIQ,4237
+keras_cv/layers/preprocessing/posterization_test.py,sha256=GX4UvVllEYKcquYouF7n0MdCBtsz-1VRiWolN7CY5Mo,3792
+keras_cv/layers/preprocessing/ragged_image_test.py,sha256=bCqlH0kqevWdHuiTIHh0xf918DT1u61bn61-CawKGyQ,5343
+keras_cv/layers/preprocessing/rand_augment.py,sha256=GhSySn_Gv2WNlO0SP2a91lwjOTWNFhHXXxi43KICNTw,10805
+keras_cv/layers/preprocessing/rand_augment_test.py,sha256=Zy-VCR9TXY2yF8ycMe2SGaWG5vT5rUMco1uglqg1Ndk,3758
+keras_cv/layers/preprocessing/random_aspect_ratio.py,sha256=cSDj28L__z33lxtDyGRc2YzovjoSJmKaq3iInKPbq6o,4683
+keras_cv/layers/preprocessing/random_aspect_ratio_test.py,sha256=R6kULrL5FggsQO88gYJhC0dIqz3FpDV6InLoL830aLw,2277
+keras_cv/layers/preprocessing/random_augmentation_pipeline.py,sha256=GmnTpiS7DW8vzbbs-PrFWN1VIIEH_kGSFYg6-t_Bojk,4754
+keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py,sha256=Y255n0YDCPFHd1rwFmKexba4PR6r5lQ3IUAzNjrS9oI,3340
+keras_cv/layers/preprocessing/random_brightness.py,sha256=YKj4hVwfm-5EPytYt2mOT241nnf3Dic2NU0C4QqvMpI,5241
+keras_cv/layers/preprocessing/random_brightness_test.py,sha256=JMN9MdzDj1zoqP4ykKuPg_LMfz99TLF3IAsfRT88xzg,3337
+keras_cv/layers/preprocessing/random_channel_shift.py,sha256=5BpP2n-P7XQoD4l0f6b-o3i7qJry44WKj4HRxpZ906g,4396
+keras_cv/layers/preprocessing/random_channel_shift_test.py,sha256=ZhLze0K1a-OM1i50F6F9SFzKVaBbUQWc3JeEs2Av-N0,3964
+keras_cv/layers/preprocessing/random_choice.py,sha256=49rSgIhgtOXpw_td1lMIIeX4xQ03UDLEBw-TqGXrCh8,4503
+keras_cv/layers/preprocessing/random_choice_test.py,sha256=rRJs_svcz6T7sx-WTXEMivjwKGQqv0wemOgkss1bns0,3316
+keras_cv/layers/preprocessing/random_color_degeneration.py,sha256=53KaEbV3Ele0x4yIc3zKRxVRH0QbbbZiiJdCoftOwbE,3392
+keras_cv/layers/preprocessing/random_color_degeneration_test.py,sha256=CGNpf979r5Zyfo05Ipgr4vBOwRKoj30atWLlSBtfBGs,2648
+keras_cv/layers/preprocessing/random_color_jitter.py,sha256=Z6TYcA-38KHwAUB57mZ1UsLpWwvNgCDyIIAr3KNowb8,6946
+keras_cv/layers/preprocessing/random_color_jitter_test.py,sha256=Z7aXMk74mCWWmXwFE46DButJpQiBPoP7yJUNqfk4QJU,3902
+keras_cv/layers/preprocessing/random_contrast.py,sha256=AcNGGlgJK-W-sg9gfJOUOo3uLzxORIEI4FLGlTw3zFM,4864
+keras_cv/layers/preprocessing/random_contrast_test.py,sha256=NVTvsRThc3WeKVz8-hMPuK5wsia_4Hkw8u0G4pzyXF8,2213
+keras_cv/layers/preprocessing/random_crop.py,sha256=Zl8ZEi_RIEUnGdPyfQ3oOuVPukH19ys3K0tYKQ_-2nk,10847
+keras_cv/layers/preprocessing/random_crop_and_resize.py,sha256=uVwAHrc4B_Ls1amzy4LHjUMuHHapJaHG20PWQCNUUso,11178
+keras_cv/layers/preprocessing/random_crop_and_resize_test.py,sha256=9UgxIClGEAuJWF_6Ny9NMCkS3mf8Hjcoha35ehhykvs,10241
+keras_cv/layers/preprocessing/random_crop_test.py,sha256=KDSXAex569_MhVVxzRAN4Vc_rXmZgoWLOTk2WuaJG8Q,9856
+keras_cv/layers/preprocessing/random_cutout.py,sha256=BGA0uDt-Lxmmns1RzELY7mM_d03JVvEOAxCftUUniaY,7024
+keras_cv/layers/preprocessing/random_cutout_test.py,sha256=9isiv_0Vj3nvZAXgxgFCoB-9iNFvlyAZ9Oiiqf-AR4s,4978
+keras_cv/layers/preprocessing/random_flip.py,sha256=R850U1PqpMhkuQ7v6vAaL4Yj_nrzo29GgfZhX-3tbkg,9003
+keras_cv/layers/preprocessing/random_flip_test.py,sha256=pjTDGOywZR19vItJ4rwNTyNbxIRB3w-8yJQpP5LUvFg,10994
+keras_cv/layers/preprocessing/random_gaussian_blur.py,sha256=Qi_xh53gz0mU3idr5pjKRdPzX07dAcPKJNeejpdQt8c,4576
+keras_cv/layers/preprocessing/random_gaussian_blur_test.py,sha256=qKXMbClcRqadaO_4Zs97G-kWMTGta2tju9J04xeNOAQ,3245
+keras_cv/layers/preprocessing/random_hue.py,sha256=GohDofOhXEHu6Pb3mP4F4IwVxZmzgfqLik28H4Bozo0,5433
+keras_cv/layers/preprocessing/random_hue_test.py,sha256=upvOGOXGsfdBkmNdkQ9E9_A0d1zInJjVDpe_BAIwIqc,4230
+keras_cv/layers/preprocessing/random_jpeg_quality.py,sha256=OJg6ldVju9p2SfEg3BzP3lpebEW4DI6S-jOlEf4EXfY,3021
 keras_cv/layers/preprocessing/random_jpeg_quality_test.py,sha256=C5MwpgOivDbwKJyh8iUjqXdBnpfdlaPDwTt6AVCumKc,1984
-keras_cv/layers/preprocessing/random_rotation.py,sha256=awhmXnLmqCuDMeiD27aNB__eUK_cn3EA-alXGupZ75E,11761
-keras_cv/layers/preprocessing/random_rotation_test.py,sha256=ZJobjWj2a9PUE_donhTZEiJdPHGJ9_Zbe_omJL-GJro,7136
-keras_cv/layers/preprocessing/random_saturation.py,sha256=xA1Vs17ikqqNjybxoaysPEB2dq2jsHsOZ83Tluikcb4,3867
-keras_cv/layers/preprocessing/random_saturation_test.py,sha256=wNUvkUv3sVv8yDCkzHXyjifBwK5O5MHZJZtaSynxu9o,3704
-keras_cv/layers/preprocessing/random_sharpness.py,sha256=-HGZtqPy8W0ffngHOo53N28at9fphjpxNkBiuw4_5GA,5286
-keras_cv/layers/preprocessing/random_sharpness_test.py,sha256=i-zjftx6MxToNw7KMcN9-7Rs7NQtKv_T1goEHo8NtqU,2348
-keras_cv/layers/preprocessing/random_shear.py,sha256=77zvkeoSWkI9_Iu3ONp_ZhspXI75m23Sy4PpJkW2TFg,12056
-keras_cv/layers/preprocessing/random_shear_test.py,sha256=EcigDqh-N031QD_fDv9eaHC0X5l6QssQJ1qCvCIKZHI,7941
-keras_cv/layers/preprocessing/random_translation.py,sha256=7ZbTIjbLur36LD3KROTGog0VGQSGrPWa5oX5yw6JVQA,10215
-keras_cv/layers/preprocessing/random_translation_test.py,sha256=696ckBupyY6XQIQfJx7mZIwvHdvjpTS-tAxOmK9bDkw,8662
-keras_cv/layers/preprocessing/random_zoom.py,sha256=enRWNxytBPzfVOeV-UezUB5ZwGiCSHSk-jziU04OBLU,9600
-keras_cv/layers/preprocessing/random_zoom_test.py,sha256=eLyh856aw_zjwrWmOUVVc3OBbW-HYxE0Ck4P8ktpCN0,5766
-keras_cv/layers/preprocessing/randomly_zoomed_crop.py,sha256=qjLWvX0K04Gvl2ctcOWFnVqQr5bEabvhjbhVQb7kXw4,9113
-keras_cv/layers/preprocessing/randomly_zoomed_crop_test.py,sha256=U1xVPzYR8GTEPOiEUJusPYFCXF2azNbj0s5PCaANlqs,5191
-keras_cv/layers/preprocessing/repeated_augmentation.py,sha256=iEc__w-yoHGGsCfEoKatBZn4UZaEqjIVchH3CicTXuk,4287
+keras_cv/layers/preprocessing/random_rotation.py,sha256=jXciaVvLmVok_dqFSEEcqt1Ca9GdY5pIrW48zz7desY,12266
+keras_cv/layers/preprocessing/random_rotation_test.py,sha256=p6uwKrYEcirGUWiW-R9g-q-4UPiCNqcfwzqCWVQAQgY,7395
+keras_cv/layers/preprocessing/random_saturation.py,sha256=0nHv80-IaL_xdY3ftHLKsHYj6OoujRyhQwrUM9C6nUo,4951
+keras_cv/layers/preprocessing/random_saturation_test.py,sha256=ojtam1LOHjSBr89i3ZpOE2fQWSIR7o8PZsmk2HOMzyI,8248
+keras_cv/layers/preprocessing/random_sharpness.py,sha256=leperIQmL7LsDO-IOHJG3iX04hlBpvk4IYWzA4TPARY,5813
+keras_cv/layers/preprocessing/random_sharpness_test.py,sha256=0Il3x6WsuvI13U-UWglj_zvqYwnGG_wV8whCLJVDII8,2724
+keras_cv/layers/preprocessing/random_shear.py,sha256=k1ljbr-qij1Ck3qRrVZQRjkh1HOyPbiw4jH9chJWZZ0,12947
+keras_cv/layers/preprocessing/random_shear_test.py,sha256=hDSaeI1uNB6YVV1_d54yhSkrZfPgqCh76oAaaAbc1ok,9331
+keras_cv/layers/preprocessing/random_translation.py,sha256=IGRJaJoqj3F9uqx996koQFv8pv7EwC6Ui5nc0SYVwAg,11148
+keras_cv/layers/preprocessing/random_translation_test.py,sha256=14K-lrHIAZvKty3Je8VE0QUs_RfZ7Rb7VwNLf4mlWGw,8917
+keras_cv/layers/preprocessing/random_zoom.py,sha256=AsHgGYsffTG_ohlDsnzMZ4mlD1gh-8SQoNAiVD9Yd80,10340
+keras_cv/layers/preprocessing/random_zoom_test.py,sha256=98ug5ZDgBI7Bq84s97tt22Y7WOA9I5r1j_qNaxNj2-Q,5859
+keras_cv/layers/preprocessing/randomly_zoomed_crop.py,sha256=iqeQXooCQQNxzZZkybxTS7wI1zwQ3--UhVhgGwMubR4,11418
+keras_cv/layers/preprocessing/randomly_zoomed_crop_test.py,sha256=OWeI-ix2Kj465GSyeDRIJYcIR3Lbj_a1DftQW9WPUNM,7266
+keras_cv/layers/preprocessing/repeated_augmentation.py,sha256=xtZy7qGY4hhrXKjzG6tCubGjOqaqVHtYoO5qjKozm-Q,4677
 keras_cv/layers/preprocessing/repeated_augmentation_test.py,sha256=HzlBGQOAv0rwGaYiVhpHmU50Q808zlrcpqBaYe2bBLE,1753
-keras_cv/layers/preprocessing/rescaling.py,sha256=JiqaH0OKvvI14CSDDchIOmE01IGKKDB8ltr6Y42Qf6w,2777
+keras_cv/layers/preprocessing/rescaling.py,sha256=hor0dxiHPGHqak6HIGKjqsDvd7XHHg2WuPmEy2Giz70,2766
 keras_cv/layers/preprocessing/rescaling_test.py,sha256=G7pJGDkFGTSqnKxpLT9VoP4433uzQGo2y9LIud8SJdI,2123
-keras_cv/layers/preprocessing/resizing.py,sha256=XAbifxbLqBZ0XiITAvDiCuJWjXAFQDZ4mxZdDr6-5zk,12849
-keras_cv/layers/preprocessing/resizing_test.py,sha256=Ga7gniQaMQQI5XISG3HqyUf7tRVULHe3BYn7zxlwai4,11721
-keras_cv/layers/preprocessing/solarization.py,sha256=HrFV-XFc1WEV1vicx4ecLPoQuijxLOIzOOkEbI8ONP4,5189
-keras_cv/layers/preprocessing/solarization_test.py,sha256=5WZsaCZcQKbK1LdaOXS-drdahtLl_jLKtZOmAWT9-BY,2718
-keras_cv/layers/preprocessing/with_labels_test.py,sha256=89vbc5r5LR32KZVWLbsEjWxXD2rynQMcEf0bWOwRxrI,4703
-keras_cv/layers/preprocessing/with_mixed_precision_test.py,sha256=CAFxz-Rm4kLhoQWR05EuJ6u_AMNyO_WGyoDoUwALQGY,5229
-keras_cv/layers/preprocessing/with_segmentation_masks_test.py,sha256=gnHYlEguFW2ko7nuQuyuJiUjF5fs1zYb7blp5cilx2A,4521
-keras_cv/layers/preprocessing_3d/__init__.py,sha256=M_FDAzTiCFPaniQs3WbhrYYvteWu_A_wa7TtYFcPnZw,1737
-keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py,sha256=mYY3EQ8simDoO--1th9FARR1aHf3qAni5AJsIWm3ewI,8050
-keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py,sha256=R4nVYkqAgPIGi-Do3_TQX6JFT0uLlo2eU7Y3bV1a6Ps,4761
-keras_cv/layers/preprocessing_3d/frustum_random_dropping_points.py,sha256=O1U8HUQ2WLwvtUGFkE2yHfexXk5vJrXuzVbnQ9IiV_M,5403
-keras_cv/layers/preprocessing_3d/frustum_random_dropping_points_test.py,sha256=mj9pfYZWhaO7ybezz6JCSvaZXfFTwbmUP7XlRth1EDU,3597
-keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise.py,sha256=_Y5mbsNkcaR1Eughmk8K895p5OpmOkzpWRwULXMR8-w,5852
-keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise_test.py,sha256=qkxk9ij5Cdfr4zDfbnNvrZyohTPzw-N-xlnLEnf6FDQ,6926
-keras_cv/layers/preprocessing_3d/global_random_dropping_points.py,sha256=Qr5-CoqiwYzDb-ZUpEvKh4rBczuk-6V7djN175lnW6g,3080
-keras_cv/layers/preprocessing_3d/global_random_dropping_points_test.py,sha256=M30m70y_BCcfEK8V-9UEvFAnHSY2RJDX58bY1JjYY8E,3494
-keras_cv/layers/preprocessing_3d/global_random_flip.py,sha256=M9CXHRjtWvnzKdw3arjWEbVl5Oa_d13LtxDmHRLyMto,4541
-keras_cv/layers/preprocessing_3d/global_random_flip_test.py,sha256=59r5PS_3r_yk_8x6Uo3x55vtzVuV2ZXnZa3Pe7oOUfI,3187
-keras_cv/layers/preprocessing_3d/global_random_rotation.py,sha256=wjjiuMzuifvHJDcJMKjSU_M_TTsPsPDSiLNoNxn1WOw,5514
-keras_cv/layers/preprocessing_3d/global_random_rotation_test.py,sha256=g-ah5_AMMKS-21duZbtKiKIpIrSn5e3EEVZfOXhsImg,3049
-keras_cv/layers/preprocessing_3d/global_random_scaling.py,sha256=7DzXhu30AjNSwvMJ4hssBq2auDho7WGjzxQ2MPEGpLs,6902
-keras_cv/layers/preprocessing_3d/global_random_scaling_test.py,sha256=1StypkQhbQYZo_pxLftqKiFr50MgFGtxiMN9RGHDsMM,4579
-keras_cv/layers/preprocessing_3d/global_random_translation.py,sha256=uTV1KAEIYNag-2x141rVJ--7m4w5h12ZHA63cDONfY0,4610
-keras_cv/layers/preprocessing_3d/global_random_translation_test.py,sha256=rdtkGOXhfYn2lpUheuyJNBBqXYjbPMs_wGv216ucuog,2847
-keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes.py,sha256=FF5RR-gRVR08tpBTA-mwfdOZCJra0wj1UTXZZNWZU14,11522
-keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes_test.py,sha256=U4-DvI_dRSahHvnk5XMi_TvYElrZTEpH32MMvoCw73c,8013
-keras_cv/layers/preprocessing_3d/random_copy_paste.py,sha256=07HbcDEm6dflu46MHKTRDd-HPKBJCB1pBsywKO6sBR0,12574
-keras_cv/layers/preprocessing_3d/random_copy_paste_test.py,sha256=uJUeIWLR97yNIJfsUZrRQQwGXAng4j3XNHMzrkqqoGg,8288
-keras_cv/layers/preprocessing_3d/random_drop_box.py,sha256=ktH3F3h8WrFm5ymAW0K81NVt2lorp6pV5M2gLmvasLI,5476
-keras_cv/layers/preprocessing_3d/random_drop_box_test.py,sha256=BAPe28buWphUM8C44mOTcjQkW9O-nRJ2cWC6uAvQv0E,12278
-keras_cv/layers/preprocessing_3d/swap_background.py,sha256=G0WG4RtjtnztKqjImNJImwip7vRG9AwWAq8Uop7CN3A,7282
-keras_cv/layers/preprocessing_3d/swap_background_test.py,sha256=uKCuyFyId99nVifJ9Whf3i8VOgtYny_7rEyfvEbVEAY,10580
+keras_cv/layers/preprocessing/resizing.py,sha256=iUnreT-7qUFmEx3_UUhstrg33-VxDDsm3sxp2CJC_LE,12328
+keras_cv/layers/preprocessing/resizing_test.py,sha256=5C1uUl7xBidiUvylV3O_XkNSNZqNbLVqW1RVth0Y6-M,11545
+keras_cv/layers/preprocessing/solarization.py,sha256=Spnuyvh3DinbV_g5cknlN7Pd5I3poQetZzJpT0EwDoU,6315
+keras_cv/layers/preprocessing/solarization_test.py,sha256=Ef4RgDgxYhrl7vOxlVjc3cbp1NF2-O74eiQyDHTzJ_c,3152
+keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py,sha256=x7ECy-QlzzZFeZ80CDp9-_3A2hKn6iqWSz1MwrhHlPg,19868
+keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py,sha256=9qnDF7HCCuUA6V1MAixPI1rFrXcia0tcdbAzr_NLUio,20975
+keras_cv/layers/preprocessing/with_labels_test.py,sha256=Ud8yOAZfLhJZjWiVAGdCIpeZ4q6u1F8uL5BrkMq2SjM,5015
+keras_cv/layers/preprocessing/with_mixed_precision_test.py,sha256=jyBM56EamQMRRyYTG4pYKcdU_T1BDtc-d3swFTATTAw,5473
+keras_cv/layers/preprocessing/with_segmentation_masks_test.py,sha256=K_qHjT4UjtLpa4QB1RAGLYiaQ33BKIgdWWYI17dDIw0,4812
+keras_cv/layers/preprocessing_3d/__init__.py,sha256=mLC9cXKiViLfAooARXo4kaQd2_e-nNgtemC3PMX3A_c,1769
+keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py,sha256=DufaGhg6MZ8WQ5qm3q8fCNNGXZj5UCB6toBNjTBzdr8,7987
+keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py,sha256=XfLsnLcSPcBMIyyyaZO6aWiL0cKBcUsfLoUUI6fAAZU,4789
+keras_cv/layers/preprocessing_3d/frustum_random_dropping_points.py,sha256=PxFqNFQJmnsBAVm4_75FO9V1UbnaSJfqVRlxrK3CV1o,6272
+keras_cv/layers/preprocessing_3d/frustum_random_dropping_points_test.py,sha256=IQVnS3_3Za-Zr8kLeENmoZ8HuxkbtkpiINqg8IO5baI,5454
+keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise.py,sha256=CtepB7NmW9ZvvE3I-kB36_oD-nSgq7ScgBwFeiLefNU,6969
+keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise_test.py,sha256=iaNAoXuPZbQ1t9btj1BVhubeSnEu6rMYKz7s78nNQPw,8880
+keras_cv/layers/preprocessing_3d/global_random_dropping_points.py,sha256=jZnfwKVongkpNKdB5aIDvCm5Qk2zI9QZ-d2MZY9YZFU,3815
+keras_cv/layers/preprocessing_3d/global_random_dropping_points_test.py,sha256=ZPQSLp60wv9MCZ8aZo1N4vqPGGxdr1j6CpsulQ9XHiQ,5100
+keras_cv/layers/preprocessing_3d/global_random_flip.py,sha256=HGsp8ryh8gT5nbfyxhvHLKckk5AVdwuFN8W1AG7tqzU,4458
+keras_cv/layers/preprocessing_3d/global_random_flip_test.py,sha256=JVUubAZfV_stcVetRkc5HoZqYX7iG7hadIcBEj0Leag,3275
+keras_cv/layers/preprocessing_3d/global_random_rotation.py,sha256=L4vihhuyDC1j-ahvbOkdKtCTCxxfQWo8-Gys19KLbLo,6016
+keras_cv/layers/preprocessing_3d/global_random_rotation_test.py,sha256=QCA7ipaULvBg6G782d_luaEFvvTywEm6TVckN6J3r04,3158
+keras_cv/layers/preprocessing_3d/global_random_scaling.py,sha256=24TOJSNvawKSnEddvuHPoX97InpraJFKff8fADasXUY,7090
+keras_cv/layers/preprocessing_3d/global_random_scaling_test.py,sha256=AyoPd0L9AGA2cOfV6ZB0-yPtjCAERFsKTDTWWobZVCg,4610
+keras_cv/layers/preprocessing_3d/global_random_translation.py,sha256=kOMMkvlufo7o2p8VyIDWiRRDrhFx2zauSmxB-7F46Uc,4733
+keras_cv/layers/preprocessing_3d/global_random_translation_test.py,sha256=9w8wkJ6xAmm_oXVucVciREZPJ9CDWSDkcw6sQlcxYYU,2935
+keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes.py,sha256=d2Bm_noNfaxDBN6C4K-Dg0093ZacmOXoFgJAak4QAvA,11464
+keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes_test.py,sha256=g40goot1tHDIzrRxnIQGqG4PqaZL2PmAJlqrqZSLU74,7460
+keras_cv/layers/preprocessing_3d/random_copy_paste.py,sha256=xbg0vj7ssyKmutP9mJleifhxT2TAKUCRLbjpucQSRKM,12606
+keras_cv/layers/preprocessing_3d/random_copy_paste_test.py,sha256=z1_NuYcD0wVBSmiQJMfaDjRL9eJUzS3joml5COZeguc,8348
+keras_cv/layers/preprocessing_3d/random_drop_box.py,sha256=rKylWskYaztEYt-F8U9PSxbZPl8-JQP20YaJoVfn0_U,5418
+keras_cv/layers/preprocessing_3d/random_drop_box_test.py,sha256=YWaRfY0DXe8mDB2cwNxmCrdnhnI0WihPGxlW2nas80o,12338
+keras_cv/layers/preprocessing_3d/swap_background.py,sha256=fpNNBNtLFzYRhk9ODVfAkFwVVL71oaQrk-evHyLIEFw,7322
+keras_cv/layers/preprocessing_3d/swap_background_test.py,sha256=uCIkasmXCl8CWE3787e2tqwxll1AWRWwWS8zgvKhsEs,11087
 keras_cv/layers/regularization/__init__.py,sha256=puQMT3krCCm0-Z6_1N296zpAZGe5FSIv-6ILRtjkG-I,868
-keras_cv/layers/regularization/drop_path.py,sha256=ElfxTmzFX6eO_wJYP4HpJUdgwOJsR4C0gwViZlbdgU8,2545
+keras_cv/layers/regularization/drop_path.py,sha256=ev56dTYJHlchvq4i4rfTBjgjFpJGMD97_WC6igt-4kE,2545
 keras_cv/layers/regularization/drop_path_test.py,sha256=xD2WretcOvKTSZNWwUTyPXh1XKjCPcBoYiP7hHBbXEk,2460
-keras_cv/layers/regularization/dropblock_2d.py,sha256=j6vKwgQ9iE33dpTxw_qHU5xqioApSoLmeopzYhiTz2o,8594
+keras_cv/layers/regularization/dropblock_2d.py,sha256=Z4ExqwQTyjFFZu1pW6caIrvWU0MnyreIzQVvnJ6qMJs,8831
 keras_cv/layers/regularization/dropblock_2d_test.py,sha256=fGYKqszc-Hd-FHuH2YK6hwXgiGGux25aDBW8BaUnnbw,3653
-keras_cv/layers/regularization/squeeze_excite.py,sha256=NuXEQrUe1hleDgkvw98uVyW09nEnsm1wLhiURiL7BZI,4227
-keras_cv/layers/regularization/squeeze_excite_test.py,sha256=lhattpBdga3IuaVlcibdxIEAVZaVT6Xia2WOjjz2oHk,1897
-keras_cv/layers/regularization/stochastic_depth.py,sha256=NMu6wXbe2uxQc0BtOArfe1svSKZpXeAmHk3qbbijn4o,2730
-keras_cv/layers/regularization/stochastic_depth_test.py,sha256=zfgUVdfXOrcADfWyJDyRkOIsL79jHz2FqOwnpk8UYuU,1758
-keras_cv/losses/__init__.py,sha256=dGY9DAYn9dS3q1Wx-B4-QMqTZmP2lGNMywg1uw6_axQ,925
-keras_cv/losses/focal.py,sha256=XU09Gb13_TJqS5tYFP6fBStJrV4pPoi8o1RN2I0n4hs,4094
-keras_cv/losses/focal_test.py,sha256=XwH9mfC9L96M3sjL6KX4ClGewH2aoQozsX5k0uFQSF8,2442
-keras_cv/losses/giou_loss.py,sha256=XxIVXAfwbrgadFLCzdH2G8MEurFhyM-BLENbiwwUGfw,6947
-keras_cv/losses/giou_loss_test.py,sha256=OCX8CY1c-ar0w5N0vmWq5Jm08901CP_MeDVrqn6NUdQ,2453
-keras_cv/losses/iou_loss.py,sha256=ytI_Q4NvYbxMmznjcECQw2yTUKfFHJPABiV0EQvZkjE,4480
-keras_cv/losses/iou_loss_test.py,sha256=NIKEo47NKC9Vog7AoUreL0yxjz9e93zPQZqwojl13C8,2433
-keras_cv/losses/penalty_reduced_focal_loss.py,sha256=s-sFr3prGd7H1MjElxEaSxJS1ADfyn-6Hq2fBYIvZYw,4195
-keras_cv/losses/penalty_reduced_focal_loss_test.py,sha256=8cJO4qbVLjsqHSFlo2jZVZ2z7JelFOc-TYSgwubFn5U,3634
-keras_cv/losses/serialization_test.py,sha256=KZgARvqIvn-GMQj7dR86AuAZvfuFMXP44_grkvg2o8I,2159
-keras_cv/losses/simclr_loss.py,sha256=LkwcqZKrfB1NeegrOjEbpkNgQfjwF0q2wPvXri9CfYM,3363
-keras_cv/losses/simclr_loss_test.py,sha256=dW2AY_RyCezbRhAbpjlP5kPgyhMIqJHAySqR9L-l_ck,2123
-keras_cv/losses/smooth_l1.py,sha256=1dw9B7rb75o_wcKwoHv6v71oL7nd_q99egtDK0MKErw,1858
+keras_cv/layers/regularization/squeeze_excite.py,sha256=1Zwhgk6mJ4Bx5cptlnVXzMOYuY9hW-UXHu3tCUBpsTo,4906
+keras_cv/layers/regularization/squeeze_excite_test.py,sha256=bK9QHsOJUe4ZIL0kaGCdjDbA9BP7LbnEkOcsIFhAzdM,1882
+keras_cv/layers/regularization/stochastic_depth.py,sha256=rRRFkhV9jPpKTLZhjUXaLMHvAePB5GsKxGnD1aiBMwE,2738
+keras_cv/layers/regularization/stochastic_depth_test.py,sha256=8ow6STkWtfuAUZGo7C1ExIFKyH32yhWDN0un9BtNXR4,1771
+keras_cv/losses/__init__.py,sha256=-hBSkcsNJDpcrCGPj8hJhLxZ4hqwU5T4SVArb9cla3E,989
+keras_cv/losses/centernet_box_loss.py,sha256=fG4Gg5i_GYTI2p9NDcKJavdl9kdeBEtjpddYjotQWm8,4854
+keras_cv/losses/centernet_box_loss_test.py,sha256=ypeCas7EODJmq-AzJ5MDjSj3K5wY1wli3gejWIewbzw,1459
+keras_cv/losses/focal.py,sha256=QMmhZ8bInDwOgFCrLHYnzfVX0lqr-IdmjMcDTvOQoxM,4140
+keras_cv/losses/focal_test.py,sha256=LUKm7_GCCFKNM3xDnFtRCN20aMHYJeX6JQPWoXQMlm4,2486
+keras_cv/losses/giou_loss.py,sha256=8vJ9wCwNXYOifbDeCNOhZ4LveizR_EReykqG08KAZZA,7410
+keras_cv/losses/giou_loss_test.py,sha256=YklDUDDqzqh1fjyqfgfrP1fBeiNK1ue_JIyVM6FOtos,2541
+keras_cv/losses/iou_loss.py,sha256=pUnEwz_QcVHTnTgDSvnQz9oNcLzoKBYnxh2190gGDNQ,4921
+keras_cv/losses/iou_loss_test.py,sha256=md0cjQUuSTvkuFgw4RAFOY2Ptqzdb_pcLtTrsv4d4B8,2521
+keras_cv/losses/penalty_reduced_focal_loss.py,sha256=tMfcAclWiPyEeiGd6ux2z1Idt3V-n3pSzvikiOua30k,4283
+keras_cv/losses/penalty_reduced_focal_loss_test.py,sha256=V6aRLeDSxm3SmxVCuf6SDWAKtbTbP4xVhFCVe9088O4,3744
+keras_cv/losses/serialization_test.py,sha256=xn8e8IB0DN5z3Q1YUTqtfzqC_XSeDBxhpHsglqFpMaE,2211
+keras_cv/losses/simclr_loss.py,sha256=V369MolbCdw4cYDTUSOC2pJbWVGQlH6dTaAGagTBw78,3468
+keras_cv/losses/simclr_loss_test.py,sha256=theY3nyff-eBEmkUdL7Q_PA_78aCLeL8U-e5nypZGUQ,2145
+keras_cv/losses/smooth_l1.py,sha256=TM_J-KT0GJpTp4xCGY3GXuQxFnfatic-YQfFK0cHNu4,1885
 keras_cv/losses/smooth_l1_test.py,sha256=1a0AjxK3AXPD760xoPnqjlbvV6xTyWuE_Xtya0c_RhU,1225
-keras_cv/metrics/__init__.py,sha256=BZOLTNZsjmUaME-5nw0dq6MfR7G9Gd7-z3XRXnJmL0g,721
-keras_cv/metrics/serialization_test.py,sha256=2Me_jl3cfKPXkPIU86ACLmv4Ru7nDRZaEnOIZGsZviQ,1416
-keras_cv/metrics/coco/__init__.py,sha256=cEeVh9Gq2e0XXoemecDwE5GfapdvW_DxjKfGSOiD8vs,1006
-keras_cv/metrics/coco/mean_average_precision.py,sha256=zxJGUC2Pb8kQvBkMGMcOQgaj3oE0xiR1QK7k3v3D3sE,15979
-keras_cv/metrics/coco/mean_average_precision_test.py,sha256=Q4AahosCd-XyRv6o4xs6qdmfu2ClvKCHzd-_YWzSVkM,8967
-keras_cv/metrics/coco/pycoco_wrapper.py,sha256=UiRkpbDUNhLAY42glbSRDoZBUDZqNp6BX2CfUzzhadI,8287
-keras_cv/metrics/coco/recall.py,sha256=fivQKW2NZ28tPg37Ik8ey-mcLpg534KsEj3hzHeyE4I,10202
-keras_cv/metrics/coco/recall_test.py,sha256=L2VSGcrll5ODPWYjP71m6d10hwCp6f3wBoXc3m8JerM,11276
-keras_cv/metrics/coco/utils.py,sha256=r_AmL3KVYD6y0VvDdAj9pnmd0wsV17GOSLtfmi_DGW8,5806
-keras_cv/metrics/coco/utils_test.py,sha256=0J17fMaIOXbJY5kStwBY7YNOAhbBjtwJqd5ep3QGbZc,4769
-keras_cv/metrics/coco/numerical_tests/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/metrics/coco/numerical_tests/mean_average_precision_test.py,sha256=05Mi8xcwGyVlNFU7_1M2kT2RL9jFBByRJd4Y0th5fDU,5341
-keras_cv/metrics/coco/numerical_tests/recall_correctness_test.py,sha256=SPX8vemdAOzUhcxPrhacrEDsKrZI1VS6PwIhsDgWyMU,4848
-keras_cv/models/__init__.py,sha256=TVz1nRf4CfuVZuM1HYqkRoBOLFLP0rVxGCphVxSl56w,5484
-keras_cv/models/convmixer.py,sha256=pVyzODtMSouTKNqvmVrCHSg31J1e3zSz7QWQWV-Z3ZM,12800
-keras_cv/models/convmixer_test.py,sha256=roFPQywwlFwfMesSla_5gztRDQrVQHj4sLgFLCiKQpQ,1828
-keras_cv/models/convnext.py,sha256=sD-1kn5xx1ffdV1cf6q9vKeDfyQklmQOoo9jUK0R48s,17867
-keras_cv/models/convnext_test.py,sha256=dEaRgjJijZQ_JT4jNDM5MqJpIhXGAE22wTNG3K7hhrk,2449
-keras_cv/models/csp_darknet.py,sha256=abLp5PsbhabfP69z7sXuG4y976qoW31s30015XgoGPA,14649
-keras_cv/models/csp_darknet_test.py,sha256=94LZ9gjdC8CPQMtig7KcrkYrwRYHRsEKaft-6D8WIXk,1878
-keras_cv/models/darknet.py,sha256=gQg7xJnHwgoRMh7az4lOTF9y1R9673Ocva5yHis5gqw,9335
-keras_cv/models/darknet_test.py,sha256=hvh3NIfwE5lOFfYLtlK0wPNGEkRYpRJqOwUcr4HPoCY,1816
-keras_cv/models/densenet.py,sha256=eQbPpuzxq5h0tbyTAlMgxu9QnK8pngdiB4zxhRNKBNs,11232
-keras_cv/models/densenet_test.py,sha256=mjiD2Uz5M3qDoH0x4rcQuJOaaYSjls8i9jRhHvZgFac,1994
-keras_cv/models/efficientnet_lite.py,sha256=0qcfTP6kDEt2DUE-IcB7jJcB5mofpLtPCi8dNM6gnTc,20354
-keras_cv/models/efficientnet_lite_test.py,sha256=c4maaedRCuv66ydqforbRLymIg4sIPv1EoVoV57OLZA,2160
-keras_cv/models/efficientnet_v1.py,sha256=XUEsPBFNe2ce2J872TUCT39dZ1XhLg6OQIpqSupp2Fs,23344
-keras_cv/models/efficientnet_v1_test.py,sha256=XvaeS5bxtDOiNSN6E4tjTJEnAqSXtVl27jHfdnqCFMg,2258
-keras_cv/models/efficientnet_v2.py,sha256=WiJy1pxxL2LEuBjNh851YNdHbucdOrVoU0Fca5EULYo,30652
-keras_cv/models/efficientnet_v2_test.py,sha256=XQZ7sXGx8dKYDzTAhbs4UxfsipMZXOg8SoHGqd3o5is,2225
-keras_cv/models/mlp_mixer.py,sha256=X7kY0Oui-ddEyPwi4lW5PR8mvZFc4NifpcaNbmWpQgA,12561
-keras_cv/models/mlp_mixer_test.py,sha256=ZS2zq_HiJqyevdiOYmS0MMv50pNoLB5sV6O3-ghwMGI,2043
-keras_cv/models/mobilenet_v3.py,sha256=jLyHIfU1znNoXZJu_XYTHFqcestghedjcSKvsNolS0I,19887
-keras_cv/models/mobilenet_v3_test.py,sha256=rLYeo8_sP0RKiDdXEj86YweGmGHc4UN0bj3sOQ1AgcU,1849
-keras_cv/models/models_test.py,sha256=MstVUEkni08f479qIE2QQS6cGULjgdm3gIb_1e4rnCg,5908
-keras_cv/models/regnet.py,sha256=9_wNSw_zd3B4eOd612hkrO6vN0wwcu6asdUmVotoFiM,45343
-keras_cv/models/regnetx_test.py,sha256=yTekDJL8KfuGfuIj6kGR5NFIKSqhCOI5gCGVKwSpnwY,2258
-keras_cv/models/regnety_test.py,sha256=1cmcXkvaZt0eQ5cROmMNu0-h22rCjlTFQzknsu78mA0,2258
-keras_cv/models/resnet_v1.py,sha256=09hTws_rbL6YsbYnMKGMLwRXQ2tTJXm3p9ReaASjvpU,18529
-keras_cv/models/resnet_v1_test.py,sha256=dCz_YB219dhcmu7IyU5z2hE0lWdfIhgSBRFlt09as5o,2054
-keras_cv/models/resnet_v2.py,sha256=5Ey56uVOkJqYg3oiHBsuOLisFhWqzU60Atd7nUHI_NA,19679
-keras_cv/models/resnet_v2_test.py,sha256=1pxgmewiPlsWrMg6HLL5El3uVnfPW018enIs9STIOOk,4156
-keras_cv/models/utils.py,sha256=WPIA617i_a2R6_fiU59HTvK_iMK-Qwe-bSI40gzCcFY,3681
-keras_cv/models/utils_test.py,sha256=Sb3FMq4LYPkjodBJcb91V7GWtxaNhTvLuEmhad0Vg3c,2268
-keras_cv/models/vgg16.py,sha256=JIpy2HuscxQ-DshZckz_d41RTmTzMXP1FRQU4302vp8,6172
-keras_cv/models/vgg16_test.py,sha256=rUYuNWf7tJPW-jtItAuXk6c3q-NtWwAzATzwLMDwZSw,1772
-keras_cv/models/vgg19.py,sha256=3KXzpm74usPI7bxmHfH-j8vEmQL_ei9FHVBw_V9S_cQ,6501
-keras_cv/models/vgg19_test.py,sha256=sjaEdS2ADAq39KrdSErajawtt4nJ5VBojnlBO5tmoTw,1772
-keras_cv/models/vit.py,sha256=b7Sj4bNh0rfgEwvmkGDPG64coyQL_zPvs0TflHlfIIc,24036
-keras_cv/models/vit_test.py,sha256=34bdRRLUf9RN-Q3X0LKVBI764L4IPjVXqodolbbHtdI,2403
-keras_cv/models/weights.py,sha256=zB959ie6XIogWIuOU3rj19vnHN_GbxHYXhFxUt843hM,9819
-keras_cv/models/__internal__/__init__.py,sha256=RkE4YDLiPrVeaIf16WBAj0R391-CSxe_pNBeTJ8b1IY,1029
-keras_cv/models/__internal__/darknet_utils.py,sha256=_0yM_JGNCwQA7yOTSDd-jC-M9FtwQmFvpfuTLYDOnDk,12046
-keras_cv/models/__internal__/unet.py,sha256=rcW7fzCFVXR9_lBkiBB4-nUYr7inn5wZi7MyssgrqyY,5966
+keras_cv/metrics/__init__.py,sha256=X8Cshzprx-J3MNKQUgGCP4STOucojE5oUGFtM-lKisY,663
+keras_cv/metrics/coco/__init__.py,sha256=7ZP_01BNrYxXkrI7kCTLsrsIxuG5lYjHwS-HzlPz0r4,719
+keras_cv/metrics/coco/pycoco_wrapper.py,sha256=vGeZ_DNWdvMuf6-0yJ6CJhNuCh7QfnFPivlCEeyTrt4,8388
+keras_cv/metrics/object_detection/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/metrics/object_detection/box_coco_metrics.py,sha256=uwOkK8pItymx8UVfzzivPC4ucTSFUYqJQp2rSX4KImk,9843
+keras_cv/metrics/object_detection/box_coco_metrics_test.py,sha256=EPfi7oFW8td_blhOPyFZbabuaC2ZUNA5DkFbIYdRBYk,4779
+keras_cv/models/__init__.py,sha256=wKQaKi3G3YMDKhqCgRC97MLC8L6WDXO995GcuzkpDkc,4079
+keras_cv/models/task.py,sha256=zlFlwK7xS6qNEQnyHPv4eh430qxb3CXiScLb248wAcg,6387
+keras_cv/models/utils.py,sha256=bxmxfjGFVQM6aO-Fh9GjrORe6Ne1Tbv5aBGz1h-Ub-I,1093
+keras_cv/models/utils_test.py,sha256=ZXmz_ngH9DYrU-axCv3IYx7RCL-iwzC8Oc_uJ_Q_eQc,1133
+keras_cv/models/__internal__/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
+keras_cv/models/__internal__/unet.py,sha256=6V7gRexsVAHdweVeI13zCz4NONheX_wUchaF6j73C3E,5966
 keras_cv/models/__internal__/unet_test.py,sha256=snnjgT9KzqeL1kpsyphVTg15JBw6oudgTvQYJKylrEE,1693
+keras_cv/models/backbones/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/backbones/backbone.py,sha256=9L_X4xHL8K1JAlf-W_mp62YJdr54fWJq57kXkyuz93E,6264
+keras_cv/models/backbones/backbone_presets.py,sha256=X6rXopPLnojShudOek_S9k6lxv7pUAlngXotrDZQOCk,1839
+keras_cv/models/backbones/csp_darknet/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py,sha256=6_8ht4jOygQU_Hi1PrwQXIiZzmqNhh54c3W0vZY1R7U,12000
+keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py,sha256=FZrBXbztCR94xvAOJiAHck8uiwQBAZD9ULG7Dt70qQ8,6518
+keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py,sha256=0uQolv9drvvg2vQncrG7rpxkwR27UpJ6K164AzH2lMo,4028
+keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py,sha256=QkzTRvM4MC7FAECiH5ZMBDNOoXaIanjs7aEXRR7gxCU,5552
+keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py,sha256=XnoHxxNt8PMVim-DRNI5synXqYcTia-rG0pjCOIAFqo,12186
+keras_cv/models/backbones/efficientnet_v2/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py,sha256=y1WusTXHwRAAeqVFUzFG9JzaJjSiDxQNl_HDZshjU9g,9316
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py,sha256=m5Z8CyHr5yoZ5Uq_n8eln0tUc0EIrQ9qNAQc7wLS_DU,12981
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets.py,sha256=ycIWMYZa47gfzCh_Y3bEE4K7Sx0xyI6WRgGkRp2hPlw,19514
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py,sha256=LeHz66FhoocjX5Z_QeqOMIMqCVuGz4fhEvp6EyKP3tY,2287
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py,sha256=rpVbuUH0RKKZIt4n_sb0BlxAEjJCQGjK_bK2vysRYFs,8546
+keras_cv/models/backbones/mobilenet_v3/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py,sha256=5ipz58pvHco8j2FfYBUZFhSSxQD-drF2RSqzpfy7QJw,15819
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py,sha256=JuGNiVNnaAO-a6W6PRKOgGjtcCL_nQ--DW6D_beX9aw,6080
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py,sha256=b0xR8W5_lv2dvrfNYyAapkB1_pcsWd82isUz3bN8zLM,1418
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py,sha256=K7VMUD1dMSG9i3uyRXi-INxpujOK0hHAuYZFVeVGF_E,3266
+keras_cv/models/backbones/resnet_v1/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py,sha256=PPAnblqvDnygkkVm5c0htdBgIROq_VSYhN4RBEa29Zo,17276
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py,sha256=uOyWXpElHcRTx81YgsHTyBY0s0jhS5VsJgT5eUYrUUU,5551
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py,sha256=PE1bcOh6AZUT5y0J4SO-ps3n1NIzOP2kzlS5WLVkUAE,3546
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py,sha256=f9MViEhHeVkGi4eMdmhEulXAssfjcSTfoG7oZwY86-w,6283
+keras_cv/models/backbones/resnet_v2/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py,sha256=37HW8D8oDRUNxYx5lT-AFgOiSkcer909ZlW-51qUvD8,18820
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py,sha256=EdRbAgMOKA2d7XtfDSjSNhrML2GSUoD86PH0_ExVTKI,5617
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py,sha256=BAEXQWNTa3pDAGDcLKlG4GALB6kSktiuDbc4sDx1vnE,3724
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py,sha256=FhKMy-ZNbGP_z-bVzgHJsmKE9iIWpZ32qe069LKoBiI,5693
+keras_cv/models/classification/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/classification/image_classifier.py,sha256=5gD2fAYb1HvIg039DXXD0sxbBljLiW3uApqCBRu_3qQ,4645
+keras_cv/models/classification/image_classifier_presets.py,sha256=FDv2YJGhr0JXHCCNJV9ZSb5OzrJqwF3rPZg_uA_myII,8454
+keras_cv/models/classification/image_classifier_test.py,sha256=kBh-jP4ZSKIbIcAUlrMYoRuyXSQlzN5uVxE3J9ShaEE,7098
+keras_cv/models/legacy/__init__.py,sha256=rk6LqZiXqzFqhIPLmflZWDKcktnVID6kZyfqZyw-MHw,4514
+keras_cv/models/legacy/convmixer.py,sha256=zF65eULXe1Jy2-iWsunOSX_lSDSwZMfqaxXw1B27ogY,14440
+keras_cv/models/legacy/convmixer_test.py,sha256=MCnQ2eEDwoSoESr50kUeJJZPosSM6CNRnx5QOoQnWXI,1835
+keras_cv/models/legacy/convnext.py,sha256=n1hJWSkLatMtciMQDCX5FG0eQpaykDMwvLErzAJageU,20264
+keras_cv/models/legacy/convnext_test.py,sha256=rBr9ka92Vd75jTkkYMCtyXvDkuFfitv9ItA3iU_L-mA,2456
+keras_cv/models/legacy/darknet.py,sha256=vo3TIrrp29sLJbyk0Vz_buCu6iu-Oa_JIbrIcq0cNiw,11050
+keras_cv/models/legacy/darknet_test.py,sha256=7QkF6BeAUTnEe1_pxmP6TwapjXKHKaGZqWIx27zCLK8,1823
+keras_cv/models/legacy/densenet.py,sha256=SCcWz8nGD6E_MjYzMf4Hg5xt4OEYxro7yF3t5A6Tx2Y,13432
+keras_cv/models/legacy/densenet_test.py,sha256=2LubeApvKim80ToW11xg9-LgHgQh-En90GAfWWcsWUY,2001
+keras_cv/models/legacy/efficientnet_lite.py,sha256=THvk6uVdDAncvFBy5sFixRCv4FlYuY-IMrHRmsyIEJA,22966
+keras_cv/models/legacy/efficientnet_lite_test.py,sha256=c1dk95sxGp73etRBFqYmk0qWzNFlo5Bijpr0874B5Jk,2173
+keras_cv/models/legacy/efficientnet_v1.py,sha256=vdRTVKLp0H2Pu_71HdhAdmT_JPFVMhza1ZtDjNCigl8,29352
+keras_cv/models/legacy/efficientnet_v1_test.py,sha256=6JmMd-_YcBhhanmqyZC4XH8zaibodPRlyfCgIGMuVQY,2265
+keras_cv/models/legacy/mlp_mixer.py,sha256=Pyp7WOxdR_9gAlkDf2FTjkihCsvJ8bMp0vU4qvbViWk,14405
+keras_cv/models/legacy/mlp_mixer_test.py,sha256=MZ7Z1ywObeloCxxSJF3yAceRWjtdPoF4molEqRMEe4w,2050
+keras_cv/models/legacy/models_test.py,sha256=-Ddf1-7SpXjdrGHORQXrRGrUq8r4nzLEHqntR-gQr40,6541
+keras_cv/models/legacy/regnet.py,sha256=wUaUYRp9n3M5J1AN7q7Ex9Bfw5D0HnoErw9ALYUG-dQ,45767
+keras_cv/models/legacy/regnetx_test.py,sha256=MENWVbPqTLY8IYUncRm3JjGAvn5MxYzPH-6TmbtNgAM,2265
+keras_cv/models/legacy/regnety_test.py,sha256=L8DKBqOtZuaTBeYGl9A3zUVE0yFnz7MiOORAyxK4dJQ,2265
+keras_cv/models/legacy/utils.py,sha256=MbjgT50Vuu1BCXxCnnZ5csFKB1F8N5DAJfQLAVDalXI,3662
+keras_cv/models/legacy/utils_test.py,sha256=-7Y1EQrxsi58gDXk_8w9q0plJWaoPs3j4AQlPXtdRqU,2320
+keras_cv/models/legacy/vgg16.py,sha256=Z-zUhy-sG1U-e6hMLD0UIFXlwrcPpKy79FWbXizuOuc,8144
+keras_cv/models/legacy/vgg16_test.py,sha256=wLZVSnrS_SiZbfWC_rUc4B-7lHXadIIVCSPE2b-AFxg,1779
+keras_cv/models/legacy/vgg19.py,sha256=qf87g78qYqAayPYR1DAYmy50ZsexBsI7OUQhXuktgF0,7103
+keras_cv/models/legacy/vgg19_test.py,sha256=F5hhA6I4PTWfOduDD7uoCaBD3FGug_6g6u_NnD_aFLc,1779
+keras_cv/models/legacy/vit.py,sha256=cRFLOH-s5qcTD7qikyOqq_HpgGqF476_qFIop8HnTe4,26159
+keras_cv/models/legacy/vit_test.py,sha256=PxknM25tlUyhDujfC3UuHR1hL55zf9OyIKQAbjBO7Tc,2410
+keras_cv/models/legacy/weights.py,sha256=G1i7XgRp2Y3kLOIEhfSAsL4F7eaqcKz-ms-FeXJX6to,8729
+keras_cv/models/legacy/segmentation/__init__.py,sha256=QGl4z6VIl5fUvjTceVVNuvYtvNiAcb-cVNehtmbbXjo,651
+keras_cv/models/legacy/segmentation/deeplab.py,sha256=fXM6hHVSmtMhQr0bQH19OnaUDkJaanNDW3vHa0Nu-GM,12412
+keras_cv/models/legacy/segmentation/deeplab_test.py,sha256=3CCny5u-YOTj5cmwfMJCM9td22NzIT2Viut2cr7Hk3Y,5955
 keras_cv/models/object_detection/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/models/object_detection/__internal__.py,sha256=QJkGcWusHSF0840kbaNTFWtHct7CjA6842PUrgqBnbU,4304
-keras_cv/models/object_detection/__test_utils__.py,sha256=Um7hyQy0vmS-Om7icxUdU0FnVHeXlh5W1ejy_RfBtDk,1574
-keras_cv/models/object_detection/faster_rcnn.py,sha256=Dr5i2BhV7K6BE2ACuzgNc964dKM-NUsDsnZzNSeoqlU,23583
-keras_cv/models/object_detection/faster_rcnn_test.py,sha256=AogCJ-7u7PWB2mdl0cHnaiB1aZZl7PIPItQ5JI7j0hY,3787
-keras_cv/models/object_detection/predict_utils.py,sha256=R4RC2WdxDacs5H_xR9Top_Tvzh6WZshIPdTeomb0ePw,3460
-keras_cv/models/object_detection/retina_net/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/models/object_detection/retina_net/retina_net.py,sha256=D4w4QBAJRWKzYu_KPw83aG1JDUnlh58XnyJ_kalUqVU,18911
-keras_cv/models/object_detection/retina_net/retina_net_inference_test.py,sha256=CIsWuYTRAuwRJQx7-LCxszaFbJrNTb0qNqYbXEhFikI,10432
-keras_cv/models/object_detection/retina_net/retina_net_test.py,sha256=49rmNcYsm7ewS27U6uLaQX-h6ttjOhl3LNH9gAhIDCw,8561
-keras_cv/models/object_detection/retina_net/__internal__/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/models/object_detection/retina_net/__internal__/layers/__init__.py,sha256=pC7bb0hSH2oxsyoxjzVZwoK5n5hLLGm6I8n1-T9Giko,817
-keras_cv/models/object_detection/retina_net/__internal__/layers/feature_pyramid.py,sha256=Cw_9TPGWIM4V74_fUrcUWoaqGQjPb8vC9cO418i6LWo,2504
-keras_cv/models/object_detection/retina_net/__internal__/layers/prediction_head.py,sha256=yvEjcYk41t-yfz7gkZgjxfDl-ZZjVhpnF5Ga5egBvsg,2482
+keras_cv/models/object_detection/__internal__.py,sha256=cjd5ZmGvv4OZ7BYySc2cvOFen2drOJgDNLPO2_edW1o,4329
+keras_cv/models/object_detection/__test_utils__.py,sha256=Eo0Vb9x12_33rXHlus3x6h7FLIhutbicZzp_X1RGQlQ,1904
+keras_cv/models/object_detection/predict_utils.py,sha256=HY5yshO7QaYDj2i-4EHHnnHc-Iq7tpe85Ez3SDD9o10,3490
+keras_cv/models/object_detection/retinanet/__init__.py,sha256=IwrDTskFA_9eIuGebwJ4-0_B8KanlWA_PRyi3Q-2M58,898
+keras_cv/models/object_detection/retinanet/feature_pyramid.py,sha256=de3zyqbuDHzsrjJZV2RhBRQI15sugICNv7V7AjCyaI4,2501
+keras_cv/models/object_detection/retinanet/prediction_head.py,sha256=-SACAmX69oyM6h4dqD5ySV-9-aKC84DhrvNFzbwHmXs,2841
+keras_cv/models/object_detection/retinanet/retinanet.py,sha256=kKfINrlrZBAGL4A5wuckpnkeXALSOIQndpmRiGRzIY4,23051
+keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py,sha256=1sB0cQRysPVi5vnnbseoBSmGgw_NNHG0Xd1hyYrYgtg,9759
+keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py,sha256=Vnr-OqPPphll3duyKmuhKYueSlgfDm2qBINzWw3OkaI,4594
+keras_cv/models/object_detection/retinanet/retinanet_presets.py,sha256=0o93_QiM3Llyn1fkbpRMYIut9GsNKRw2YO2h9vHTWcQ,1634
+keras_cv/models/object_detection/retinanet/retinanet_test.py,sha256=2YmmMIMp_opViDqVobfo87q7FWFqFAVvRoGIPwclasA,7822
+keras_cv/models/object_detection/yolo_v8/__init__.py,sha256=eUL-SD8orSL53eH7fPXWySgO2rHkJzkT3y8rL4llKtk,687
+keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py,sha256=H6-jUCoTzmwvDHZmi-JDf9iYYCkbB_PAn-gs7Kw9KZU,7018
+keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone_presets.py,sha256=LIXCwGg4xJHdibMFXB2m_KPQZLG7xbypFzZk1XfIsUc,6577
+keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py,sha256=SJOWcaBIXIWszDln2tr-bEnYlrXkBat_E0tlEPG_gS4,22502
+keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py,sha256=JTXcX7vZ3WJuTFVtVxN5V1TPJ_wR10EBSRFUeMqTprQ,1595
+keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py,sha256=aDM_zTRdzukNZBTvSreKazUJpfV0Cz51en0dljTBUPI,4708
+keras_cv/models/object_detection/yolo_v8/yolo_v8_iou_loss.py,sha256=_7TEgmiHm00KxuoBPSSdp2rkzWE6kicXRU8QPXIe5PE,2090
+keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py,sha256=w3iauBbrc0CsbMHt2XCQr4mJbD6ncdt9Pq6XTiJZpYo,14891
+keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py,sha256=DHmCqzwBwYi_lWuqLn8FUwGMNsXQ1I03pbzOucE3ktY,2884
+keras_cv/models/object_detection/yolox/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/object_detection/yolox/binary_crossentropy.py,sha256=HcGHiKXtAXufxvXy7FR6wOKdmXx0GODyH74xtUAJrzg,3419
+keras_cv/models/object_detection/yolox/layers/__init__.py,sha256=SRs1LNrQrUKFqBMxfq2syl2gdClaqXZdHEDJ7f35L2k,954
+keras_cv/models/object_detection/yolox/layers/yolox_decoder.py,sha256=Qvlf1j8jBYpL9iZmK_6tWKgKbR1v0dmhqWmLdxH8eOg,6313
+keras_cv/models/object_detection/yolox/layers/yolox_head.py,sha256=mHO6fiwqQ-4ZZlUA2_uPnBeLFPLGk5nr-iKl8TNtprE,5423
+keras_cv/models/object_detection/yolox/layers/yolox_head_test.py,sha256=6R9-KhIpJ7skYmmjz41vsNMKVokKAg9FrBN8oflUXts,1859
+keras_cv/models/object_detection/yolox/layers/yolox_label_encoder.py,sha256=Zx10iyK4gULCV5RD97PuswQSSwbAqSr4Bp95I4VCroU,1923
+keras_cv/models/object_detection/yolox/layers/yolox_label_encoder_test.py,sha256=7wI8QJbPl6HEG4EX7WZG1lHMQGk28KEmvg2aFEJbl8w,2986
+keras_cv/models/object_detection/yolox/layers/yolox_pafpn.py,sha256=WKfWMWIduint2dVLjmpnSajNfWC3gMbn23y-gY6f1vc,5198
+keras_cv/models/object_detection/yolox/layers/yolox_pafpn_test.py,sha256=dJ6HZb9ae06DYf6bPVK68OYQHYcax103t5Z2niO6TQQ,1806
 keras_cv/models/object_detection_3d/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/models/object_detection_3d/center_pillar.py,sha256=7RKRdawUrJXkxpgNyIUEJkI60ESh8ioxSRaZ-Fi2GoE,8569
-keras_cv/models/object_detection_3d/center_pillar_test.py,sha256=oLufn0ein0f-LM0yXDtCLoadYFkL2EekMOC_LQP4-lU,5337
-keras_cv/models/segmentation/__init__.py,sha256=J7BJekzydMnIP3xyS5g6NKPxMTWxCdECBLlOVaVcgqw,644
-keras_cv/models/segmentation/deeplab.py,sha256=owNy3RQbYUfhdKVWC52DUX8AcP5Na4Ssl6RAuQh_rxQ,12653
-keras_cv/models/segmentation/deeplab_test.py,sha256=Y2k01w6yoVCZr22FPW3BaOMgWyCFMqtwU3CrFhhRmwg,6215
+keras_cv/models/object_detection_3d/center_pillar.py,sha256=gPiRs0ufYeHpVdg6XQOOBKkuCQru4lIa2aiHBnlO6xA,10038
+keras_cv/models/object_detection_3d/center_pillar_test.py,sha256=70X28vklYDrir7sEHKvC1_ZitnUHmiccHUyUNPJUYE4,5577
 keras_cv/models/stable_diffusion/__init__.py,sha256=_8wBV8X3b3r40BjkJXJqaYSU7JfclxRz8mVo81507uQ,1324
-keras_cv/models/stable_diffusion/clip_tokenizer.py,sha256=KPLUhM5cUkIpPiEeo1WwKneT5e4qI0XStY79fN4tVF4,6863
+keras_cv/models/stable_diffusion/clip_tokenizer.py,sha256=v-V0BQzodwjL-Akfhnc0-ywzbA6wrESKEuejwkSld7E,7025
 keras_cv/models/stable_diffusion/constants.py,sha256=AcJgrYp5XM_kckkSxEtVHfL9_a2LWxySq-08DOWZnyM,17410
-keras_cv/models/stable_diffusion/decoder.py,sha256=K_xJWqgWooA3FoDwdaG8VibvAsWU2frWCOPx08YU7oE,2648
-keras_cv/models/stable_diffusion/diffusion_model.py,sha256=B98jNHWNHzHVTr0PF0J2SM6Vq3UP3gbPRo60szRJdko,13001
-keras_cv/models/stable_diffusion/image_encoder.py,sha256=vzaNiDAtDqU2OpQa2dk7Xd8KbT1NgUGN-eZSKbSIsvw,2691
-keras_cv/models/stable_diffusion/noise_scheduler.py,sha256=ZI9cX6e19TZQLl6DySgXufqSsGqHvbClEEqbnfYHsNw,7420
-keras_cv/models/stable_diffusion/stable_diffusion.py,sha256=xE8Nq7rIj3ANTF8VF5wrXa4t6jJ1PsUnBtlkQnytnXY,26051
-keras_cv/models/stable_diffusion/stable_diffusion_test.py,sha256=T_uNJ7Wir2V-KG4VqlJOcWOC7j9QT3-MW5B_LM9fVrc,2043
-keras_cv/models/stable_diffusion/text_encoder.py,sha256=bwq7tAFg1If7gQE5RtOke5CbrcbUjXIVacWKpIglCpY,6456
+keras_cv/models/stable_diffusion/decoder.py,sha256=hUbi8IfQdjn4tBTv8sHH-mRcHHywcCr8SSeaU2pF_4g,2690
+keras_cv/models/stable_diffusion/diffusion_model.py,sha256=5dW0HHFj8oPj0k4erJzNYl1wd01oBlVJ_wE_eOSZZEA,13271
+keras_cv/models/stable_diffusion/image_encoder.py,sha256=NVC2v-ClmpSoeZQWG7870nQmiAyGycoEYzYhmoN0z6Q,2757
+keras_cv/models/stable_diffusion/noise_scheduler.py,sha256=3kjdFSIuRa2odHRlJJB9qOBaNOguHKkz0aIoCrN23_c,7770
+keras_cv/models/stable_diffusion/stable_diffusion.py,sha256=XQYP_QuPZNblo6Y-gq1jLL6G3Kob3kDbOJL7dPb8MZ4,19421
+keras_cv/models/stable_diffusion/stable_diffusion_test.py,sha256=vwjDgMpJ14CmB-S1i10Oeh1THoELUNafMk2lp5CQAl8,2414
+keras_cv/models/stable_diffusion/text_encoder.py,sha256=yaXriTtB3vVeReZxZpNk5Y0xRD6JhEWbZkckgXxiWRM,6680
 keras_cv/models/stable_diffusion/__internal__/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/stable_diffusion/__internal__/layers/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py,sha256=wdnQ98ietbvBzktlNRuYCGjsWq93_SciXnMmlle92M8,1868
+keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py,sha256=sdDVdeS51RI1n5mNX4glafyHlaW2LkU5PgAVBmrVmgA,1948
 keras_cv/models/stable_diffusion/__internal__/layers/padded_conv2d.py,sha256=z3AO3IHvkhcfhU6om6729QqqIHhR6hwwVhek5s8jtcA,1005
 keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py,sha256=SOvE8_rMAHZIYrDxNRGTvfJa-W0mPljG5l7kCWF2t-o,1560
-keras_cv/ops/__init__.py,sha256=zBgjv4uB_ywPPg1IEIbSHkO7g0h2zlIFDaUiIJBvA6U,1313
-keras_cv/ops/iou_3d.py,sha256=h971AvpGZ1nScdU1Y2QkgOARSxCTAiJ3vMWJZ8SCSio,1589
-keras_cv/ops/iou_3d_test.py,sha256=pe15YjhCtWXo0NsuXxfbjF6BswfmGv7Glh3eMJ8l0Do,2199
-keras_cv/ops/point_cloud.py,sha256=EUSIO8xU8B1QF9T6DBIStO2EyWkAMbtBRayVpwyggxA,17543
-keras_cv/ops/point_cloud_test.py,sha256=DIaxfyK5qPLUn-6zCYDFt0IjpmFRnFbdnAPcca8DYhE,13690
-keras_cv/ops/within_box_3d_test.py,sha256=SX6kGOSHQ1BzBUXDgubNKzBPLif43eq2QV458i-5Rf8,7275
+keras_cv/ops/__init__.py,sha256=ygu6MeZEtohuDvOabajq4E-FHQ7xOI2O_3Z-3JjRwNs,624
+keras_cv/ops/iou_3d.py,sha256=Eshs7-xaC7iS6u0uE09_r8akjwE9-lXFOGI7VUxTcVk,1589
+keras_cv/ops/iou_3d_test.py,sha256=P36M1YrbNVpFJKB4dGUdgAHt79OZ4CeE7SIPBU7luuw,2242
+keras_cv/point_cloud/__init__.py,sha256=u8QJ5UeFdH4x4XEk0sgG6zX0PRCPo1QlnsDESNCEm-Y,1522
+keras_cv/point_cloud/point_cloud.py,sha256=wTRpBWV010EEy_GwP-yib1YEQrqtdkRt5CYxcnVL28k,18327
+keras_cv/point_cloud/point_cloud_test.py,sha256=ygXBsoK5ADFZfow9c1mqtOFp86dbGRahOWn79g9sJKw,14038
+keras_cv/point_cloud/within_box_3d_test.py,sha256=vovw5NQR5AWQdmYf_g6E7hjRUoiwbUJzO0PRMQVe_WQ,7774
 keras_cv/training/__init__.py,sha256=zKTTo5nwGqrjrKUlSCvH5A64ypUlIVO_LqpTTqWKrgE,810
 keras_cv/training/contrastive/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/training/contrastive/contrastive_trainer.py,sha256=wIs0X7p99N3h0cGeloAxoCg3uIX2SOX2tAE8vbK7UYc,9017
-keras_cv/training/contrastive/contrastive_trainer_test.py,sha256=Qelz8w_m59SdlrOAIpDJCfgjMpHBXOoqM80tetfo-BA,6011
-keras_cv/training/contrastive/simclr_trainer.py,sha256=XoE6goCWbt8012xwY6Aom_npgfPPvenzxALKzP3tpiQ,3183
-keras_cv/training/contrastive/simclr_trainer_test.py,sha256=um6cE0xNfY8289kEjcp4zfo-z9kCzp0gOfpesbFoaAk,1425
-keras_cv/utils/__init__.py,sha256=EAOOyNdh7R0MNLt9JENxE9NJSmBoxhyZ6fled-iOwPE,1128
-keras_cv/utils/conv_utils.py,sha256=eLedO73JdlGUb-4tZadWeuA2tHmbVCMiEr_ZiGl7nbE,2446
-keras_cv/utils/fill_utils.py,sha256=vtI_SifgILh8iETjJzfEPBfAGb9jKXw8SC292IzLlRQ,3073
-keras_cv/utils/fill_utils_test.py,sha256=Tp6UGO4RM7DDvqDIGTQUq29gh0GBjdOY9oZGFlEisfA,11243
-keras_cv/utils/preprocessing.py,sha256=btMLYj0IaxJC1NGdA_Od0qBLg4hNST4DvWWjpsagqjE,13664
+keras_cv/training/contrastive/contrastive_trainer.py,sha256=cJLddEMPwBM5-YizBG5PlIRQxCcYaebYbstro2RbBWM,9414
+keras_cv/training/contrastive/contrastive_trainer_test.py,sha256=v1JWSoF70AZl3_IQIfVSVcYDTSfpBx3NbYUMjvarCkA,6082
+keras_cv/training/contrastive/simclr_trainer.py,sha256=lZREjE7gqKfsplgbivqan4baLA6KTzblDujVGD3JcfQ,3197
+keras_cv/training/contrastive/simclr_trainer_test.py,sha256=uqsYPR2z29O6LqSdIf-mv8v5yFe0hEl1JsvFSjeeNWw,1609
+keras_cv/utils/__init__.py,sha256=T29RuoUBnjaqMUw6d6_6PHf-SKQY0lvI7uOB7zyMw9Y,1408
+keras_cv/utils/conditional_imports.py,sha256=tm1Vn8ntsmw1eW2ewoZ-DRKTj2rTXipoHTamKrrI1pw,2080
+keras_cv/utils/conv_utils.py,sha256=bHkcdIJrC2XDByFQ6Lx52rKruXBJLkSq30erTSkeOko,2474
+keras_cv/utils/fill_utils.py,sha256=umSUsnPbqznINNhXjBRB-grPvdufXsbni8N4UuI_r8Y,3105
+keras_cv/utils/fill_utils_test.py,sha256=gJ8oq1yZIfBQyMpI61QnPKW4oujcJI19us_ghBVauHA,11265
+keras_cv/utils/preprocessing.py,sha256=yEXLCfwNnPlnnVD77hlmr3ZkZOtxlOIz3aAbz2pM21s,14465
 keras_cv/utils/preprocessing_test.py,sha256=eFL6dlg9kFJ2zuHYutvz8qyNy9Skiqr8ZpLZql_7LPs,2303
-keras_cv/utils/resource_loader.py,sha256=4Go6Zepf2CtDXJg4E1vQqdBaHckrEXhOlhSP5-L6_Z8,2797
-keras_cv/utils/target_gather.py,sha256=Gre8jmbRIg7GdQBRn3CWz2CBfNb5EtCnD3QPhl3_hQA,4690
+keras_cv/utils/python_utils.py,sha256=d1ILxSZg7zQN5tu0OZnYg9wOfmat4P8A3DadX-1MRhc,1802
+keras_cv/utils/resource_loader.py,sha256=LYsQkuTV30JJ1lIRfOdlhBhP2MrswTNMJ5p5YAgL1ts,2843
+keras_cv/utils/target_gather.py,sha256=b9gN5J6Y8fA1iuYAY3RtuUnrb79XfFO8rLjJGbHDvsM,4731
 keras_cv/utils/target_gather_test.py,sha256=-Gp8CBl1maluIpJbW7IT6djyOlIY7V6vCW2FkSMkvDo,5346
-keras_cv/utils/test_utils.py,sha256=a6SBER_khglESYLWtKCZ6Nq3WoCksXvYou-NYxkrIus,3639
-keras_cv/utils/train.py,sha256=Q_72Com8l7hetB5IkeaSYoc7wbCuc-1qATJD_K_Agrw,1771
-keras_cv-0.4.2.dist-info/LICENSE,sha256=2zOp5rCyz7FJvND3_eb184w-K3tdLuJy_3lhQ58OVn0,11412
-keras_cv-0.4.2.dist-info/METADATA,sha256=y_6lC3T6zMFG7zNZIdLYRxLKs1iSUi3Og-nvs9eOoMY,9740
-keras_cv-0.4.2.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-keras_cv-0.4.2.dist-info/top_level.txt,sha256=uKfJekc1tcN8PbI4ul-NL7P0z6JrS58MuciUKPPRKxY,9
-keras_cv-0.4.2.dist-info/RECORD,,
+keras_cv/utils/test_utils.py,sha256=InZNiCk3HMZZ2PqTf1U_lThmXKYiPxcUhoYjxqxddAg,3628
+keras_cv/utils/to_numpy.py,sha256=Wa9tvz3RKEmIBdsaej5zzlADWkpVWSk_xL2lIAMOvMY,991
+keras_cv/utils/train.py,sha256=EsvRoaRvgeO-gaZVjQqTIBXCymOYC30MiaNg59y0G4Q,2813
+keras_cv/visualization/__init__.py,sha256=h4QC2aOrQu4TmHFV44FpJQlVWncQIG5wh3PDprsFGpM,754
+keras_cv/visualization/draw_bounding_boxes.py,sha256=YqZGsARY1B47RRNsC75M50AUI0P-hCkCtNFGT5MLUH8,5497
+keras_cv/visualization/plot_bounding_box_gallery.py,sha256=yzuZVIBWLfhYqopbGPZjehPOaUYgrPdr-c2sxvl1jso,6120
+keras_cv/visualization/plot_image_gallery.py,sha256=Bbnrm1QvUCmUvnXXqbuOX9_ozOwrdBL1-fsIc5sEeEg,3945
+keras_cv-0.5.0.dist-info/LICENSE,sha256=2zOp5rCyz7FJvND3_eb184w-K3tdLuJy_3lhQ58OVn0,11412
+keras_cv-0.5.0.dist-info/METADATA,sha256=1Jz8LBcVJhmEMbPv2BfMh337cVyN04Xc-kRt8ww3luY,10792
+keras_cv-0.5.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+keras_cv-0.5.0.dist-info/top_level.txt,sha256=uKfJekc1tcN8PbI4ul-NL7P0z6JrS58MuciUKPPRKxY,9
+keras_cv-0.5.0.dist-info/RECORD,,
```

